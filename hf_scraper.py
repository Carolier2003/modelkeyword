#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HuggingFace æ¨¡å‹ä¿¡æ¯çˆ¬è™«
æ”¯æŒçˆ¬å–æ¨¡å‹åç§°ã€æ ‡ç­¾åˆ—è¡¨å’ŒREADMEå†…å®¹
"""

import re
import asyncio
import json
from typing import Dict, Optional
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

async def scrape_hf_model(url: str, token: Optional[str] = None) -> Dict[str, str]:
    """
    çˆ¬å– HuggingFace æ¨¡å‹ä¿¡æ¯
    
    Args:
        url: æ¨¡å‹é¡µé¢URL
        token: å¯é€‰çš„è®¤è¯token
    
    Returns:
        DictåŒ…å«ä»¥ä¸‹å­—æ®µ:
        - url: åŸå§‹URL
        - name: æ¨¡å‹å…¨ç§°
        - tags: æ ‡ç­¾åˆ—è¡¨(JSONå­—ç¬¦ä¸²)
        - readme: READMEå†…å®¹
    """
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        page = await context.new_page()
        
        try:
            # åŠ è½½é¡µé¢ï¼Œä½¿ç”¨æ›´å®½æ¾çš„ç­‰å¾…æ¡ä»¶
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)
            
            # è®¾ç½®è®¤è¯tokenï¼ˆå¦‚æœæä¾›ï¼‰
            if token:
                await page.evaluate(f"""
                    localStorage.setItem('token', '{token}');
                    localStorage.setItem('auth_token', '{token}');
                    localStorage.setItem('access_token', '{token}');
                """)
                
                # åˆ·æ–°é¡µé¢ä»¥åº”ç”¨token
                await page.reload(wait_until="domcontentloaded", timeout=30000)
                await page.wait_for_timeout(3000)
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            await page.wait_for_timeout(5000)
            
            # å°è¯•ç›´æ¥è·å–READMEå†…å®¹
            try:
                readme_md = await page.evaluate("""
                    () => {
                        // å°è¯•å¤šä¸ªé€‰æ‹©å™¨
                        const selectors = [
                            'div.dp-editor-md-preview-container',
                            'div.gitCode-MdRender-container',
                            'div[class*=\"readme\"]',
                            'div[class*=\"markdown\"]',
                            '.repo-file-markdown-content'
                        ];
                        
                        for (const selector of selectors) {
                            const element = document.querySelector(selector);
                            if (element) {
                                const text = element.innerText || element.textContent || '';
                                if (text.length > 100) {  // ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
                                    return text;
                                }
                            }
                        }
                        return '';
                    }
                """)
                # print(f"ğŸ” ç›´æ¥è·å–READMEï¼Œé•¿åº¦: {len(readme_md)}")
            except Exception as e:
                print(f"âŒ ç›´æ¥è·å–READMEå¤±è´¥: {e}")
                readme_md = ""
            
            # è·å–é¡µé¢å†…å®¹ç”¨äºè§£æå…¶ä»–ä¿¡æ¯
            content = await page.content()
            soup = BeautifulSoup(content, "html.parser")
            
            # å¦‚æœç›´æ¥è·å–å¤±è´¥ï¼Œä½¿ç”¨BeautifulSoupä½œä¸ºå¤‡ç”¨
            if len(readme_md) == 0:
                # å°è¯•å¤šç§æ–‡æœ¬æå–æ–¹æ³•
                readme_div = soup.find("div", class_=re.compile(r"dp-editor-md-preview-container"))
                if readme_div:
                    readme_md = readme_div.get_text(strip=False)
                    if len(readme_md) == 0:
                        # å¦‚æœget_textä¸ºç©ºï¼Œå°è¯•è·å–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
                        readme_md = ""
                        for text_node in readme_div.find_all(text=True):
                            readme_md += text_node
                    # print(f"ğŸ” BeautifulSoupæ‰¾åˆ°READMEï¼Œé•¿åº¦: {len(readme_md)}")
                else:
                    # å¤‡ç”¨é€‰æ‹©å™¨ï¼šä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
                    readme_div = soup.find("div", class_=re.compile(r"gitCode-MdRender-container"))
                    if readme_div:
                        readme_md = readme_div.get_text(strip=False)
                        if len(readme_md) == 0:
                            readme_md = ""
                            for text_node in readme_div.find_all(text=True):
                                readme_md += text_node
                        # print(f"ğŸ” å¤‡ç”¨é€‰æ‹©å™¨æ‰¾åˆ°READMEï¼Œé•¿åº¦: {len(readme_md)}")
                    else:
                        readme_md = ""
                        # print("âŒ æœªæ‰¾åˆ°README div")

            # 1. æ¨¡å‹åç§° ----------------------------------------------------------
            # é¢åŒ…å±‘æœ€åä¸€èŠ‚ <a><span class="linkTx font-bold ...">GLM-4.6</span></a>
            model_name_element = soup.select_one("div.breadcrumb p a span.linkTx")
            if model_name_element:
                model_name = model_name_element.get_text(strip=True)
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä»æ ‡é¢˜æå–
                title = await page.title()
                model_match = re.search(r"GLM[-\w\.]*", title)
                model_name = model_match.group() if model_match else "Unknown"
            
            # ä»URLæå–ç»„ç»‡åå’Œä»“åº“å
            url_parts = url.rstrip('/').split('/')
            if len(url_parts) >= 2:
                org_name = url_parts[-2]
                repo_name = url_parts[-1]
                full_name = f"{org_name}/{repo_name}"
            else:
                full_name = model_name

            # 2. æ ‡ç­¾åˆ—è¡¨ ----------------------------------------------------------
            # æ¯ä¸ªæ ‡ç­¾å¯¹åº”ä¸€ä¸ª <div class="topic-tag ..."> ä¸‹çš„ <span>
            tag_elements = soup.select("div.topic-tag span")
            if tag_elements:
                tags = [span.get_text(strip=True) for span in tag_elements]
            else:
                # å¤‡ç”¨é€‰æ‹©å™¨
                tags = [elem.get_text(strip=True) for elem in soup.select(".tag, .label, .badge")]
            
            # 3. README Markdown åŸæ–‡ ----------------------------------------------
            # GitCode æŠŠ README å¡åœ¨ä¸€ä¸ª <div class="dp-editor-md-preview-container ...">
            readme_div = soup.find("div", class_=re.compile(r"dp-editor-md-preview-container"))
            if readme_div:
                # å°è¯•å¤šç§æ–‡æœ¬æå–æ–¹æ³•
                readme_md = readme_div.get_text(strip=False)
                if len(readme_md) == 0:
                    # å¦‚æœget_textä¸ºç©ºï¼Œå°è¯•è·å–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
                    readme_md = ""
                    for text_node in readme_div.find_all(text=True):
                        readme_md += text_node
                print(f"ğŸ” æ‰¾åˆ°README divï¼Œé•¿åº¦: {len(readme_md)}")
            else:
                # å¤‡ç”¨é€‰æ‹©å™¨ï¼šä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
                readme_div = soup.find("div", class_=re.compile(r"gitCode-MdRender-container"))
                if readme_div:
                    readme_md = readme_div.get_text(strip=False)
                    if len(readme_md) == 0:
                        readme_md = ""
                        for text_node in readme_div.find_all(text=True):
                            readme_md += text_node
                    print(f"ğŸ” å¤‡ç”¨é€‰æ‹©å™¨æ‰¾åˆ°READMEï¼Œé•¿åº¦: {len(readme_md)}")
                else:
                    readme_md = ""
                    print("âŒ æœªæ‰¾åˆ°README div")
            
            # è¿”å›ç»“æœ
            result = {
                "url": url,
                "name": full_name,
                "tags": json.dumps(tags, ensure_ascii=False),
                "readme": readme_md
            }
            
            return result

        except Exception as e:
            # è¿”å›é”™è¯¯ä¿¡æ¯
            return {
                "url": url,
                "name": "Error",
                "tags": json.dumps([]),
                "readme": f"Error: {str(e)}"
            }
        
        finally:
            await browser.close()

def scrape_hf_model_sync(url: str, token: Optional[str] = None) -> Dict[str, str]:
    """
    åŒæ­¥ç‰ˆæœ¬çš„çˆ¬è™«å‡½æ•°
    
    Args:
        url: æ¨¡å‹é¡µé¢URL
        token: å¯é€‰çš„è®¤è¯token
    
    Returns:
        DictåŒ…å«æ¨¡å‹ä¿¡æ¯
    """
    return asyncio.run(scrape_hf_model(url, token))

async def main():
    """æµ‹è¯•å‡½æ•°"""
    url = "https://ai.gitcode.com/hf_mirrors/zai-org/GLM-4.6"
    token = "eyJhbGciOiJIUzUxMiJ9.eyJqdGkiOiI2NzMwNTkzOTY4ZjYwYzcyYTZkNjY0YjAiLCJzdWIiOiJDYXJvbGllciIsImF1dGhvcml0aWVzIjpbXSwib2JqZWN0SWQiOiI2OGU3NjAwMmEzYzAyMjFmZTc5NTQ0NzgiLCJpYXQiOjE3NTk5OTM4NTgsImV4cCI6MTc2MDA4MDI1OH0.Gx_-yrMRyUhqHDg7TjDQkAY5QK2z-l2ZHHNdQD9K0DgKShp0qrjHLpNlQEfjZJMokQm5-gzMsbvXZwHKB2sdeQ"
    
    result = await scrape_hf_model(url, token)
    print(json.dumps(result, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    asyncio.run(main())
