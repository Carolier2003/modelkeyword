#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HuggingFace æ¨¡å‹ä¿¡æ¯çˆ¬è™«
æ”¯æŒçˆ¬å–æ¨¡å‹åç§°ã€æ ‡ç­¾åˆ—è¡¨å’ŒREADMEå†…å®¹
"""

import re
import asyncio
import json
from typing import Dict, Optional
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

async def scrape_hf_model(url: str, token: Optional[str] = None) -> Dict[str, str]:
    """
    çˆ¬å– HuggingFace æ¨¡å‹ä¿¡æ¯
    
    Args:
        url: æ¨¡å‹é¡µé¢URL
        token: å¯é€‰çš„è®¤è¯token
    
    Returns:
        DictåŒ…å«ä»¥ä¸‹å­—æ®µ:
        - url: åŸå§‹URL
        - name: æ¨¡å‹å…¨ç§°
        - tags: æ ‡ç­¾åˆ—è¡¨(JSONå­—ç¬¦ä¸²)
        - readme: READMEå†…å®¹
    """
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        page = await context.new_page()
        
        try:
            # åŠ è½½é¡µé¢ï¼Œä½¿ç”¨æ›´å®½æ¾çš„ç­‰å¾…æ¡ä»¶
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)
            
            # è®¾ç½®è®¤è¯tokenï¼ˆå¦‚æœæä¾›ï¼‰
            if token:
                await page.evaluate(f"""
                    localStorage.setItem('token', '{token}');
                    localStorage.setItem('auth_token', '{token}');
                    localStorage.setItem('access_token', '{token}');
                """)
                
                # åˆ·æ–°é¡µé¢ä»¥åº”ç”¨token
                await page.reload(wait_until="domcontentloaded", timeout=30000)
                await page.wait_for_timeout(3000)
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            await page.wait_for_timeout(5000)
            
            # æ£€æŸ¥æ˜¯å¦è·³è½¬åˆ°äº†/model-inferenceé¡µé¢ï¼Œå¦‚æœæ˜¯åˆ™ç‚¹å‡»"æ¨¡å‹ä»‹ç»"æŒ‰é’®è¿”å›ä¸»é¡µé¢
            current_url = page.url
            if '/model-inference' in current_url:
                print(f"âš ï¸  æ£€æµ‹åˆ°è·³è½¬åˆ°/model-inferenceé¡µé¢: {current_url}")
                print("   æ­£åœ¨ç‚¹å‡»'æ¨¡å‹ä»‹ç»'æŒ‰é’®è¿”å›ä¸»é¡µé¢...")
                
                try:
                    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½ï¼Œ"æ¨¡å‹ä»‹ç»"æŒ‰é’®å‡ºç°
                    await page.wait_for_timeout(3000)
                    
                    # ä½¿ç”¨Playwrightçš„locator APIæŸ¥æ‰¾å¹¶ç‚¹å‡»"æ¨¡å‹ä»‹ç»"æŒ‰é’®
                    intro_button_clicked = False
                    
                    try:
                        # æ–¹æ³•1: ä½¿ç”¨get_by_textæŸ¥æ‰¾åŒ…å«"æ¨¡å‹ä»‹ç»"æ–‡æœ¬çš„å…ƒç´ 
                        try:
                            intro_button = page.get_by_text("æ¨¡å‹ä»‹ç»", exact=False)
                            if await intro_button.count() > 0:
                                # æ‰¾åˆ°çˆ¶divå¹¶ç‚¹å‡»
                                await intro_button.first.click(timeout=5000)
                                intro_button_clicked = True
                                print("   âœ… æˆåŠŸç‚¹å‡»'æ¨¡å‹ä»‹ç»'æŒ‰é’®ï¼ˆæ–¹æ³•1ï¼šget_by_textï¼‰")
                        except Exception as e1:
                            # æ–¹æ³•2: ä½¿ç”¨JavaScriptæŸ¥æ‰¾å¹¶ç‚¹å‡»
                            try:
                                clicked = await page.evaluate("""
                                    () => {
                                        // æŸ¥æ‰¾åŒ…å«"æ¨¡å‹ä»‹ç»"æ–‡æœ¬çš„spanå…ƒç´ 
                                        const spans = Array.from(document.querySelectorAll('span'));
                                        const introSpan = spans.find(span => {
                                            const text = span.textContent || span.innerText || '';
                                            return text.trim() === 'æ¨¡å‹ä»‹ç»' || text.includes('æ¨¡å‹ä»‹ç»');
                                        });
                                        
                                        if (introSpan) {
                                            // æ‰¾åˆ°å¯ç‚¹å‡»çš„çˆ¶å…ƒç´ ï¼ˆé€šå¸¸æ˜¯åŒ…å«flex classçš„divï¼‰
                                            let clickable = introSpan.closest('div');
                                            // å‘ä¸ŠæŸ¥æ‰¾ï¼Œç›´åˆ°æ‰¾åˆ°åŒ…å«flexçš„div
                                            while (clickable && !clickable.classList.contains('flex')) {
                                                clickable = clickable.parentElement;
                                            }
                                            
                                            if (clickable) {
                                                clickable.click();
                                                return true;
                                            }
                                        }
                                        
                                        // é€šè¿‡classæŸ¥æ‰¾åŒ…å«flex gap-2çš„div
                                        const flexDivs = Array.from(document.querySelectorAll('div.flex.gap-2.items-center'));
                                        for (const div of flexDivs) {
                                            const span = div.querySelector('span');
                                            if (span && (span.textContent || span.innerText || '').includes('æ¨¡å‹ä»‹ç»')) {
                                                div.click();
                                                return true;
                                            }
                                        }
                                        
                                        // é€šè¿‡SVGçš„xlink:hrefæŸ¥æ‰¾
                                        const allUses = Array.from(document.querySelectorAll('use'));
                                        const svgUse = allUses.find(use => {
                                            const href = use.getAttribute('xlink:href') || use.getAttribute('href');
                                            return href === '#gt-plane-models';
                                        });
                                        if (svgUse) {
                                            const svg = svgUse.closest('svg');
                                            if (svg) {
                                                const parentDiv = svg.closest('div');
                                                if (parentDiv) {
                                                    parentDiv.click();
                                                    return true;
                                                }
                                            }
                                        }
                                        
                                        return false;
                                    }
                                """)
                                
                                if clicked:
                                    intro_button_clicked = True
                                    print("   âœ… æˆåŠŸç‚¹å‡»'æ¨¡å‹ä»‹ç»'æŒ‰é’®ï¼ˆæ–¹æ³•2ï¼šJavaScriptï¼‰")
                                else:
                                    print("   âš ï¸  æœªæ‰¾åˆ°'æ¨¡å‹ä»‹ç»'æŒ‰é’®")
                            except Exception as e2:
                                print(f"   âš ï¸  JavaScriptæ–¹æ³•å¤±è´¥: {e2}")
                    except Exception as e:
                        print(f"   âŒ ç‚¹å‡»'æ¨¡å‹ä»‹ç»'æŒ‰é’®å¤±è´¥: {e}")
                    
                    if intro_button_clicked:
                        # ç­‰å¾…é¡µé¢è·³è½¬å›ä¸»é¡µé¢
                        await page.wait_for_timeout(2000)
                        
                        # ç­‰å¾…URLå˜åŒ–ï¼ˆå»æ‰/model-inferenceï¼‰
                        try:
                            await page.wait_for_function(
                                "() => !window.location.href.includes('/model-inference')",
                                timeout=10000
                            )
                            print("   âœ… å·²è·³è½¬å›ä¸»é¡µé¢ï¼ˆç­‰å¾…URLå˜åŒ–ï¼‰")
                        except Exception:
                            # å¦‚æœç­‰å¾…è¶…æ—¶ï¼Œæ£€æŸ¥å½“å‰URL
                            current_url_after = page.url
                            if '/model-inference' not in current_url_after:
                                print("   âœ… å·²è·³è½¬å›ä¸»é¡µé¢ï¼ˆURLæ£€æŸ¥ï¼‰")
                            else:
                                print(f"   âš ï¸  ä»åœ¨/model-inferenceé¡µé¢: {current_url_after}")
                        
                        # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                        await page.wait_for_timeout(5000)
                    else:
                        print("   âš ï¸  æœªèƒ½ç‚¹å‡»'æ¨¡å‹ä»‹ç»'æŒ‰é’®ï¼Œå°è¯•ç›´æ¥è®¿é—®ä¸»é¡µé¢URL...")
                        # å¦‚æœç‚¹å‡»å¤±è´¥ï¼Œå°è¯•ç›´æ¥è®¿é—®ä¸»é¡µé¢URL
                        base_url = url.rstrip('/')
                        if '/model-inference' in base_url:
                            base_url = base_url.split('/model-inference')[0]
                        try:
                            await page.goto(base_url, wait_until="domcontentloaded", timeout=30000)
                            await page.wait_for_timeout(5000)
                            
                            # æ£€æŸ¥æ˜¯å¦åˆè·³è½¬å›äº†/model-inference
                            current_url_check = page.url
                            if '/model-inference' in current_url_check:
                                print(f"   âš ï¸  ç›´æ¥è®¿é—®åä»è·³è½¬åˆ°/model-inferenceï¼Œå†æ¬¡å°è¯•ç‚¹å‡»æŒ‰é’®...")
                                # å†æ¬¡å°è¯•ç‚¹å‡»æŒ‰é’®
                                try:
                                    intro_button = page.get_by_text("æ¨¡å‹ä»‹ç»", exact=False)
                                    if await intro_button.count() > 0:
                                        await intro_button.first.click(timeout=5000)
                                        await page.wait_for_timeout(3000)
                                        print("   âœ… å†æ¬¡ç‚¹å‡»'æ¨¡å‹ä»‹ç»'æŒ‰é’®æˆåŠŸ")
                                except Exception:
                                    pass
                            else:
                                print(f"   âœ… ç›´æ¥è®¿é—®ä¸»é¡µé¢æˆåŠŸ: {base_url}")
                        except Exception as e:
                            print(f"   âš ï¸  ç›´æ¥è®¿é—®ä¸»é¡µé¢å¤±è´¥: {e}")
                except Exception as e:
                    print(f"   âš ï¸  å¤„ç†/model-inferenceé¡µé¢æ—¶å‡ºé”™: {e}")
            
            # å†æ¬¡ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½ï¼ˆç¡®ä¿READMEå†…å®¹å·²æ¸²æŸ“ï¼‰
            await page.wait_for_timeout(5000)
            
            # ç¡®ä¿å½“å‰ä¸åœ¨/model-inferenceé¡µé¢
            final_url = page.url
            if '/model-inference' in final_url:
                print(f"   âš ï¸  æœ€ç»ˆä»åœ¨/model-inferenceé¡µé¢ï¼ŒREADMEå¯èƒ½æ— æ³•è·å–")
            
            # å°è¯•ç›´æ¥è·å–READMEå†…å®¹
            try:
                readme_md = await page.evaluate("""
                    () => {
                        // å°è¯•å¤šä¸ªé€‰æ‹©å™¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
                        const selectors = [
                            'div.markdown-card',
                            'div[class*="markdown-card"]',
                            'div.dp-editor-md-preview-container',
                            'div.gitCode-MdRender-container',
                            'div[class*="readme"]',
                            'div[class*="markdown"]',
                            '.repo-file-markdown-content',
                            'article',
                            'main div[class*="content"]'
                        ];
                        
                        for (const selector of selectors) {
                            const element = document.querySelector(selector);
                            if (element) {
                                const text = element.innerText || element.textContent || '';
                                if (text.length > 50) {  // é™ä½é•¿åº¦è¦æ±‚
                                    return text;
                                }
                            }
                        }
                        
                        // å¦‚æœä¸Šè¿°é€‰æ‹©å™¨éƒ½å¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«å¤§é‡æ–‡æœ¬çš„div
                        const allDivs = Array.from(document.querySelectorAll('div'));
                        for (const div of allDivs) {
                            const text = div.innerText || div.textContent || '';
                            // å¦‚æœdivåŒ…å«å¤§é‡æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯READMEï¼‰ï¼Œä¸”ä¸æ˜¯å¯¼èˆªæ ç­‰
                            if (text.length > 200 && 
                                !div.classList.contains('header') && 
                                !div.classList.contains('nav') &&
                                !div.classList.contains('footer')) {
                                return text;
                            }
                        }
                        
                        return '';
                    }
                """)
                # print(f"ğŸ” ç›´æ¥è·å–READMEï¼Œé•¿åº¦: {len(readme_md)}")
            except Exception as e:
                print(f"âŒ ç›´æ¥è·å–READMEå¤±è´¥: {e}")
                readme_md = ""
            
            # è·å–é¡µé¢å†…å®¹ç”¨äºè§£æå…¶ä»–ä¿¡æ¯
            content = await page.content()
            soup = BeautifulSoup(content, "html.parser")
            
            # å¦‚æœç›´æ¥è·å–å¤±è´¥ï¼Œä½¿ç”¨BeautifulSoupä½œä¸ºå¤‡ç”¨
            if len(readme_md) == 0:
                # å°è¯•å¤šç§æ–‡æœ¬æå–æ–¹æ³•ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
                selectors_to_try = [
                    r"markdown-card",
                    r"dp-editor-md-preview-container", 
                    r"gitCode-MdRender-container"
                ]
                
                for selector_pattern in selectors_to_try:
                    readme_div = soup.find("div", class_=re.compile(selector_pattern))
                    if readme_div:
                        readme_md = readme_div.get_text(strip=False)
                        if len(readme_md) == 0:
                            # å¦‚æœget_textä¸ºç©ºï¼Œå°è¯•è·å–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
                            readme_md = ""
                            for text_node in readme_div.find_all(text=True):
                                readme_md += text_node
                        if len(readme_md) > 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
                            # print(f"ğŸ” BeautifulSoupæ‰¾åˆ°READMEï¼Œé•¿åº¦: {len(readme_md)}")
                            break
                        else:
                            readme_md = ""  # é‡ç½®ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªé€‰æ‹©å™¨
                
                if len(readme_md) == 0:
                    # print("âŒ æœªæ‰¾åˆ°README div")
                    pass

            # 1. æ¨¡å‹åç§° ----------------------------------------------------------
            # é¢åŒ…å±‘æœ€åä¸€èŠ‚ <a><span class="linkTx font-bold ...">GLM-4.6</span></a>
            model_name_element = soup.select_one("div.breadcrumb p a span.linkTx")
            if model_name_element:
                model_name = model_name_element.get_text(strip=True)
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä»æ ‡é¢˜æå–
                title = await page.title()
                model_match = re.search(r"GLM[-\w\.]*", title)
                model_name = model_match.group() if model_match else "Unknown"
            
            # ä»URLæå–ç»„ç»‡åå’Œä»“åº“å
            url_parts = url.rstrip('/').split('/')
            if len(url_parts) >= 2:
                org_name = url_parts[-2]
                repo_name = url_parts[-1]
                full_name = f"{org_name}/{repo_name}"
            else:
                full_name = model_name

            # 2. æ ‡ç­¾åˆ—è¡¨ ----------------------------------------------------------
            # æ¯ä¸ªæ ‡ç­¾å¯¹åº”ä¸€ä¸ª <div class="topic-tag ..."> ä¸‹çš„ <span>
            tag_elements = soup.select("div.topic-tag span")
            if tag_elements:
                tags = [span.get_text(strip=True) for span in tag_elements]
            else:
                # å¤‡ç”¨é€‰æ‹©å™¨
                tags = [elem.get_text(strip=True) for elem in soup.select(".tag, .label, .badge")]
            
            # 3. README Markdown åŸæ–‡ ----------------------------------------------
            # READMEå†…å®¹å·²ç»åœ¨ä¸Šé¢æå–è¿‡äº†ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤æå–
            
            # è¿”å›ç»“æœ
            result = {
                "url": url,
                "name": full_name,
                "tags": json.dumps(tags, ensure_ascii=False),
                "readme": readme_md
            }
            
            return result

        except Exception as e:
            # è¿”å›é”™è¯¯ä¿¡æ¯
            return {
                "url": url,
                "name": "Error",
                "tags": json.dumps([]),
                "readme": f"Error: {str(e)}"
            }
        
        finally:
            await browser.close()

def scrape_hf_model_sync(url: str, token: Optional[str] = None) -> Dict[str, str]:
    """
    åŒæ­¥ç‰ˆæœ¬çš„çˆ¬è™«å‡½æ•°
    
    Args:
        url: æ¨¡å‹é¡µé¢URL
        token: å¯é€‰çš„è®¤è¯token
    
    Returns:
        DictåŒ…å«æ¨¡å‹ä¿¡æ¯
    """
    return asyncio.run(scrape_hf_model(url, token))

async def main():
    """æµ‹è¯•å‡½æ•°"""
    url = "https://ai.gitcode.com/hf_mirrors/zai-org/GLM-4.6"
    token = "eyJhbGciOiJIUzUxMiJ9.eyJqdGkiOiI2NzMwNTkzOTY4ZjYwYzcyYTZkNjY0YjAiLCJzdWIiOiJDYXJvbGllciIsImF1dGhvcml0aWVzIjpbXSwib2JqZWN0SWQiOiI2OGU3NjAwMmEzYzAyMjFmZTc5NTQ0NzgiLCJpYXQiOjE3NTk5OTM4NTgsImV4cCI6MTc2MDA4MDI1OH0.Gx_-yrMRyUhqHDg7TjDQkAY5QK2z-l2ZHHNdQD9K0DgKShp0qrjHLpNlQEfjZJMokQm5-gzMsbvXZwHKB2sdeQ"
    
    result = await scrape_hf_model(url, token)
    print(json.dumps(result, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    asyncio.run(main())
