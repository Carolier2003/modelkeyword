"""
åŸºç¡€å…³é”®è¯æå–å™¨ - æå–å…¬å…±ä»£ç 
"""
import os
import re
import json
from typing import List, Dict, Any, Optional
from abc import ABC, abstractmethod

from models import ModelInfo, KeywordResult


class BaseKeywordExtractor(ABC):
    """åŸºç¡€å…³é”®è¯æå–å™¨æŠ½è±¡ç±»"""
    
    def build_prompt(self, model_info: ModelInfo) -> str:
        """
        æ ¹æ®éœ€æ±‚æ–‡æ¡£æ„å»ºPrompt
        
        Args:
            model_info: æ¨¡å‹ä¿¡æ¯
            
        Returns:
            æ„å»ºå¥½çš„prompt
        """
        prompt = f"""# è§’è‰²
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„AIé¡¹ç›®è¿è¥ä¸“å®¶å’ŒSEOå¤§å¸ˆï¼Œä¸“é—¨ä¸ºCSDNåšå®¢å’ŒGitCodeç½‘ç«™å¼•æµä¼˜åŒ–å…³é”®è¯ã€‚

# èƒŒæ™¯
"é«˜äº®è¯"æ˜¯åœ¨CSDNåšå®¢æ–‡ç« ä¸­å¯ä»¥è¢«ç‚¹å‡»å¹¶è·³è½¬åˆ°GitCodeé¡¹ç›®é¡µé¢çš„å…³é”®è¯ï¼Œç”¨äºå¼•æµã€‚ä¸€ä¸ªå¥½çš„é«˜äº®è¯å¿…é¡»å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1. **é«˜æœç´¢ä»·å€¼**: ç”¨æˆ·ä¼šä¸»åŠ¨æœç´¢çš„æŠ€æœ¯å…³é”®è¯
2. **å¼ºå¼•æµæ•ˆæœ**: èƒ½å¸å¼•å¼€å‘è€…ç‚¹å‡»äº†è§£é¡¹ç›®
3. **æŠ€æœ¯çƒ­ç‚¹**: å½“å‰AI/MLé¢†åŸŸçš„çƒ­é—¨æŠ€æœ¯
4. **å®ç”¨æ€§å¼º**: å¼€å‘è€…çœŸæ­£éœ€è¦çš„å·¥å…·å’ŒæŠ€æœ¯

# ä»»åŠ¡

è¯·åˆ†æä»¥ä¸‹AIæ¨¡å‹é¡¹ç›®ä¿¡æ¯ï¼Œæå–å¼•æµå…³é”®è¯ï¼š

**[é¡¹ç›®åç§°]**: {model_info.project_name}
**[é¡¹ç›®ç½‘å€]**: {model_info.url}

## é¡¹ç›®è¯¦ç»†ä¿¡æ¯

### READMEå†…å®¹ï¼ˆå‰2000å­—ç¬¦ï¼‰ï¼š
{(model_info.readme[:2000] + "...") if model_info.readme and len(model_info.readme) > 2000 else (model_info.readme if model_info.readme else "æš‚æ— READMEå†…å®¹")}

### é¡¹ç›®æ ‡ç­¾ï¼š
{', '.join(model_info.tags) if model_info.tags else "æš‚æ— æ ‡ç­¾ä¿¡æ¯"}

è¯·åŸºäºä»¥ä¸Šæä¾›çš„READMEå†…å®¹å’Œæ ‡ç­¾ä¿¡æ¯ï¼Œç†è§£è¿™ä¸ªAIæ¨¡å‹çš„åŠŸèƒ½ã€ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯ã€‚

## âš ï¸ é‡è¦æå–è§„åˆ™

1. **å¼•æµä»·å€¼ä¼˜å…ˆ**ï¼šä¼˜å…ˆæå–ç”¨æˆ·ä¼šæœç´¢ã€æœ‰å¼•æµä»·å€¼çš„æŠ€æœ¯å…³é”®è¯
2. **ä¸¥æ ¼åŸºäºåŸæ–‡**ï¼šåªèƒ½æå–é¡µé¢å®é™…å­˜åœ¨çš„å†…å®¹ï¼Œç»å¯¹ç¦æ­¢ç¼–é€ æˆ–çŒœæµ‹
3. **æ•°æ®å‡†ç¡®æ€§**ï¼šå‚æ•°æ•°é‡ç­‰æ•°å­—ä¿¡æ¯å¿…é¡»ä¸é¡µé¢å®Œå…¨ä¸€è‡´ä¸”æœ‰æ˜ç¡®å«ä¹‰
4. **æ ¼å¼è§„èŒƒ**ï¼š
   - ä¸­æ–‡å…³é”®è¯ä½¿ç”¨çº¯ä¸­æ–‡ï¼šå¦‚"æ¨ç†åŠ é€Ÿ"ã€"æ–‡æœ¬ç”Ÿæˆ"
   - è‹±æ–‡å…³é”®è¯ä½¿ç”¨çº¯è‹±æ–‡ï¼šå¦‚"Transformer"ã€"BERT"
   - ä¸“ä¸šæœ¯è¯­ä¿æŒåŸæ ¼å¼ï¼šå¦‚"Apache License 2.0"ã€"MIT License"ã€"FLUX.1"ã€"GPT-4.0"
   - å¯ä½¿ç”¨è¿å­—ç¬¦å’Œç‚¹å·ï¼šå¦‚"GPT-4"ã€"FLUX.1"ã€"Claude-3.5"
   - é¿å…è‹±æ–‡-ä¸­æ–‡æ··åˆæ ¼å¼ï¼šâŒ"FP8-æ¨ç†" âœ…"FP8æ¨ç†"æˆ–"FP8"
5. **æœ¯è¯­å‡†ç¡®æ€§**ï¼šè®¸å¯è¯ã€æ¨¡å‹æ¶æ„ç­‰ä¸“ä¸šæœ¯è¯­å¿…é¡»ä½¿ç”¨æ ‡å‡†è¡¨è¾¾
6. **å“ç‰Œåç§°è§„èŒƒ**ï¼š
   - NVIDIA â†’ "NVIDIAå¤§æ¨¡å‹"
   - OpenAI â†’ "OpenAIå¤§æ¨¡å‹"
   - å…¶ä»–å“ç‰Œå â†’ "å“ç‰Œåå¤§æ¨¡å‹"
7. **å‚æ•°è§„æ ¼ä¼˜åŒ–**ï¼š
   - "6710äº¿å‚æ•°" â†’ "671Bå‚æ•°"
   - "1060äº¿å‚æ•°" â†’ "106Bå‚æ•°"
   - ä¿ç•™æœ‰æ„ä¹‰çš„å‚æ•°è§„æ ¼ï¼Œå¦‚"6Bå‚æ•°"ã€"128Kä¸Šä¸‹æ–‡"
8. **ä¸¥æ ¼ç¦æ­¢æå–çš„å†…å®¹**ï¼š
   - âŒ è®¸å¯è¯ä¿¡æ¯ï¼šå¦‚"MITè®¸å¯è¯"ã€"Apache License 2.0"ã€"éå•†ä¸šè®¸å¯"
   - âŒ é•œåƒç›¸å…³ï¼šå¦‚"HuggingFaceé•œåƒ"ã€"openai-mirror"
   - âŒ é€šç”¨åŠŸèƒ½ï¼šå¦‚"å·¥å…·è°ƒç”¨"ã€"æ¨ç†åŠ é€Ÿ"ç­‰è¿‡äºé€šç”¨çš„è¯
   - âŒ å•çº¯ç‰ˆæœ¬å·ï¼šå¦‚"v2.0"ã€"v1.5"ã€"2.1"ç­‰çº¯ç‰ˆæœ¬æ ‡è¯†
   - âŒ æ— æ„ä¹‰æ•°å­—ï¼šå¦‚"1024"ã€"512512"ã€"10241024"ç­‰çº¯æ•°å­—ç»„åˆ
   - âŒ æ— æ„ä¹‰æ•°å­—æè¿°ï¼šå¦‚"68ä¸‡å°æ—¶"ã€"100ä¸‡å‚æ•°"ç­‰è®­ç»ƒæ•°æ®é‡æè¿°
   - âŒ è‹±æ–‡å“ç‰Œåï¼šå¦‚"baidu"åº”ä¸º"ç™¾åº¦å¤§æ¨¡å‹"ã€"tencent"åº”ä¸º"è…¾è®¯å¤§æ¨¡å‹"
   - âŒ æŠ€æœ¯å‚æ•°æ•°å­—ï¼šå¦‚"16kHz"ã€"720p"ã€"480P"ç­‰çº¯æŠ€æœ¯è§„æ ¼
   - âŒ éƒ¨ç½²å·¥å…·ï¼šå¦‚"Ollama"åº”ä¸º"Ollamaéƒ¨ç½²"

ä»ä»¥ä¸‹6ä¸ªç»´åº¦åˆ†æå¹¶æå–å‡º**æ°å¥½6-8ä¸ª**æœ€ä¼˜è´¨çš„é«˜äº®è¯ã€‚

âš ï¸âš ï¸âš ï¸ **ä¸¥æ ¼æ•°é‡è¦æ±‚** âš ï¸âš ï¸âš ï¸
- ğŸš¨ **å¿…é¡»æå–æ°å¥½6-8ä¸ªå…³é”®è¯ï¼Œç»å¯¹ä¸å…è®¸å°‘äº6ä¸ªï¼**
- ğŸš¨ **å¦‚æœåªèƒ½æ‰¾åˆ°å°‘äº6ä¸ªå…³é”®è¯ï¼Œè¯·é‡æ–°ä»”ç»†é˜…è¯»é¡µé¢å†…å®¹ï¼**
- ğŸš¨ **å®å¯ä»åŒä¸€ç»´åº¦æå–å¤šä¸ªå…³é”®è¯ï¼Œä¹Ÿä¸è¦å°‘äº6ä¸ªï¼**
- ğŸš¨ **æ¯ä¸ªç»´åº¦è‡³å°‘æå–1ä¸ªå…³é”®è¯ï¼**

# æå–ç»´åº¦ä¸æŒ‡å—ï¼ˆé‡ç‚¹å…³æ³¨å¼•æµä»·å€¼ï¼‰
1. **çƒ­é—¨æ¨¡å‹å“ç‰Œ** (å¼•æµä»·å€¼æœ€é«˜):
* æå–ç”¨æˆ·ä¼šæœç´¢çš„çŸ¥åæ¨¡å‹å’Œå“ç‰Œï¼Œå¿…é¡»åŠ "å¤§æ¨¡å‹"åç¼€
* âœ… æ­£ç¡®: "InternLMå¤§æ¨¡å‹", "GLMå¤§æ¨¡å‹", "Qwenå¤§æ¨¡å‹", "NVIDIAå¤§æ¨¡å‹", "OpenAIå¤§æ¨¡å‹"
* âŒ é”™è¯¯: "InternLM"(ç¼ºå°‘å¤§æ¨¡å‹), "NVIDIA"(åº”ä¸ºNVIDIAå¤§æ¨¡å‹), "MITè®¸å¯è¯"(æ— å¼•æµä»·å€¼)

2. **æ ¸å¿ƒæŠ€æœ¯æ¶æ„** (æŠ€æœ¯çƒ­ç‚¹):
* æå–å½“å‰AIé¢†åŸŸçš„çƒ­é—¨æŠ€æœ¯æ¶æ„å’Œç®—æ³•
* âœ… æ­£ç¡®: "Transformer", "MoEæ¶æ„", "FP8é‡åŒ–", "LoRAå¾®è°ƒ", "å¤šæ¨¡æ€"
* âŒ é”™è¯¯: "å·¥å…·è°ƒç”¨"(è¿‡äºé€šç”¨), "æ¨ç†åŠ é€Ÿ"(æ— å…·ä½“æŠ€æœ¯ä»·å€¼)

3. **åº”ç”¨åœºæ™¯** (ç”¨æˆ·éœ€æ±‚):
* æå–ç”¨æˆ·å…³å¿ƒçš„å…·ä½“åº”ç”¨åœºæ™¯å’ŒåŠŸèƒ½
* âœ… æ­£ç¡®: "æ–‡æœ¬ç”Ÿæˆ", "å›¾åƒç†è§£", "ç§‘å­¦è®¡ç®—", "ä»£ç ç”Ÿæˆ", "å¯¹è¯ç³»ç»Ÿ"
* âŒ é”™è¯¯: "å¤šæ¨¡æ€å¯¹è¯"(è¿‡äºé€šç”¨), "æ™ºèƒ½åŠ©æ‰‹"(æ— å…·ä½“ä»·å€¼)

4. **éƒ¨ç½²ä¸é›†æˆ** (å®ç”¨å·¥å…·):
* æå–å¼€å‘è€…å…³å¿ƒçš„éƒ¨ç½²å·¥å…·å’Œé›†æˆæ–¹æ¡ˆ
* âœ… æ­£ç¡®: "Ollamaéƒ¨ç½²", "Transformers", "Diffusers", "ComfyUI", "vLLM"
* âŒ é”™è¯¯: "HuggingFaceé•œåƒ"(æ— å¼•æµä»·å€¼), "openai-mirror"(é•œåƒæ— æ„ä¹‰)

5. **æ€§èƒ½è§„æ ¼** (æŠ€æœ¯å‚æ•°):
* æå–æœ‰æ„ä¹‰çš„æ€§èƒ½å‚æ•°ï¼Œä¼˜åŒ–æ•°å­—æ ¼å¼
* âœ… æ­£ç¡®: "671Bå‚æ•°", "106Bå‚æ•°", "128Kä¸Šä¸‹æ–‡", "6Bå‚æ•°"
* âŒ é”™è¯¯: "6710äº¿å‚æ•°"(åº”ä¼˜åŒ–ä¸º671B), "1060äº¿å‚æ•°"(åº”ä¼˜åŒ–ä¸º106B), "MITè®¸å¯è¯"(æ— æŠ€æœ¯ä»·å€¼)

6. **ä¸“ä¸šé¢†åŸŸ** (å‚ç›´åº”ç”¨):
* æå–ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šåº”ç”¨
* âœ… æ­£ç¡®: "ç§‘å­¦è®¡ç®—", "åŒ–å­¦åˆ†æ", "è›‹ç™½è´¨ç ”ç©¶", "æ•°å­¦æ¨ç†", "ä»£ç ç¼–ç¨‹"
* âŒ é”™è¯¯: "éå•†ä¸šè®¸å¯"(æ— æŠ€æœ¯ä»·å€¼), "Apache License 2.0"(è®¸å¯è¯æ— æ„ä¹‰)

# è¾“å‡ºè¦æ±‚
- ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä¸€ä¸ªkeywordsæ•°ç»„
- **æ•°ç»„å¿…é¡»åŒ…å«6-8ä¸ªå…³é”®è¯å¯¹è±¡ï¼Œä¸èƒ½å°‘äº6ä¸ªï¼Œä¸èƒ½å¤šäº10ä¸ª**
- æ¯ä¸ªå…³é”®è¯å¯¹è±¡åŒ…å«ï¼škeywordï¼ˆå…³é”®è¯ï¼‰ã€dimensionï¼ˆæå–ç»´åº¦ï¼‰ã€reasonï¼ˆæ¨èç†ç”±ï¼‰
- æ¨èç†ç”±å¿…é¡»è¯´æ˜ä¸ºä»€ä¹ˆè¿™ä¸ªè¯æœ‰å¼•æµä»·å€¼ï¼Œç”¨æˆ·ä¼šæœç´¢ä»€ä¹ˆ

# å…³é”®è¯æ ¼å¼ä¸¥æ ¼è¦æ±‚
1. **æ ¼å¼è§„èŒƒ**ï¼š
   - ä¸­æ–‡å…³é”®è¯ï¼šçº¯ä¸­æ–‡ï¼Œå¦‚"æ–‡æœ¬ç”Ÿæˆ"ã€"å¤šæ¨¡æ€"
   - è‹±æ–‡å…³é”®è¯ï¼šçº¯è‹±æ–‡ï¼Œå¦‚"Transformer"ã€"BERT"
   - å“ç‰Œåç§°ï¼šå®Œæ•´æ¨¡å‹åå¦‚"FLUX.1"ã€"GPT-4.0"ã€"Claude-3.5"ï¼ˆåŒ…å«ç‰ˆæœ¬çš„å®Œæ•´åç§°ï¼‰
   - å‚æ•°è§„æ ¼ï¼šæœ‰æ˜ç¡®å«ä¹‰çš„å‚æ•°ï¼Œå¦‚"6Bå‚æ•°"ã€"128Kä¸Šä¸‹æ–‡"
   - ä¸“ä¸šæœ¯è¯­ï¼šä½¿ç”¨æ ‡å‡†æ ¼å¼ï¼Œå¦‚"Apache License 2.0"
   - ä¸­æ–‡å“ç‰Œï¼šä½¿ç”¨æ ‡å‡†ä¸­æ–‡ï¼Œå¦‚"ç™¾åº¦"ã€"è…¾è®¯"ã€"é˜¿é‡Œå·´å·´"

2. **ä¸¥ç¦æ ¼å¼**ï¼š
   - âŒ è‹±æ–‡-ä¸­æ–‡æ··åˆï¼šå¦‚"FP8-æ¨ç†"ã€"llamacpp-å…¼å®¹"
   - âŒ ç¼–é€ å†…å®¹ï¼šå¦‚"å°é’¢ç‚®"ã€"æé€Ÿå‡ºå›¾"
   - âŒ æ•°å­—é”™è¯¯ï¼šå®é™…6Bå†™æˆ8B
   - âŒ æ ¼å¼é”™è¯¯ï¼šå¦‚"Apache-2-0-å•†ç”¨"
   - âŒ çº¯ç‰ˆæœ¬å·ï¼šå¦‚"v2.0"ã€"v1.5"ã€"2.1"ç­‰å•ç‹¬ç‰ˆæœ¬æ ‡è¯†
   - âŒ æ— æ„ä¹‰æ•°å­—ï¼šå¦‚"1024"ã€"512512"ã€"10241024"ã€"16kHz"ã€"720p"
   - âŒ è‹±æ–‡å“ç‰Œåï¼šå¦‚"baidu"(åº”ä¸ºç™¾åº¦)ã€"tencent"(åº”ä¸ºè…¾è®¯)
   - âŒ æ•°æ®é‡æè¿°ï¼šå¦‚"68ä¸‡å°æ—¶"ã€"100ä¸‡å‚æ•°"ç­‰è®­ç»ƒæ•°æ®æè¿°

3. **å†…å®¹è¦æ±‚**ï¼š
   - å¿…é¡»åŸºäºé¡µé¢å®é™…å­˜åœ¨çš„å†…å®¹
   - æ•°å­—ä¿¡æ¯å¿…é¡»å‡†ç¡®ä¸”æœ‰æ˜ç¡®ä¸šåŠ¡å«ä¹‰
   - ä¸“ä¸šæœ¯è¯­å¿…é¡»è§„èŒƒ
   - é¿å…å¤¸å¼ å’Œè¥é”€æ€§æè¿°
   - é¿å…æ— æ„ä¹‰çš„çº¯æ•°å­—å’ŒæŠ€æœ¯è§„æ ¼
   - å“ç‰Œåç§°å¿…é¡»ä½¿ç”¨æ ‡å‡†ä¸­æ–‡å½¢å¼

## ğŸš¨ è¾“å‡ºæ ¼å¼ä¸¥æ ¼è¦æ±‚

**å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸å¾—æœ‰ä»»ä½•åå·®ï¼š**

1. **ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦ä½¿ç”¨```jsonä»£ç å—åŒ…è£…**
2. **ä½¿ç”¨æ ‡å‡†è‹±æ–‡åŒå¼•å· " è€Œä¸æ˜¯ä¸­æ–‡å¼•å· " "**
3. **ç¡®ä¿JSONç»“æ„å®Œæ•´ï¼ŒåŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ**
4. **ç¡®ä¿æ‹¬å·å’Œå¼•å·æ­£ç¡®é…å¯¹**

**æ ‡å‡†è¾“å‡ºæ ¼å¼ï¼ˆè¯·å®Œå…¨æŒ‰æ­¤æ ¼å¼è¾“å‡ºï¼‰ï¼š**

{{
  "keywords": [
    {{
      "keyword": "InternLMå¤§æ¨¡å‹",
      "dimension": "çƒ­é—¨æ¨¡å‹å“ç‰Œ",
      "reason": "çŸ¥åå¼€æºå¤§æ¨¡å‹å“ç‰Œï¼Œç”¨æˆ·æœç´¢çƒ­åº¦é«˜ï¼Œå¼•æµä»·å€¼å¤§"
    }},
    {{
      "keyword": "å¤šæ¨¡æ€æ¨ç†",
      "dimension": "æ ¸å¿ƒæŠ€æœ¯æ¶æ„",
      "reason": "å½“å‰AIæŠ€æœ¯çƒ­ç‚¹ï¼Œå¼€å‘è€…å…³æ³¨åº¦é«˜ï¼Œæœç´¢éœ€æ±‚å¤§"
    }},
    {{
      "keyword": "ç§‘å­¦è®¡ç®—",
      "dimension": "ä¸“ä¸šé¢†åŸŸ",
      "reason": "ä¸“ä¸šåº”ç”¨åœºæ™¯ï¼Œç§‘ç ”ç”¨æˆ·æœç´¢éœ€æ±‚å¼ºçƒˆ"
    }},
    {{
      "keyword": "FP8é‡åŒ–",
      "dimension": "æ ¸å¿ƒæŠ€æœ¯æ¶æ„",
      "reason": "å‰æ²¿é‡åŒ–æŠ€æœ¯ï¼ŒAIä¼˜åŒ–çƒ­ç‚¹ï¼ŒæŠ€æœ¯æœç´¢ä»·å€¼é«˜"
    }},
    {{
      "keyword": "671Bå‚æ•°",
      "dimension": "æ€§èƒ½è§„æ ¼",
      "reason": "å¤§è§„æ¨¡å‚æ•°æ¨¡å‹ï¼ŒæŠ€æœ¯è§„æ ¼å…³é”®ä¿¡æ¯ï¼Œç”¨æˆ·å…³æ³¨åº¦é«˜"
    }},
    {{
      "keyword": "Transformers",
      "dimension": "éƒ¨ç½²ä¸é›†æˆ",
      "reason": "ä¸»æµAIæ¡†æ¶ï¼Œå¼€å‘è€…å¿…å¤‡å·¥å…·ï¼Œæœç´¢ä½¿ç”¨é¢‘ç‡é«˜"
    }}
  ]
}}

âš ï¸âš ï¸âš ï¸ **æœ€ç»ˆæ£€æŸ¥æ¸…å•** âš ï¸âš ï¸âš ï¸
åœ¨è¾“å‡ºå‰è¯·åŠ¡å¿…ç¡®è®¤ï¼š
1. âœ… å…³é”®è¯æ•°é‡ï¼šå¿…é¡»æ˜¯6-8ä¸ªï¼ˆæ•°ä¸€æ•°ï¼ï¼‰
2. âœ… JSONæ ¼å¼ï¼šå¼€å¤´æ˜¯{{ï¼Œç»“å°¾æ˜¯}}ï¼Œæ— ä»£ç å—æ ‡è®°
3. âœ… å¼•å·æ ¼å¼ï¼šä½¿ç”¨è‹±æ–‡åŒå¼•å·"ï¼Œä¸æ˜¯ä¸­æ–‡å¼•å·
4. âœ… æ¯ä¸ªå…³é”®è¯åŒ…å«ï¼škeyword, dimension, reasonä¸‰ä¸ªå­—æ®µ
5. âœ… å¼•æµä»·å€¼ï¼šæ¯ä¸ªå…³é”®è¯éƒ½æœ‰æ˜ç¡®çš„æœç´¢ä»·å€¼å’Œå¼•æµæ•ˆæœ
6. âœ… å“ç‰Œè§„èŒƒï¼šå“ç‰Œåå¿…é¡»åŠ "å¤§æ¨¡å‹"åç¼€ï¼Œå¦‚"NVIDIAå¤§æ¨¡å‹"
7. âœ… å‚æ•°ä¼˜åŒ–ï¼šå¤§æ•°å­—å‚æ•°å·²ä¼˜åŒ–ï¼Œå¦‚"671Bå‚æ•°"è€Œé"6710äº¿å‚æ•°"
8. âœ… æ— è®¸å¯è¯ï¼šå·²æ’é™¤"MITè®¸å¯è¯"ã€"Apache License 2.0"ç­‰æ— ä»·å€¼å†…å®¹
9. âœ… æ— é•œåƒï¼šå·²æ’é™¤"HuggingFaceé•œåƒ"ã€"openai-mirror"ç­‰æ— æ„ä¹‰å†…å®¹
10. âœ… æ— é€šç”¨è¯ï¼šå·²æ’é™¤"å·¥å…·è°ƒç”¨"ã€"æ¨ç†åŠ é€Ÿ"ç­‰è¿‡äºé€šç”¨çš„è¯
11. âœ… æ— ç‰ˆæœ¬å·ï¼šå·²æ’é™¤"v2.0"ã€"v1.5"ç­‰çº¯ç‰ˆæœ¬æ ‡è¯†
12. âœ… æ— æŠ€æœ¯å‚æ•°ï¼šå·²æ’é™¤"16kHz"ã€"720p"ç­‰çº¯æŠ€æœ¯è§„æ ¼

ğŸš¨ å¦‚æœå…³é”®è¯æ•°é‡ä¸è¶³6ä¸ªï¼Œè¯·é‡æ–°æå–ï¼
ğŸš¨ å¦‚æœåŒ…å«ä¸Šè¿°ç¦æ­¢å†…å®¹ï¼Œè¯·é‡æ–°ç­›é€‰ï¼
ğŸš¨ å¦‚æœå…³é”®è¯ç¼ºä¹å¼•æµä»·å€¼ï¼Œè¯·é‡æ–°ä¼˜åŒ–ï¼"""

        return prompt
    
    def _parse_keywords_response(self, response: str) -> List[Dict[str, str]]:
        """
        è§£æAIå“åº”ä¸­çš„å…³é”®è¯JSON
        
        Args:
            response: AIå“åº”å†…å®¹
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        try:
            # ç¬¬ä¸€æ­¥ï¼šå°è¯•ç›´æ¥è§£æï¼ˆAIåº”è¯¥è¿”å›æ ‡å‡†JSONï¼‰
            cleaned_response = response.strip()
            
            # æ¸…ç†å¯èƒ½çš„ä¸­æ–‡å¼•å·é—®é¢˜
            json_str = cleaned_response.replace('"', '"').replace('"', '"')
            json_str = json_str.replace(''', "'").replace(''', "'")
            
            # ç›´æ¥å°è¯•è§£æ
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                if '```json' in json_str:
                    start = json_str.find('```json') + 7
                    end = json_str.find('```', start)
                    if end != -1:
                        json_str = json_str[start:end].strip()
                    else:
                        json_str = json_str[start:].strip()
                else:
                    # æŸ¥æ‰¾JSONå¯¹è±¡è¾¹ç•Œ
                    start_pos = json_str.find('{')
                    if start_pos != -1:
                        end_pos = json_str.rfind('}')
                        if end_pos != -1 and end_pos > start_pos:
                            json_str = json_str[start_pos:end_pos+1]
                
                # å†æ¬¡æ¸…ç†å’Œè§£æ
                json_str = json_str.replace('"', '"').replace('"', '"')
                json_str = json_str.replace(''', "'").replace(''', "'")
                
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é”™è¯¯
                json_str = self._fix_common_json_errors(json_str)
                
                # å°è¯•ä¿®å¤æˆªæ–­çš„JSON
                json_str = self._fix_truncated_json(json_str)
                
                data = json.loads(json_str)
            
            keywords = data.get('keywords', [])
            
            # éªŒè¯å…³é”®è¯æ•°é‡ï¼ˆé€‚åº¦æ”¾å®½è‡³3-8ä¸ªä»¥å‡å°‘å¤±è´¥ç‡ï¼‰
            if len(keywords) < 3:
                print(f"âš ï¸ å…³é”®è¯æ•°é‡ä¸è¶³ï¼šåªæœ‰{len(keywords)}ä¸ªï¼Œè¦æ±‚è‡³å°‘3ä¸ª")
                return []
            elif len(keywords) > 8:
                print(f"âš ï¸ å…³é”®è¯æ•°é‡è¿‡å¤šï¼šæœ‰{len(keywords)}ä¸ªï¼Œè¦æ±‚3-8ä¸ªï¼Œå–å‰8ä¸ª")
                keywords = keywords[:8]
            else:
                print(f"âœ… å…³é”®è¯æ•°é‡ç¬¦åˆè¦æ±‚ï¼š{len(keywords)}ä¸ª")
            
            # éªŒè¯å’Œæ¸…ç†å…³é”®è¯
            cleaned_keywords = []
            for kw in keywords:
                if self._validate_keyword(kw):
                    cleaned_kw = self._clean_keyword(kw)
                    # åªæ£€æŸ¥å½“å‰æ¨¡å‹å†…çš„é‡å¤ï¼Œä¸è·¨æ¨¡å‹å»é‡
                    current_keywords = [k['keyword'] for k in cleaned_keywords]
                    if cleaned_kw['keyword'] not in current_keywords:
                        cleaned_keywords.append(cleaned_kw)
            
            return cleaned_keywords
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"ğŸ“ AIå®Œæ•´è¿”å›å†…å®¹:")
            print("=" * 80)
            print(response[:1500] + ("..." if len(response) > 1500 else ""))
            print("=" * 80)
            print("ğŸ’¡ æç¤ºï¼šAIå¯èƒ½æ²¡æœ‰æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¿”å›")
            return []
        except Exception as e:
            print(f"âŒ è§£æå“åº”æ—¶å‡ºé”™: {e}")
            print(f"ğŸ“ AIå®Œæ•´è¿”å›å†…å®¹:")
            print("=" * 80)
            print(response[:1500] + ("..." if len(response) > 1500 else ""))
            print("=" * 80)
            return []
    
    def _validate_keyword(self, keyword_obj: Dict[str, str]) -> bool:
        """
        éªŒè¯å…³é”®è¯å¯¹è±¡æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            keyword_obj: å…³é”®è¯å¯¹è±¡
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        required_fields = ['keyword', 'dimension', 'reason']
        return all(field in keyword_obj and keyword_obj[field].strip() for field in required_fields)
    
    def _clean_keyword(self, keyword_obj: Dict[str, str]) -> Dict[str, str]:
        """
        æ¸…ç†å…³é”®è¯ï¼Œç¡®ä¿ç¬¦åˆæ ¼å¼è¦æ±‚
        
        Args:
            keyword_obj: åŸå§‹å…³é”®è¯å¯¹è±¡
            
        Returns:
            æ¸…ç†åçš„å…³é”®è¯å¯¹è±¡
        """
        keyword = keyword_obj['keyword'].strip()
        
        # ç§»é™¤æ‹¬å·
        keyword = re.sub(r'[()ï¼ˆï¼‰]', '', keyword)
        
        # æ›¿æ¢ç©ºæ ¼ä¸ºè¿å­—ç¬¦
        keyword = re.sub(r'\s+', '-', keyword)
        
        # åªä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€è¿å­—ç¬¦ã€ç‚¹å·
        keyword = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\.-]', '', keyword)
        
        # ç§»é™¤è¿ç»­çš„è¿å­—ç¬¦å’Œç‚¹å·
        keyword = re.sub(r'-+', '-', keyword)
        keyword = re.sub(r'\.+', '.', keyword)
        
        # ç§»é™¤é¦–å°¾è¿å­—ç¬¦å’Œç‚¹å·
        keyword = keyword.strip('-.')
        
        # ä½†ä¿ç•™ç‰ˆæœ¬å·æ ¼å¼ï¼ˆå¦‚v2.0, FLUX.1ç­‰ï¼‰
        # å¦‚æœå…³é”®è¯ä»¥vå¼€å¤´ä¸”åŒ…å«ç‚¹å·ï¼Œä¿ç•™å®ƒ
        if keyword.lower().startswith('v') and '.' in keyword:
            pass  # ä¿æŒåŸæ ·ï¼Œä¸å†å¤„ç†
        # å¦‚æœæ˜¯å¸¸è§çš„ç‰ˆæœ¬å·æ ¼å¼ï¼Œä¹Ÿä¿ç•™
        elif re.match(r'^[A-Za-z0-9]+\.[0-9]+$', keyword):
            pass  # ä¿æŒåŸæ ·ï¼Œå¦‚FLUX.1, GPT.4ç­‰
        
        # å“ç‰Œåç§°æ™ºèƒ½æ‰©å±•ç­–ç•¥
        keyword = self._enhance_brand_keywords(keyword, keyword_obj['dimension'])
        
        return {
            'keyword': keyword,
            'dimension': keyword_obj['dimension'].strip(),
            'reason': keyword_obj['reason'].strip()
        }
    
    def _fix_common_json_errors(self, json_str: str) -> str:
        """
        ä¿®å¤AIç”ŸæˆJSONä¸­çš„å¸¸è§é”™è¯¯
        
        Args:
            json_str: å¾…ä¿®å¤çš„JSONå­—ç¬¦ä¸²
            
        Returns:
            ä¿®å¤åçš„JSONå­—ç¬¦ä¸²
        """
        import re
        
        # ä¿®å¤ç¼ºå¤±å¼€æ‹¬å·çš„æƒ…å†µï¼š},\n  "keyword" â†’ },\n  {"keyword"
        # åŒ¹é…ï¼š},åé¢è·Ÿç€æ¢è¡Œå’Œç©ºæ ¼ï¼Œç„¶åç›´æ¥æ˜¯"keyword"ï¼ˆè€Œä¸æ˜¯{ï¼‰
        pattern = r'(\},\s*\n\s*)("keyword":)'
        json_str = re.sub(pattern, r'\1{\2', json_str)
        
        # ä¿®å¤å¤šä½™é€—å·çš„æƒ…å†µï¼š},\n  }\n] â†’ }\n  }\n]
        json_str = re.sub(r',(\s*\}\s*\])', r'\1', json_str)
        
        # ä¿®å¤ç¼ºå¤±é€—å·çš„æƒ…å†µï¼š}\n  { â†’ },\n  {
        json_str = re.sub(r'(\})\s*\n\s*(\{)', r'\1,\n  \2', json_str)
        
        return json_str
    
    def _fix_truncated_json(self, json_str: str) -> str:
        """ä¿®å¤æˆªæ–­çš„JSON"""
        import re
        
        # å¦‚æœJSONè¢«æˆªæ–­ï¼Œå°è¯•ä¿®å¤
        if not json_str.endswith('}'):
            # æŸ¥æ‰¾æœ€åä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
            last_complete_obj = json_str.rfind('}')
            if last_complete_obj != -1:
                # æ£€æŸ¥æ˜¯å¦åœ¨keywordsæ•°ç»„ä¸­
                before_last = json_str[:last_complete_obj]
                if '"keywords"' in before_last:
                    # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„keywordå¯¹è±¡
                    keyword_objects = re.findall(r'\{[^}]*"keyword"[^}]*\}', before_last)
                    if keyword_objects:
                        # ä½¿ç”¨æœ€åä¸€ä¸ªå®Œæ•´çš„keywordå¯¹è±¡
                        last_keyword = keyword_objects[-1]
                        # é‡æ–°æ„å»ºJSON
                        json_str = before_last[:before_last.rfind(last_keyword)] + last_keyword + '}]}'
                    else:
                        # å¦‚æœæ²¡æœ‰å®Œæ•´çš„keywordå¯¹è±¡ï¼Œå°è¯•æ·»åŠ ç¼ºå¤±çš„ç»“æŸç¬¦
                        json_str = before_last + '}]}'
        
        return json_str
    
    def _enhance_brand_keywords(self, keyword: str, dimension: str) -> str:
        """
        å“ç‰Œå…³é”®è¯æ™ºèƒ½æ‰©å±•ç­–ç•¥
        
        Args:
            keyword: åŸå§‹å…³é”®è¯
            dimension: å…³é”®è¯ç»´åº¦
            
        Returns:
            æ‰©å±•åçš„å…³é”®è¯
        """
        # åªå¯¹"å“ç‰Œä¸èº«ä»½"ç»´åº¦çš„å…³é”®è¯è¿›è¡Œæ‰©å±•
        if dimension != "å“ç‰Œä¸èº«ä»½":
            return keyword
        
        # å®šä¹‰éœ€è¦æ‰©å±•çš„å“ç‰Œåç§°åˆ—è¡¨
        brand_names = {
            # ä¸­å›½å¤§å‚
            "ç™¾åº¦": "ç™¾åº¦å¤§æ¨¡å‹",
            "è…¾è®¯": "è…¾è®¯å¤§æ¨¡å‹", 
            "é˜¿é‡Œ": "é˜¿é‡Œå¤§æ¨¡å‹",
            "é˜¿é‡Œå·´å·´": "é˜¿é‡Œå·´å·´å¤§æ¨¡å‹",
            "å­—èŠ‚": "å­—èŠ‚å¤§æ¨¡å‹",
            "å­—èŠ‚è·³åŠ¨": "å­—èŠ‚è·³åŠ¨å¤§æ¨¡å‹",
            "åä¸º": "åä¸ºå¤§æ¨¡å‹",
            "å°ç±³": "å°ç±³å¤§æ¨¡å‹",
            "å¿«æ‰‹": "å¿«æ‰‹å¤§æ¨¡å‹",
            "ç½‘æ˜“": "ç½‘æ˜“å¤§æ¨¡å‹",
            "äº¬ä¸œ": "äº¬ä¸œå¤§æ¨¡å‹",
            "ç¾å›¢": "ç¾å›¢å¤§æ¨¡å‹",
            "æ»´æ»´": "æ»´æ»´å¤§æ¨¡å‹",
            
            # å›½é™…å¤§å‚
            "OpenAI": "OpenAIå¤§æ¨¡å‹",
            "Google": "Googleå¤§æ¨¡å‹", 
            "è°·æ­Œ": "è°·æ­Œå¤§æ¨¡å‹",
            "Microsoft": "Microsoftå¤§æ¨¡å‹",
            "å¾®è½¯": "å¾®è½¯å¤§æ¨¡å‹",
            "Meta": "Metaå¤§æ¨¡å‹",
            "Facebook": "Facebookå¤§æ¨¡å‹",
            "Amazon": "Amazonå¤§æ¨¡å‹",
            "äºšé©¬é€Š": "äºšé©¬é€Šå¤§æ¨¡å‹",
            "Apple": "Appleå¤§æ¨¡å‹",
            "è‹¹æœ": "è‹¹æœå¤§æ¨¡å‹",
            "NVIDIA": "NVIDIAå¤§æ¨¡å‹",
            "è‹±ä¼Ÿè¾¾": "è‹±ä¼Ÿè¾¾å¤§æ¨¡å‹",
            
            # AIåˆ›ä¸šå…¬å¸
            "æ™ºè°±": "æ™ºè°±å¤§æ¨¡å‹",
            "æœˆä¹‹æš—é¢": "æœˆä¹‹æš—é¢å¤§æ¨¡å‹",
            "é›¶ä¸€ä¸‡ç‰©": "é›¶ä¸€ä¸‡ç‰©å¤§æ¨¡å‹",
            "æ·±åº¦æ±‚ç´¢": "æ·±åº¦æ±‚ç´¢å¤§æ¨¡å‹",
            "å•†æ±¤": "å•†æ±¤å¤§æ¨¡å‹",
            "æ—·è§†": "æ—·è§†å¤§æ¨¡å‹",
            "ç§‘å¤§è®¯é£": "ç§‘å¤§è®¯é£å¤§æ¨¡å‹",
            "äº‘çŸ¥å£°": "äº‘çŸ¥å£°å¤§æ¨¡å‹",
            "å‡ºé—¨é—®é—®": "å‡ºé—¨é—®é—®å¤§æ¨¡å‹",
            "å°å†°": "å°å†°å¤§æ¨¡å‹"
        }
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºéœ€è¦æ‰©å±•çš„å“ç‰Œåç§°
        for brand, enhanced in brand_names.items():
            if keyword == brand:
                print(f"ğŸ”„ å“ç‰Œæ‰©å±•: {brand} â†’ {enhanced}")
                return enhanced
        
        return keyword
    
    def deduplicate_keywords(self, keyword_results: List[KeywordResult]) -> List[KeywordResult]:
        """
        å¯¹æ‰€æœ‰æå–çš„å…³é”®è¯è¿›è¡Œå»é‡
        
        Args:
            keyword_results: å…³é”®è¯æå–ç»“æœåˆ—è¡¨
            
        Returns:
            å»é‡åçš„ç»“æœåˆ—è¡¨
        """
        print("å¼€å§‹è¿›è¡Œå…³é”®è¯å»é‡...")
        
        # ç»Ÿè®¡æ‰€æœ‰å…³é”®è¯çš„å‡ºç°é¢‘ç‡
        keyword_count = {}
        for result in keyword_results:
            for kw in result.keywords:
                keyword = kw['keyword']
                keyword_count[keyword] = keyword_count.get(keyword, 0) + 1
        
        # å¯¹äºå‡ºç°æ¬¡æ•°è¿‡å¤šçš„é€šç”¨å…³é”®è¯ï¼Œåªä¿ç•™æœ€ä¼˜çš„å‡ ä¸ª
        # æ”¾å®½é˜ˆå€¼ï¼šåªæœ‰å‡ºç°è¶…è¿‡30%çš„å…³é”®è¯æ‰ç®—é«˜é¢‘
        common_threshold = max(5, len(keyword_results) // 3)  # å‡ºç°é¢‘ç‡é˜ˆå€¼
        
        deduplicated_results = []
        global_keywords = set()
        
        for result in keyword_results:
            filtered_keywords = []
            
            for kw in result.keywords:
                keyword = kw['keyword']
                
                # å¦‚æœæ˜¯é«˜é¢‘è¯ä¸”å·²ç»æœ‰å…¶ä»–æ¨¡å‹ä½¿ç”¨ï¼Œè·³è¿‡
                if keyword_count[keyword] >= common_threshold and keyword in global_keywords:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰å…³é”®è¯è¿‡äºç›¸ä¼¼
                if not self._is_similar_keyword_exists(keyword, global_keywords):
                    filtered_keywords.append(kw)
                    global_keywords.add(keyword)
            
            # ç¡®ä¿æ¯ä¸ªæ¨¡å‹è‡³å°‘ä¿ç•™2ä¸ªå…³é”®è¯
            if not filtered_keywords and len(result.keywords) >= 2:
                # å¦‚æœæ‰€æœ‰å…³é”®è¯éƒ½è¢«è¿‡æ»¤ï¼Œå¼ºåˆ¶ä¿ç•™å‰2ä¸ªæœ€é‡è¦çš„
                filtered_keywords = result.keywords[:2]
                print(f"è­¦å‘Šï¼šæ¨¡å‹æ‰€æœ‰å…³é”®è¯éƒ½æ˜¯é«˜é¢‘è¯ï¼Œå¼ºåˆ¶ä¿ç•™å‰2ä¸ª")
            
            if filtered_keywords:
                result.keywords = filtered_keywords
                deduplicated_results.append(result)
        
        print(f"å»é‡å®Œæˆï¼Œä¿ç•™ {len(deduplicated_results)} ä¸ªæ¨¡å‹çš„å…³é”®è¯")
        return deduplicated_results
    
    def _is_similar_keyword_exists(self, keyword: str, existing_keywords: set) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸ä¼¼çš„å…³é”®è¯
        
        Args:
            keyword: å¾…æ£€æŸ¥çš„å…³é”®è¯
            existing_keywords: å·²å­˜åœ¨çš„å…³é”®è¯é›†åˆ
            
        Returns:
            æ˜¯å¦å­˜åœ¨ç›¸ä¼¼å…³é”®è¯
        """
        keyword_lower = keyword.lower()
        
        for existing in existing_keywords:
            existing_lower = existing.lower()
            
            # å®Œå…¨ç›¸åŒ
            if keyword_lower == existing_lower:
                return True
            
            # åŒ…å«å…³ç³»ï¼ˆè¾ƒçŸ­çš„è¯åŒ…å«åœ¨è¾ƒé•¿çš„è¯ä¸­ï¼‰
            if len(keyword_lower) >= 3 and len(existing_lower) >= 3:
                if keyword_lower in existing_lower or existing_lower in keyword_lower:
                    return True
        
        return False
    
    @abstractmethod
    def extract_keywords(self, model_info: ModelInfo) -> Optional[KeywordResult]:
        """æå–å…³é”®è¯çš„æŠ½è±¡æ–¹æ³•"""
        pass
    
    @abstractmethod
    def extract_batch_keywords(self, model_infos: List[ModelInfo]) -> List[KeywordResult]:
        """æ‰¹é‡æå–å…³é”®è¯çš„æŠ½è±¡æ–¹æ³•"""
        pass
