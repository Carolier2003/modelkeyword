"""
å¤šå¹³å°AIå…³é”®è¯æå–æ¨¡å— - æ”¯æŒå¹¶å‘è°ƒç”¨å¤šä¸ªå¤§æ¨¡å‹å¹³å°
"""
import os
import re
import json
import asyncio
import aiohttp
from typing import List, Dict, Any, Optional, Tuple
from openai import AsyncOpenAI
from dotenv import load_dotenv

from models import ModelInfo, KeywordResult
from base_extractor import BaseKeywordExtractor

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class MultiPlatformExtractor(BaseKeywordExtractor):
    """å¤šå¹³å°å…³é”®è¯æå–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤šä¸ªAIå®¢æˆ·ç«¯"""
        super().__init__()  # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        self.platforms = self._init_platforms()
        
    def _init_platforms(self) -> Dict[str, Dict]:
        """åˆå§‹åŒ–æ”¯æŒçš„å¹³å°é…ç½®"""
        platforms = {}
        
        # æœˆä¹‹æš—é¢ (Moonshot)
        if os.getenv("MOONSHOT_API_KEY"):
            platforms["moonshot"] = {
                "client": AsyncOpenAI(
                    api_key=os.getenv("MOONSHOT_API_KEY"),
                    base_url=os.getenv("MOONSHOT_BASE_URL", "https://api.moonshot.cn/v1"),
                ),
                "model": "kimi-k2-0905-preview",
                "name": "æœˆä¹‹æš—é¢",
                "enabled": True
            }
        
        # é˜¿é‡Œç™¾ç‚¼ (DashScope)
        if os.getenv("DASHSCOPE_API_KEY"):
            platforms["dashscope"] = {
                "client": AsyncOpenAI(
                    api_key=os.getenv("DASHSCOPE_API_KEY"),
                    base_url=os.getenv("DASHSCOPE_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"),
                ),
                "model": os.getenv("DASHSCOPE_MODEL", "qwen-plus"),
                "name": "é˜¿é‡Œç™¾ç‚¼",
                "enabled": True
            }
        
        # OpenAI (å¦‚æœé…ç½®äº†)
        if os.getenv("OPENAI_API_KEY"):
            platforms["openai"] = {
                "client": AsyncOpenAI(
                    api_key=os.getenv("OPENAI_API_KEY"),
                    base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
                ),
                "model": os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
                "name": "OpenAI",
                "enabled": True
            }
        
        # æ™ºè°±AI (å¦‚æœé…ç½®äº†)
        if os.getenv("ZHIPU_API_KEY"):
            platforms["zhipu"] = {
                "client": AsyncOpenAI(
                    api_key=os.getenv("ZHIPU_API_KEY"),
                    base_url=os.getenv("ZHIPU_BASE_URL", "https://open.bigmodel.cn/api/paas/v4"),
                ),
                "model": os.getenv("ZHIPU_MODEL", "glm-4"),
                "name": "æ™ºè°±AI",
                "enabled": True
            }
        
        # ä¸ƒç‰›äº‘ (å¦‚æœé…ç½®äº†)
        if os.getenv("QINIU_API_KEY"):
            platforms["qiniu"] = {
                "client": AsyncOpenAI(
                    api_key=os.getenv("QINIU_API_KEY"),
                    base_url=os.getenv("QINIU_BASE_URL", "https://openai.qiniu.com/v1"),
                ),
                "model": os.getenv("QINIU_MODEL", "gpt-oss-120b"),
                "name": "ä¸ƒç‰›äº‘",
                "enabled": True
            }
        
        # è…¾è®¯æ··å…ƒ (å¦‚æœé…ç½®äº†)
        if os.getenv("HUNYUAN_API_KEY"):
            platforms["hunyuan"] = {
                "client": AsyncOpenAI(
                    api_key=os.getenv("HUNYUAN_API_KEY"),
                    base_url=os.getenv("HUNYUAN_BASE_URL", "https://api.hunyuan.cloud.tencent.com/v1"),
                ),
                "model": os.getenv("HUNYUAN_MODEL", "hunyuan-turbos-latest"),
                "name": "è…¾è®¯æ··å…ƒ",
                "enabled": True
            }
        
        # ç¡…åŸºæµåŠ¨ (å¦‚æœé…ç½®äº†)
        if os.getenv("SILICONFLOW_API_KEY"):
            platforms["siliconflow"] = {
                "client": AsyncOpenAI(
                    api_key=os.getenv("SILICONFLOW_API_KEY"),
                    base_url=os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1"),
                ),
                "model": os.getenv("SILICONFLOW_MODEL", "Qwen/Qwen3-Next-80B-A3B-Instruct"),
                "name": "ç¡…åŸºæµåŠ¨",
                "enabled": True
            }
        
        # ç«å±±å¼•æ“ (å¦‚æœé…ç½®äº†)
        if os.getenv("VOLCENGINE_API_KEY"):
            platforms["volcengine"] = {
                "client": AsyncOpenAI(
                    api_key=os.getenv("VOLCENGINE_API_KEY"),
                    base_url=os.getenv("VOLCENGINE_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3"),
                ),
                "model": os.getenv("VOLCENGINE_MODEL", "doubao-1-5-pro-32k-250115"),
                "name": "ç«å±±å¼•æ“",
                "enabled": True
            }
        
        # ç™¾åº¦åƒå¸† (å¦‚æœé…ç½®äº†)
        if os.getenv("QIANFAN_API_KEY"):
            platforms["qianfan"] = {
                "client": AsyncOpenAI(
                    api_key=os.getenv("QIANFAN_API_KEY"),
                    base_url=os.getenv("QIANFAN_BASE_URL", "https://qianfan.baidubce.com"),
                ),
                "model": os.getenv("QIANFAN_MODEL", "ernie-4.5-turbo-128k"),
                "name": "ç™¾åº¦åƒå¸†",
                "enabled": True
            }
        
        # è®¯é£æ˜Ÿç« (å¦‚æœé…ç½®äº†)
        if os.getenv("SPARK_API_KEY"):
            platforms["spark"] = {
                "client": AsyncOpenAI(
                    api_key=os.getenv("SPARK_API_KEY"),
                    base_url=os.getenv("SPARK_BASE_URL", "https://spark-api-open.xf-yun.com/v2"),
                ),
                "model": os.getenv("SPARK_MODEL", "x1"),
                "name": "è®¯é£æ˜Ÿç«",
                "enabled": True
            }
        
        print(f"ğŸš€ åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒ {len(platforms)} ä¸ªå¹³å°:")
        for platform_id, config in platforms.items():
            print(f"   - {config['name']} ({platform_id}): {config['model']}")
        
        return platforms
    
    # build_prompt æ–¹æ³•å·²ç§»è‡³ BaseKeywordExtractor
    
    async def extract_keywords_single_platform(self, model_info: ModelInfo, platform_id: str) -> Optional[Tuple[str, List[Dict[str, str]]]]:
        """ä½¿ç”¨å•ä¸ªå¹³å°æå–å…³é”®è¯"""
        import time
        start_time = time.time()
        
        if platform_id not in self.platforms or not self.platforms[platform_id]["enabled"]:
            return None
        
        platform = self.platforms[platform_id]
        client = platform["client"]
        model = platform["model"]
        platform_name = platform["name"]
        
        # æå–æ¨¡å‹åç§°ç”¨äºæ˜¾ç¤º
        model_name = model_info.url.split('/')[-2:] if '/' in model_info.url else [model_info.url]
        model_name = '/'.join(model_name)
        
        try:
            print(f"ğŸ”„ ä½¿ç”¨ {platform_name} å¤„ç† {model_name}...")
            
            prompt = self.build_prompt(model_info)
            
            # ä¸ºè…¾è®¯æ··å…ƒå’Œç™¾åº¦åƒå¸†æ·»åŠ ç‰¹æ®Šå‚æ•°
            extra_params = {}
            if platform_id == "hunyuan":
                extra_params["extra_body"] = {"enable_enhancement": True}
            elif platform_id == "qianfan":
                extra_params["extra_body"] = {
                    "penalty_score": 1,
                    "stop": [],
                    "web_search": {
                        "enable": False,
                        "enable_trace": False
                    }
                }
            
            completion = await client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIé¡¹ç›®è¿è¥ä¸“å®¶å’ŒSEOå¤§å¸ˆï¼Œä¸“é—¨è´Ÿè´£ä»AIæ¨¡å‹é¡¹ç›®ä¸­æå–é«˜ä»·å€¼çš„å…³é”®è¯ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1200,  # è¿›ä¸€æ­¥å¢åŠ tokenæ•°é‡ï¼Œé¿å…å“åº”è¢«æˆªæ–­
                **extra_params
            )
            
            response_content = completion.choices[0].message.content
            
            # ä¸ºæ™ºè°±AIæ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            if platform_id == "zhipu":
                print(f"ğŸ” æ™ºè°±AIè°ƒè¯•ä¿¡æ¯:")
                print(f"   å“åº”é•¿åº¦: {len(response_content)} å­—ç¬¦")
                print(f"   å“åº”å†…å®¹å‰500å­—ç¬¦:")
                print(f"   {response_content[:500]}")
                print(f"   å“åº”å†…å®¹å500å­—ç¬¦:")
                print(f"   {response_content[-500:]}")
                print(f"   å®Œæ•´å“åº”å†…å®¹:")
                print(f"   {response_content}")
                print(f"   å“åº”å†…å®¹ç±»å‹: {type(response_content)}")
            
            keywords = self._parse_keywords_response(response_content)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            if keywords:
                print(f"âœ… {platform_name} æˆåŠŸå¤„ç† {model_name} ({processing_time:.2f}s) - æå– {len(keywords)} ä¸ªå…³é”®è¯")
                return platform_id, keywords
            else:
                print(f"âŒ {platform_name} å¤„ç† {model_name} ({processing_time:.2f}s) - æœªèƒ½æå–åˆ°æœ‰æ•ˆå…³é”®è¯")
                return None
                
        except Exception as e:
            end_time = time.time()
            processing_time = end_time - start_time
            error_msg = str(e)
            print(f"âŒ {platform_name} å¤„ç† {model_name} ({processing_time:.2f}s) - æå–å¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯APIé™åˆ¶é”™è¯¯ï¼ˆ429/503ï¼‰
            if "429" in error_msg or "503" in error_msg or "rate_limit" in error_msg.lower() or "too busy" in error_msg.lower():
                # è®¡ç®—å»¶è¿Ÿæ—¶é—´ï¼šåŸºç¡€å»¶è¿Ÿ + éšæœºå»¶è¿Ÿ
                import random
                base_delay = 1  # å‡å°‘åŸºç¡€å»¶è¿Ÿåˆ°1ç§’
                random_delay = random.uniform(0.5, 1.5)  # å‡å°‘éšæœºå»¶è¿Ÿ
                total_delay = base_delay + random_delay
                
                print(f"â³ {platform_name} é‡åˆ°APIé™åˆ¶ï¼Œç­‰å¾… {total_delay:.1f} ç§’åé‡è¯•...")
                await asyncio.sleep(total_delay)
            
            return None
    
    async def extract_keywords_concurrent(self, model_info: ModelInfo) -> Optional[KeywordResult]:
        """å¹¶å‘è°ƒç”¨å¤šä¸ªå¹³å°æå–å…³é”®è¯"""
        import time
        start_time = time.time()
        
        print(f"ğŸš€ å¹¶å‘è°ƒç”¨ {len(self.platforms)} ä¸ªå¹³å°æå–å…³é”®è¯...")
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for platform_id in self.platforms.keys():
            if self.platforms[platform_id]["enabled"]:
                task = self.extract_keywords_single_platform(model_info, platform_id)
                tasks.append(task)
        
        if not tasks:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„å¹³å°")
            return None
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        successful_results = []
        for result in results:
            if isinstance(result, Exception):
                print(f"âŒ å¹³å°è°ƒç”¨å¼‚å¸¸: {result}")
            elif result is not None:
                platform_id, keywords = result
                successful_results.append((platform_id, keywords))
        
        if not successful_results:
            print("âŒ æ‰€æœ‰å¹³å°éƒ½æå–å¤±è´¥")
            return None
        
        # é€‰æ‹©æœ€ä½³ç»“æœï¼ˆä¼˜å…ˆé€‰æ‹©å…³é”®è¯æ•°é‡æœ€å¤šçš„ï¼‰
        best_platform_id, best_keywords = max(successful_results, key=lambda x: len(x[1]))
        best_platform_name = self.platforms[best_platform_id]["name"]
        
        elapsed_time = time.time() - start_time
        print(f"âœ… æœ€ä½³ç»“æœæ¥è‡ª {best_platform_name}: {len(best_keywords)} ä¸ªå…³é”®è¯ (æ€»è€—æ—¶: {elapsed_time:.1f}ç§’)")
        
        return KeywordResult(
            model_url=model_info.url,
            keywords=best_keywords
        )
    
    # _parse_keywords_response æ–¹æ³•å·²ç§»è‡³ BaseKeywordExtractor
    
    # _validate_keyword, _clean_keyword, _fix_common_json_errors, _fix_truncated_json æ–¹æ³•å·²ç§»è‡³ BaseKeywordExtractor
    
    def extract_keywords(self, model_info: ModelInfo) -> Optional[KeywordResult]:
        """å®ç°æŠ½è±¡æ–¹æ³• - åŒæ­¥ç‰ˆæœ¬çš„å…³é”®è¯æå–"""
        return asyncio.run(self.extract_keywords_concurrent(model_info))
    
    async def extract_batch_keywords(self, model_infos: List[ModelInfo]) -> List[KeywordResult]:
        """æ‰¹é‡æå–å…³é”®è¯ï¼ˆwork-stealingç‰ˆæœ¬ï¼‰"""
        return await self._work_stealing_main(model_infos)
    
    async def _work_stealing_main(self, model_infos: List[ModelInfo]) -> List[KeywordResult]:
        """ä»»åŠ¡æ±  + work-stealing ä¸»é€»è¾‘"""
        import time
        start_time = time.time()
        
        total = len(model_infos)
        
        # è·å–å¯ç”¨çš„å¹³å°
        available_platforms = [pid for pid, config in self.platforms.items() if config["enabled"]]
        platform_count = len(available_platforms)
        
        if platform_count == 0:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„å¹³å°")
            return []
        
        print(f"ğŸš€ ä»»åŠ¡æ± å¯åŠ¨ï¼Œæ¨¡å‹ {total} ä¸ªï¼Œå¹³å° {platform_count} ä¸ª")
        
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ— (ModelInfo, retry_count)
        queue = asyncio.Queue()
        for model_info in model_infos:
            await queue.put((model_info, 0))
        
        # å…±äº«ç»“æœåˆ—è¡¨å’Œé”
        results = []
        lock = asyncio.Lock()
        
        # è¿›åº¦è·Ÿè¸ª
        progress_lock = asyncio.Lock()
        completed_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨ä¸åŒåç¨‹é—´å…±äº«
        
        # åˆ›å»ºworkerä»»åŠ¡
        workers = []
        for platform_id in available_platforms:
            worker = asyncio.create_task(
                self._worker(platform_id, queue, results, lock, platform_count, progress_lock, completed_count, total)
            )
            workers.append(worker)
        
        # å¯åŠ¨è¿›åº¦ç›‘æ§ä»»åŠ¡
        progress_task = asyncio.create_task(
            self._progress_monitor(progress_lock, completed_count, total, start_time)
        )
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await queue.join()
        
        # å–æ¶ˆæ‰€æœ‰workerå’Œè¿›åº¦ç›‘æ§
        for worker in workers:
            worker.cancel()
        progress_task.cancel()
        
        # ç­‰å¾…workeræ¸…ç†å®Œæˆ
        await asyncio.gather(*workers, return_exceptions=True)
        
        # è®¡ç®—è€—æ—¶
        end_time = time.time()
        total_time = end_time - start_time
        avg_time = total_time / len(results) if results else 0
        
        print(f"\nğŸš€ ä»»åŠ¡æ± å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(results)} ä¸ªæ¨¡å‹")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’ï¼Œå¹³å‡è€—æ—¶: {avg_time:.2f}ç§’/æ¨¡å‹")
        return results
    
    async def _progress_monitor(self, progress_lock: asyncio.Lock, completed_count: int, total: int, start_time: float):
        """è¿›åº¦ç›‘æ§ä»»åŠ¡"""
        import time
        
        while True:
            try:
                await asyncio.sleep(2)  # æ¯2ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦
                
                async with progress_lock:
                    current_completed = completed_count[0]
                
                if current_completed >= total:
                    break
                
                # è®¡ç®—è¿›åº¦
                progress_percent = (current_completed / total) * 100
                elapsed_time = time.time() - start_time
                
                # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´
                if current_completed > 0:
                    avg_time_per_model = elapsed_time / current_completed
                    remaining_models = total - current_completed
                    estimated_remaining = avg_time_per_model * remaining_models
                else:
                    estimated_remaining = 0
                
                # åˆ›å»ºè¿›åº¦æ¡
                bar_length = 30
                filled_length = int(bar_length * current_completed // total)
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                
                # æ˜¾ç¤ºè¿›åº¦
                print(f"\rğŸ“Š è¿›åº¦: [{bar}] {current_completed}/{total} ({progress_percent:.1f}%) | å·²ç”¨æ—¶: {elapsed_time:.1f}s | é¢„è®¡å‰©ä½™: {estimated_remaining:.1f}s", end='', flush=True)
                
            except asyncio.CancelledError:
                break
            except Exception:
                break
    
    async def _worker(self, platform_id: str, queue: asyncio.Queue, results: List[KeywordResult], 
                     lock: asyncio.Lock, max_retries: int, progress_lock: asyncio.Lock, completed_count: int, total: int):
        """å•ä¸ªå¹³å°çš„workeråç¨‹"""
        platform_name = self.platforms[platform_id]["name"]
        success_count = 0
        consecutive_failures = 0  # è¿ç»­å¤±è´¥è®¡æ•°
        
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                model_info, retry_count = queue.get_nowait()
                
                # å¦‚æœè¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œå¢åŠ å»¶è¿Ÿ
                if consecutive_failures > 2:
                    delay = min(consecutive_failures * 0.5, 3.0)  # æœ€å¤šå»¶è¿Ÿ3ç§’
                    print(f"â³ {platform_name} è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œå»¶è¿Ÿ {delay:.1f} ç§’...")
                    await asyncio.sleep(delay)
                
                # å°è¯•å¤„ç†æ¨¡å‹
                result = await self.extract_keywords_single_platform(model_info, platform_id)
                
                if result:
                    # æˆåŠŸå¤„ç†ï¼Œé‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
                    consecutive_failures = 0
                    platform_id_result, keywords = result
                    keyword_result = KeywordResult(
                        model_url=model_info.url,
                        keywords=keywords
                    )
                    
                    # çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ ç»“æœ
                    async with lock:
                        results.append(keyword_result)
                        # âœ¨ å®æ—¶æ›´æ–°æ’é™¤é˜Ÿåˆ—
                        self.update_exclusion_queue(keywords)
                    
                    # æ›´æ–°è¿›åº¦è®¡æ•°
                    async with progress_lock:
                        completed_count[0] += 1
                    
                    success_count += 1
                    queue.task_done()
                else:
                    # å¤„ç†å¤±è´¥ï¼Œå¢åŠ è¿ç»­å¤±è´¥è®¡æ•°
                    consecutive_failures += 1
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
                    if retry_count < max_retries - 1:
                        # é‡æ–°æ”¾å›é˜Ÿåˆ—ï¼Œå¢åŠ é‡è¯•æ¬¡æ•°
                        await queue.put((model_info, retry_count + 1))
                        queue.task_done()
                    else:
                        # æ‰€æœ‰å¹³å°éƒ½è¯•è¿‡äº†ï¼Œä¸¢å¼ƒ
                        print(f"\nâš ï¸  {model_info.project_name} æ‰€æœ‰å¹³å°å‡å¤±è´¥ï¼Œå·²ä¸¢å¼ƒ")
                        
                        # æ›´æ–°è¿›åº¦è®¡æ•°ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿç®—å®Œæˆï¼‰
                        async with progress_lock:
                            completed_count[0] += 1
                        
                        queue.task_done()
                        
            except asyncio.QueueEmpty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œworkeré€€å‡º
                break
            except Exception as e:
                # å•ä¸ªä»»åŠ¡å¼‚å¸¸ï¼Œå¢åŠ è¿ç»­å¤±è´¥è®¡æ•°
                consecutive_failures += 1
                print(f"âŒ {platform_name} å¤„ç†å¼‚å¸¸: {e}")
                try:
                    queue.task_done()
                except ValueError:
                    pass  # å¦‚æœtask_done()è¢«è°ƒç”¨å¤šæ¬¡ï¼Œå¿½ç•¥é”™è¯¯
        
        print(f"âœ… {platform_name} æˆåŠŸå¤„ç† {success_count} ä¸ª")
    
    async def extract_keywords_shard(self, platform_id: str, model_infos: List[ModelInfo], start_index: int) -> List[KeywordResult]:
        """å•ä¸ªå¹³å°å¤„ç†åˆ†ç‰‡"""
        platform_name = self.platforms[platform_id]["name"]
        shard_size = len(model_infos)
        
        print(f"ğŸ”„ {platform_name} å¼€å§‹å¤„ç†åˆ†ç‰‡ (æ¨¡å‹ {start_index+1}-{start_index+shard_size})")
        
        results = []
        for i, model_info in enumerate(model_infos):
            model_index = start_index + i + 1
            print(f"   è¿›åº¦: {model_index}/{start_index+shard_size} - {model_info.project_name}")
            
            # ä½¿ç”¨å•ä¸ªå¹³å°æå–å…³é”®è¯
            result = await self.extract_keywords_single_platform(model_info, platform_id)
            
            if result:
                platform_id_result, keywords = result
                keyword_result = KeywordResult(
                    model_url=model_info.url,
                    keywords=keywords
                )
                results.append(keyword_result)
                print(f"   âœ… æˆåŠŸæå– {len(keywords)} ä¸ªå…³é”®è¯")
            else:
                print(f"   âŒ æå–å¤±è´¥")
        
        print(f"âœ… {platform_name} åˆ†ç‰‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸ {len(results)}/{shard_size} ä¸ªæ¨¡å‹")
        return results


# åŒæ­¥åŒ…è£…å™¨ï¼Œä¿æŒä¸åŸç‰ˆå…¼å®¹
class MultiPlatformExtractorSync:
    """å¤šå¹³å°å…³é”®è¯æå–å™¨ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self):
        self.async_extractor = MultiPlatformExtractor()
    
    def extract_keywords(self, model_info: ModelInfo) -> Optional[KeywordResult]:
        """åŒæ­¥ç‰ˆæœ¬çš„å…³é”®è¯æå–"""
        return asyncio.run(self.async_extractor.extract_keywords_concurrent(model_info))
    
    def extract_batch_keywords(self, model_infos: List[ModelInfo]) -> List[KeywordResult]:
        """åŒæ­¥ç‰ˆæœ¬çš„æ‰¹é‡æå–"""
        return asyncio.run(self.async_extractor.extract_batch_keywords(model_infos))
    
    def deduplicate_keywords(self, keyword_results: List[KeywordResult]) -> List[KeywordResult]:
        """åŒæ­¥ç‰ˆæœ¬çš„å…³é”®è¯å»é‡"""
        return self.async_extractor.deduplicate_keywords(keyword_results)


def test_multi_platform():
    """æµ‹è¯•å¤šå¹³å°æå–åŠŸèƒ½"""
    import asyncio
    from models import ModelInfo
    
    async def test_async():
        extractor = MultiPlatformExtractor()
        
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹ä¿¡æ¯
        test_model = ModelInfo(
            url="https://gitcode.com/hf_mirrors/internlm/Intern-S1-FP8",
            project_name="internlm/Intern-S1-FP8",
            readme="Intern-S1 æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘è¾“å…¥ã€‚",
            tags=["å¤šæ¨¡æ€", "æ¨ç†", "Transformers"]
        )
        
        # æµ‹è¯•å¹¶å‘æå–
        result = await extractor.extract_keywords_concurrent(test_model)
        
        if result:
            print(f"\næ¨¡å‹: {result.model_url}")
            print("æå–çš„å…³é”®è¯:")
            for kw in result.keywords:
                print(f"- {kw['keyword']} ({kw['dimension']}): {kw['reason']}")
        else:
            print("å…³é”®è¯æå–å¤±è´¥")
    
    asyncio.run(test_async())


if __name__ == "__main__":
    test_multi_platform()
