#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¢„çˆ¬å–è„šæœ¬ - æ‰¹é‡çˆ¬å–æ¨¡å‹ç½‘é¡µæ•°æ®å¹¶ä¿å­˜åˆ°ç¼“å­˜

åŠŸèƒ½ï¼š
- ä»æ¨¡å‹æç¤ºè¯.csvè¯»å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹
- ä½¿ç”¨çˆ¬è™«è·å–READMEå’Œæ ‡ç­¾ä¿¡æ¯
- æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œåˆ†æ‰¹å¤„ç†
- å®æ—¶ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
"""

import os
import json
import time
import argparse
from datetime import datetime
from typing import List, Dict, Set
from tqdm import tqdm

from models import ModelInfo
from csv_reader import CSVModelReader, DEFAULT_CSV_FILE
from hf_scraper import scrape_hf_model_sync


class PreCrawler:
    """é¢„çˆ¬å–å™¨"""
    
    def __init__(self, csv_file: str = None, cache_file: str = "output/models_cache.json", 
                 delay: float = 0.5, token: str = None):
        """
        åˆå§‹åŒ–é¢„çˆ¬å–å™¨
        
        Args:
            csv_file: CSVæ–‡ä»¶è·¯å¾„ï¼ˆNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
            cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            delay: çˆ¬å–å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            token: å¯é€‰çš„è®¤è¯token
        """
        self.csv_file = csv_file or DEFAULT_CSV_FILE
        self.cache_file = cache_file
        self.delay = delay
        self.token = token
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(cache_file), exist_ok=True)
        
        # åˆå§‹åŒ–CSVè¯»å–å™¨
        self.csv_reader = CSVModelReader(csv_file=csv_file, delay=delay, token=token)
    
    def load_existing_cache(self) -> Dict[str, ModelInfo]:
        """
        åŠ è½½ç°æœ‰ç¼“å­˜
        
        Returns:
            å·²ç¼“å­˜çš„æ¨¡å‹å­—å…¸ {url: ModelInfo}
        """
        if not os.path.exists(self.cache_file):
            print("ğŸ“ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°ç¼“å­˜")
            return {}
        
        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            
            cached_models = {}
            for data in cached_data:
                model_info = ModelInfo.from_dict(data)
                cached_models[model_info.url] = model_info
            
            print(f"ğŸ“ åŠ è½½ç°æœ‰ç¼“å­˜: {len(cached_models)} ä¸ªæ¨¡å‹")
            return cached_models
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return {}
    
    def get_all_models_from_csv(self) -> List[ModelInfo]:
        """
        ä»CSVæ–‡ä»¶è·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹
        
        Returns:
            æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
        """
        print(f"ğŸ“– ä»CSVæ–‡ä»¶è¯»å–æ¨¡å‹: {self.csv_file}")
        
        # è¯»å–CSVæ•°æ®
        csv_models = self.csv_reader.read_csv_data(max_models=10000)  # è®¾ç½®ä¸€ä¸ªå¾ˆå¤§çš„æ•°å­—
        
        if not csv_models:
            print("âŒ æ— æ³•ä»CSVè·å–æ¨¡å‹æ•°æ®")
            return []
        
        # è½¬æ¢ä¸ºModelInfoå¯¹è±¡ï¼ˆåªè½¬æ¢åŸºæœ¬ä¿¡æ¯ï¼Œä¸çˆ¬å–è¯¦æƒ…ï¼‰
        model_infos = []
        for csv_model in csv_models:
            try:
                model_info = self.csv_reader.convert_csv_to_model_info(csv_model)
                model_infos.append(model_info)
            except Exception as e:
                print(f"âš ï¸ è½¬æ¢æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ“Š CSVä¸­ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹: {len(model_infos)} ä¸ª")
        return model_infos
    
    def filter_uncached_models(self, all_models: List[ModelInfo], 
                              cached_models: Dict[str, ModelInfo]) -> List[ModelInfo]:
        """
        è¿‡æ»¤å‡ºæœªç¼“å­˜çš„æ¨¡å‹
        
        Args:
            all_models: æ‰€æœ‰æ¨¡å‹åˆ—è¡¨
            cached_models: å·²ç¼“å­˜çš„æ¨¡å‹å­—å…¸
            
        Returns:
            æœªç¼“å­˜çš„æ¨¡å‹åˆ—è¡¨
        """
        uncached_models = []
        cached_urls = set(cached_models.keys())
        
        for model in all_models:
            if model.url not in cached_urls:
                uncached_models.append(model)
        
        print(f"ğŸ“‹ éœ€è¦çˆ¬å–çš„æ¨¡å‹: {len(uncached_models)} ä¸ª")
        print(f"ğŸ“ å·²ç¼“å­˜çš„æ¨¡å‹: {len(cached_models)} ä¸ª")
        
        return uncached_models
    
    def crawl_models_batch(self, models: List[ModelInfo], batch_size: int = 50, 
                          cached_models: Dict[str, ModelInfo] = None) -> List[ModelInfo]:
        """
        åˆ†æ‰¹çˆ¬å–æ¨¡å‹æ•°æ®ï¼Œæ¯çˆ¬å–ä¸€ä¸ªç«‹å³ä¿å­˜åˆ°ç¼“å­˜
        
        Args:
            models: è¦çˆ¬å–çš„æ¨¡å‹åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            cached_models: å·²ç¼“å­˜çš„æ¨¡å‹å­—å…¸ï¼Œç”¨äºå®æ—¶æ›´æ–°
            
        Returns:
            æˆåŠŸçˆ¬å–çš„æ¨¡å‹åˆ—è¡¨
        """
        if not models:
            print("âœ… æ²¡æœ‰éœ€è¦çˆ¬å–çš„æ¨¡å‹")
            return []
        
        print(f"ğŸš€ å¼€å§‹åˆ†æ‰¹çˆ¬å– {len(models)} ä¸ªæ¨¡å‹ (æ‰¹æ¬¡å¤§å°: {batch_size})")
        print("ğŸ’¾ å®æ—¶ä¿å­˜æ¨¡å¼ï¼šæ¯çˆ¬å–ä¸€ä¸ªæ¨¡å‹ç«‹å³ä¿å­˜åˆ°ç¼“å­˜")
        
        successful_models = []
        total_batches = (len(models) + batch_size - 1) // batch_size
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(models))
            batch_models = models[start_idx:end_idx]
            
            print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch_models)} ä¸ªæ¨¡å‹)")
            
            batch_successful = []
            for i, model in enumerate(tqdm(batch_models, desc=f"æ‰¹æ¬¡ {batch_idx + 1}")):
                try:
                    # ä½¿ç”¨çˆ¬è™«è·å–è¯¦ç»†ä¿¡æ¯
                    detailed_model = self.csv_reader.get_model_detail_from_scraper(model)
                    
                    if detailed_model.readme or detailed_model.tags:
                        batch_successful.append(detailed_model)
                        
                        # ç«‹å³ä¿å­˜åˆ°ç¼“å­˜
                        if cached_models is not None:
                            cached_models[detailed_model.url] = detailed_model
                            self.save_cache_immediate(cached_models)
                        
                        print(f"âœ… {model.project_name}: README={len(detailed_model.readme)}, æ ‡ç­¾={len(detailed_model.tags)} [å·²ä¿å­˜]")
                    else:
                        print(f"âš ï¸ {model.project_name}: çˆ¬å–å¤±è´¥ï¼Œè·³è¿‡")
                    
                    # æ·»åŠ å»¶è¿Ÿ
                    if i < len(batch_models) - 1:
                        time.sleep(self.delay)
                        
                except Exception as e:
                    print(f"âŒ {model.project_name}: çˆ¬å–å¼‚å¸¸ - {e}")
                    continue
            
            successful_models.extend(batch_successful)
            print(f"ğŸ“Š æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆ: {len(batch_successful)}/{len(batch_models)} æˆåŠŸ")
            
            # æ‰¹æ¬¡é—´ç¨é•¿å»¶è¿Ÿ
            if batch_idx < total_batches - 1:
                print("â¸ï¸ æ‰¹æ¬¡é—´ä¼‘æ¯ 2 ç§’...")
                time.sleep(2)
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆ: {len(successful_models)}/{len(models)} ä¸ªæ¨¡å‹æˆåŠŸ")
        return successful_models
    
    def save_cache_immediate(self, cached_models: Dict[str, ModelInfo]):
        """
        ç«‹å³ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶ï¼ˆå®æ—¶ä¿å­˜ï¼‰
        
        Args:
            cached_models: å·²ç¼“å­˜çš„æ¨¡å‹å­—å…¸
        """
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        model_dicts = [model.to_dict() for model in cached_models.values()]
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(model_dicts, f, ensure_ascii=False, indent=2)
            
            # ä¸æ‰“å°å¤ªå¤šä¿¡æ¯ï¼Œé¿å…åˆ·å±
            # print(f"ğŸ’¾ å®æ—¶ä¿å­˜: {len(model_dicts)} ä¸ªæ¨¡å‹")
            
        except Exception as e:
            print(f"âŒ å®æ—¶ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def save_cache(self, cached_models: Dict[str, ModelInfo], new_models: List[ModelInfo]):
        """
        ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶ï¼ˆæ‰¹é‡ä¿å­˜ï¼Œå…¼å®¹æ—§æ¥å£ï¼‰
        
        Args:
            cached_models: å·²ç¼“å­˜çš„æ¨¡å‹å­—å…¸
            new_models: æ–°çˆ¬å–çš„æ¨¡å‹åˆ—è¡¨
        """
        # åˆå¹¶æ–°æ—§æ¨¡å‹
        for model in new_models:
            cached_models[model.url] = model
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        model_dicts = [model.to_dict() for model in cached_models.values()]
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(model_dicts, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {len(model_dicts)} ä¸ªæ¨¡å‹ -> {self.cache_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def run(self, max_models: int = None, batch_size: int = 50, force_crawl: bool = False):
        """
        è¿è¡Œé¢„çˆ¬å–æµç¨‹
        
        Args:
            max_models: æœ€å¤§æ¨¡å‹æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°
            force_crawl: æ˜¯å¦å¼ºåˆ¶é‡æ–°çˆ¬å–
        """
        start_time = time.time()
        
        print("=" * 60)
        print("ğŸš€ æ¨¡å‹æ•°æ®é¢„çˆ¬å–ç³»ç»Ÿ")
        print("=" * 60)
        print(f"ğŸ“ CSVæ–‡ä»¶: {self.csv_file}")
        print(f"ğŸ’¾ ç¼“å­˜æ–‡ä»¶: {self.cache_file}")
        print(f"â±ï¸ çˆ¬å–å»¶è¿Ÿ: {self.delay}ç§’")
        print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        print("=" * 60)
        
        try:
            # 1. åŠ è½½ç°æœ‰ç¼“å­˜
            cached_models = {} if force_crawl else self.load_existing_cache()
            
            # 2. è·å–æ‰€æœ‰æ¨¡å‹
            all_models = self.get_all_models_from_csv()
            if not all_models:
                print("âŒ æ— æ³•è·å–æ¨¡å‹æ•°æ®")
                return
            
            # 3. é™åˆ¶æ¨¡å‹æ•°é‡
            if max_models and max_models < len(all_models):
                all_models = all_models[:max_models]
                print(f"ğŸ“Š é™åˆ¶å¤„ç†æ•°é‡: {len(all_models)} ä¸ªæ¨¡å‹")
            
            # 4. è¿‡æ»¤æœªç¼“å­˜çš„æ¨¡å‹
            uncached_models = self.filter_uncached_models(all_models, cached_models)
            
            if not uncached_models:
                print("âœ… æ‰€æœ‰æ¨¡å‹éƒ½å·²ç¼“å­˜ï¼Œæ— éœ€çˆ¬å–")
                return
            
            # 5. åˆ†æ‰¹çˆ¬å–ï¼ˆå®æ—¶ä¿å­˜æ¨¡å¼ï¼‰
            new_models = self.crawl_models_batch(uncached_models, batch_size, cached_models)
            
            # 6. æœ€ç»ˆä¿å­˜ç¼“å­˜ï¼ˆç¡®ä¿æ•°æ®å®Œæ•´æ€§ï¼‰
            if new_models:
                self.save_cache(cached_models, [])  # ä¼ å…¥ç©ºåˆ—è¡¨ï¼Œå› ä¸ºå·²ç»å®æ—¶ä¿å­˜äº†
            
            # 7. ç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            total_time = end_time - start_time
            
            print("\n" + "=" * 60)
            print("ğŸ“Š çˆ¬å–ç»Ÿè®¡")
            print("=" * 60)
            print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"ğŸ“Š å¤„ç†æ¨¡å‹: {len(all_models)} ä¸ª")
            print(f"ğŸ†• æ–°çˆ¬å–: {len(new_models)} ä¸ª")
            print(f"ğŸ“ ç¼“å­˜æ€»æ•°: {len(cached_models) + len(new_models)} ä¸ª")
            print(f"âš¡ å¹³å‡é€Ÿåº¦: {len(new_models)/total_time:.2f} ä¸ª/ç§’")
            print("=" * 60)
            
        except Exception as e:
            print(f"âŒ é¢„çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é¢„çˆ¬å–æ¨¡å‹ç½‘é¡µæ•°æ®")
    parser.add_argument("--csv-file", default=DEFAULT_CSV_FILE, help="CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--cache-file", default="output/models_cache.json", help="ç¼“å­˜æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max-models", type=int, help="æœ€å¤§æ¨¡å‹æ•°é‡")
    parser.add_argument("--batch-size", type=int, default=50, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--delay", type=float, default=0.5, help="çˆ¬å–å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--force-crawl", action="store_true", help="å¼ºåˆ¶é‡æ–°çˆ¬å–æ‰€æœ‰æ¨¡å‹")
    parser.add_argument("--token", help="å¯é€‰çš„è®¤è¯token")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé¢„çˆ¬å–å™¨
    crawler = PreCrawler(
        csv_file=args.csv_file,
        cache_file=args.cache_file,
        delay=args.delay,
        token=args.token
    )
    
    # è¿è¡Œé¢„çˆ¬å–
    crawler.run(
        max_models=args.max_models,
        batch_size=args.batch_size,
        force_crawl=args.force_crawl
    )


if __name__ == "__main__":
    main()
