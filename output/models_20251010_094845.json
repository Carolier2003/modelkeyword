[
  {
    "url": "https://gitcode.com/hf_mirrors/internlm/Intern-S1-FP8",
    "project_name": "internlm/Intern-S1-FP8",
    "readme": "\n\nIntern-S1\n\n\nÂ \nğŸ’»Github ä»“åº“ â€¢ ğŸ¤—æ¨¡å‹é›†åˆ â€¢ ğŸ“œæŠ€æœ¯æŠ¥å‘Šï¼ˆå³å°†å‘å¸ƒï¼‰ â€¢ ğŸ’¬åœ¨çº¿äº¤æµ\n\n\n    ğŸ‘‹ æ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„ Discord å’Œ å¾®ä¿¡ ç¤¾åŒº\n\n\n\nç®€ä»‹\næˆ‘ä»¬éš†é‡æ¨å‡º Intern-S1â€”â€”è¿„ä»Šä¸ºæ­¢æœ€å…ˆè¿›çš„å¼€æºå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚Intern-S1 å…¼å…·å¼ºå¤§çš„é€šç”¨ä»»åŠ¡èƒ½åŠ›ä¸é¡¶å°–çš„ç§‘å­¦ä»»åŠ¡æ€§èƒ½ï¼Œè¶³ä»¥åª²ç¾é¢†å…ˆçš„é—­æºå•†ä¸šæ¨¡å‹ã€‚\nåŸºäº 235B MoE è¯­è¨€æ¨¡å‹ï¼ˆQwen3ï¼‰å’Œ 6B è§†è§‰ç¼–ç å™¨ï¼ˆInternViTï¼‰æ„å»ºï¼ŒIntern-S1 ç»è¿‡5ä¸‡äº¿ token çš„å¤šæ¨¡æ€æ•°æ®ç»§ç»­é¢„è®­ç»ƒï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡2.5ä¸‡äº¿ç§‘å­¦é¢†åŸŸ tokenã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨ä¿æŒå¼ºå¤§é€šç”¨èƒ½åŠ›çš„åŒæ—¶ï¼Œåœ¨åŒ–å­¦ç»“æ„è§£æã€è›‹ç™½è´¨åºåˆ—ç†è§£ã€åŒ–åˆç‰©åˆæˆè·¯çº¿è§„åˆ’ç­‰ä¸“ä¸šç§‘å­¦é¢†åŸŸè¡¨ç°å“è¶Šï¼Œæˆä¸ºç°å®ç§‘ç ”åº”ç”¨ä¸­å¾—åŠ›çš„ç ”ç©¶åŠ©æ‰‹ã€‚\n\n\nç‰¹æ€§\n\nåœ¨è¯­è¨€ä¸è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å¼ºåŠ²ï¼Œå°¤å…¶åœ¨ç§‘å­¦ä»»åŠ¡ä¸Šä¼˜åŠ¿æ˜¾è‘—\nåŸºäº 5T token å¤§è§„æ¨¡æ•°æ®é›†æŒç»­é¢„è®­ç»ƒï¼Œå…¶ä¸­è¶… 50% ä¸ºä¸“ä¸šç§‘å­¦æ•°æ®ï¼Œæ·±åº¦èåˆé¢†åŸŸçŸ¥è¯†\nåŠ¨æ€åˆ†è¯å™¨åŸç”Ÿæ”¯æŒåˆ†å­å¼ã€è›‹ç™½è´¨åºåˆ—å’Œåœ°éœ‡ä¿¡å·çš„ç†è§£\n\n\n\næ€§èƒ½è¡¨ç°\næˆ‘ä»¬åœ¨é€šç”¨æ•°æ®é›†å’Œç§‘å­¦æ•°æ®é›†ä¸Šå¯¹ Intern-S1 è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ä»¥ä¸‹ä¸ºä¸è¿‘æœŸä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åŠå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½å¯¹æ¯”ï¼š\n\n\n\nåŸºå‡†æµ‹è¯•\nIntern-S1\nInternVL3-78B\nQwen2.5-VL-72B\nDS-R1-0528\nQwen3-235B-A22B\nKimi-K2-Instruct\nGemini-2.5 Pro\no3\nGrok-4\n\n\n\nMMLU-Pro83.5 âœ…73.072.183.482.282.786.085.085.9\nMMMU77.7 âœ…72.270.2---81.980.877.9\nGPQA77.349.949.080.671.177.883.883.387.5\nMMStar74.9 âœ…72.570.8---79.375.169.6\nMathVista81.5 ğŸ‘‘79.074.8---80.377.572.5\nAIME202586.010.710.987.581.551.483.088.991.7\nMathVision62.5 âœ…43.138.1---73.067.767.3\nIFEval86.775.683.979.785.090.291.592.292.8\nSFE44.3 ğŸ‘‘36.230.5---43.037.731.2\nPhysics44.0 âœ…23.115.7---40.047.942.8\nSmolInstruct51.0 ğŸ‘‘19.421.030.728.748.140.443.947.3\nChemBench83.4 ğŸ‘‘61.361.675.675.875.382.881.683.3\nMatBench75.0 ğŸ‘‘49.351.557.752.161.761.761.667.9\nMicroVQA63.9 ğŸ‘‘59.153.0---63.158.359.5\nProteinLMBench63.161.661.061.459.866.762.967.766.2\nMSEarthMCQ65.7 ğŸ‘‘57.237.6---59.961.058.0\nXLRS-Bench55.0 ğŸ‘‘49.350.9---45.243.645.4\n\n\n\nè¯´æ˜ï¼šâœ… ä»£è¡¨å¼€æºæ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½ï¼ŒğŸ‘‘ ä»£è¡¨æ‰€æœ‰æ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½\n\næˆ‘ä»¬ä½¿ç”¨ OpenCompass å’Œ VLMEvalkit å¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚\n\n\nå¿«é€Ÿå¼€å§‹\n\n\né‡‡æ ·å‚æ•°\næˆ‘ä»¬æ¨èä½¿ç”¨ä»¥ä¸‹è¶…å‚æ•°ä»¥ç¡®ä¿è·å¾—æ›´ä½³ç»“æœ\ntop_p = 1.0\ntop_k = 50\nmin_p = 0.0\ntemperature = 0.7\n\n\n\nTransformers\nä»¥ä¸‹æä¾›ç¤ºä¾‹ä»£ç ï¼Œå±•ç¤ºå¦‚ä½•åŸºäºæ–‡æœ¬å’Œå¤šæ¨¡æ€è¾“å…¥è¿›è¡Œç”Ÿæˆã€‚\n\nè¯·ä½¿ç”¨ transformers>=4.53.0 ç‰ˆæœ¬ä»¥ç¡®ä¿æ¨¡å‹æ­£å¸¸è¿è¡Œã€‚\n\n\n\næ–‡æœ¬è¾“å…¥\nfrom transformers import AutoProcessor, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"internlm/Intern-S1\"\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"tell me about an interesting physical phenomenon.\"},\n        ],\n    }\n]\n\ninputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n\ngenerate_ids = model.generate(**inputs, max_new_tokens=32768)\ndecoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\nprint(decoded_output)\n\n\n\nå›¾åƒè¾“å…¥\nfrom transformers import AutoProcessor, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"internlm/Intern-S1\"\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n            {\"type\": \"text\", \"text\": \"Please describe the image explicitly.\"},\n        ],\n    }\n]\n\ninputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n\ngenerate_ids = model.generate(**inputs, max_new_tokens=32768)\ndecoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\nprint(decoded_output)\n\n\n\nè§†é¢‘è¾“å…¥\nè¯·ç¡®ä¿å·²é€šè¿‡ pip install decord å®‰è£… decord è§†é¢‘è§£ç åº“ã€‚\nfrom transformers import AutoProcessor, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"internlm/Intern-S1\"\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\n\nmessages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"video\",\n                    \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\",\n                },\n                {\"type\": \"text\", \"text\": \"What type of shot is the man performing?\"},\n            ],\n        }\n    ]\n\ninputs = processor.apply_chat_template(\n        messages,\n        return_tensors=\"pt\",\n        add_generation_prompt=True,\n        video_load_backend=\"decord\",\n        tokenize=True,\n        return_dict=True,\n    ).to(model.device, dtype=torch.float16)\n\ngenerate_ids = model.generate(**inputs, max_new_tokens=32768)\ndecoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\nprint(decoded_output)\n\n\n\néƒ¨ç½²æœåŠ¡\néƒ¨ç½² Intern-S1 ç³»åˆ—æ¨¡å‹çš„æœ€ä½ç¡¬ä»¶è¦æ±‚å¦‚ä¸‹ï¼š\n\n\n\næ¨¡å‹\nA100(GPUs)\nH800(GPUs)\nH100(GPUs)\nH200(GPUs)\n\n\n\n\ninternlm/Intern-S1\n8\n8\n8\n4\n\n\ninternlm/Intern-S1-FP8\n-\n4\n4\n2\n\n\n\næ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»»æ„ä¸€ä¸ª LLM æ¨ç†æ¡†æ¶æ¥åˆ›å»ºå…¼å®¹ OpenAI çš„æœåŠ¡å™¨ï¼š\n\n\nlmdeploy(>=0.9.2)\nlmdeploy serve api_server internlm/Intern-S1-FP8 --reasoning-parser intern-s1 --tool-call-parser intern-s1 --tp 4\n\n\n\nvllm\nvllm serve internlm/Intern-S1-FP8 --tensor-parallel-size 4 --trust-remote-code\n\n\n\nsglang\nå¯¹ Intern-S1 çš„ SGLang æ”¯æŒä»åœ¨å¼€å‘ä¸­ã€‚è¯·å‚è€ƒæ­¤ PRã€‚\nCUDA_VISIBLE_DEVICES=0,1,2,3 \\\n    python3 -m sglang.launch_server \\\n    --model-path internlm/Intern-S1-FP8 \\\n    --trust-remote-code \\\n    --tp 4 \\\n    --port 8001 \\\n    --mem-fraction-static 0.85 \\\n    --enable-multimodal \\\n    --grammar-backend none\n\n\n\né«˜çº§ç”¨æ³•\n\n\nå·¥å…·è°ƒç”¨\nå½“å‰è®¸å¤šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éƒ½å…·å¤‡å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œè¿™é¡¹å¼ºå¤§åŠŸèƒ½ä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ä¸å¤–éƒ¨å·¥å…·åŠAPIäº¤äº’æ¥æ‰©å±•å…¶åŠŸèƒ½ã€‚å€ŸåŠ©è¯¥èƒ½åŠ›ï¼Œæ¨¡å‹å¯æ‰§è¡Œè¯¸å¦‚è·å–å®æ—¶ä¿¡æ¯ã€è¿è¡Œä»£ç æˆ–è°ƒç”¨å…¶ä»–åº”ç”¨ç¨‹åºä¸­çš„å‡½æ•°ç­‰ä»»åŠ¡ã€‚\nå¯¹å¼€å‘è€…è€Œè¨€ï¼Œä¸€ä¸ªå…³é”®ä¼˜åŠ¿åœ¨äºè¶Šæ¥è¶Šå¤šçš„å¼€æºLLMsè¢«è®¾è®¡ä¸ºä¸OpenAI APIå…¼å®¹ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥ä½¿ç”¨OpenAIåº“ä¸­ç†Ÿæ‚‰çš„è¯­æ³•å’Œç»“æ„ï¼Œåœ¨è¿™äº›å¼€æºæ¨¡å‹ä¸Šå®ç°å·¥å…·è°ƒç”¨åŠŸèƒ½ã€‚å› æ­¤ï¼Œæœ¬æ•™ç¨‹æ¼”ç¤ºçš„ä»£ç å…·æœ‰é€šç”¨æ€§â€”â€”å®ƒä¸ä»…é€‚ç”¨äºOpenAIæ¨¡å‹ï¼Œä¹Ÿå…¼å®¹éµå¾ªç›¸åŒæ¥å£æ ‡å‡†çš„ä»»ä½•æ¨¡å‹ã€‚\nä¸ºå…·ä½“è¯´æ˜å…¶å®ç°æ–¹å¼ï¼Œè®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®é™…ä»£ç ç¤ºä¾‹æ¥æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å·¥å…·è°ƒç”¨è·å–æœ€æ–°å¤©æ°”é¢„æŠ¥ï¼ˆåŸºäºlmdeploy api serverï¼‰ã€‚\n      \nfrom openai import OpenAI\nimport json\n\n\ndef get_current_temperature(location: str, unit: str = \"celsius\"):\n    \"\"\"Get current temperature at a location.\n\n    Args:\n        location: The location to get the temperature for, in the format \"City, State, Country\".\n        unit: The unit to return the temperature in. Defaults to \"celsius\". (choices: [\"celsius\", \"fahrenheit\"])\n\n    Returns:\n        the temperature, the location, and the unit in a dict\n    \"\"\"\n    return {\n        \"temperature\": 26.1,\n        \"location\": location,\n        \"unit\": unit,\n    }\n\n\ndef get_temperature_date(location: str, date: str, unit: str = \"celsius\"):\n    \"\"\"Get temperature at a location and date.\n\n    Args:\n        location: The location to get the temperature for, in the format \"City, State, Country\".\n        date: The date to get the temperature for, in the format \"Year-Month-Day\".\n        unit: The unit to return the temperature in. Defaults to \"celsius\". (choices: [\"celsius\", \"fahrenheit\"])\n\n    Returns:\n        the temperature, the location, the date and the unit in a dict\n    \"\"\"\n    return {\n        \"temperature\": 25.9,\n        \"location\": location,\n        \"date\": date,\n        \"unit\": unit,\n    }\n\ndef get_function_by_name(name):\n    if name == \"get_current_temperature\":\n        return get_current_temperature\n    if name == \"get_temperature_date\":\n        return get_temperature_date\n\ntools = [{\n    'type': 'function',\n    'function': {\n        'name': 'get_current_temperature',\n        'description': 'Get current temperature at a location.',\n        'parameters': {\n            'type': 'object',\n            'properties': {\n                'location': {\n                    'type': 'string',\n                    'description': 'The location to get the temperature for, in the format \\'City, State, Country\\'.'\n                },\n                'unit': {\n                    'type': 'string',\n                    'enum': [\n                        'celsius',\n                        'fahrenheit'\n                    ],\n                    'description': 'The unit to return the temperature in. Defaults to \\'celsius\\'.'\n                }\n            },\n            'required': [\n                'location'\n            ]\n        }\n    }\n}, {\n    'type': 'function',\n    'function': {\n        'name': 'get_temperature_date',\n        'description': 'Get temperature at a location and date.',\n        'parameters': {\n            'type': 'object',\n            'properties': {\n                'location': {\n                    'type': 'string',\n                    'description': 'The location to get the temperature for, in the format \\'City, State, Country\\'.'\n                },\n                'date': {\n                    'type': 'string',\n                    'description': 'The date to get the temperature for, in the format \\'Year-Month-Day\\'.'\n                },\n                'unit': {\n                    'type': 'string',\n                    'enum': [\n                        'celsius',\n                        'fahrenheit'\n                    ],\n                    'description': 'The unit to return the temperature in. Defaults to \\'celsius\\'.'\n                }\n            },\n            'required': [\n                'location',\n                'date'\n            ]\n        }\n    }\n}]\n\n\n\nmessages = [\n    {'role': 'user', 'content': 'Today is 2024-11-14, What\\'s the temperature in San Francisco now? How about tomorrow?'}\n]\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://0.0.0.0:23333/v1\"\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\nmodel_name = client.models.list().data[0].id\nresponse = client.chat.completions.create(\n    model=model_name,\n    messages=messages,\n    max_tokens=32768,\n    temperature=0.8,\n    top_p=0.8,\n    stream=False,\n    extra_body=dict(spaces_between_special_tokens=False, enable_thinking=False),\n    tools=tools)\nprint(response.choices[0].message)\nmessages.append(response.choices[0].message)\n\nfor tool_call in response.choices[0].message.tool_calls:\n    tool_call_args = json.loads(tool_call.function.arguments)\n    tool_call_result = get_function_by_name(tool_call.function.name)(**tool_call_args)\n    tool_call_result = json.dumps(tool_call_result, ensure_ascii=False)\n    messages.append({\n        'role': 'tool',\n        'name': tool_call.function.name,\n        'content': tool_call_result,\n        'tool_call_id': tool_call.id\n    })\n\nresponse = client.chat.completions.create(\n    model=model_name,\n    messages=messages,\n    temperature=0.8,\n    top_p=0.8,\n    stream=False,\n    extra_body=dict(spaces_between_special_tokens=False, enable_thinking=False),\n    tools=tools)\nprint(response.choices[0].message.content)\n\n\n\næ€è€ƒæ¨¡å¼ä¸éæ€è€ƒæ¨¡å¼åˆ‡æ¢\nIntern-S1 é»˜è®¤å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œé€šè¿‡å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ¥ç”Ÿæˆæ›´é«˜è´¨é‡çš„å›å¤ã€‚å¦‚éœ€ç¦ç”¨æ­¤åŠŸèƒ½ï¼Œå¯åœ¨ tokenizer.apply_chat_template ä¸­è®¾ç½® enable_thinking=Falseã€‚\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # think mode indicator\n)\n\nä½¿ç”¨LMDeployæœåŠ¡Intern-S1æ¨¡å‹æ—¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡åœ¨è¯·æ±‚ä¸­è°ƒæ•´enable_thinkingå‚æ•°æ¥åŠ¨æ€æ§åˆ¶æ€è€ƒæ¨¡å¼ã€‚\nfrom openai import OpenAI\nimport json\n\nmessages = [\n{\n    'role': 'user',\n    'content': 'who are you'\n}, {\n    'role': 'assistant',\n    'content': 'I am an AI'\n}, {\n    'role': 'user',\n    'content': 'AGI is?'\n}]\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://0.0.0.0:23333/v1\"\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\nmodel_name = client.models.list().data[0].id\n\nresponse = client.chat.completions.create(\n    model=model_name,\n    messages=messages,\n    temperature=0.7,\n    top_p=0.8,\n    max_tokens=2048,\n    extra_body={\n        \"enable_thinking\": False,\n    }\n)\nprint(json.dumps(response.model_dump(), indent=2, ensure_ascii=False))\n\nå¯¹äº vllm å’Œ sglang ç”¨æˆ·ï¼Œå¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›è¡Œé…ç½®ï¼š\nextra_body={\n    \"chat_template_kwargs\": {\"enable_thinking\": False}\n}\n\n",
    "tags": [
      "Image-Text-to-Text",
      "Transformers",
      "Safetensors",
      "Apache License 2.0",
      "arxiv:2508.1576"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/nunchaku-tech/nunchaku-flux.1-krea-dev",
    "project_name": "nunchaku-tech/nunchaku-flux.1-krea-dev",
    "readme": "\n\n\n\n\nnunchaku-flux.1-krea-dev æ¨¡å‹å¡ç‰‡\n\næœ¬ä»“åº“åŒ…å« FLUX.1-Krea-dev çš„ Nunchaku é‡åŒ–ç‰ˆæœ¬ï¼Œæ—¨åœ¨é€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚è¯¥æ¨¡å‹ç»è¿‡ä¼˜åŒ–ï¼Œå¯å®ç°é«˜æ•ˆæ¨ç†ï¼ŒåŒæ—¶å°†æ€§èƒ½æŸå¤±é™è‡³æœ€ä½ã€‚\n\n\næ¨¡å‹è¯¦æƒ…\n\n\næ¨¡å‹æè¿°\n\nå¼€å‘è€…ï¼š Nunchaku å›¢é˜Ÿ\næ¨¡å‹ç±»å‹ï¼š æ–‡æœ¬åˆ°å›¾åƒ\nè®¸å¯è¯ï¼š flux-1-krea-dev-non-commercial-license\né‡åŒ–æºæ¨¡å‹ï¼š FLUX.1-Krea-dev\n\n\n\næ¨¡å‹æ–‡ä»¶\n\nsvdq-int4_r32-flux.1-krea-dev.safetensorsï¼šSVDQuant é‡åŒ–çš„ INT4 FLUX.1-Krea-dev æ¨¡å‹ã€‚é€‚ç”¨äºé Blackwell GPUï¼ˆ50 ç³»åˆ—ä¹‹å‰ï¼‰ç”¨æˆ·ã€‚\nsvdq-fp4_r32-flux.1-krea-dev.safetensorsï¼šSVDQuant é‡åŒ–çš„ NVFP4 FLUX.1-Krea-dev æ¨¡å‹ã€‚é€‚ç”¨äº Blackwell GPUï¼ˆ50 ç³»åˆ—ï¼‰ç”¨æˆ·ã€‚\n\n\n\næ¨¡å‹æ¥æº\n\næ¨ç†å¼•æ“ï¼š nunchaku\né‡åŒ–åº“ï¼š deepcompressor\nè®ºæ–‡ï¼š SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models\næ¼”ç¤ºï¼š svdquant.mit.edu\n\n\n\nä½¿ç”¨æ–¹æ³•\n\nDiffusers ä½¿ç”¨ï¼šå‚è§ flux.1-krea-dev.pyã€‚åªéœ€å°† safetensors æ–‡ä»¶æ›¿æ¢ä¸ºæœ¬ä»“åº“ä¸­çš„æ–‡ä»¶å³å¯ã€‚æœ‰å…³æ›´é«˜çº§çš„ç”¨æ³•ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ æ•™ç¨‹ã€‚\nComfyUI ä½¿ç”¨ï¼šå‚è§ nunchaku-flux.1-dev.jsonã€‚åªéœ€å°† safetensors æ–‡ä»¶æ›¿æ¢ä¸ºæœ¬ä»“åº“ä¸­çš„æ–‡ä»¶å³å¯ã€‚\n\n\n\næ€§èƒ½\n\n\n\nå¼•ç”¨\n@inproceedings{\n  li2024svdquant,\n  title={SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models},\n  author={Li*, Muyang and Lin*, Yujun and Zhang*, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},\n  booktitle={The Thirteenth International Conference on Learning Representations},\n  year={2025}\n}\n\n\n\nå½’å±å£°æ˜\nFLUX.1 [dev] æ¨¡å‹ç”± Black Forest Labs Inc. æ ¹æ® FLUX.1 [dev] éå•†ä¸šè®¸å¯åè®®æˆæƒã€‚ç‰ˆæƒæ‰€æœ‰ Black Forest Labs Inc.ã€‚åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œå¯¹äºå› ä½¿ç”¨æœ¬æ¨¡å‹è€Œå¼•èµ·çš„ã€ä¸ä¹‹ç›¸å…³çš„æˆ–ç”±æ­¤äº§ç”Ÿçš„ä»»ä½•ç´¢èµ”ã€æŸå®³æˆ–å…¶ä»–è´£ä»»ï¼Œæ— è®ºæ˜¯åŸºäºåˆåŒã€ä¾µæƒè¡Œä¸ºè¿˜æ˜¯å…¶ä»–åŸå› ï¼ŒBlack Forest Labs Inc. å‡ä¸æ‰¿æ‹…è´£ä»»ã€‚\n",
    "tags": [
      "Text-to-Image",
      "Diffusers",
      "Safetensors",
      "English",
      "Other",
      "mit-han-lab/svdquant-datasets",
      "Quantization",
      "FLUX.1",
      "Diffusion",
      "SVDQuant",
      "FLUX.1-Krea-dev",
      "ICLR2025",
      "arxiv:2411.05007",
      "arxiv:2411.0500"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/zai-org/GLM-4.5V",
    "project_name": "zai-org/GLM-4.5V",
    "readme": "\n\nGLM-4.5V\n\n\n\næœ¬æ¨¡å‹æ˜¯ GLM-V æ¨¡å‹ç³»åˆ—çš„ä¸€éƒ¨åˆ†ï¼Œç›¸å…³è®ºæ–‡ä¸º GLM-4.1V-Thinking and GLM-4.5V: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learningã€‚\n\nè®ºæ–‡ï¼šhttps://huggingface.co/papers/2507.01006\nGitHub ä»“åº“ï¼šhttps://github.com/zai-org/GLM-V/\nåœ¨çº¿æ¼”ç¤ºï¼šhttps://chat.z.ai/\nAPI è®¿é—®ï¼šæ™ºè°±AIå¼€æ”¾å¹³å°\næ¡Œé¢åŠ©æ‰‹åº”ç”¨ï¼šhttps://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App\nDiscord ç¤¾åŒºï¼šhttps://discord.com/invite/8cnQKdAprg\n\n\n\nç®€ä»‹ä¸æ¨¡å‹æ¦‚è¿°\nè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²æˆä¸ºæ™ºèƒ½ç³»ç»Ÿçš„æ ¸å¿ƒåŸºçŸ³ã€‚éšç€ç°å®ä¸–ç•ŒAIä»»åŠ¡æ—¥ç›Šå¤æ‚ï¼ŒVLMsè¿«åˆ‡éœ€è¦åœ¨åŸºç¡€å¤šæ¨¡æ€æ„ŸçŸ¥ä¹‹å¤–å¢å¼ºæ¨ç†èƒ½åŠ›â€”â€”æå‡å‡†ç¡®æ€§ã€å…¨é¢æ€§å’Œæ™ºèƒ½æ°´å¹³â€”â€”ä»¥å®ç°å¤æ‚é—®é¢˜è§£å†³ã€é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œå¤šæ¨¡æ€æ™ºèƒ½ä½“åŠŸèƒ½ã€‚\né€šè¿‡å¼€æºå·¥ä½œï¼Œæˆ‘ä»¬æ—¨åœ¨ä¸ç¤¾åŒºå…±åŒæ¢ç´¢æŠ€æœ¯å‰æ²¿ï¼ŒåŒæ—¶èµ‹èƒ½æ›´å¤šå¼€å‘è€…åˆ›å»ºä»¤äººå…´å¥‹çš„åˆ›æ–°åº”ç”¨ã€‚\næœ¬ Hugging Face ä»“åº“æ‰˜ç®¡ GLM-4.5V æ¨¡å‹ï¼Œå®ƒæ˜¯ GLM-V ç³»åˆ—çš„ä¸€éƒ¨åˆ†ã€‚\n\n\nGLM-4.5V\nGLM-4.5V åŸºäºæ™ºè°±AIä¸‹ä¸€ä»£æ——èˆ°æ–‡æœ¬åŸºç¡€æ¨¡å‹ GLM-4.5-Airï¼ˆ1060äº¿å‚æ•°ï¼Œ120äº¿æ¿€æ´»å‚æ•°ï¼‰æ„å»ºã€‚å®ƒå»¶ç»­äº† GLM-4.1V-Thinking çš„æŠ€æœ¯è·¯çº¿ï¼Œåœ¨42é¡¹å…¬å¼€è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†åŒè§„æ¨¡æ¨¡å‹çš„SOTAæ€§èƒ½ã€‚å…¶èƒ½åŠ›è¦†ç›–å›¾åƒã€è§†é¢‘ã€æ–‡æ¡£ç†è§£ç­‰å¸¸è§ä»»åŠ¡ï¼Œä»¥åŠGUIæ™ºèƒ½ä½“æ“ä½œã€‚\n\né™¤åŸºå‡†æµ‹è¯•æ€§èƒ½å¤–ï¼ŒGLM-4.5V æ³¨é‡å®é™…åœºæ™¯å¯ç”¨æ€§ã€‚é€šè¿‡é«˜æ•ˆæ··åˆè®­ç»ƒï¼Œå®ƒèƒ½å¤Ÿå¤„ç†å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ï¼Œå®ç°å…¨è°±ç³»è§†è§‰æ¨ç†ï¼ŒåŒ…æ‹¬ï¼š\n\nå›¾åƒæ¨ç†ï¼ˆåœºæ™¯ç†è§£ã€å¤æ‚å¤šå›¾åˆ†æã€ç©ºé—´è¯†åˆ«ï¼‰\nè§†é¢‘ç†è§£ï¼ˆé•¿è§†é¢‘åˆ†å‰²ä¸äº‹ä»¶è¯†åˆ«ï¼‰\nGUIä»»åŠ¡ï¼ˆå±å¹•å†…å®¹è¯»å–ã€å›¾æ ‡è¯†åˆ«ã€æ¡Œé¢æ“ä½œè¾…åŠ©ï¼‰\nå¤æ‚å›¾è¡¨ä¸é•¿æ–‡æ¡£è§£æï¼ˆç ”ç©¶æŠ¥å‘Šåˆ†æã€ä¿¡æ¯æå–ï¼‰\nè§†è§‰å®šä½ï¼ˆç²¾ç¡®è§†è§‰å…ƒç´ å®šä½ï¼‰\n\nè¯¥æ¨¡å‹è¿˜å¼•å…¥äº†æ€è€ƒæ¨¡å¼å¼€å…³ï¼Œå…è®¸ç”¨æˆ·åœ¨å¿«é€Ÿå“åº”å’Œæ·±åº¦æ¨ç†ä¹‹é—´è¿›è¡Œå¹³è¡¡ã€‚æ­¤å¼€å…³çš„åŠŸèƒ½ä¸ GLM-4.5 è¯­è¨€æ¨¡å‹ä¸­çš„ä¿æŒä¸€è‡´ã€‚\n\n\nGLM-4.1V-9B\næä¾›æœ‰å…³ GLM-4.1V-9B çš„èƒŒæ™¯ä¿¡æ¯æ˜¯ä¸ºäº†å†…å®¹çš„å®Œæ•´æ€§ï¼Œå› ä¸ºå®ƒæ˜¯ GLM-V ç³»åˆ—çš„ä¸€éƒ¨åˆ†ï¼Œä¹Ÿæ˜¯ GLM-4.5V å¼€å‘çš„åŸºç¡€ã€‚\nGLM-4.1V-9B-Thinking æ¨¡å‹åŸºäº GLM-4-9B-0414 åŸºç¡€æ¨¡å‹æ„å»ºï¼Œå¼•å…¥äº†æ¨ç†èŒƒå¼ï¼Œå¹¶é‡‡ç”¨ RLCSï¼ˆè¯¾ç¨‹é‡‡æ ·å¼ºåŒ–å­¦ä¹ ï¼‰å…¨é¢å¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚å®ƒåœ¨ 100 äº¿å‚æ•°çº§åˆ«çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­æ€§èƒ½æœ€å¼ºï¼Œåœ¨ 18 é¡¹åŸºå‡†ä»»åŠ¡ä¸­è¾¾åˆ°æˆ–è¶…è¶Šäº†è§„æ¨¡å¤§å¾—å¤šçš„ Qwen-2.5-VL-72Bã€‚\næˆ‘ä»¬è¿˜å¼€æºäº†åŸºç¡€æ¨¡å‹ GLM-4.1V-9B-Baseï¼Œä»¥æ”¯æŒç ”ç©¶äººå‘˜æ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æé™ã€‚\n\nä¸ä¸Šä¸€ä»£ CogVLM2 å’Œ GLM-4V ç³»åˆ—ç›¸æ¯”ï¼ŒGLM-4.1V-Thinking å¸¦æ¥äº†ä»¥ä¸‹ç‰¹æ€§ï¼š\n\nç³»åˆ—ä¸­é¦–ä¸ªä¸“æ³¨äºæ¨ç†çš„æ¨¡å‹ï¼Œåœ¨æ•°å­¦ä¹‹å¤–çš„å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚\næ”¯æŒ 64k ä¸Šä¸‹æ–‡é•¿åº¦ã€‚\næ”¯æŒ ä»»æ„å®½é«˜æ¯” ä»¥åŠæœ€é«˜ 4k å›¾åƒåˆ†è¾¨ç‡ã€‚\nåŒè¯­ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰å¼€æºç‰ˆæœ¬ã€‚\n\nGLM-4.1V-9B-Thinking é›†æˆäº† æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ æ¨ç†æœºåˆ¶ï¼Œæå‡äº†å‡†ç¡®æ€§ã€ä¸°å¯Œåº¦å’Œå¯è§£é‡Šæ€§ã€‚åœ¨ 100 äº¿å‚æ•°è§„æ¨¡ä¸‹ï¼Œå®ƒåœ¨ 28 é¡¹åŸºå‡†ä»»åŠ¡ä¸­çš„ 23 é¡¹ä½å±…æ¦œé¦–ï¼›å°½ç®¡æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä½†åœ¨ 18 é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äº Qwen-2.5-VL-72Bã€‚\n\n\n\né¡¹ç›®æ›´æ–°\n\nğŸ”¥ æ–°é—»ï¼š2025/08/11ï¼šæˆ‘ä»¬å‘å¸ƒäº† GLM-4.5Vï¼Œåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å‡æœ‰æ˜¾è‘—æå‡ã€‚åŒæ—¶å¼€æºäº†æ‰‹å·¥æ‰“é€ çš„ æ¡Œé¢åŠ©æ‰‹åº”ç”¨ï¼Œç”¨äºè°ƒè¯•ã€‚è¿æ¥ GLM-4.5V åï¼Œå®ƒå¯é€šè¿‡æˆªå›¾æˆ–å½•å±æ•è·ç”µè„‘å±å¹•çš„è§†è§‰ä¿¡æ¯ã€‚æ¬¢è¿è¯•ç”¨æˆ–å°†å…¶å®šåˆ¶ä¸ºæ‚¨è‡ªå·±çš„å¤šæ¨¡æ€åŠ©æ‰‹ã€‚ç‚¹å‡» æ­¤å¤„ ä¸‹è½½å®‰è£…ç¨‹åºï¼Œæˆ– ä»æºç æ„å»ºï¼\næ–°é—»ï¼š2025/07/16ï¼šæˆ‘ä»¬å¼€æºäº†ç”¨äºè®­ç»ƒ GLM-4.1V-Thinking çš„ VLM å¥–åŠ±ç³»ç»Ÿã€‚æŸ¥çœ‹ ä»£ç ä»“åº“ å¹¶æœ¬åœ°è¿è¡Œï¼špython examples/reward_system_demo.pyã€‚\næ–°é—»ï¼š2025/07/01ï¼šæˆ‘ä»¬å‘å¸ƒäº† GLM-4.1V-9B-Thinking åŠå…¶ æŠ€æœ¯æŠ¥å‘Šã€‚\n\n\n\næ¨¡å‹å®ç°ä»£ç \n\nGLM-4.5V æ¨¡å‹ç®—æ³•ï¼šè¯¦è§ transformers ä¸­çš„å®Œæ•´å®ç°ã€‚\nGLM-4.1V-9B-Thinking æ¨¡å‹ç®—æ³•ï¼šè¯¦è§ transformers ä¸­çš„å®Œæ•´å®ç°ã€‚\nä¸¤ä¸ªæ¨¡å‹å…±äº«ç›¸åŒçš„å¤šæ¨¡æ€é¢„å¤„ç†æµç¨‹ï¼Œä½†é‡‡ç”¨ä¸åŒçš„å¯¹è¯æ¨¡æ¿â€”â€”è¯·åŠ¡å¿…ä»”ç»†åŒºåˆ†ã€‚\n\n\n\nä½¿ç”¨æ–¹æ³•\n\n\nç¯å¢ƒå®‰è£…\né€‚ç”¨äº SGLang å’Œ transformersï¼š\npip install -r https://raw.githubusercontent.com/zai-org/GLM-V/main/requirements.txt\n\nå¯¹äº vLLMï¼š\npip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\npip install transformers-v4.55.0-GLM-4.5V-preview\n\n\n\nä½¿ç”¨ Transformers å¿«é€Ÿå…¥é—¨\nfrom transformers import AutoProcessor, Glm4vMoeForConditionalGeneration\nimport torch\n\nMODEL_PATH = \"zai-org/GLM-4.5V\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png\"\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"describe this image\"\n            }\n        ],\n    }\n]\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\nmodel = Glm4vMoeForConditionalGeneration.from_pretrained(\n    pretrained_model_name_or_path=MODEL_PATH,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\ninputs = processor.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_dict=True,\n    return_tensors=\"pt\"\n).to(model.device)\ninputs.pop(\"token_type_ids\", None)\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\noutput_text = processor.decode(generated_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\nprint(output_text)\n\nå“åº”ä¸­çš„ç‰¹æ®Šæ ‡è®° <|begin_of_box|> å’Œ <|end_of_box|> ç”¨äºæ ‡è¯†å›¾åƒä¸­ç­”æ¡ˆçš„è¾¹ç•Œæ¡†ã€‚è¾¹ç•Œæ¡†ä»¥å››ä¸ªæ•°å­—è¡¨ç¤ºï¼Œä¾‹å¦‚ [x1, y1, x2, y2]ï¼Œå…¶ä¸­ (x1, y1) ä¸ºå·¦ä¸Šè§’åæ ‡ï¼Œ(x2, y2) ä¸ºå³ä¸‹è§’åæ ‡ã€‚æ‹¬å·æ ·å¼å¯èƒ½æœ‰æ‰€ä¸åŒï¼ˆ[]ã€[[]]ã€()ã€<> ç­‰ï¼‰ï¼Œä½†å«ä¹‰ç›¸åŒï¼šç”¨äºåŒ…è£¹è¾¹ç•Œæ¡†çš„åæ ‡ã€‚è¿™äº›åæ ‡æ˜¯ä»‹äº 0 åˆ° 1000 ä¹‹é—´çš„ç›¸å¯¹å€¼ï¼Œå·²æ ¹æ®å›¾åƒå°ºå¯¸è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚\næ›´å¤šä»£ç ä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„ GitHubã€‚\n\n\nå®šä½ç¤ºä¾‹\nGLM-4.5V å…·å¤‡ç²¾ç¡®çš„å®šä½èƒ½åŠ›ã€‚å½“ç»™å®šä¸€ä¸ªè¯·æ±‚ç‰¹å®šç‰©ä½“ä½ç½®çš„æç¤ºæ—¶ï¼ŒGLM-4.5V èƒ½å¤Ÿé€æ­¥æ¨ç†å¹¶è¯†åˆ«å‡ºç›®æ ‡ç‰©ä½“çš„è¾¹ç•Œæ¡†ã€‚æŸ¥è¯¢æç¤ºæ”¯æŒå¯¹ç›®æ ‡ç‰©ä½“çš„å¤æ‚æè¿°ä»¥åŠæŒ‡å®šè¾“å‡ºæ ¼å¼ï¼Œä¾‹å¦‚ï¼š\n\n\nå¸®æˆ‘åœ¨å›¾åƒä¸­å®šä½  å¹¶ç»™å‡ºå…¶è¾¹ç•Œæ¡†ã€‚\nè¯·æ ¹æ®ç»™å®šæè¿°åœ¨å›¾åƒä¸­ç²¾ç¡®å®šä½è¾¹ç•Œæ¡† [[x1,y1,x2,y2], â€¦]ã€‚\n\n\næ­¤å¤„ï¼Œ<expr> æ˜¯ç›®æ ‡ç‰©ä½“çš„æè¿°ã€‚è¾“å‡ºçš„è¾¹ç•Œæ¡†æ˜¯ä¸€ä¸ªç”±å·¦ä¸Šè§’å’Œå³ä¸‹è§’åæ ‡ç»„æˆçš„å››å…ƒç»„ $$[x_1,y_1,x_2,y_2]$$ï¼Œå…¶ä¸­æ¯ä¸ªå€¼å‡æŒ‰å›¾åƒå®½åº¦ï¼ˆå¯¹äº xï¼‰æˆ–é«˜åº¦ï¼ˆå¯¹äº yï¼‰è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œå¹¶ç¼©æ”¾è‡³ 1000ã€‚\nåœ¨å“åº”ä¸­ï¼Œç‰¹æ®Šæ ‡è®° <|begin_of_box|> å’Œ <|end_of_box|> ç”¨äºæ ‡è¯†ç­”æ¡ˆä¸­çš„å›¾åƒè¾¹ç•Œæ¡†ã€‚æ‹¬å·æ ·å¼å¯èƒ½æœ‰æ‰€ä¸åŒï¼ˆ[]ã€[[]]ã€()ã€<> ç­‰ï¼‰ï¼Œä½†å«ä¹‰ç›¸åŒï¼šç”¨äºåŒ…è£¹è¾¹ç•Œæ¡†çš„åæ ‡ã€‚\n\n\nGUI æ™ºèƒ½ä½“ç¤ºä¾‹\n\nexamples/gui-agentï¼šæ¼”ç¤º GUI æ™ºèƒ½ä½“çš„æç¤ºæ„å»ºå’Œè¾“å‡ºå¤„ç†ï¼ŒåŒ…æ‹¬é’ˆå¯¹ç§»åŠ¨ç«¯ã€ä¸ªäººç”µè„‘ç«¯å’Œç½‘é¡µç«¯çš„ç­–ç•¥ã€‚GLM-4.1V å’Œ GLM-4.5V çš„æç¤ºæ¨¡æ¿æœ‰æ‰€ä¸åŒã€‚\n\n\n\nå¿«é€Ÿæ¼”ç¤ºåº”ç”¨\n\nexamples/vlm-helperï¼šä¸€æ¬¾é€‚ç”¨äº GLM å¤šæ¨¡æ€æ¨¡å‹çš„æ¡Œé¢åŠ©æ‰‹ï¼ˆä¸»è¦é’ˆå¯¹ GLM-4.5Vï¼Œå…¼å®¹ GLM-4.1Vï¼‰ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€PDFã€PPT ç­‰å¤šç§æ ¼å¼ã€‚é€šè¿‡è¿æ¥ GLM å¤šæ¨¡æ€ APIï¼Œå¯æä¾›è·¨åœºæ™¯çš„æ™ºèƒ½æœåŠ¡ã€‚ä¸‹è½½ å®‰è£…ç¨‹åº æˆ– ä»æºç æ„å»ºã€‚\n\n\n\nvLLM\nvllm serve zai-org/GLM-4.5V \\\n     --tensor-parallel-size 4 \\\n     --tool-call-parser glm45 \\\n     --reasoning-parser glm45 \\\n     --enable-auto-tool-choice \\\n     --served-model-name glm-4.5v \\\n     --allowed-local-media-path / \\\n     --media-io-kwargs '{\"video\": {\"num_frames\": -1}}'\n\n\n\nSGLang\npython3 -m sglang.launch_server --model-path zai-org/GLM-4.5V \\\n     --tp-size 4 \\\n     --tool-call-parser glm45 \\\n     --reasoning-parser glm45 \\\n     --served-model-name glm-4.5v \\\n     --port 8000 \\\n     --host 0.0.0.0\n\n\n\næ³¨æ„äº‹é¡¹ï¼š\n\nå»ºè®®åœ¨ SGLang ä¸­ä½¿ç”¨ FA3 æ³¨æ„åŠ›åç«¯ä»¥è·å¾—æ›´é«˜çš„æ¨ç†æ€§èƒ½å’Œæ›´ä½çš„å†…å­˜å ç”¨ï¼š\n--attention-backend fa3 --mm-attention-backend fa3 --enable-torch-compile\nè‹¥ä¸ä½¿ç”¨ FA3ï¼Œå¤§å‹è§†é¢‘æ¨ç†å¯èƒ½ä¼šå¯¼è‡´å†…å­˜æº¢å‡ºï¼ˆOOMï¼‰é”™è¯¯ã€‚\nåŒæ—¶å»ºè®®å¢åŠ  SGLANG_VLM_CACHE_SIZE_MBï¼ˆä¾‹å¦‚è®¾ä¸º 1024ï¼‰ï¼Œä¸ºè§†é¢‘ç†è§£æä¾›å……è¶³çš„ç¼“å­˜ç©ºé—´ã€‚\nä½¿ç”¨ vLLM å’Œ SGLang æ—¶ï¼Œé»˜è®¤å¯ç”¨æ€ç»´é“¾æ¨¡å¼ã€‚å¦‚éœ€å…³é—­æ€ç»´é“¾åˆ‡æ¢ï¼Œè¯·æ·»åŠ ï¼š\nextra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n\n\n\næ¨¡å‹å¾®è°ƒ\nLLaMA-Factory å·²æ”¯æŒ GLM-4.5V å’Œ GLM-4.1V-9B-Thinking æ¨¡å‹çš„å¾®è°ƒã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨ä¸¤å¼ å›¾åƒæ„å»ºæ•°æ®é›†çš„ç¤ºä¾‹ã€‚è¯·å°†æ•°æ®é›†æ•´ç†ä¸º finetune.json å¹¶éµå¾ªä»¥ä¸‹æ ¼å¼ï¼Œæ­¤ç¤ºä¾‹é€‚ç”¨äº GLM-4.1V-9B çš„å¾®è°ƒã€‚\n[\n  {\n    \"messages\": [\n      {\n        \"content\": \"<image>Who are they?\",\n        \"role\": \"user\"\n      },\n      {\n        \"content\": \"<think>\nUser asked me to observe the image and find the answer. I know they are Kane and Goretzka from Bayern Munich.</think>\n<answer>They're Kane and Goretzka from Bayern Munich.</answer>\",\n        \"role\": \"assistant\"\n      },\n      {\n        \"content\": \"<image>What are they doing?\",\n        \"role\": \"user\"\n      },\n      {\n        \"content\": \"<think>\nI need to observe what these people are doing. Oh, they are celebrating on the soccer field.</think>\n<answer>They are celebrating on the soccer field.</answer>\",\n        \"role\": \"assistant\"\n      }\n    ],\n    \"images\": [\n      \"mllm_demo_data/1.jpg\",\n      \"mllm_demo_data/2.jpg\"\n    ]\n  }\n]\n\n\n</think> ... </think> å†…çš„å†…å®¹ä¸ä¼šè¢«å­˜å‚¨ä¸ºå¯¹è¯å†å²æˆ–ç”¨äºå¾®è°ƒæ•°æ®ã€‚\n<image> æ ‡ç­¾å°†è¢«æ›¿æ¢ä¸ºç›¸åº”çš„å›¾åƒä¿¡æ¯ã€‚\nå¯¹äº GLM-4.5V æ¨¡å‹ï¼Œåº”ç§»é™¤  å’Œ superscript: æ ‡ç­¾ã€‚\n\nä¹‹åï¼Œæ‚¨å¯ä»¥æŒ‰ç…§æ ‡å‡†çš„ LLaMA-Factory æµç¨‹è¿›è¡Œå¾®è°ƒã€‚\n\n\nå·²ä¿®å¤é—®é¢˜ä¸é—ç•™é—®é¢˜\nè‡ª GLM-4.1V å‘å¸ƒä»¥æ¥ï¼Œæˆ‘ä»¬å·²è§£å†³äº†ç¤¾åŒºåé¦ˆçš„è¯¸å¤šé—®é¢˜ã€‚åœ¨ GLM-4.5V ä¸­ï¼Œé‡å¤æ€è€ƒã€è¾“å‡ºæ ¼å¼é”™è¯¯ç­‰å¸¸è§é—®é¢˜å¾—åˆ°ç¼“è§£ã€‚ä½†ä»å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼š\n\nåœ¨å‰ç«¯ä»£ç å¤ç°åœºæ™¯ä¸­ï¼Œæ¨¡å‹å¯èƒ½è¾“å‡ºåŸå§‹ HTML ä»£ç è€Œæœªä½¿ç”¨æ­£ç¡®çš„ Markdown æ ¼å¼åŒ…è£¹ï¼Œè¿˜å¯èƒ½å­˜åœ¨å­—ç¬¦è½¬ä¹‰é—®é¢˜ï¼Œä»è€Œå¯¼è‡´æ¸²æŸ“é”™è¯¯ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª è¡¥ä¸ ä»¥ä¿®å¤å¤§éƒ¨åˆ†æ­¤ç±»æƒ…å†µã€‚\nçº¯æ–‡æœ¬é—®ç­”èƒ½åŠ›ä»æœ‰æå‡ç©ºé—´ï¼Œå› ä¸ºæœ¬ç‰ˆæœ¬ä¸»è¦èšç„¦äºå¤šæ¨¡æ€åœºæ™¯çš„ä¼˜åŒ–ã€‚\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¿‡åº¦æ€è€ƒæˆ–é‡å¤å†…å®¹ï¼Œå°¤å…¶æ˜¯é¢å¯¹å¤æ‚æç¤ºæ—¶ã€‚\nå¶å°”ï¼Œæ¨¡å‹å¯èƒ½ä¼šåœ¨ç»“å°¾å¤„å†æ¬¡å¤è¿°ç­”æ¡ˆã€‚\nå­˜åœ¨ä¸€äº›æ„ŸçŸ¥ç±»é—®é¢˜ï¼Œåœ¨è®¡æ•°ã€ç‰¹å®šäººç‰©è¯†åˆ«ç­‰ä»»åŠ¡ä¸Šä»æœ‰æ”¹è¿›ä½™åœ°ã€‚\n\næˆ‘ä»¬æ¬¢è¿åœ¨ issue ä¸“åŒºæä¾›åé¦ˆï¼Œå¹¶å°†å°½å¿«è§£å†³ç›¸å…³é—®é¢˜ã€‚\n\n\nå¼•ç”¨è¯´æ˜\nå¦‚æœæ‚¨ä½¿ç”¨æœ¬æ¨¡å‹ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼š\n@misc{vteam2025glm45vglm41vthinkingversatilemultimodal,\n      title={GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning}, \n      author={V Team and Wenyi Hong and Wenmeng Yu and Xiaotao Gu and Guo Wang and Guobing Gan and Haomiao Tang and Jiale Cheng and Ji Qi and Junhui Ji and Lihang Pan and Shuaiqi Duan and Weihan Wang and Yan Wang and Yean Cheng and Zehai He and Zhe Su and Zhen Yang and Ziyang Pan and Aohan Zeng and Baoxu Wang and Bin Chen and Boyan Shi and Changyu Pang and Chenhui Zhang and Da Yin and Fan Yang and Guoqing Chen and Jiazheng Xu and Jiale Zhu and Jiali Chen and Jing Chen and Jinhao Chen and Jinghao Lin and Jinjiang Wang and Junjie Chen and Leqi Lei and Letian Gong and Leyi Pan and Mingdao Liu and Mingde Xu and Mingzhi Zhang and Qinkai Zheng and Sheng Yang and Shi Zhong and Shiyu Huang and Shuyuan Zhao and Siyan Xue and Shangqin Tu and Shengbiao Meng and Tianshu Zhang and Tianwei Luo and Tianxiang Hao and Tianyu Tong and Wenkai Li and Wei Jia and Xiao Liu and Xiaohan Zhang and Xin Lyu and Xinyue Fan and Xuancheng Huang and Yanling Wang and Yadong Xue and Yanfeng Wang and Yanzi Wang and Yifan An and Yifan Du and Yiming Shi and Yiheng Huang and Yilin Niu and Yuan Wang and Yuanchang Yue and Yuchen Li and Yutao Zhang and Yuting Wang and Yu Wang and Yuxuan Zhang and Zhao Xue and Zhenyu Hou and Zhengxiao Du and Zihan Wang and Peng Zhang and Debing Liu and Bin Xu and Juanzi Li and Minlie Huang and Yuxiao Dong and Jie Tang},\n      year={2025},\n      eprint={2507.01006},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2507.01006}, \n}\n\n",
    "tags": [
      "Image-Text-to-Text",
      "Transformers",
      "Safetensors",
      "English",
      "Chinese",
      "MIT",
      "arxiv:2507.01006",
      "arxiv:2507.0100"
    ]
  },
  {
    "url": "https://gitcode.com/mirrors/sentence-transformers/all-MiniLM-L6-v2",
    "project_name": "sentence-transformers/all-MiniLM-L6-v2",
    "readme": "Sign up to GitCodeDiscover high-quality open-source projects easily and host them with one clické¦–é¡µ å·¥ä½œå° ...AIç¤¾åŒº å¤§èµ›å¹³å° ...é¡¹ç›® ç»„ç»‡ ...SettingsThis is not the web page you are looking forBack to HomeSelectedzorm17Goè½»é‡ORM,æ”¯æŒè¾¾æ¢¦(dm),é‡‘ä»“(kingbase),ç¥é€š(shentong),å—å¤§é€šç”¨(gbase),TDengine,mysql,postgresql,oracle,mssql,sqlite,db2,clickhouse...Go172hello-algo478ã€ŠHello ç®—æ³•ã€‹ï¼šåŠ¨ç”»å›¾è§£ã€ä¸€é”®è¿è¡Œçš„æ•°æ®ç»“æ„ä¸ç®—æ³•æ•™ç¨‹ï¼Œæ”¯æŒ Java, C++, Python, Go, JS, TS, C#, Swift, Rust, Dart, Zig ç­‰è¯­è¨€ã€‚Java47820WxJava847å¾®ä¿¡å¼€å‘ Java SDKï¼Œæ”¯æŒå¾®ä¿¡æ”¯ä»˜ã€å¼€æ”¾å¹³å°ã€å…¬ä¼—å·ã€è§†é¢‘å·ã€ä¼ä¸šå¾®ä¿¡ã€å°ç¨‹åºç­‰çš„åç«¯å¼€å‘ï¼Œè®°å¾—å…³æ³¨å…¬ä¼—å·åŠæ—¶æ¥å—ç‰ˆæœ¬æ›´æ–°ä¿¡æ¯ï¼Œä»¥åŠåŠ å…¥å¾®ä¿¡ç¾¤è¿›è¡Œæ·±å…¥è®¨è®ºJava84724MiniLLMs0No description0smart-sso30SpringBoot SSO å•ç‚¹ç™»å½• æƒé™è®¤è¯ï¼ŒOAuth2å®ç°ï¼Œæ”¯æŒè·¨åŸŸã€å‰åç«¯åˆ†ç¦»ã€åˆ†å¸ƒå¼éƒ¨ç½²JavaScript3015minillm0MiniLLM is a minimal system for running modern LLMs on consumer-grade GPUsPython0dy-java68DyJavaæ˜¯ä¸€æ¬¾åŠŸèƒ½å¼ºå¤§çš„æŠ–éŸ³Javaå¼€å‘å·¥å…·åŒ…ï¼ˆSDKï¼‰ï¼Œæ”¯æŒæŠ–éŸ³å„ä¸ªåº”ç”¨OpenAPIå¿«é€Ÿè°ƒç”¨ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºç§»åŠ¨/ç½‘ç«™åº”ç”¨ã€æŠ–éŸ³å¼€æ”¾å¹³å°ã€æŠ–åº—ã€TikTokå’ŒæŠ–éŸ³å°ç¨‹åºç­‰ã€‚DyJavaè‡´åŠ›äºç®€åŒ–å¼€å‘æµç¨‹ï¼Œæé«˜å¼€å‘æ•ˆç‡ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿæ›´ä¸“æ³¨äºåˆ›æ–°å’Œä¸šåŠ¡é€»è¾‘çš„å®ç°ã€‚Java6810ruoyi-spring-boot-all22èŠ‹é“æºç (æ— é®ç¾å¸ƒç‰ˆ)221DrissionPage132åŸºäºpythonçš„ç½‘é¡µè‡ªåŠ¨åŒ–å·¥å…·ã€‚æ—¢èƒ½æ§åˆ¶æµè§ˆå™¨ï¼Œä¹Ÿèƒ½æ”¶å‘æ•°æ®åŒ…ã€‚å¯å…¼é¡¾æµè§ˆå™¨è‡ªåŠ¨åŒ–çš„ä¾¿åˆ©æ€§å’Œrequestsçš„é«˜æ•ˆç‡ã€‚åŠŸèƒ½å¼ºå¤§ï¼Œå†…ç½®æ— æ•°äººæ€§åŒ–è®¾è®¡å’Œä¾¿æ·åŠŸèƒ½ã€‚è¯­æ³•ç®€æ´è€Œä¼˜é›…ï¼Œä»£ç é‡å°‘ã€‚Python13215More",
    "tags": []
  },
  {
    "url": "https://gitcode.com/hf_mirrors/tencent/HunyuanVideo-Foley",
    "project_name": "tencent/HunyuanVideo-Foley",
    "readme": "Sign up to GitCodeDiscover high-quality open-source projects easily and host them with one clické¦–é¡µ å·¥ä½œå° ...AIç¤¾åŒº å¤§èµ›å¹³å° ...é¡¹ç›® ç»„ç»‡ ...SettingsThis is not the web page you are looking forBack to HomeSelectedPandaX50ğŸ‰ğŸ”¥PandaXæ˜¯Goè¯­è¨€å¼€æºçš„ä¼ä¸šçº§ç‰©è”ç½‘å¹³å°ä½ä»£ç å¼€å‘åŸºåº§ï¼ŒåŸºäºgo-restful+Vue3.0+TypeScript+vite3+element-Plusçš„å‰åç«¯åˆ†ç¦»å¼€å‘ã€‚æ”¯æŒè®¾å¤‡ç®¡æ§ï¼Œè§„åˆ™é“¾ï¼Œäº‘ç»„æ€ï¼Œå¯è§†åŒ–å¤§å±ï¼ŒæŠ¥è¡¨è®¾è®¡å™¨ï¼Œè¡¨å•è®¾è®¡å™¨ï¼Œä»£ç ç”Ÿæˆå™¨ç­‰åŠŸèƒ½ã€‚èƒ½å¸®åŠ©ä½ å¿«é€Ÿå»ºç«‹IOTç‰©è”ç½‘å¹³å°ç­‰ç›¸å…³ä¸šåŠ¡ç³»ç»Ÿã€‚Go5010ComfyUI_FoleyCrafter0No description0smart-sso30SpringBoot SSO å•ç‚¹ç™»å½• æƒé™è®¤è¯ï¼ŒOAuth2å®ç°ï¼Œæ”¯æŒè·¨åŸŸã€å‰åç«¯åˆ†ç¦»ã€åˆ†å¸ƒå¼éƒ¨ç½²JavaScript3015ComfyUI-HunyuanVideoMultiLora0No description0VideoFusion15ä¸€ç«™å¼çŸ­è§†é¢‘æ‹¼æ¥è½¯ä»¶ æ— ä¾èµ–,ç‚¹å‡»å³ç”¨,è‡ªåŠ¨å»é»‘è¾¹,è‡ªåŠ¨å¸§åŒæ­¥,è‡ªåŠ¨è°ƒæ•´åˆ†è¾¨ç‡,æ‰¹é‡å˜æ›´è§†é¢‘ä¸ºæ¨ªå±/ç«–å±\nhttps://271374667.github.io/VideoFusion/Python157ComfyUI-HunyuanVideoWrapper0No descriptionPython0geekai43AI åŠ©æ‰‹å…¨å¥—å¼€æºè§£å†³æ–¹æ¡ˆï¼Œè‡ªå¸¦è¿è¥ç®¡ç†åå°ï¼Œé‡‡ç”¨ Go + Vue3 + element-plus å®ç°ã€‚å¼€ç®±å³ç”¨ã€‚é›†æˆäº† OpenGPT, Azure, ChatGLM,è®¯é£æ˜Ÿç«,æ–‡å¿ƒä¸€è¨€ç­‰å¤šä¸ªå¹³å°çš„å¤§è¯­è¨€æ¨¡å‹ã€‚æ”¯æŒ MJ AI ç»˜ç”»ï¼ŒStable Diffusion AI  ç»˜ç”»ï¼Œå¾®åšçƒ­æœç­‰æ’ä»¶å·¥å…·ã€‚åç»­è¿˜å°†æ”¯æŒAiç”ŸæˆPPT/Aiç”Ÿæˆæ•°å­—äººè§†é¢‘/Aiç”Ÿæˆé£æ ¼è§†é¢‘/Aiç”Ÿæˆä¸‡å­—é•¿ç¯‡å†™ä½œç­‰å®ç”¨åŠŸèƒ½Vue436folly5An open-source C++ library developed and used at Facebook.C++5AgileBPM-OA41AgileBPM å·¥ä½œæµ ä½ä»£ç  å¿«é€Ÿå¼€å‘å¹³å°ï¼Œ åŒ…å« èµ„äº§ç®¡ç†ã€å®¢æˆ·å…³ç³» ç­‰ç”Ÿæ€åº”ç”¨ç³»ç»ŸJava415More",
    "tags": []
  },
  {
    "url": "https://gitcode.com/hf_mirrors/deepseek-ai/DeepSeek-V3.1",
    "project_name": "deepseek-ai/DeepSeek-V3.1",
    "readme": "\n\nDeepSeek-V3.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nç®€ä»‹\nDeepSeek-V3.1 æ˜¯ä¸€æ¬¾æ··åˆæ¨¡å‹ï¼ŒåŒæ—¶æ”¯æŒæ€è€ƒæ¨¡å¼ä¸éæ€è€ƒæ¨¡å¼ã€‚ç›¸è¾ƒäºä¸Šä¸€ç‰ˆæœ¬ï¼Œæœ¬æ¬¡å‡çº§åœ¨å¤šä¸ªæ–¹é¢å¸¦æ¥äº†æ”¹è¿›ï¼š\n\n\næ··åˆæ€è€ƒæ¨¡å¼ï¼šé€šè¿‡åˆ‡æ¢å¯¹è¯æ¨¡æ¿ï¼Œå•ä¸€æ¨¡å‹å³å¯æ”¯æŒæ€è€ƒæ¨¡å¼ä¸éæ€è€ƒæ¨¡å¼ã€‚\n\n\næ›´æ™ºèƒ½çš„å·¥å…·è°ƒç”¨ï¼šç»è¿‡è®­ç»ƒåä¼˜åŒ–ï¼Œæ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨å’Œæ™ºèƒ½ä½“ä»»åŠ¡ä¸Šçš„è¡¨ç°å¾—åˆ°æ˜¾è‘—æå‡ã€‚\n\n\næ›´é«˜çš„æ€è€ƒæ•ˆç‡ï¼šDeepSeek-V3.1-Think èƒ½å¤Ÿè¾¾åˆ°ä¸ DeepSeek-R1-0528 ç›¸å½“çš„å›ç­”è´¨é‡ï¼ŒåŒæ—¶å“åº”é€Ÿåº¦æ›´å¿«ã€‚\n\n\nDeepSeek-V3.1 æ˜¯åœ¨ DeepSeek-V3.1-Base çš„åŸºç¡€ä¸Šè¿›è¡Œè®­ç»ƒåä¼˜åŒ–å¾—åˆ°çš„ã€‚DeepSeek-V3.1-Base åˆ™æ˜¯åœ¨åŸå§‹ V3 åŸºç¡€æ¨¡å‹ checkpoint ä¹‹ä¸Šï¼Œéµå¾ªåŸå§‹ DeepSeek-V3 æŠ¥å‘Šä¸­é˜è¿°çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸¤é˜¶æ®µé•¿ä¸Šä¸‹æ–‡æ‰©å±•æ–¹æ¡ˆæ„å»ºè€Œæˆã€‚æˆ‘ä»¬é€šè¿‡æ”¶é›†é¢å¤–çš„é•¿æ–‡æ¡£å¹¶å¤§å¹…æ‰©å±•ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µæ¥æ‰©å……æ•°æ®é›†ã€‚32K æ‰©å±•é˜¶æ®µçš„è®­ç»ƒæ•°æ®é‡å¢åŠ äº† 10 å€ï¼Œè¾¾åˆ° 6300 äº¿ tokensï¼›128K æ‰©å±•é˜¶æ®µçš„è®­ç»ƒæ•°æ®é‡æ‰©å±•äº† 3.3 å€ï¼Œè¾¾åˆ° 2090 äº¿ tokensã€‚\næ­¤å¤–ï¼ŒDeepSeek-V3.1 åœ¨è®­ç»ƒæ—¶ï¼Œæ¨¡å‹æƒé‡å’Œæ¿€æ´»å€¼å‡é‡‡ç”¨ UE8M0 FP8 ç²¾åº¦æ•°æ®æ ¼å¼ï¼Œä»¥ç¡®ä¿ä¸å¾®ç¼©æ”¾æ•°æ®æ ¼å¼çš„å…¼å®¹æ€§ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚è€ƒ DeepGEMMã€‚\n\n\næ¨¡å‹ä¸‹è½½\n\n\n\n\næ¨¡å‹\næ€»å‚æ•°é‡\næ¿€æ´»å‚æ•°é‡\nä¸Šä¸‹æ–‡é•¿åº¦\nä¸‹è½½é“¾æ¥\n\n\n\n\nDeepSeek-V3.1-Base\n6710 äº¿\n370 äº¿\n128K\nHuggingFace | ModelScope\n\n\nDeepSeek-V3.1\n6710 äº¿\n370 äº¿\n128K\nHuggingFace | ModelScope\n\n\n\n\n\n\nå¯¹è¯æ¨¡æ¿\nå¯¹è¯æ¨¡æ¿çš„è¯¦ç»†ä¿¡æ¯æè¿°äº tokenizer_config.json å’Œ assets/chat_template.jinja æ–‡ä»¶ä¸­ã€‚ä»¥ä¸‹æ˜¯ç®€è¦è¯´æ˜ã€‚\n\n\néæ€è€ƒæ¨¡å¼\n\n\né¦–è½®å¯¹è¯\nå‰ç¼€ï¼š\n<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>\nå€ŸåŠ©ç»™å®šçš„å‰ç¼€ï¼ŒDeepSeek V3.1 èƒ½å¤Ÿä»¥éæ€è€ƒæ¨¡å¼å¯¹æŸ¥è¯¢ç”Ÿæˆå“åº”ã€‚ä¸ DeepSeek V3 ä¸åŒï¼Œè¯¥ç‰ˆæœ¬æ–°å¢äº†ä¸€ä¸ªé¢å¤–çš„æ ‡è®° </think>ã€‚\n\n\nå¤šè½®å¯¹è¯\nä¸Šä¸‹æ–‡ï¼š\n<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ>superscript:{response}<ï½œendâ–ofâ–sentenceï½œ>...<ï½œUserï½œ>{query}<ï½œAssistantï½œ>superscript:{response}<ï½œendâ–ofâ–sentenceï½œ>\nå‰ç¼€ï¼š\n<ï½œUserï½œ>{query}<ï½œAssistantï½œ>superscript:\né€šè¿‡å°†ä¸Šä¸‹æ–‡ä¸å‰ç¼€è¿›è¡Œæ‹¼æ¥ï¼Œæˆ‘ä»¬å³å¯å¾—åˆ°è¯¥æŸ¥è¯¢å¯¹åº”çš„æ­£ç¡®æç¤ºè¯ã€‚\n\n\næ€è€ƒæ¨¡å¼\n\n\né¦–è½®å¯¹è¯\nå‰ç¼€ï¼š\n<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>\næ€è€ƒæ¨¡å¼çš„å‰ç¼€ä¸ DeepSeek-R1 ç±»ä¼¼ã€‚\n\n\nå¤šè½®å¯¹è¯\nä¸Šä¸‹æ–‡ï¼š\n<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ>superscript:{response}<ï½œendâ–ofâ–sentenceï½œ>...<ï½œUserï½œ>{query}<ï½œAssistantï½œ>superscript:{response}<ï½œendâ–ofâ–sentenceï½œ>\nå‰ç¼€ï¼š\n<ï½œUserï½œ>{query}<ï½œAssistantï½œ>superscript:\nå¤šè½®å¯¹è¯æ¨¡æ¿ä¸éæ€è€ƒæ¨¡å¼ä¸‹çš„å¤šè½®å¯¹è¯æ¨¡æ¿ç›¸åŒã€‚è¿™æ„å‘³ç€æœ€åä¸€è½®ä¸­çš„æ€è€ƒæ ‡è®°ä¼šè¢«å»é™¤ï¼Œä½† </think> ä¼šä¿ç•™åœ¨ä¸Šä¸‹æ–‡ä¸­çš„æ¯ä¸€è½®å¯¹è¯é‡Œã€‚\n\n\nå·¥å…·è°ƒç”¨ï¼ˆToolCallï¼‰\néæ€è€ƒæ¨¡å¼æ”¯æŒå·¥å…·è°ƒç”¨åŠŸèƒ½ã€‚å…¶æ ¼å¼å¦‚ä¸‹ï¼š\n<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}\\n\\n{tool_description}<ï½œUserï½œ>{query}<ï½œAssistantï½œ>superscript: ï¼Œå…¶ä¸­ tool_description ä¸º\n## Tools\nYou have access to the following tools:\n\n### {tool_name1}\nDescription: {description}\n\nParameters: {json.dumps(parameters)}\n\nIMPORTANT: ALWAYS adhere to this exact format for tool use:\n<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>tool_call_name<ï½œtoolâ–sepï½œ>tool_call_arguments<ï½œtoolâ–callâ–endï½œ>{additional_tool_calls}<ï½œtoolâ–callsâ–endï½œ>\n\nWhere:\n- `tool_call_name` must be an exact match to one of the available tools\n- `tool_call_arguments` must be valid JSON that strictly follows the tool's Parameters Schema\n- For multiple tool calls, chain them directly without separators or spaces\n\n\n\nä»£ç æ™ºèƒ½ä½“ï¼ˆCode-Agentï¼‰\næˆ‘ä»¬æ”¯æŒå¤šç§ä»£ç æ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯·å‚è€ƒä¸Šè¿°å·¥å…·è°ƒç”¨æ ¼å¼åˆ›å»ºæ‚¨è‡ªå·±çš„ä»£ç æ™ºèƒ½ä½“ã€‚ç¤ºä¾‹å¯å‚è§assets/code_agent_trajectory.htmlã€‚\n\n\næœç´¢æ™ºèƒ½ä½“ï¼ˆSearch-Agentï¼‰\nä¸ºæ”¯æŒæœç´¢æ™ºèƒ½ä½“ï¼Œæˆ‘ä»¬åœ¨æ€è€ƒæ¨¡å¼ä¸­è®¾è®¡äº†ç‰¹å®šçš„æœç´¢å·¥å…·è°ƒç”¨æ ¼å¼ã€‚\nå¯¹äºéœ€è¦è®¿é—®å¤–éƒ¨ä¿¡æ¯æˆ–æœ€æ–°èµ„è®¯çš„å¤æ‚é—®é¢˜ï¼ŒDeepSeek-V3.1 å¯é€šè¿‡å¤šè½®å·¥å…·è°ƒç”¨æµç¨‹ï¼Œåˆ©ç”¨ç”¨æˆ·æä¾›çš„æœç´¢å·¥å…·æ¥è·å–ç­”æ¡ˆã€‚\nè¯¦ç»†æ¨¡æ¿è¯·å‚è€ƒassets/search_tool_trajectory.htmlå’Œassets/search_python_tool_trajectory.htmlã€‚\n\n\nè¯„ä¼°ç»“æœ\n\n\n\nç±»åˆ«\nè¯„æµ‹åŸºå‡†ï¼ˆæŒ‡æ ‡ï¼‰\nDeepSeek V3.1-NonThinking\nDeepSeek V3 0324\nDeepSeek V3.1-Thinking\nDeepSeek R1 0528\n\n\n\n\né€šç”¨èƒ½åŠ›\n\n\n\n\n\n\n\n\nMMLU-Reduxï¼ˆç²¾ç¡®åŒ¹é…ï¼‰\n91.8\n90.5\n93.7\n93.4\n\n\n\nMMLU-Proï¼ˆç²¾ç¡®åŒ¹é…ï¼‰\n83.7\n81.2\n84.8\n85.0\n\n\n\nGPQA-Diamondï¼ˆPass@1ï¼‰\n74.9\n68.4\n80.1\n81.0\n\n\n\nHumanity's Last Examï¼ˆPass@1ï¼‰\n-\n-\n15.9\n17.7\n\n\næœç´¢æ™ºèƒ½ä½“\n\n\n\n\n\n\n\n\nBrowseComp\n-\n-\n30.0\n8.9\n\n\n\nBrowseComp_zh\n-\n-\n49.2\n35.7\n\n\n\nHumanity's Last Examï¼ˆPython + æœç´¢ï¼‰\n-\n-\n29.8\n24.8\n\n\n\nSimpleQA\n-\n-\n93.4\n92.3\n\n\nä»£ç èƒ½åŠ›\n\n\n\n\n\n\n\n\nLiveCodeBenchï¼ˆ2408-2505ï¼‰ï¼ˆPass@1ï¼‰\n56.4\n43.0\n74.8\n73.3\n\n\n\nCodeforces-Div1ï¼ˆRatingï¼‰\n-\n-\n2091\n1930\n\n\n\nAider-Polyglotï¼ˆå‡†ç¡®ç‡ï¼‰\n68.4\n55.1\n76.3\n71.6\n\n\nä»£ç æ™ºèƒ½ä½“\n\n\n\n\n\n\n\n\nSWE Verifiedï¼ˆæ™ºèƒ½ä½“æ¨¡å¼ï¼‰\n66.0\n45.4\n-\n44.6\n\n\n\nSWE-bench Multilingualï¼ˆæ™ºèƒ½ä½“æ¨¡å¼ï¼‰\n54.5\n29.3\n-\n30.5\n\n\n\nTerminal-benchï¼ˆTerminus 1 æ¡†æ¶ï¼‰\n31.3\n13.3\n-\n5.7\n\n\næ•°å­¦èƒ½åŠ›\n\n\n\n\n\n\n\n\nAIME 2024ï¼ˆPass@1ï¼‰\n66.3\n59.4\n93.1\n91.4\n\n\n\nAIME 2025ï¼ˆPass@1ï¼‰\n49.8\n51.3\n88.4\n87.5\n\n\n\nHMMT 2025ï¼ˆPass@1ï¼‰\n33.5\n29.2\n84.2\n79.4\n\n\n\næ³¨ï¼š\n\næœç´¢æ™ºèƒ½ä½“çš„è¯„ä¼°åŸºäºæˆ‘ä»¬çš„å†…éƒ¨æœç´¢æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å•†ä¸šæœç´¢ API + ç½‘é¡µè¿‡æ»¤å™¨ + 128K ä¸Šä¸‹æ–‡çª—å£ã€‚R1-0528 çš„æœç´¢æ™ºèƒ½ä½“ç»“æœé€šè¿‡é¢„å®šä¹‰å·¥ä½œæµè¿›è¡Œè¯„ä¼°ã€‚\nSWE-bench çš„è¯„ä¼°åŸºäºæˆ‘ä»¬çš„å†…éƒ¨ä»£ç æ™ºèƒ½ä½“æ¡†æ¶ã€‚\nHLE ä»…è¯„ä¼°æ–‡æœ¬å­é›†ã€‚\n\n\n\nä½¿ç”¨ç¤ºä¾‹\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.1\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n    {\"role\": \"assistant\", \"content\": \"<think>Hmm</think>I am DeepSeek\"},\n    {\"role\": \"user\", \"content\": \"1+1=?\"}\n]\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=True, add_generation_prompt=True)\n# '<ï½œbeginâ–ofâ–sentenceï½œ>You are a helpful assistant<ï½œUserï½œ>Who are you?<ï½œAssistantï½œ></think>I am DeepSeek<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>1+1=?<ï½œAssistantï½œ><think>'\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=False, add_generation_prompt=True)\n# '<ï½œbeginâ–ofâ–sentenceï½œ>You are a helpful assistant<ï½œUserï½œ>Who are you?<ï½œAssistantï½œ></think>I am DeepSeek<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>1+1=?<ï½œAssistantï½œ></think>'\n\n\n\nå¦‚ä½•æœ¬åœ°è¿è¡Œ\nDeepSeek-V3.1 çš„æ¨¡å‹ç»“æ„ä¸ DeepSeek-V3 ä¸€è‡´ã€‚æœ‰å…³æœ¬åœ°è¿è¡Œæ­¤æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—® DeepSeek-V3 ä»£ç åº“ã€‚\nä½¿ç”¨å»ºè®®ï¼š\n\nmlp.gate.e_score_correction_bias å‚æ•°åº”é‡‡ç”¨ FP32 ç²¾åº¦åŠ è½½å’Œè®¡ç®—ã€‚\nç¡®ä¿ FP8 æ¨¡å‹æƒé‡å’Œæ¿€æ´»å€¼ä½¿ç”¨ UE8M0 å°ºåº¦æ ¼å¼ã€‚\n\n\n\nè®¸å¯è¯\næœ¬ä»£ç åº“åŠæ¨¡å‹æƒé‡é‡‡ç”¨ MIT è®¸å¯è¯ æˆæƒã€‚\n\n\nå¼•ç”¨\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n\n\n\nè”ç³»æ–¹å¼\nè‹¥æ‚¨æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·æäº¤ issue æˆ–é€šè¿‡ service@deepseek.com ä¸æˆ‘ä»¬è”ç³»ã€‚\n",
    "tags": [
      "Transformers",
      "Safetensors",
      "MIT",
      "arxiv:2412.1943"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/stepfun-ai/NextStep-1-Large",
    "project_name": "stepfun-ai/NextStep-1-Large",
    "readme": "\n\nNextStep-1ï¼šè¿ˆå‘å¤§è§„æ¨¡è¿ç»­ä»¤ç‰Œè‡ªå›å½’å›¾åƒç”Ÿæˆ\nä¸»é¡µÂ \n| GitHubÂ \n| è®ºæ–‡Â \næˆ‘ä»¬æå‡ºNextStep-1ï¼Œè¿™æ˜¯ä¸€ä¸ª140äº¿å‚æ•°çš„è‡ªå›å½’æ¨¡å‹ï¼Œæ­é…1.57äº¿å‚æ•°çš„æµåŒ¹é…å¤´ï¼Œé€šè¿‡ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ç›®æ ‡å¯¹ç¦»æ•£æ–‡æœ¬ä»¤ç‰Œå’Œè¿ç»­å›¾åƒä»¤ç‰Œè¿›è¡Œè®­ç»ƒã€‚\nNextStep-1åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†è‡ªå›å½’æ¨¡å‹çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é«˜ä¿çœŸå›¾åƒåˆæˆèƒ½åŠ›ã€‚\n\n\n\n\n\nç¯å¢ƒè®¾ç½®\nä¸ºé¿å…åŠ è½½å’Œè¿è¡Œæ¨¡å‹æ—¶å¯èƒ½å‡ºç°çš„é”™è¯¯ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ä»¥ä¸‹è®¾ç½®ï¼š\nconda create -n nextstep python=3.11 -y\nconda activate nextstep\n\npip install uv # optional\n\nGIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/stepfun-ai/NextStep-1-Large && cd NextStep-1-Large\nuv pip install -r requirements.txt\n\nhf download stepfun-ai/NextStep-1-Large \"vae/checkpoint.pt\" --local-dir ./\n\n\n\nä½¿ç”¨æ–¹æ³•\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom models.gen_pipeline import NextStepPipeline\n\nHF_HUB = \"stepfun-ai/NextStep-1-Large\"\n\n# load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(HF_HUB, local_files_only=True, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(HF_HUB, local_files_only=True, trust_remote_code=True)\npipeline = NextStepPipeline(tokenizer=tokenizer, model=model).to(device=\"cuda\", dtype=torch.bfloat16)\n\n# set prompts\npositive_prompt = \"masterpiece, film grained, best quality.\"\nnegative_prompt = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry.\"\nexample_prompt = \"A realistic photograph of a wall with \\\"NextStep-1.1 is coming\\\" prominently displayed\"\n\n# generate image from text\nIMG_SIZE = 512\nimage = pipeline.generate_image(\n    example_prompt,\n    hw=(IMG_SIZE, IMG_SIZE),\n    num_images_per_caption=1,\n    positive_prompt=positive_prompt,\n    negative_prompt=negative_prompt,\n    cfg=7.5,\n    cfg_img=1.0,\n    cfg_schedule=\"constant\",\n    use_norm=False,\n    num_sampling_steps=28,\n    timesteps_shift=1.0,\n    seed=3407,\n)[0]\nimage.save(\"./assets/output.jpg\")\n\n\n\nå¼•ç”¨\nå¦‚æœæ‚¨å‘ç°NextStepå¯¹æ‚¨çš„ç ”ç©¶å’Œåº”ç”¨æœ‰æ‰€å¸®åŠ©ï¼Œè¯·è€ƒè™‘ä¸ºè¯¥ä»“åº“ç‚¹äº®æ˜Ÿæ ‡å¹¶å¼•ç”¨ï¼š\n@article{nextstepteam2025nextstep1,\n  title={NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale},\n  author={NextStep Team and Chunrui Han and Guopeng Li and Jingwei Wu and Quan Sun and Yan Cai and Yuang Peng and Zheng Ge and Deyu Zhou and Haomiao Tang and Hongyu Zhou and Kenkun Liu and Ailin Huang and Bin Wang and Changxin Miao and Deshan Sun and En Yu and Fukun Yin and Gang Yu and Hao Nie and Haoran Lv and Hanpeng Hu and Jia Wang and Jian Zhou and Jianjian Sun and Kaijun Tan and Kang An and Kangheng Lin and Liang Zhao and Mei Chen and Peng Xing and Rui Wang and Shiyu Liu and Shutao Xia and Tianhao You and Wei Ji and Xianfang Zeng and Xin Han and Xuelin Zhang and Yana Wei and Yanming Xu and Yimin Jiang and Yingming Wang and Yu Zhou and Yucheng Han and Ziyang Meng and Binxing Jiao and Daxin Jiang and Xiangyu Zhang and Yibo Zhu},\n  journal={arXiv preprint arXiv:2508.10711},\n  year={2025}\n}\n\n",
    "tags": [
      "Text-to-Image",
      "Transformers",
      "Safetensors",
      "Apache License 2.0",
      "arxiv:2508.1071"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/unsloth/grok-2",
    "project_name": "unsloth/grok-2",
    "readme": "\n\nGrok-2 Tokenizer\nA ğŸ¤—-compatible version of the Grok-2 tokenizer (adapted from xai-org/grok-2).\nThis means it can be used with Hugging Face libraries including Transformers,\nTokenizers, and Transformers.js.\n\n\nMotivation\nAs Grok 2.5 aka. xai-org/grok-2 has been recently released on the ğŸ¤— Hub with SGLang\nnative support, but the checkpoints on the Hub won't come with a Hugging Face compatible tokenizer, but rather with a tiktoken-based\nJSON export, which is internally read and patched in SGLang.\nThis repository then contains the Hugging Face compatible export so that users can easily interact and play around with the Grok-2 tokenizer,\nbesides that allowing to use it via SGLang without having to pull the repository manually from the Hub and then using a mount, to prevent from directly having\nto point to the tokenizer path, so that Grok-2 can be deployed as:\npython3 -m sglang.launch_server --model-path xai-org/grok-2 --tokenizer-path alvarobartt/grok-2-tokenizer --tp-size 8 --quantization fp8 --attention-backend triton\n\nRather than the former 2-step process:\nhf download xai-org/grok-2 --local-dir /local/grok-2\n\npython3 -m sglang.launch_server --model-path /local/grok-2 --tokenizer-path /local/grok-2/tokenizer.tok.json --tp-size 8 --quantization fp8 --attention-backend triton\n\n\n\nExample\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"alvarobartt/grok-2-tokenizer\")\n\nassert tokenizer.encode(\"Human: What is Deep Learning?<|separator|>\\n\\n\") == [\n    35406,\n    186,\n    2171,\n    458,\n    17454,\n    14803,\n    191,\n    1,\n    417,\n]\n\nassert (\n    tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}], tokenize=False\n    )\n    == \"Human: What is the capital of France?<|separator|>\\n\\n\"\n)\n\nNoteThis repository has been inspired by earlier similar work by Xenova in Xenova/grok-1-tokenizer.\n\n",
    "tags": [
      "PyTorch",
      "Transformers",
      "Safetensors",
      "Grok",
      "Other",
      "sglang",
      "tokenizers"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/Qwen/Qwen2.5-VL-7B-Instruct",
    "project_name": "Qwen/Qwen2.5-VL-7B-Instruct",
    "readme": "\n\nQwen2.5-VL-7B-Instruct\n\n\n\n\n\nç®€ä»‹\nè‡ªQwen2-VLå‘å¸ƒä»¥æ¥çš„äº”ä¸ªæœˆé‡Œï¼Œä¼—å¤šå¼€å‘è€…åŸºäºå…¶è§†è§‰è¯­è¨€æ¨¡å‹æ„å»ºäº†æ–°æ¨¡å‹ï¼Œå¹¶ä¸ºæˆ‘ä»¬æä¾›äº†å®è´µåé¦ˆã€‚åœ¨æ­¤æœŸé—´ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ‰“é€ æ›´å…·å®ç”¨ä»·å€¼çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´æ¨å‡ºQwenå®¶æ—çš„æœ€æ–°æˆå‘˜ï¼šQwen2.5-VLã€‚\n\n\næ ¸å¿ƒå‡çº§ï¼š\n\n\nè§†è§‰ç†è§£èƒ½åŠ›ï¼šQwen2.5-VLä¸ä»…æ“…é•¿è¯†åˆ«å¸¸è§ç‰©ä½“å¦‚èŠ±é¸Ÿé±¼è™«ï¼Œæ›´èƒ½ç²¾å‡†åˆ†æå›¾åƒä¸­çš„æ–‡æœ¬ã€å›¾è¡¨ã€å›¾æ ‡ã€å›¾å½¢ä¸ç‰ˆå¼å¸ƒå±€ã€‚\n\n\næ™ºèƒ½ä½“åŒ–èƒ½åŠ›ï¼šå¯ç›´æ¥ä½œä¸ºè§†è§‰æ™ºèƒ½ä½“è¿›è¡Œæ¨ç†å’ŒåŠ¨æ€è°ƒç”¨å·¥å…·ï¼Œå®ç°è®¡ç®—æœºä¸æ‰‹æœºæ“ä½œã€‚\n\n\né•¿è§†é¢‘ç†è§£ä¸äº‹ä»¶æ•æ‰ï¼šèƒ½è§£æè¶…è¿‡1å°æ—¶çš„è§†é¢‘å†…å®¹ï¼Œæ–°å¢ç²¾å‡†å®šä½ç›¸å…³ç‰‡æ®µçš„äº‹ä»¶æ•æ‰èƒ½åŠ›ã€‚\n\n\nå¤šæ ¼å¼è§†è§‰å®šä½ï¼šé€šè¿‡ç”Ÿæˆè¾¹ç•Œæ¡†æˆ–åæ ‡ç‚¹ç²¾ç¡®å®šä½å›¾åƒå¯¹è±¡ï¼Œå¹¶è¾“å‡ºç¨³å®šçš„JSONæ ¼å¼åæ ‡ä¸å±æ€§æ•°æ®ã€‚\n\n\nç»“æ„åŒ–è¾“å‡ºèƒ½åŠ›ï¼šé’ˆå¯¹å‘ç¥¨æ‰«æä»¶ã€è¡¨å•ã€è¡¨æ ¼ç­‰æ•°æ®ï¼Œæ”¯æŒå†…å®¹ç»“æ„åŒ–è¾“å‡ºï¼Œä¸ºé‡‘èã€å•†ä¸šç­‰é¢†åŸŸæä¾›ä¾¿åˆ©ã€‚\n\n\n\n\næ¶æ„æ›´æ–°ï¼š\n\nè§†é¢‘ç†è§£çš„åŠ¨æ€åˆ†è¾¨ç‡ä¸å¸§ç‡è®­ç»ƒï¼š\n\né€šè¿‡åŠ¨æ€FPSé‡‡æ ·å°†åŠ¨æ€åˆ†è¾¨ç‡æ‰©å±•è‡³æ—¶é—´ç»´åº¦ï¼Œä½¿æ¨¡å‹èƒ½é€‚åº”ä¸åŒé‡‡æ ·ç‡çš„è§†é¢‘ã€‚åŒæ—¶å‡çº§æ—¶é—´ç»´åº¦çš„mRoPEæŠ€æœ¯ï¼Œç»“åˆIDä¸ç»å¯¹æ—¶é—´å¯¹é½ï¼Œèµ‹äºˆæ¨¡å‹æ—¶åºæ„ŸçŸ¥ä¸é€Ÿåº¦ç†è§£èƒ½åŠ›ï¼Œæœ€ç»ˆå®ç°ç²¾å‡†æ—¶é—´ç‚¹å®šä½ã€‚\n\n\n\n\né«˜æ•ˆç²¾ç®€çš„è§†è§‰ç¼–ç å™¨ï¼š\n\nåœ¨ViTä¸­æ™ºèƒ½åº”ç”¨çª—å£æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ˜¾è‘—æå‡è®­ç»ƒä¸æ¨ç†é€Ÿåº¦ã€‚é€šè¿‡SwiGLUå’ŒRMSNormä¼˜åŒ–ViTæ¶æ„ï¼Œä½¿å…¶ä¸Qwen2.5å¤§è¯­è¨€æ¨¡å‹ç»“æ„ä¿æŒä¸€è‡´ã€‚\næˆ‘ä»¬æä¾›30äº¿ã€70äº¿å’Œ720äº¿å‚æ•°çš„ä¸‰æ¬¾æ¨¡å‹ã€‚æœ¬ä»“åº“åŒ…å«70äº¿å‚æ•°çš„æŒ‡ä»¤è°ƒä¼˜ç‰ˆQwen2.5-VLæ¨¡å‹ã€‚æ›´å¤šè¯¦æƒ…è¯·è®¿é—®åšå®¢å’ŒGitHubã€‚\n\n\næ€§èƒ½è¯„ä¼°\n\n\nå›¾åƒåŸºå‡†æµ‹è¯•\n\n\n\næµ‹è¯•é›†\nInternVL2.5-8B\nMiniCPM-o 2.6\nGPT-4o-mini\nQwen2-VL-7B\nQwen2.5-VL-7B\n\n\n\n\nMMMUval\n56\n50.4\n60\n54.1\n58.6\n\n\nMMMU-Proval\n34.3\n-\n37.6\n30.5\n41.0\n\n\nDocVQAtest\n93\n93\n-\n94.5\n95.7\n\n\nInfoVQAtest\n77.6\n-\n-\n76.5\n82.6\n\n\nChartQAtest\n84.8\n-\n-\n83.0\n87.3\n\n\nTextVQAval\n79.1\n80.1\n-\n84.3\n84.9\n\n\nOCRBench\n822\n852\n785\n845\n864\n\n\nCC_OCR\n57.7\n\n\n61.6\n77.8\n\n\nMMStar\n62.8\n\n\n60.7\n63.9\n\n\nMMBench-V1.1-Entest\n79.4\n78.0\n76.0\n80.7\n82.6\n\n\nMMT-Benchtest\n-\n-\n-\n63.7\n63.6\n\n\nMMStar\n61.5\n57.5\n54.8\n60.7\n63.9\n\n\nMMVetGPT-4-Turbo\n54.2\n60.0\n66.9\n62.0\n67.1\n\n\nHallBenchavg\n45.2\n48.1\n46.1\n50.6\n52.9\n\n\nMathVistatestmini\n58.3\n60.6\n52.4\n58.2\n68.2\n\n\nMathVision\n-\n-\n-\n16.3\n25.07\n\n\n\n\n\nè§†é¢‘åŸºå‡†æµ‹è¯•\n\n\n\næµ‹è¯•é›†\nQwen2-VL-7B\nQwen2.5-VL-7B\n\n\n\n\nMVBench\n67.0\n69.6\n\n\nPerceptionTesttest\n66.9\n70.5\n\n\nVideo-MMEwo/w subs\n63.3/69.0\n65.1/71.6\n\n\nLVBench\n\n45.3\n\n\nLongVideoBench\n\n54.7\n\n\nMMBench-Video\n1.44\n1.79\n\n\nTempCompass\n\n71.7\n\n\nMLVU\n\n70.2\n\n\nCharadesSTA/mIoU\n43.6\n\n\n\n\n\n\næ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•\n\n\n\næµ‹è¯•é›†\nQwen2.5-VL-7B\n\n\n\n\nScreenSpot\n84.7\n\n\nScreenSpot Pro\n29.0\n\n\nAITZ_EM\n81.9\n\n\nAndroid Control High_EM\n60.1\n\n\nAndroid Control Low_EM\n93.7\n\n\nAndroidWorld_SR\n25.5\n\n\nMobileMiniWob++_SR\n91.4\n\n\n\n\n\nç¯å¢ƒè¦æ±‚\nQwen2.5-VLçš„ä»£ç å·²é›†æˆè‡³æœ€æ–°ç‰ˆHugging Face transformersåº“ï¼Œå»ºè®®é€šè¿‡ä»¥ä¸‹å‘½ä»¤ä»æºç æ„å»ºï¼š\npip install git+https://github.com/huggingface/transformers accelerate\n\nå¦åˆ™ï¼Œä½ å¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š\nKeyError: 'qwen2_5_vl'\n\n\n\nå¿«é€Ÿå¼€å§‹\nä»¥ä¸‹æˆ‘ä»¬æä¾›ç®€å•ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ Qwen2.5-VL é…åˆ ğŸ¤– ModelScope å’Œ ğŸ¤— Transformersã€‚\nQwen2.5-VL çš„ä»£ç å·²é›†æˆè‡³æœ€æ–°ç‰ˆ Hugging Face transformersï¼Œå»ºè®®æ‚¨é€šè¿‡ä»¥ä¸‹å‘½ä»¤ä»æºç å®‰è£…ï¼š\npip install git+https://github.com/huggingface/transformers accelerate\n\nå¦åˆ™ï¼Œä½ å¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š\nKeyError: 'qwen2_5_vl'\n\næˆ‘ä»¬æä¾›äº†ä¸€å¥—å·¥å…·åŒ…ï¼ŒåŠ©æ‚¨åƒè°ƒç”¨APIèˆ¬ä¾¿æ·å¤„ç†å„ç±»è§†è§‰è¾“å…¥ã€‚æ— è®ºæ˜¯base64ç¼–ç ã€URLé“¾æ¥ï¼Œè¿˜æ˜¯äº¤é”™æ’åˆ—çš„å›¾ç‰‡ä¸è§†é¢‘ï¼Œéƒ½èƒ½è½»æ¾åº”å¯¹ã€‚æ‚¨å¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…ï¼š\n# It's highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]==0.0.8\n\nå¦‚æœæ‚¨æœªä½¿ç”¨ Linux ç³»ç»Ÿï¼Œå¯èƒ½æ— æ³•é€šè¿‡ PyPI ç›´æ¥å®‰è£… decordã€‚æ­¤æ—¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡ pip install qwen-vl-utils å›é€€åˆ°ä½¿ç”¨ torchvision è¿›è¡Œè§†é¢‘å¤„ç†ã€‚ä¸è¿‡ï¼Œæ‚¨ä»å¯ä»æºç å®‰è£… decord ä»¥å®ç°è§†é¢‘åŠ è½½æ—¶è°ƒç”¨ decord çš„åŠŸèƒ½ã€‚\n\n\nä½¿ç”¨ ğŸ¤— Transformers è¿›è¡Œå¯¹è¯\nä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•ç»“åˆ transformers å’Œ qwen_vl_utils ä½¿ç”¨å¯¹è¯æ¨¡å‹ï¼š\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n\n\nå¤šå›¾åƒæ¨ç†\n# Messages containing multiple images and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"Identify the similarities between these images.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n\n\n\nè§†é¢‘æ¨ç†\n# Messages containing a images list as a video and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": [\n                    \"file:///path/to/frame1.jpg\",\n                    \"file:///path/to/frame2.jpg\",\n                    \"file:///path/to/frame3.jpg\",\n                    \"file:///path/to/frame4.jpg\",\n                ],\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a local video path and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"file:///path/to/video1.mp4\",\n                \"max_pixels\": 360 * 420,\n                \"fps\": 1.0,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a video url and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n#In Qwen 2.5 VL, frame rate information is also input into the model to align with absolute time.\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    fps=fps,\n    padding=True,\n    return_tensors=\"pt\",\n    **video_kwargs,\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n\nè§†é¢‘ URL çš„å…¼å®¹æ€§ä¸»è¦å–å†³äºç¬¬ä¸‰æ–¹åº“ç‰ˆæœ¬ã€‚å…·ä½“ç»†èŠ‚å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚è‹¥æ‚¨ä¸å¸Œæœ›ä½¿ç”¨é»˜è®¤åç«¯ï¼Œå¯é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡ FORCE_QWENVL_VIDEO_READER=torchvision æˆ– FORCE_QWENVL_VIDEO_READER=decord æ¥åˆ‡æ¢åç«¯ã€‚\n\n\n\nåç«¯\nHTTP\nHTTPS\n\n\n\n\ntorchvision >= 0.19.0\nâœ…\nâœ…\n\n\ntorchvision < 0.19.0\nâŒ\nâŒ\n\n\ndecord\nâœ…\nâŒ\n\n\n\n\n\næ‰¹é‡æ¨ç†\n# Sample messages for batch inference\nmessages1 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"What are the common elements in these pictures?\"},\n        ],\n    }\n]\nmessages2 = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n\n# Preparation for batch inference\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\n\n\n\n\nğŸ¤– ModelScope\næˆ‘ä»¬å¼ºçƒˆå»ºè®®ç”¨æˆ·ï¼Œç‰¹åˆ«æ˜¯ä¸­å›½å¤§é™†åœ°åŒºçš„ç”¨æˆ·ä½¿ç”¨ModelScopeã€‚snapshot_downloadåŠŸèƒ½å¯å¸®åŠ©æ‚¨è§£å†³æ¨¡å‹æ£€æŸ¥ç‚¹ä¸‹è½½çš„ç›¸å…³é—®é¢˜ã€‚\n\n\næ›´å¤šä½¿ç”¨æŠ€å·§\nå¯¹äºè¾“å…¥å›¾åƒï¼Œæˆ‘ä»¬æ”¯æŒæœ¬åœ°æ–‡ä»¶ã€base64ç¼–ç åŠURLé“¾æ¥ã€‚è§†é¢‘æ–¹é¢ï¼Œç›®å‰ä»…æ”¯æŒæœ¬åœ°æ–‡ä»¶å¤„ç†ã€‚\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Image URL\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Base64 encoded image\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n\n\nå›¾åƒåˆ†è¾¨ç‡æå‡æ€§èƒ½\nè¯¥æ¨¡å‹æ”¯æŒå¤šç§åˆ†è¾¨ç‡è¾“å…¥ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒä½¿ç”¨åŸç”Ÿåˆ†è¾¨ç‡è¿›è¡Œå¤„ç†ï¼Œä½†æ›´é«˜çš„åˆ†è¾¨ç‡å¯ä»¥æå‡æ€§èƒ½ï¼Œä»£ä»·æ˜¯å¢åŠ è®¡ç®—é‡ã€‚ç”¨æˆ·å¯é€šè¿‡è®¾ç½®åƒç´ èŒƒå›´ï¼ˆä¾‹å¦‚256-1280çš„tokenè®¡æ•°åŒºé—´ï¼‰æ¥ä¼˜åŒ–é…ç½®ï¼Œä»è€Œåœ¨è¿è¡Œé€Ÿåº¦å’Œå†…å­˜å ç”¨ä¹‹é—´å–å¾—å¹³è¡¡ã€‚\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n)\n\næ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸¤ç§æ–¹æ³•ï¼Œç”¨äºç²¾ç»†æ§åˆ¶è¾“å…¥æ¨¡å‹çš„å›¾åƒå°ºå¯¸ï¼š\n\n\nå®šä¹‰æœ€å°åƒç´ å’Œæœ€å¤§åƒç´ ï¼šå›¾åƒå°†æŒ‰æ¯”ä¾‹è°ƒæ•´å¤§å°ï¼Œç¡®ä¿å…¶åƒç´ æ•°ä»‹äºè®¾å®šçš„æœ€å°å€¼å’Œæœ€å¤§å€¼ä¹‹é—´ã€‚\n\n\næŒ‡å®šç²¾ç¡®å°ºå¯¸ï¼šç›´æ¥è®¾ç½® resized_height å’Œ resized_widthã€‚è¿™äº›æ•°å€¼å°†è¢«å››èˆäº”å…¥è‡³æœ€æ¥è¿‘çš„28çš„å€æ•°ã€‚\n\n\n# min_pixels and max_pixels\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"resized_height\": 280,\n                \"resized_width\": 420,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n# resized_height and resized_width\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"min_pixels\": 50176,\n                \"max_pixels\": 50176,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n\n\nå¤„ç†é•¿æ–‡æœ¬\nå½“å‰çš„ config.json è®¾ç½®ä¸ºæ”¯æŒæœ€å¤š 32,768 ä¸ª token çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚\nè‹¥éœ€å¤„ç†è¶…è¿‡ 32,768 ä¸ª token çš„é•¿æ–‡æœ¬è¾“å…¥ï¼Œæˆ‘ä»¬é‡‡ç”¨ YaRN æŠ€æœ¯å¢å¼ºæ¨¡å‹çš„é•¿è·ç¦»å¤–æ¨èƒ½åŠ›ï¼Œç¡®ä¿åœ¨é•¿æ–‡æœ¬ä¸Šçš„æ€§èƒ½è¡¨ç°æœ€ä¼˜ã€‚\nå¯¹äºæ”¯æŒçš„æ¡†æ¶ï¼Œæ‚¨å¯ä»¥åœ¨ config.json ä¸­æ·»åŠ ä»¥ä¸‹é…ç½®ä»¥å¯ç”¨ YaRNï¼š\n{\n\t...,\n    \"type\": \"yarn\",\n    \"mrope_section\": [\n        16,\n        24,\n        24\n    ],\n    \"factor\": 4,\n    \"original_max_position_embeddings\": 32768\n}\n\nä½†éœ€æ³¨æ„ï¼Œè¯¥æ–¹æ³•å¯¹æ—¶åºå’Œç©ºé—´å®šä½ä»»åŠ¡çš„æ€§èƒ½å½±å“è¾ƒå¤§ï¼Œå› æ­¤ä¸å»ºè®®ä½¿ç”¨ã€‚\nåŒæ—¶ï¼Œå¯¹äºé•¿è§†é¢‘è¾“å…¥ï¼Œç”±äº MRoPE æœ¬èº«å¯¹ id çš„æ¶ˆè€—æ›´ç»æµï¼Œå¯ç›´æ¥å°† max_position_embeddings ä¿®æ”¹ä¸ºæ›´å¤§çš„å€¼ï¼ˆä¾‹å¦‚ 64kï¼‰ã€‚\n\n\nå¼•ç”¨\nå¦‚æœæ‚¨è®¤ä¸ºæˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚\n@misc{qwen2.5-VL,\n    title = {Qwen2.5-VL},\n    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n\n",
    "tags": [
      "Image-Text-to-Text",
      "Transformers",
      "Safetensors",
      "English",
      "Apache License 2.0",
      "multimodal",
      "arxiv:2309.0007",
      "arxiv:2409.1219",
      "arxiv:2308.1296"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/jinaai/jina-embeddings-v4",
    "project_name": "jinaai/jina-embeddings-v4",
    "readme": "\n\n\n\n\nç”± Jina AI è®­ç»ƒçš„åµŒå…¥æ¨¡å‹ã€‚\n\n\n\nJina Embeddings v4ï¼šé¢å‘å¤šæ¨¡æ€å¤šè¯­è¨€æ£€ç´¢çš„é€šç”¨åµŒå…¥æ¨¡å‹\nGGUF | åšå®¢ | æŠ€æœ¯æŠ¥å‘Š | API\n\n\né¢„æœŸç”¨é€”ä¸æ¨¡å‹ä¿¡æ¯\njina-embeddings-v4 æ˜¯ä¸€æ¬¾é¢å‘å¤šæ¨¡æ€ä¸å¤šè¯­è¨€æ£€ç´¢çš„é€šç”¨åµŒå…¥æ¨¡å‹ã€‚\nè¯¥æ¨¡å‹ä¸“ä¸ºå¤æ‚æ–‡æ¡£æ£€ç´¢åœºæ™¯è®¾è®¡ï¼Œæ”¯æŒåŒ…å«å›¾è¡¨ã€è¡¨æ ¼å’Œæ’å›¾çš„è§†è§‰ä¸°å¯Œæ–‡æ¡£ã€‚\nåŸºäº Qwen/Qwen2.5-VL-3B-Instruct æ„å»ºçš„ jina-embeddings-v4 å…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š\n\nç»Ÿä¸€åµŒå…¥è¡¨ç¤ºï¼šæ”¯æŒæ–‡æœ¬ã€å›¾åƒåŠè§†è§‰æ–‡æ¡£çš„è”åˆåµŒå…¥ï¼Œå…¼å®¹ç¨ å¯†ï¼ˆå•å‘é‡ï¼‰ä¸å»¶è¿Ÿäº¤äº’ï¼ˆå¤šå‘é‡ï¼‰æ£€ç´¢æ¨¡å¼\nå¤šè¯­è¨€æ”¯æŒï¼šè¦†ç›–30+ç§è¯­è¨€ï¼Œå¹¿æ³›é€‚é…æŠ€æœ¯æ–‡æ¡£ä¸è§†è§‰å¤æ‚æ–‡æ¡£ç­‰å¤šå…ƒåœºæ™¯\nä»»åŠ¡é€‚é…å™¨ï¼šæä¾›æ£€ç´¢ã€æ–‡æœ¬åŒ¹é…åŠä»£ç ç›¸å…³ä»»åŠ¡çš„ä¸“ç”¨é€‚é…å™¨ï¼Œå¯åœ¨æ¨ç†æ—¶åŠ¨æ€é€‰æ‹©\nçµæ´»åµŒå…¥ç»´åº¦ï¼šç¨ å¯†åµŒå…¥é»˜è®¤2048ç»´ï¼Œå¯æ— æŸå‹ç¼©è‡³æœ€ä½128ç»´\n\næ ¸å¿ƒç‰¹æ€§æ¦‚è§ˆï¼š\n\n\n\nç‰¹æ€§\nJina Embeddings V4\n\n\n\n\nåŸºç¡€æ¨¡å‹\nQwen2.5-VL-3B-Instruct\n\n\næ”¯æŒä»»åŠ¡\nretrieval, text-matching, code\n\n\næ¨¡å‹ç²¾åº¦\nBFloat 16\n\n\næœ€å¤§åºåˆ—é•¿åº¦\n32768\n\n\nå•å‘é‡ç»´åº¦\n2048\n\n\nå¤šå‘é‡ç»´åº¦\n128\n\n\nå¥—å¨ƒç»´åº¦\n128, 256, 512, 1024, 2048\n\n\næ± åŒ–ç­–ç•¥\nå‡å€¼æ± åŒ–\n\n\næ³¨æ„åŠ›æœºåˆ¶\nFlashAttention2\n\n\n\n\n\nè®­ç»ƒä¸è¯„ä¼°\nå…³äºè®­ç»ƒç»†èŠ‚å’ŒåŸºå‡†æµ‹è¯•ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„ jina-embeddings-v4 æŠ€æœ¯æŠ¥å‘Šã€‚\n\n\nä½¿ç”¨æ–¹æ³•\n\nç¯å¢ƒè¦æ±‚\néœ€è¦å®‰è£…ä»¥ä¸‹ Python åŒ…ï¼š\n\ntransformers>=4.52.0\ntorch>=2.6.0\npeft>=0.15.2\ntorchvision\npillow\n\n\n\nå¯é€‰/æ¨èç»„ä»¶\n\nflash-attentionï¼šå»ºè®®å®‰è£… flash-attention ä»¥æå‡æ¨ç†é€Ÿåº¦å’Œæ•ˆç‡ï¼Œä½†éå¼ºåˆ¶è¦æ±‚ã€‚\nsentence-transformersï¼šå¦‚éœ€é€šè¿‡ sentence-transformers æ¥å£ä½¿ç”¨æ¨¡å‹ï¼Œè¯·é¢å¤–å®‰è£…æ­¤åŒ…ã€‚\n\n\n\né€šè¿‡ Jina AI Embeddings API ä½¿ç”¨\ncurl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $JINA_AI_API_TOKEN\" \\\n  -d @- <<EOFEOF\n  {\n    \"model\": \"jina-embeddings-v4\",\n    \"task\": \"text-matching\",\n    \"input\": [\n        {\n            \"text\": \"ØºØ±ÙˆØ¨ Ø¬Ù…ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø·Ø¦\"\n        },\n        {\n            \"text\": \"æµ·æ»©ä¸Šç¾ä¸½çš„æ—¥è½\"\n        },\n        {\n            \"text\": \"A beautiful sunset over the beach\"\n        },\n        {\n            \"text\": \"Un beau coucher de soleil sur la plage\"\n        },\n        {\n            \"text\": \"Ein wunderschÃ¶ner Sonnenuntergang am Strand\"\n        },\n        {\n            \"text\": \"ÎˆÎ½Î± ÏŒÎ¼Î¿ÏÏ†Î¿ Î·Î»Î¹Î¿Î²Î±ÏƒÎ¯Î»ÎµÎ¼Î± Ï€Î¬Î½Ï‰ Î±Ï€ÏŒ Ï„Î·Î½ Ï€Î±ÏÎ±Î»Î¯Î±\"\n        },\n        {\n            \"text\": \"à¤¸à¤®à¥à¤¦à¥à¤° à¤¤à¤Ÿ à¤ªà¤° à¤à¤• à¤–à¥‚à¤¬à¤¸à¥‚à¤°à¤¤ à¤¸à¥‚à¤°à¥à¤¯à¤¾à¤¸à¥à¤¤\"\n        },\n        {\n            \"text\": \"Un bellissimo tramonto sulla spiaggia\"\n        },\n        {\n            \"text\": \"æµœè¾ºã«æ²ˆã‚€ç¾ã—ã„å¤•æ—¥\"\n        },\n        {\n            \"text\": \"í•´ë³€ ìœ„ë¡œ ì•„ë¦„ë‹¤ìš´ ì¼ëª°\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/nQNGqL0/beach1.jpg\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/r5w8hG8/beach2.jpg\"\n        }\n    ]\n  }\nEOFEOF\n\n\n\né€šè¿‡ transformers\n# !pip install transformers>=4.52.0 torch>=2.6.0 peft>=0.15.2 torchvision pillow\n# !pip install\nfrom transformers import AutoModel\nimport torch\n\n# Initialize the model\nmodel = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v4\", trust_remote_code=True, torch_dtype=torch.float16)\n\nmodel.to(\"cuda\")\n\n# ========================\n# 1. Retrieval Task\n# ========================\n# Configure truncate_dim, max_length (for texts), max_pixels (for images), vector_type, batch_size in the encode function if needed\n\n# Encode query\nquery_embeddings = model.encode_text(\n    texts=[\"Overview of climate change impacts on coastal cities\"],\n    task=\"retrieval\",\n    prompt_name=\"query\",\n)\n\n# Encode passage (text)\npassage_embeddings = model.encode_text(\n    texts=[\n        \"Climate change has led to rising sea levels, increased frequency of extreme weather events...\"\n    ],\n    task=\"retrieval\",\n    prompt_name=\"passage\",\n)\n\n# Encode image/document\nimage_embeddings = model.encode_image(\n    images=[\"https://i.ibb.co/nQNGqL0/beach1.jpg\"],\n    task=\"retrieval\",\n)\n\n# ========================\n# 2. Text Matching Task\n# ========================\ntexts = [\n    \"ØºØ±ÙˆØ¨ Ø¬Ù…ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø·Ø¦\",  # Arabic\n    \"æµ·æ»©ä¸Šç¾ä¸½çš„æ—¥è½\",  # Chinese\n    \"Un beau coucher de soleil sur la plage\",  # French\n    \"Ein wunderschÃ¶ner Sonnenuntergang am Strand\",  # German\n    \"ÎˆÎ½Î± ÏŒÎ¼Î¿ÏÏ†Î¿ Î·Î»Î¹Î¿Î²Î±ÏƒÎ¯Î»ÎµÎ¼Î± Ï€Î¬Î½Ï‰ Î±Ï€ÏŒ Ï„Î·Î½ Ï€Î±ÏÎ±Î»Î¯Î±\",  # Greek\n    \"à¤¸à¤®à¥à¤¦à¥à¤° à¤¤à¤Ÿ à¤ªà¤° à¤à¤• à¤–à¥‚à¤¬à¤¸à¥‚à¤°à¤¤ à¤¸à¥‚à¤°à¥à¤¯à¤¾à¤¸à¥à¤¤\",  # Hindi\n    \"Un bellissimo tramonto sulla spiaggia\",  # Italian\n    \"æµœè¾ºã«æ²ˆã‚€ç¾ã—ã„å¤•æ—¥\",  # Japanese\n    \"í•´ë³€ ìœ„ë¡œ ì•„ë¦„ë‹¤ìš´ ì¼ëª°\",  # Korean\n]\n\ntext_embeddings = model.encode_text(texts=texts, task=\"text-matching\")\n\n# ========================\n# 3. Code Understanding Task\n# ========================\n\n# Encode query\nquery_embedding = model.encode_text(\n    texts=[\"Find a function that prints a greeting message to the console\"],\n    task=\"code\",\n    prompt_name=\"query\",\n)\n\n# Encode code\ncode_embeddings = model.encode_text(\n    texts=[\"def hello_world():\\n    print('Hello, World!')\"],\n    task=\"code\",\n    prompt_name=\"passage\",\n)\n\n# ========================\n# 4. Use multivectors\n# ========================\n\nmultivector_embeddings = model.encode_text(\n    texts=texts,\n    task=\"retrieval\",\n    prompt_name=\"query\",\n    return_multivector=True,\n)\n\nimages = [\"https://i.ibb.co/nQNGqL0/beach1.jpg\", \"https://i.ibb.co/r5w8hG8/beach2.jpg\"]\nmultivector_image_embeddings = model.encode_image(\n    images=images,\n    task=\"retrieval\",\n    return_multivector=True,\n)\n\n\n\né€šè¿‡ sentence-transformers\nfrom sentence_transformers import SentenceTransformer\n\n# Initialize the model\nmodel = SentenceTransformer(\"jinaai/jina-embeddings-v4\", trust_remote_code=True)\n# ========================\n# 1. Retrieval Task\n# ========================\n# Encode query\nquery_embeddings = model.encode(\n    sentences=[\"Overview of climate change impacts on coastal cities\"],\n    task=\"retrieval\",\n    prompt_name=\"query\",\n)\n\nprint(f\"query_embeddings.shape = {query_embeddings.shape}\")\n\n# Encode passage (text)\npassage_embeddings = model.encode(\n    sentences=[\n        \"Climate change has led to rising sea levels, increased frequency of extreme weather events...\"\n    ],\n    task=\"retrieval\",\n    prompt_name=\"passage\",\n)\n\nprint(f\"passage_embeddings.shape = {passage_embeddings.shape}\")\n\n# Encode image/document\nimage_embeddings = model.encode(\n    sentences=[\"https://i.ibb.co/nQNGqL0/beach1.jpg\"],\n    task=\"retrieval\",\n)\n\nprint(f\"image_embeddings.shape = {image_embeddings.shape}\")\n\n# ========================\n# 2. Text Matching Task\n# ========================\ntexts = [\n    \"ØºØ±ÙˆØ¨ Ø¬Ù…ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø·Ø¦\",  # Arabic\n    \"æµ·æ»©ä¸Šç¾ä¸½çš„æ—¥è½\",  # Chinese\n    \"Un beau coucher de soleil sur la plage\",  # French\n    \"Ein wunderschÃ¶ner Sonnenuntergang am Strand\",  # German\n    \"ÎˆÎ½Î± ÏŒÎ¼Î¿ÏÏ†Î¿ Î·Î»Î¹Î¿Î²Î±ÏƒÎ¯Î»ÎµÎ¼Î± Ï€Î¬Î½Ï‰ Î±Ï€ÏŒ Ï„Î·Î½ Ï€Î±ÏÎ±Î»Î¯Î±\",  # Greek\n    \"à¤¸à¤®à¥à¤¦à¥à¤° à¤¤à¤Ÿ à¤ªà¤° à¤à¤• à¤–à¥‚à¤¬à¤¸à¥‚à¤°à¤¤ à¤¸à¥‚à¤°à¥à¤¯à¤¾à¤¸à¥à¤¤\",  # Hindi\n    \"Un bellissimo tramonto sulla spiaggia\",  # Italian\n    \"æµœè¾ºã«æ²ˆã‚€ç¾ã—ã„å¤•æ—¥\",  # Japanese\n    \"í•´ë³€ ìœ„ë¡œ ì•„ë¦„ë‹¤ìš´ ì¼ëª°\",  # Korean\n]\n\ntext_embeddings = model.encode(sentences=texts, task=\"text-matching\")\n\n# ========================\n# 3. Code Understanding Task\n# ========================\n\n# Encode query\nquery_embeddings = model.encode(\n    sentences=[\"Find a function that prints a greeting message to the console\"],\n    task=\"code\",\n    prompt_name=\"query\",\n)\n\n# Encode code\ncode_embeddings = model.encode(\n    sentences=[\"def hello_world():\\n    print('Hello, World!')\"],\n    task=\"code\",\n    prompt_name=\"passage\",\n)\n\n# ========================\n# 4. Use multivectors\n# ========================\n# If you want to use multi-vector embeddings, please use the Hugging Face model directly.\n\n\n\né€šè¿‡ vLLM\næˆ‘ä»¬ä¸ºä¸åŒä»»åŠ¡ï¼ˆretrievalæ£€ç´¢ã€text-matchingæ–‡æœ¬åŒ¹é…ã€codeä»£ç ï¼‰åˆ†åˆ«æä¾›äº†é€‚é…å™¨å·²åˆå¹¶è‡³åŸºç¡€Qwen2.5-VLæƒé‡çš„ç‹¬ç«‹æ¨¡å‹ç‰ˆæœ¬ã€‚\nè¿™ä¸€æ”¹è¿›ä½¿å…¶åŸç”Ÿå…¼å®¹vLLMæ¨ç†æ¡†æ¶ã€‚\nå„ä»»åŠ¡çš„ä½¿ç”¨è¯´æ˜å’Œç¤ºä¾‹è¯¦è§å¯¹åº”ç›®å½•ï¼š\n\njina-embeddings-v4-vllm-retrieval\njina-embeddings-v4-vllm-text-matching\njina-embeddings-v4-vllm-code\n\nè¯·æ ¹æ®æ‚¨çš„å…·ä½“ä»»åŠ¡é€‰æ‹©å¯¹åº”ç›®å½•æŸ¥çœ‹è¯¦æƒ…ã€‚\n\n\n\nJina-VDR\nä¼´éšjina-embeddings-v4çš„å‘å¸ƒï¼Œæˆ‘ä»¬åŒæ­¥æ¨å‡ºJina VDRâ€”â€”ä¸€ä¸ªå¤šè¯­è¨€ã€å¤šé¢†åŸŸçš„è§†è§‰æ–‡æ¡£æ£€ç´¢åŸºå‡†æµ‹è¯•é›†ã€‚ä»»åŠ¡é›†å¯åœ¨æ­¤æŸ¥çœ‹ï¼Œè¯„ä¼°æŒ‡å—è¯·å‚é˜…æ­¤å¤„ã€‚\n\n\nè®¸å¯è¯\næœ¬æ¨¡å‹éµå¾ªCC BY-NC 4.0åè®®å¼€æ”¾ä¸‹è½½ä¸è¿è¡Œã€‚å•†ä¸šä½¿ç”¨å¯é€šè¿‡Jina Embeddings APIã€AWSã€AzureåŠGCPå®ç°ã€‚å¦‚éœ€å•†ä¸šç”¨é€”ä¸‹è½½ï¼Œè¯·è”ç³»æˆ‘ä»¬ã€‚\n\n\nè”ç³»æˆ‘ä»¬\næ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„Discordç¤¾åŒºï¼Œä¸å…¶ä»–ç¤¾åŒºæˆå‘˜äº¤æµåˆ›æ„ã€‚\n\n\nå¼•ç”¨\nå¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†jina-embeddings-v4ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼š\n@misc{gÃ¼nther2025jinaembeddingsv4universalembeddingsmultimodal,\n      title={jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval}, \n      author={Michael GÃ¼nther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Sedigheh Eslami and Scott Martens and Bo Wang and Nan Wang and Han Xiao},\n      year={2025},\n      eprint={2506.18902},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2506.18902}, \n}\n\n",
    "tags": [
      "Visual Document Retrieval",
      "Transformers",
      "PEFT",
      "Safetensors",
      "sentence-transformers",
      "ColPali",
      "multilingual",
      "Creative Commons Attribution Non Commercial 4.0",
      "multimodal-embedding",
      "mteb",
      "vidore",
      "multilingual-embedding",
      "feature-extraction",
      "sentence-similarity",
      "Text-to-Visual Document (Tâ†’VD) retrieval"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "project_name": "mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "readme": "\n\nMistral-Small-3.2-24B-Instruct-2506\nMistral-Small-3.2-24B-Instruct-2506 æ˜¯ Mistral-Small-3.1-24B-Instruct-2503 çš„å°å¹…æ›´æ–°ç‰ˆæœ¬ã€‚\nSmall-3.2 åœ¨ä»¥ä¸‹æ–¹é¢æœ‰æ‰€æ”¹è¿›ï¼š\n\næŒ‡ä»¤éµå¾ªï¼šSmall-3.2 æ›´æ“…é•¿éµå¾ªç²¾ç¡®æŒ‡ä»¤\né‡å¤é”™è¯¯ï¼šSmall-3.2 å‡å°‘äº†æ— é™ç”Ÿæˆæˆ–é‡å¤å›ç­”çš„æƒ…å†µ\nå‡½æ•°è°ƒç”¨ï¼šSmall-3.2 çš„å‡½æ•°è°ƒç”¨æ¨¡æ¿æ›´åŠ ç¨³å¥ï¼ˆå‚è§ æ­¤å¤„ å’Œ ç¤ºä¾‹ï¼‰\n\nåœ¨æ‰€æœ‰å…¶ä»–æ–¹é¢ï¼ŒSmall-3.2 åº”ä¸ Mistral-Small-3.1-24B-Instruct-2503 ç›¸å½“æˆ–ç•¥æœ‰æå‡ã€‚\n\n\nä¸»è¦ç‰¹æ€§\n\nä¸ Mistral-Small-3.1-24B-Instruct-2503 ç›¸åŒ\n\n\n\nåŸºå‡†æµ‹è¯•ç»“æœ\næˆ‘ä»¬å°† Mistral-Small-3.2-24B ä¸ Mistral-Small-3.1-24B-Instruct-2503 è¿›è¡Œå¯¹æ¯”ã€‚\nå¦‚éœ€ä¸å…¶ä»–ç±»ä¼¼è§„æ¨¡æ¨¡å‹çš„æ›´å¤šå¯¹æ¯”ï¼Œè¯·æŸ¥çœ‹ Mistral-Small-3.1 çš„åŸºå‡†æµ‹è¯•\n\n\næ–‡æœ¬\n\n\næŒ‡ä»¤éµå¾ª / å¯¹è¯ / è¯­æ°”\n\n\n\næ¨¡å‹\nWildbench v2\nArena Hard v2\nIFï¼ˆå†…éƒ¨ï¼›å‡†ç¡®ç‡ï¼‰\n\n\n\n\nSmall 3.1 24B Instruct\n55.6%\n19.56%\n82.75%\n\n\nSmall 3.2 24B Instruct\n65.33%\n43.1%\n84.78%\n\n\n\n\n\næ— é™ç”Ÿæˆ\nåœ¨å…·æœ‰æŒ‘æˆ˜æ€§ã€å†—é•¿ä¸”é‡å¤çš„æç¤ºä¸‹ï¼ŒSmall 3.2 å°†æ— é™ç”Ÿæˆæƒ…å†µå‡å°‘äº† 2 å€ã€‚\n\n\n\næ¨¡å‹\næ— é™ç”Ÿæˆï¼ˆå†…éƒ¨ï¼›æ•°å€¼è¶Šä½è¶Šå¥½ï¼‰\n\n\n\n\nSmall 3.1 24B Instruct\n2.11%\n\n\nSmall 3.2 24B Instruct\n1.29%\n\n\n\n\n\nSTEM\n\n\n\næ¨¡å‹\nMMLU\nMMLU Proï¼ˆ5-shot CoTï¼‰\nMATH\nGPQA Mainï¼ˆ5-shot CoTï¼‰\nGPQA Diamondï¼ˆ5-shot CoTï¼‰\nMBPP Plus - Pass@5\nHumanEval Plus - Pass@5\nSimpleQAï¼ˆTotalAccï¼‰\n\n\n\n\nSmall 3.1 24B Instruct\n80.62%\n66.76%\n69.30%\n44.42%\n45.96%\n74.63%\n88.99%\n10.43%\n\n\nSmall 3.2 24B Instruct\n80.50%\n69.06%\n69.42%\n44.22%\n46.13%\n78.33%\n92.90%\n12.10%\n\n\n\n\n\næ„¿æ™¯\n\n\n\næ¨¡å‹\nMMMU\nMathvista\nChartQA\nDocVQA\nAI2D\n\n\n\n\nSmall 3.1 24B Instruct\n64.00%\n68.91%\n86.24%\n94.08%\n93.72%\n\n\nSmall 3.2 24B Instruct\n62.50%\n67.09%\n87.4%\n94.86%\n92.91%\n\n\n\n\n\nä½¿ç”¨æ–¹æ³•\nè¯¥æ¨¡å‹å¯é€šè¿‡ä»¥ä¸‹æ¡†æ¶ä½¿ç”¨ï¼š\n\nvllmï¼ˆæ¨èï¼‰ï¼šè¯¦è§æ­¤å¤„\ntransformersï¼šè¯¦è§æ­¤å¤„\n\næ³¨æ„ 1ï¼šå»ºè®®ä½¿ç”¨ç›¸å¯¹è¾ƒä½çš„æ¸©åº¦å‚æ•°ï¼Œä¾‹å¦‚ temperature=0.15ã€‚\næ³¨æ„ 2ï¼šè¯·åŠ¡å¿…ä¸ºæ¨¡å‹æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼Œä»¥ä½¿å…¶æ›´å¥½åœ°æ»¡è¶³æ‚¨çš„éœ€æ±‚ã€‚å¦‚æœæ‚¨å¸Œæœ›å°†è¯¥æ¨¡å‹ç”¨ä½œé€šç”¨åŠ©æ‰‹ï¼Œå»ºè®®ä½¿ç”¨ SYSTEM_PROMPT.txt æ–‡ä»¶ä¸­æä¾›çš„ç³»ç»Ÿæç¤ºè¯ã€‚\n\n\nvLLMï¼ˆæ¨èï¼‰\næˆ‘ä»¬å»ºè®®å°†æ­¤æ¨¡å‹ä¸ vLLM ç»“åˆä½¿ç”¨ã€‚\n\n\nå®‰è£…\nè¯·ç¡®ä¿å®‰è£… vLLM >= 0.9.1 ç‰ˆæœ¬ï¼š\npip install vllm --upgrade\n\næ‰§è¡Œæ­¤æ“ä½œåº”ä¼šè‡ªåŠ¨å®‰è£… mistral_common >= 1.6.2ã€‚\næ£€æŸ¥æ–¹æ³•ï¼š\npython -c \"import mistral_common; print(mistral_common.__version__)\"\n\næ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ç°æˆçš„ docker image æˆ– docker hub ä¸Šçš„é•œåƒã€‚\n\n\næœåŠ¡éƒ¨ç½²\næˆ‘ä»¬å»ºè®®æ‚¨åœ¨æœåŠ¡å™¨/å®¢æˆ·ç«¯ç¯å¢ƒä¸­ä½¿ç”¨ Mistral-Small-3.2-24B-Instruct-2506ã€‚\n\nå¯åŠ¨æœåŠ¡å™¨ï¼š\n\nvllm serve mistralai/Mistral-Small-3.2-24B-Instruct-2506 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --limit_mm_per_prompt 'image=10' --tensor-parallel-size 2\n\næ³¨æ„ï¼š åœ¨ GPU ä¸Šè¿è¡Œ Mistral-Small-3.2-24B-Instruct-2506 æ—¶ï¼Œé‡‡ç”¨ bf16 æˆ– fp16 ç²¾åº¦éœ€è¦çº¦ 55 GB çš„ GPU å†…å­˜ã€‚\n\nè‹¥è¦å¯¹å®¢æˆ·ç«¯æ‰§è¡Œ ping æ“ä½œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸€æ®µç®€å•çš„ Python ä»£ç ç‰‡æ®µã€‚è¯·å‚è§ä»¥ä¸‹ç¤ºä¾‹ã€‚\n\n\n\nè§†è§‰æ¨ç†\nåˆ©ç”¨ Mistral-Small-3.2-24B-Instruct-2506 çš„è§†è§‰åŠŸèƒ½ï¼Œåœ¨ç»™å®šåœºæ™¯ä¸‹åšå‡ºæœ€ä½³é€‰æ‹©ï¼Œå°†å®ƒä»¬å…¨éƒ¨æŒæ¡ï¼\n\nPython ä»£ç ç‰‡æ®µ\n\nfrom datetime import datetime, timedelta\n\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime(\"%Y-%m-%d\")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    model_name = repo_id.split(\"/\")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n            },\n            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n        ],\n    },\n]\n\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n)\n\nprint(response.choices[0].message.content)\n# In this situation, you are playing a PokÃ©mon game where your Pikachu (Level 42) is facing a wild Pidgey (Level 17). Here are the possible actions you can take and an analysis of each:\n\n# 1. **FIGHT**:\n#    - **Pros**: Pikachu is significantly higher level than the wild Pidgey, which suggests that it should be able to defeat Pidgey easily. This could be a good opportunity to gain experience points and possibly items or money.\n#    - **Cons**: There is always a small risk of Pikachu fainting, especially if Pidgey has a powerful move or a status effect that could hinder Pikachu. However, given the large level difference, this risk is minimal.\n\n# 2. **BAG**:\n#    - **Pros**: You might have items in your bag that could help in this battle, such as Potions, PokÃ© Balls, or Berries. Using an item could help you capture the Pidgey or heal your Pikachu if needed.\n#    - **Cons**: Using items might not be necessary given the level difference. It could be more efficient to just fight and defeat the Pidgey quickly.\n\n# 3. **POKÃ‰MON**:\n#    - **Pros**: You might have another PokÃ©mon in your party that is better suited for this battle or that you want to gain experience. Switching PokÃ©mon could also be a strategic move if you want to train a lower-level PokÃ©mon.\n#    - **Cons**: Switching PokÃ©mon might not be necessary since Pikachu is at a significant advantage. It could also waste time and potentially give Pidgey a turn to attack.\n\n# 4. **RUN**:\n#    - **Pros**: Running away could save time and conserve your PokÃ©mon's health and resources. If you are in a hurry or do not need the experience or items, running away is a safe option.\n#    - **Cons**: Running away means you miss out on the experience points and potential items or money that you could gain from defeating the Pidgey. It also means you do not get the chance to capture the Pidgey if you wanted to.\n\n# ### Recommendation:\n# Given the significant level advantage, the best action is likely to **FIGHT**. This will allow you to quickly defeat the Pidgey, gain experience points, and potentially earn items or money. If you are concerned about Pikachu's health, you could use an item from your **BAG** to heal it before or during the battle. Running away or switching PokÃ©mon does not seem necessary in this situation.\n\n\n\nå‡½æ•°è°ƒç”¨\nMistral-Small-3.2-24B-Instruct-2506 å€ŸåŠ© vLLMï¼Œåœ¨å‡½æ•°/å·¥å…·è°ƒç”¨ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¾‹å¦‚ï¼š\n\nPython ä»£ç ç‰‡æ®µ - ç®€å•\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    return system_prompt\n\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\n\nimage_url = \"https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/europe.png\"\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_population\",\n            \"description\": \"Get the up-to-date population of a given country.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"country\": {\n                        \"type\": \"string\",\n                        \"description\": \"The country to find the population of.\",\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unit for the population.\",\n                        \"enum\": [\"millions\", \"thousands\"],\n                    },\n                },\n                \"required\": [\"country\", \"unit\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"rewrite\",\n            \"description\": \"Rewrite a given text for improved clarity\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"text\": {\n                        \"type\": \"string\",\n                        \"description\": \"The input text to rewrite\",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": \"Could you please make the below article more concise?\\n\\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"id\": \"bbc5b7ede\",\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"rewrite\",\n                    \"arguments\": '{\"text\": \"OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\"}',\n                },\n            }\n        ],\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": '{\"action\":\"rewrite\",\"outcome\":\"OpenAI is a FOR-profit company.\"}',\n        \"tool_call_id\": \"bbc5b7ede\",\n        \"name\": \"rewrite\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"---\\n\\nOpenAI is a FOR-profit company.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Can you tell me what is the biggest country depicted on the map?\",\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": image_url,\n                },\n            },\n        ],\n    }\n]\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n    tools=tools,\n    tool_choice=\"auto\",\n)\n\nassistant_message = response.choices[0].message.content\nprint(assistant_message)\n# The biggest country depicted on the map is Russia.\n\nmessages.extend([\n    {\"role\": \"assistant\", \"content\": assistant_message},\n    {\"role\": \"user\", \"content\": \"What is the population of that country in millions?\"},\n])\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n    tools=tools,\n    tool_choice=\"auto\",\n)\n\nprint(response.choices[0].message.tool_calls)\n# [ChatCompletionMessageToolCall(id='3e92V6Vfo', function=Function(arguments='{\"country\": \"Russia\", \"unit\": \"millions\"}', name='get_current_population'), type='function')]\n\n\n\nPython ä»£ç ç‰‡æ®µ - å¤æ‚\n\nimport json\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    return system_prompt\n\n\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\n\nimage_url = \"https://math-coaching.com/img/fiche/46/expressions-mathematiques.jpg\"\n\n\ndef my_calculator(expression: str) -> str:\n    return str(eval(expression))\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"my_calculator\",\n            \"description\": \"A calculator that can evaluate a mathematical expression.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"expression\": {\n                        \"type\": \"string\",\n                        \"description\": \"The mathematical expression to evaluate.\",\n                    },\n                },\n                \"required\": [\"expression\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"rewrite\",\n            \"description\": \"Rewrite a given text for improved clarity\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"text\": {\n                        \"type\": \"string\",\n                        \"description\": \"The input text to rewrite\",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Can you calculate the results for all the equations displayed in the image? Only compute the ones that involve numbers.\",\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": image_url,\n                },\n            },\n        ],\n    },\n]\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n    tools=tools,\n    tool_choice=\"auto\",\n)\n\ntool_calls = response.choices[0].message.tool_calls\nprint(tool_calls)\n# [ChatCompletionMessageToolCall(id='CyQBSAtGh', function=Function(arguments='{\"expression\": \"6 + 2 * 3\"}', name='my_calculator'), type='function'), ChatCompletionMessageToolCall(id='KQqRCqvzc', function=Function(arguments='{\"expression\": \"19 - (8 + 2) + 1\"}', name='my_calculator'), type='function')]\n\nresults = []\nfor tool_call in tool_calls:\n    function_name = tool_call.function.name\n    function_args = tool_call.function.arguments\n    if function_name == \"my_calculator\":\n        result = my_calculator(**json.loads(function_args))\n        results.append(result)\n\nmessages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\nfor tool_call, result in zip(tool_calls, results):\n    messages.append(\n        {\n            \"role\": \"tool\",\n            \"tool_call_id\": tool_call.id,\n            \"name\": tool_call.function.name,\n            \"content\": result,\n        }\n    )\n\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n)\n\nprint(response.choices[0].message.content)\n# Here are the results for the equations that involve numbers:\n\n# 1. \\( 6 + 2 \\times 3 = 12 \\)\n# 3. \\( 19 - (8 + 2) + 1 = 10 \\)\n\n# For the other equations, you need to substitute the variables with specific values to compute the results.\n\n\n\næŒ‡ä»¤éµå¾ª\nMistral-Small-3.2-24B-Instruct-2506 å°†ä¸¥æ ¼æŒ‰ç…§æ‚¨çš„æŒ‡ä»¤æ‰§è¡Œï¼Œç¡®ä¿åˆ†æ¯«ä¸å·®ï¼\n\nPython ä»£ç ç‰‡æ®µ\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    return system_prompt\n\n\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": \"Write me a sentence where every word starts with the next letter in the alphabet - start with 'a' and end with 'z'.\",\n    },\n]\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n)\n\nassistant_message = response.choices[0].message.content\nprint(assistant_message)\n\n# Here's a sentence where each word starts with the next letter of the alphabet, starting from 'a' and ending with 'z':\n\n# \"Always brave cats dance elegantly, fluffy giraffes happily ignore jungle kites, lovingly munching nuts, observing playful quails racing swiftly, tiny unicorns vaulting while xylophones yodel zealously.\"\n\n# This sentence follows the sequence from A to Z without skipping any letters.\n\n\n\n\nTransformers\næ‚¨ä¹Ÿå¯ä»¥æ­é… Transformers ä½¿ç”¨ Mistral-Small-3.2-24B-Instruct-2506ï¼\nä¸ºäº†å……åˆ†å‘æŒ¥æ¨¡å‹åœ¨ Transformers ä¸­çš„æ€§èƒ½ï¼Œè¯·ç¡®ä¿å·²å®‰è£… mistral-common >= 1.6.2 ä»¥ä½¿ç”¨æˆ‘ä»¬çš„åˆ†è¯å™¨ã€‚\npip install mistral-common --upgrade\n\nç„¶ååŠ è½½æˆ‘ä»¬çš„åˆ†è¯å™¨å’Œæ¨¡å‹å¹¶è¿›è¡Œç”Ÿæˆï¼š\n\nPython ä»£ç ç‰‡æ®µ\n\nfrom datetime import datetime, timedelta\nimport torch\n\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import Mistral3ForConditionalGeneration\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime(\"%Y-%m-%d\")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    model_name = repo_id.split(\"/\")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\n\ntokenizer = MistralTokenizer.from_hf_hub(model_id)\n\nmodel = Mistral3ForConditionalGeneration.from_pretrained(\n    model_id, torch_dtype=torch.bfloat16\n)\n\nimage_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n            },\n            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n        ],\n    },\n]\n\ntokenized = tokenizer.encode_chat_completion(ChatCompletionRequest(messages=messages))\n\ninput_ids = torch.tensor([tokenized.tokens])\nattention_mask = torch.ones_like(input_ids)\npixel_values = torch.tensor(tokenized.images[0], dtype=torch.bfloat16).unsqueeze(0)\nimage_sizes = torch.tensor([pixel_values.shape[-2:]])\n\noutput = model.generate(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    pixel_values=pixel_values,\n    image_sizes=image_sizes,\n    max_new_tokens=1000,\n)[0]\n\ndecoded_output = tokenizer.decode(output[len(tokenized.tokens) :])\nprint(decoded_output)\n# In this situation, you are playing a PokÃ©mon game where your Pikachu (Level 42) is facing a wild Pidgey (Level 17). Here are the possible actions you can take and an analysis of each:\n\n# 1. **FIGHT**:\n#    - **Pros**: Pikachu is significantly higher level than the wild Pidgey, which suggests that it should be able to defeat Pidgey easily. This could be a good opportunity to gain experience points and possibly items or money.\n#    - **Cons**: There is always a small risk of Pikachu fainting, especially if Pidgey has a powerful move or a status effect that could hinder Pikachu. However, given the large level difference, this risk is minimal.\n\n# 2. **BAG**:\n#    - **Pros**: You might have items in your bag that could help in this battle, such as Potions, PokÃ© Balls, or Berries. Using an item could help you capture Pidgey or heal Pikachu if needed.\n#    - **Cons**: Using items might not be necessary given the level difference. It could be more efficient to just fight and defeat Pidgey quickly.\n\n# 3. **POKÃ‰MON**:\n#    - **Pros**: You might have another PokÃ©mon in your party that is better suited for this battle or that you want to gain experience. Switching PokÃ©mon could also be strategic if you want to train a lower-level PokÃ©mon.\n#    - **Cons**: Switching PokÃ©mon might not be necessary since Pikachu is at a significant advantage. It could also waste time and potentially give Pidgey a turn to attack.\n\n# 4. **RUN**:\n#    - **Pros**: Running away could be a quick way to avoid the battle altogether. This might be useful if you are trying to conserve resources or if you are in a hurry to get to another location.\n#    - **Cons**: Running away means you miss out on the experience points, items, or money that you could gain from defeating Pidgey. It also might not be the most efficient use of your time if you are trying to train your PokÃ©mon.\n\n# ### Recommendation:\n# Given the significant level advantage, the best action to take is likely **FIGHT**. This will allow you to quickly defeat Pidgey and gain experience points for Pikachu. If you are concerned about Pikachu's health, you could use the **BAG** to heal Pikachu before or during the battle. Running away or switching PokÃ©mon does not seem necessary in this situation.\n\n",
    "tags": [
      "Transformers",
      "Safetensors",
      "24 languages",
      "Apache License 2.0",
      "mistral-common"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/t-tech/T-one",
    "project_name": "t-tech/T-one",
    "readme": "\n\nT-one: é¢å‘ä¿„è¯­ç”µè¯é¢†åŸŸçš„æµå¼è¯­éŸ³è¯†åˆ«\nğŸš€ T-one æ˜¯ä¸€æ¬¾é«˜æ€§èƒ½ä¿„è¯­æµå¼è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æµæ°´çº¿ï¼Œä¸“ä¸ºç”µè¯é¢†åŸŸä¼˜åŒ–ã€‚\nT-one æä¾›å®Œæ•´çš„ä½å»¶è¿Ÿå®æ—¶è½¬å½•è§£å†³æ–¹æ¡ˆã€‚å®ƒå…·å¤‡é¢„è®­ç»ƒçš„æµå¼ Conformer å£°å­¦æ¨¡å‹ã€å®šåˆ¶çš„è¯­å¥è¾¹ç•Œæ£€æµ‹å™¨å’Œè§£ç å™¨ï¼Œæ˜¯ä¸€å¥—å¯ç›´æ¥ç”¨äºç”Ÿäº§ç¯å¢ƒçš„å³ç”¨å‹æ–¹æ¡ˆã€‚é™¤é¢„è®­ç»ƒæ¨¡å‹å¤–ï¼ŒT-one è¿˜æä¾›å…¨å¥—æ¨ç†ã€å¾®è°ƒåŠéƒ¨ç½²å·¥å…·ã€‚\nè¯¥é¡¹ç›®ç”± T-Software DC å¼€å‘ï¼Œæ˜¯ä¸€å¥—å®ç”¨çš„ä½å»¶è¿Ÿã€é«˜ååé‡ ASR è§£å†³æ–¹æ¡ˆï¼Œç»„ä»¶æ¨¡å—åŒ–ã€‚\næ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è§ GitHub ä»“åº“ã€‚\n\n\nç›®å½•\n\né¡¹ç›®æ¦‚è¿°\nè´¨é‡åŸºå‡†\næ¨ç†ç¤ºä¾‹\nå¾®è°ƒ\nå£°å­¦æ¨¡å‹\nè®­ç»ƒè¯¦æƒ…\nè®¸å¯è¯\n\n\n\nğŸ“ é¡¹ç›®æ¦‚è¿°\næ ¸å¿ƒç‰¹æ€§ï¼š\n\næµå¼ä¼˜å…ˆæ¶æ„ï¼š ä¸“ä¸ºä½å»¶è¿Ÿã€å®æ—¶åº”ç”¨æ„å»ºã€‚\nå³ç”¨å‹æµæ°´çº¿ï¼š åŒ…å«é¢„è®­ç»ƒå£°å­¦æ¨¡å‹ã€è¯­å¥åˆ†å‰²å™¨ï¼Œä»¥åŠåŸºäº KenLM çš„ CTC æŸæœç´¢è§£ç å™¨ï¼Œå¹¶æä¾›ç¦»çº¿å’Œæµå¼è¯­éŸ³è¯†åˆ«æ¨ç†ç¤ºä¾‹ã€‚\næ¼”ç¤º â€” é€šè¿‡ Docker ç«‹å³å¯åŠ¨æœ¬åœ°è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼Œè½¬å½•éŸ³é¢‘æ–‡ä»¶æˆ–å®æ—¶éº¦å…‹é£è¾“å…¥ã€‚\nå¾®è°ƒï¼šåˆ©ç”¨ ğŸ¤— ç”Ÿæ€ç³»ç»Ÿï¼Œå¯è½»æ¾åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒ T-oneã€‚\néƒ¨ç½²ä¾¿æ·ï¼š åŒ…å«ä½¿ç”¨ Triton Inference Server è¿›è¡Œé«˜ååé‡åœºæ™¯éƒ¨ç½²çš„ç¤ºä¾‹ã€‚\nå®Œå…¨å¼€æºæ¶æ„ï¼š æ‰€æœ‰æ¨¡å‹å’Œæµæ°´çº¿ä»£ç å‡å¼€æ”¾å¯ç”¨ã€‚\n\n\n\nğŸ“Š è´¨é‡åŸºå‡†\nè¯é”™è¯¯ç‡ (WER) ç”¨äºè¯„ä¼°è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„è´¨é‡ï¼Œå¯ç†è§£ä¸ºä¸å‚è€ƒè½¬å½•æ–‡æœ¬ç›¸æ¯”ï¼Œé”™è¯¯è¯†åˆ«å•è¯æ‰€å çš„ç™¾åˆ†æ¯”ã€‚æ•°å€¼è¶Šä½ï¼Œè¡¨ç¤ºå‡†ç¡®ç‡è¶Šé«˜ã€‚T-one å±•ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å…¶ç›®æ ‡é¢†åŸŸç”µè¯è¯­éŸ³ä¸Šè¡¨ç°å“è¶Šï¼ŒåŒæ—¶åœ¨é€šç”¨åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿä¿æŒç«äº‰åŠ›ã€‚\n\n\n\nç±»åˆ«\nT-one (71M)\nGigaAM-RNNT v2 (243M)\nGigaAM-CTC v2 (242M)\nVosk-model-ru 0.54 (65M)\nVosk-model-small-streaming-ru 0.54 (20M)\nWhisper large-v3 (1540M)\n\n\n\n\nå‘¼å«ä¸­å¿ƒ\n8.63\n10.22\n10.57\n11.28\n15.53\n19.39\n\n\nå…¶ä»–ç”µè¯è¯­éŸ³\n6.20\n7.88\n8.15\n8.69\n13.49\n17.29\n\n\nå‘½åå®ä½“\n5.83\n9.55\n9.81\n12.12\n17.65\n17.87\n\n\nCommonVoice 19ï¼ˆæµ‹è¯•é›†ï¼‰\n5.32\n2.68\n3.14\n6.22\n11.3\n5.78\n\n\nOpenSTT asr_calls_2_val åŸå§‹ç‰ˆ\n20.27\n20.07\n21.24\n22.64\n29.45\n29.02\n\n\nOpenSTT asr_calls_2_val é‡æ ‡æ³¨ç‰ˆ\n7.94\n11.14\n12.43\n13.22\n21.03\n20.82\n\n\n\n\n\nğŸ‘¨â€ğŸ’» æ¨ç†ç¤ºä¾‹\n\n\nç¦»çº¿æ¨ç†ï¼ˆé€‚ç”¨äºå®Œæ•´éŸ³é¢‘æ–‡ä»¶ï¼‰\nfrom tone import StreamingCTCPipeline, read_audio, read_example_audio\n\n\naudio = read_example_audio() # or read_audio(\"your_audio.flac\")\n\npipeline = StreamingCTCPipeline.from_hugging_face()\nprint(pipeline.forward_offline(audio))  # run offline recognition\n\nè¾“å‡ºï¼š\n[TextPhrase(text='Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚', start_time=1.79, end_time=2.04), TextPhrase(text='ÑÑ‚Ğ¾ Ñ', start_time=3.72, end_time=4.26), TextPhrase(text='Ñ Ğ¿Ğ¾Ğ´ÑƒĞ¼Ğ°Ğ»Ğ° Ğ½Ğµ Ñ…Ğ¾Ñ‡ĞµÑˆÑŒ Ğ»Ğ¸ Ñ‚Ñ‹ Ğ²ÑÑ‚Ñ€ĞµÑ‚Ğ¸Ñ‚ÑŒÑÑ ÑĞ¿ÑƒÑÑ‚Ñ Ğ²ÑĞµ ÑÑ‚Ğ¸ Ğ³Ğ¾Ğ´Ñ‹', start_time=5.88, end_time=10.59)]\n\n\n\næµå¼æ¨ç†ï¼ˆé€‚ç”¨äºå®æ—¶éŸ³é¢‘ï¼‰\nfrom tone import StreamingCTCPipeline, read_stream_example_audio\n\n\npipeline = StreamingCTCPipeline.from_hugging_face()\n\nstate = None  # Current state of the ASR pipeline (None - initial)\nfor audio_chunk in read_stream_example_audio():  # Use any source of audio chunks\n    new_phrases, state = pipeline.forward(audio_chunk, state)\n    print(new_phrases)\n\n# Finalize the pipeline and get the remaining phrases\nnew_phrases, _ = pipeline.finalize(state)\nprint(new_phrases)\n\nè¾“å‡ºï¼š\nTextPhrase(text='Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚', start_time=1.79, end_time=2.04)\nTextPhrase(text='ÑÑ‚Ğ¾ Ñ', start_time=3.72, end_time=4.26)\nTextPhrase(text='Ñ Ğ¿Ğ¾Ğ´ÑƒĞ¼Ğ°Ğ»Ğ° Ğ½Ğµ Ñ…Ğ¾Ñ‡ĞµÑˆÑŒ Ğ»Ğ¸ Ñ‚Ñ‹ Ğ²ÑÑ‚Ñ€ĞµÑ‚Ğ¸Ñ‚ÑŒÑÑ ÑĞ¿ÑƒÑÑ‚Ñ Ğ²ÑĞµ ÑÑ‚Ğ¸ Ğ³Ğ¾Ğ´Ñ‹', start_time=5.88, end_time=10.59)\n\n\n\nğŸ”§ å¾®è°ƒ\nè‹¥è¦ä»é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å¾®è°ƒ T-oneï¼Œæ‚¨éœ€è¦å‡†å¤‡è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶ä» t-tech/T-one ğŸ¤— ä»“åº“åŠ è½½åˆ†è¯å™¨å’Œç‰¹å¾æå–å™¨ã€‚\nimport torch\n\nfrom tone.training.model_wrapper import ToneForCTC\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ToneForCTC.from_pretrained(\"t-tech/T-one\").to(device)\n\nè®¾ç½®æ•°æ®æ•´ç†å™¨ã€è¯„ä¼°æŒ‡æ ‡ã€è®­ç»ƒå‚æ•°å’Œ ğŸ¤— Trainerã€‚\nå®Œæ•´æŒ‡å—è¯·å‚è€ƒ å¾®è°ƒç¤ºä¾‹ç¬”è®°æœ¬ã€‚\n\n\nğŸ™ å£°å­¦æ¨¡å‹\n\n\næ¶æ„\nT-one æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 7100 ä¸‡å‚æ•°çš„å£°å­¦æ¨¡å‹ï¼ŒåŸºäº Conformer æ¶æ„æ„å»ºï¼Œå¹¶èå…¥äº†å¤šé¡¹å…³é”®åˆ›æ–°ä»¥æå‡æ€§èƒ½å’Œæ•ˆç‡ï¼š\n\nSwiGLU æ¿€æ´»å‡½æ•°ï¼šå°†å‰é¦ˆæ¨¡å—æ›¿æ¢ä¸º SwiGLU æ¨¡å—ï¼Œä»¥è·å¾—æ›´ä¼˜æ€§èƒ½ã€‚\nç°ä»£å½’ä¸€åŒ–æŠ€æœ¯ï¼šé‡‡ç”¨ SiLUï¼ˆSwishï¼‰æ¿€æ´»å‡½æ•°å’Œ RMSNormï¼Œæ›¿ä»£ä¼ ç»Ÿçš„ ReLU å’Œ LayerNormã€‚\nRoPE åµŒå…¥ï¼šå°† Transformer-XL ä¸­çš„ç›¸å¯¹ä½ç½®åµŒå…¥æ›¿æ¢ä¸ºé€Ÿåº¦æ›´å¿«çš„ Rotary Position Embeddingsï¼ˆRoPEï¼‰ã€‚\nU-Net ç»“æ„ï¼šåœ¨ Conformer å—å†…éƒ¨å¯¹æ—¶é—´ç»´åº¦è¿›è¡Œä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·æ“ä½œï¼Œå¢å¼ºæ¨¡å‹çš„æ„Ÿå—é‡ã€‚\næ³¨æ„åŠ›åˆ†æ•°å¤ç”¨ï¼šå¯¹å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚è¿›è¡Œåˆ†ç»„ï¼Œæ¯ç»„ä»…è®¡ç®—ä¸€æ¬¡æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»¥å‡å°‘è®¡ç®—é‡ã€‚\né«˜æ•ˆçŠ¶æ€ç®¡ç†ï¼šä»…åœ¨æ¨¡å‹çš„æœ€åä¸¤å±‚ä½¿ç”¨æµå¼çŠ¶æ€ã€‚\n\nå®ƒä»¥ 300 æ¯«ç§’çš„éŸ³é¢‘å—ä¸ºå•ä½è¿›è¡Œå¤„ç†ï¼Œå¹¶ä½¿ç”¨è´ªå©ªè§£ç æˆ–åŸºäº KenLM çš„ CTC æŸæœç´¢è§£ç å™¨ç”Ÿæˆè½¬å½•æ–‡æœ¬ã€‚\nè¯¥æ¨¡å‹ä½¿ç”¨ CTC æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚\nT-one ä¸»è¦è®¾è®¡ç”¨äºç”µè¯ä¿¡é“éŸ³é¢‘ã€‚ä½†ç”±äºå…¶è®­ç»ƒæ•°æ®æ¥æºå¤šæ ·ï¼Œå› æ­¤åœ¨ä¸åŒé¢†åŸŸéƒ½å…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ï¼Œä¸ä»…é™äºç”µè¯è¯­éŸ³åœºæ™¯ã€‚\nè¯¥æ¨¡å‹æ”¯æŒæµå¼æ¨ç†ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥å¼€ç®±å³ç”¨åœ°ä»¥å®æ—¶æ–¹å¼å¤„ç†é•¿éŸ³é¢‘æ–‡ä»¶ã€‚\næ­¤æ¨¡å‹çš„ä¸»è¦åº”ç”¨åœºæ™¯æ˜¯é€šè¯çš„æµå¼è¯­éŸ³è¯†åˆ«ã€‚ç”¨æˆ·å‘æ¨¡å‹å‘é€å°çš„éŸ³é¢‘å—ï¼Œæ¨¡å‹å¯¹æ¯ä¸ªç‰‡æ®µè¿›è¡Œå¢é‡å¤„ç†ï¼Œå¹¶å®æ—¶è¿”å›å·²ç¡®å®šçš„æ–‡æœ¬å’Œè¯çº§æ—¶é—´æˆ³ã€‚\nT-one å¯ä»¥è½»æ¾é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œå¾®è°ƒã€‚\nå¦‚éœ€è¯¦ç»†äº†è§£æˆ‘ä»¬çš„æ¶æ„ã€è®¾è®¡é€‰æ‹©å’Œå®ç°ç»†èŠ‚ï¼Œè¯·æŸ¥é˜…æˆ‘ä»¬çš„é…å¥— æ–‡ç« ã€‚æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥å‚è€ƒæˆ‘ä»¬åœ¨ YouTube ä¸Šçš„ æŠ€æœ¯æ·±åº¦æ¢è®¨ï¼Œäº†è§£å¦‚ä½•æå‡æµå¼ ASR æ¨¡å‹çš„è´¨é‡å’Œè®­ç»ƒé€Ÿåº¦ã€‚\n\n\nğŸ“‰ è®­ç»ƒè¯¦æƒ…\n\n\nè®­ç»ƒæ•°æ®\nå£°å­¦æ¨¡å‹çš„è®­ç»ƒåŸºäºè¶…è¿‡80,000å°æ—¶çš„ä¿„è¯­è¯­éŸ³æ•°æ®ã€‚å…¶ä¸­å¾ˆå¤§ä¸€éƒ¨åˆ†ï¼ˆé«˜è¾¾64%ï¼‰æ˜¯ä½¿ç”¨ç¨³å¥çš„ROVERæ¨¡å‹é›†æˆè¿›è¡Œä¼ªæ ‡æ³¨çš„ã€‚\n\n\n\né¢†åŸŸ\næ—¶é•¿\næ¥æº\n\n\n\n\nç”µè¯è¯­éŸ³\n57.9k\nå†…éƒ¨\n\n\nè¿œåœºè¯­éŸ³\n2.2K\nå†…éƒ¨\n\n\næ··åˆè¯­éŸ³\n18.4K\nå†…éƒ¨\n\n\næ··åˆè¯­éŸ³\n2.3K\nå¼€æº\n\n\n\n\n\nè®­ç»ƒè¿‡ç¨‹\nè¯¥æ¨¡å‹åŸºäºNVIDIA NeMoæ¡†æ¶ï¼Œä»é›¶å¼€å§‹ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰åœ¨8å—A100 GPUä¸Šè®­ç»ƒäº†7å¤©ã€‚ä¸»è¦è®­ç»ƒå‚æ•°åŒ…æ‹¬ï¼š\n\nä¼˜åŒ–å™¨ï¼š AdamW\nè°ƒåº¦å™¨ï¼š å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«\nç²¾åº¦ï¼š 16ä½æ··åˆç²¾åº¦\næ‰¹å¤„ç†ï¼š åŠæ’åºæ‰¹å¤„ç†ä»¥æé«˜æ•ˆç‡\n\n\n\nğŸ“œ è®¸å¯è¯\næœ¬é¡¹ç›®ï¼ˆåŒ…æ‹¬ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼‰åŸºäºApache 2.0 Licenseå‘å¸ƒã€‚\n",
    "tags": [
      "Automatic Speech Recognition",
      "Transformers",
      "ONNX",
      "Safetensors",
      "Russian",
      "Apache License 2.0",
      "asr",
      "speech",
      "streaming",
      "stt",
      "conformer",
      "russian",
      "t-one",
      "telephony",
      "t-tech"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/baidu/ERNIE-4.5-VL-424B-A47B-PT",
    "project_name": "baidu/ERNIE-4.5-VL-424B-A47B-PT",
    "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nERNIE-4.5-VL-424B-A47B\nNoteæ³¨ï¼šå¸¦â€œ-Paddleâ€åç¼€çš„æ¨¡å‹ä½¿ç”¨é£æ¡¨æ¡†æ¶æƒé‡ï¼Œè€Œå¸¦â€œ-PTâ€åç¼€çš„æ¨¡å‹é‡‡ç”¨Transformeré£æ ¼çš„PyTorchæƒé‡ã€‚\n\n\n\nERNIE 4.5 æ ¸å¿ƒäº®ç‚¹\nERNIE 4.5ç³»åˆ—æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯åŸºäºMoEæ¶æ„çš„A47Bå’ŒA3Bç³»åˆ—ï¼‰çš„å“è¶Šèƒ½åŠ›æºäºå¤šé¡¹å…³é”®æŠ€æœ¯çªç ´ï¼š\n\n\nå¤šæ¨¡æ€å¼‚æ„MoEé¢„è®­ç»ƒï¼šæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡æ–‡æœ¬ä¸è§†è§‰æ¨¡æ€çš„è”åˆè®­ç»ƒï¼Œæ›´ç²¾å‡†æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯ç»†èŠ‚ï¼Œæ˜¾è‘—æå‡æ–‡æœ¬ç†è§£ç”Ÿæˆã€å›¾åƒè§£æåŠè·¨æ¨¡æ€æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚ä¸ºé¿å…ä¸åŒæ¨¡æ€é—´çš„ç›¸äº’å¹²æ‰°ï¼Œæˆ‘ä»¬åˆ›æ–°æ€§åœ°è®¾è®¡äº†å¼‚æ„MoEç»“æ„ï¼Œé‡‡ç”¨æ¨¡æ€éš”ç¦»è·¯ç”±æœºåˆ¶ï¼Œå¹¶ç»“åˆè·¯ç”±å™¨æ­£äº¤æŸå¤±ä¸å¤šæ¨¡æ€ä»¤ç‰Œå‡è¡¡æŸå¤±ã€‚è¿™äº›æ¶æ„è®¾è®¡ç¡®ä¿åŒæ¨¡æ€ç‰¹å¾å¾—åˆ°æœ‰æ•ˆè¡¨å¾ï¼Œå®ç°è®­ç»ƒè¿‡ç¨‹ä¸­çš„ååŒå¢å¼ºã€‚\n\n\né«˜æ•ˆæ‰©å±•åŸºç¡€è®¾æ–½ï¼šæˆ‘ä»¬æå‡ºæ–°å‹å¼‚æ„æ··åˆå¹¶è¡Œä¸åˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œä¸ºERNIE 4.5æ¨¡å‹æä¾›é«˜æ•ˆè®­ç»ƒæ”¯æŒã€‚é€šè¿‡èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜ä¼˜åŒ–çš„æµæ°´çº¿è°ƒåº¦ã€FP8æ··åˆç²¾åº¦è®­ç»ƒåŠç»†ç²’åº¦é‡è®¡ç®—æŠ€æœ¯ï¼Œå®ç°äº†å“è¶Šçš„é¢„è®­ç»ƒååæ•ˆç‡ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œé‡‡ç”¨å¤šä¸“å®¶å¹¶è¡Œåä½œæ–¹æ³•ä¸å·ç§¯ç é‡åŒ–ç®—æ³•ï¼Œè¾¾æˆ4æ¯”ç‰¹/2æ¯”ç‰¹æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œåˆ›æ–°æ€§å¼•å…¥åŠ¨æ€è§’è‰²åˆ‡æ¢çš„PDè§£è€¦æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡ERNIE 4.5 MoEæ¨¡å‹çš„æ¨ç†èµ„æºåˆ©ç”¨ç‡ã€‚åŸºäºé£æ¡¨æ¡†æ¶æ„å»ºçš„ERNIE 4.5ï¼Œå¯åœ¨å¤šæ ·åŒ–çš„ç¡¬ä»¶å¹³å°ä¸Šå®ç°é«˜æ€§èƒ½æ¨ç†ã€‚\n\n\næ¨¡æ€ä¸“é¡¹åè®­ç»ƒï¼šä¸ºæ»¡è¶³å®é™…åº”ç”¨çš„å¤šå…ƒåŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨¡æ€ä¸“é¡¹å¾®è°ƒã€‚å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸“æ³¨äºé€šç”¨è¯­è¨€ç†è§£ä¸ç”Ÿæˆä¼˜åŒ–ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åˆ™å¼ºåŒ–è§†è§‰-è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œæ”¯æŒæ€ç»´é“¾ä¸éæ€ç»´é“¾åŒæ¨¡å¼ã€‚å„æ¨¡å‹å‡ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ æˆ–æ”¹è¿›å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰ è¿›è¡Œåè®­ç»ƒã€‚\n\n\nåœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å¾®è°ƒé˜¶æ®µï¼Œè§†è§‰ä¸è¯­è¨€çš„æ·±åº¦èåˆå¯¹æ¨¡å‹åœ¨ç†è§£ã€æ¨ç†ã€ç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°å…·æœ‰å†³å®šæ€§ä½œç”¨ã€‚ä¸ºå¢å¼ºæ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æ³›åŒ–æ€§ä¸é€‚åº”æ€§ï¼Œæˆ‘ä»¬èšç„¦å›¾åƒè§£æã€ä»»åŠ¡ä¸“é¡¹å¾®è°ƒå’Œå¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†ä¸‰å¤§æ ¸å¿ƒèƒ½åŠ›ï¼Œå¼€å±•äº†ç³»ç»ŸåŒ–çš„æ•°æ®æ„å»ºä¸è®­ç»ƒç­–ç•¥ä¼˜åŒ–ã€‚åŒæ—¶é‡‡ç”¨å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–å¯¹é½æ•ˆæœä¸æ€§èƒ½è¡¨ç°ã€‚ç»è¿‡SFTä¸RLé˜¶æ®µè®­ç»ƒåï¼Œæœ€ç»ˆè·å¾—ERNIE-4.5-VL-424B-A47Bæ¨¡å‹ã€‚\n\n\næ¨¡å‹æ¦‚è¿°\nERNIE-4.5-VL-424B-A47B æ˜¯åŸºäº ERNIE-4.5-VL-424B-A47B-Base çš„å¤šæ¨¡æ€ MoE å¯¹è¯æ¨¡å‹ï¼Œæ€»å‚æ•°é‡è¾¾ 424Bï¼Œæ¯ token æ¿€æ´»å‚æ•°é‡ä¸º 47Bã€‚ä»¥ä¸‹æ˜¯æ¨¡å‹é…ç½®è¯¦æƒ…ï¼š\n\n\n\nå‚æ•°é¡¹\nå€¼\n\n\n\n\næ¨¡æ€æ”¯æŒ\næ–‡æœ¬ä¸è§†è§‰\n\n\nè®­ç»ƒé˜¶æ®µ\nåè®­ç»ƒé˜¶æ®µ\n\n\nå‚æ•°é‡ï¼ˆæ€»é‡/æ¿€æ´»é‡ï¼‰\n424B / 47B\n\n\nå±‚æ•°\n54\n\n\næ³¨æ„åŠ›å¤´æ•°ï¼ˆQ/KVï¼‰\n64 / 8\n\n\næ–‡æœ¬ä¸“å®¶æ•°ï¼ˆæ€»é‡/æ¿€æ´»é‡ï¼‰\n64 / 8\n\n\nè§†è§‰ä¸“å®¶æ•°ï¼ˆæ€»é‡/æ¿€æ´»é‡ï¼‰\n64 / 8\n\n\nä¸Šä¸‹æ–‡é•¿åº¦\n131072\n\n\n\n\n\nå¿«é€Ÿå¼€å§‹\n\n\nvLLM æ¨ç†\nä½¿ç”¨ vllm GitHub åº“ã€‚æ”¯æŒçº¯ Python ç¯å¢ƒæ„å»ºã€‚\n# 80G * 16 GPU\nvllm serve baidu/ERNIE-4.5-VL-424B-A47B-PT --tensor-parallel-size 16 --trust-remote-code\n\n\n\nè®¸å¯åè®®\nERNIE 4.5 æ¨¡å‹åŸºäº Apache 2.0 è®¸å¯è¯æä¾›ã€‚è¯¥è®¸å¯è¯å…è®¸å•†ä¸šä½¿ç”¨ï¼Œä½†é¡»éµå®ˆå…¶æ¡æ¬¾ä¸æ¡ä»¶ã€‚ç‰ˆæƒæ‰€æœ‰ (c) 2025 ç™¾åº¦å…¬å¸ï¼Œä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\n\nå¼•ç”¨è¯´æ˜\nå¦‚æœæ‚¨è®¤ä¸º ERNIE 4.5 å¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¸®åŠ©æˆ–å¸Œæœ›åœ¨å…¶é¡¹ç›®ä¸­ä½¿ç”¨ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n\n",
    "tags": [
      "Image-Text-to-Text",
      "Transformers",
      "Safetensors",
      "English",
      "Chinese",
      "Apache License 2.0",
      "ERNIE4.5"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/nvidia/canary-qwen-2.5b",
    "project_name": "nvidia/canary-qwen-2.5b",
    "readme": "img {\n display: inline;\n}\n\n\n| \n| \n\n\næ¨¡å‹æ¦‚è¿°\n\n\næè¿°ï¼š\nNVIDIA NeMo Canary-Qwen-2.5B æ˜¯ä¸€æ¬¾è‹±è¯­è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œåœ¨å¤šä¸ªè‹±è¯­è¯­éŸ³åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹æ‹¥æœ‰ 25 äº¿å‚æ•°ï¼Œè¿è¡Œé€Ÿåº¦ä¸º 418 RTFxï¼Œæ”¯æŒå¸¦æ ‡ç‚¹å’Œå¤§å°å†™ï¼ˆPnCï¼‰çš„è‹±è¯­è‡ªåŠ¨è¯­éŸ³è½¬æ–‡æœ¬è¯†åˆ«ï¼ˆASRï¼‰ã€‚è¯¥æ¨¡å‹æœ‰ä¸¤ç§å·¥ä½œæ¨¡å¼ï¼šè½¬å½•å·¥å…·æ¨¡å¼ï¼ˆASR æ¨¡å¼ï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¨¡å¼ï¼ˆLLM æ¨¡å¼ï¼‰ã€‚åœ¨ ASR æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ä»…èƒ½å°†è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬ï¼Œä¸å…·å¤‡æ¨ç†ç­‰ä»»ä½• LLM ç‰¹æœ‰æŠ€èƒ½ã€‚åœ¨ LLM æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ä¿ç•™æ‰€æœ‰åŸå§‹ LLM èƒ½åŠ›ï¼Œå¯ç”¨äºå¯¹è½¬å½•æ–‡æœ¬è¿›è¡Œåå¤„ç†ï¼Œä¾‹å¦‚æ€»ç»“æˆ–å›ç­”ç›¸å…³é—®é¢˜ã€‚åœ¨ LLM æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ä¸å†â€œç†è§£â€åŸå§‹éŸ³é¢‘ï¼Œä»…èƒ½å¤„ç†å…¶è½¬å½•æ–‡æœ¬ã€‚æœ¬æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼Œå¯æŠ•å…¥å•†ä¸šä½¿ç”¨ã€‚\n\n\nè®¸å¯è¯/ä½¿ç”¨æ¡æ¬¾ï¼š\nCanary-Qwen-2.5B åŸºäº CC-BY-4.0 è®¸å¯è¯å‘å¸ƒã€‚ä½¿ç”¨æœ¬æ¨¡å‹å³è¡¨ç¤ºæ‚¨åŒæ„è¯¥è®¸å¯è¯çš„æ¡æ¬¾ä¸æ¡ä»¶ã€‚\n\n\nå‚è€ƒæ–‡çŒ®ï¼š\n[1] å°‘å³æ˜¯å¤šï¼šæ— éœ€ç½‘ç»œçº§æ•°æ®çš„ç²¾å‡†è¯­éŸ³è¯†åˆ«ä¸ç¿»è¯‘\n[2] å…·æœ‰çº¿æ€§å¯æ‰©å±•æ³¨æ„åŠ›çš„å¿«é€Ÿ Conformer ç”¨äºé«˜æ•ˆè¯­éŸ³è¯†åˆ«\n[3] Attention Is All You Need\n[4] Qwen/Qwen3-1.7B æ¨¡å‹å¡ç‰‡\n[5] ç¼–ç å™¨-è§£ç å™¨è¯­éŸ³æ¨¡å‹çš„è®­ç»ƒä¸æ¨ç†æ•ˆç‡\n[6] NVIDIA NeMo å·¥å…·åŒ…\n[7] Granaryï¼š25 ç§æ¬§æ´²è¯­è¨€çš„è¯­éŸ³è¯†åˆ«ä¸ç¿»è¯‘æ•°æ®é›†\n[8] è¿ˆå‘ AI å…¬å¹³æ€§è¡¡é‡ï¼š Casual Conversations æ•°æ®é›†\n[9] SALMï¼šç”¨äºè¯­éŸ³è¯†åˆ«ä¸ç¿»è¯‘çš„å¸¦ä¸Šä¸‹æ–‡å­¦ä¹ çš„è¯­éŸ³å¢å¼ºè¯­è¨€æ¨¡å‹\n\n\néƒ¨ç½²åœ°åŒºï¼š\nå…¨çƒ\n\n\nä½¿ç”¨åœºæ™¯ï¼š\nè¯¥æ¨¡å‹é¢å‘éœ€è¦è‹±è¯­è¯­éŸ³è½¬æ–‡æœ¬è½¬å½•åŠŸèƒ½å’Œ/æˆ–é€šè¿‡æç¤ºåº•å±‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°è½¬å½•åå¤„ç†åŠŸèƒ½çš„ç”¨æˆ·ã€‚å…¸å‹ä½¿ç”¨åœºæ™¯åŒ…æ‹¬ï¼šè½¬å½•ã€æ€»ç»“ã€å›ç­”ç”¨æˆ·å…³äºè½¬å½•æ–‡æœ¬çš„é—®é¢˜ã€‚\n\n\nå‘å¸ƒæ—¥æœŸï¼š\nHuggingface 2025å¹´7æœˆ17æ—¥ï¼Œåœ°å€ï¼šhttps://huggingface.co/nvidia/canary-qwen-2.5b\n\n\næ¨¡å‹æ¶æ„ï¼š\nCanary-Qwen æ˜¯ä¸€ç§è¯­éŸ³å¢å¼ºè¯­è¨€æ¨¡å‹ï¼ˆSALMï¼‰[9]ï¼ŒåŒ…å« FastConformer [2] ç¼–ç å™¨å’Œ Transformer è§£ç å™¨ [3]ã€‚å®ƒåŸºäºä¸¤ä¸ªåŸºç¡€æ¨¡å‹æ„å»ºï¼šnvidia/canary-1b-flash [1,5] å’Œ Qwen/Qwen3-1.7B [4]ï¼Œå¹¶ç»“åˆäº†çº¿æ€§æŠ•å½±å’Œåº”ç”¨äº LLM çš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ã€‚éŸ³é¢‘ç¼–ç å™¨è®¡ç®—éŸ³é¢‘è¡¨ç¤ºï¼Œé€šè¿‡çº¿æ€§æŠ•å½±æ˜ å°„åˆ° LLM åµŒå…¥ç©ºé—´ï¼Œå¹¶ä¸æ–‡æœ¬æ ‡è®°çš„åµŒå…¥è¿›è¡Œæ‹¼æ¥ã€‚æ¨¡å‹ä½¿ç”¨ Qwen çš„èŠå¤©æ¨¡æ¿ï¼Œæç¤ºè¯ä¸ºâ€œTranscribe the following: â€ã€‚\n\n\nå±€é™æ€§\nè¾“å…¥é•¿åº¦ï¼šè®­ç»ƒä¸­çš„æœ€å¤§éŸ³é¢‘æ—¶é•¿ä¸º 40 ç§’ï¼Œæœ€å¤§æ ‡è®°åºåˆ—é•¿åº¦ä¸º 1024 ä¸ªæ ‡è®°ï¼ˆåŒ…æ‹¬æç¤ºè¯ã€éŸ³é¢‘å’Œå“åº”ï¼‰ã€‚è¯¥æ¨¡å‹åœ¨æŠ€æœ¯ä¸Šå¯èƒ½èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„åºåˆ—ï¼Œä½†å…¶å‡†ç¡®æ€§å¯èƒ½ä¼šä¸‹é™ã€‚\nä»…é™ ASR ç›¸å…³åŠŸèƒ½ï¼šé¢„è®¡è¯¥æ¨¡å‹ä¸ä¼šå°†åº•å±‚ LLM çš„ä»»ä½•åŠŸèƒ½ä¿ç•™åˆ°è¯­éŸ³æ¨¡æ€ä¸­ã€‚\nä»…æ”¯æŒè‹±è¯­ï¼šè¯¥æ¨¡å‹ä»…ä½¿ç”¨è‹±è¯­æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ç”±äºåº•å±‚ç¼–ç å™¨é™¤è‹±è¯­å¤–ï¼Œè¿˜ä½¿ç”¨å¾·è¯­ã€æ³•è¯­å’Œè¥¿ç­ç‰™è¯­è¯­éŸ³è¿›è¡Œè¿‡é¢„è®­ç»ƒï¼Œå› æ­¤å®ƒå¯èƒ½ä¼šé”™è¯¯åœ°è½¬å½•å…¶ä»–è¯­è¨€ï¼Œä½†ä½œä¸ºå¤šè¯­è¨€æ¨¡å‹ä¸å¤ªå¯èƒ½å¯é ã€‚\n\n\nNVIDIA NeMo\nè¦ä½¿ç”¨ Canary-Qwen-2.5B è¿›è¡Œè®­ç»ƒã€å¾®è°ƒæˆ–è½¬å½•ï¼Œæ‚¨éœ€è¦å®‰è£… NVIDIA NeMoã€‚\n# Currently requires installing the latest trunk version of NeMo, and PyTorch 2.6+ for FSDP2 support.\npython -m pip install \"nemo_toolkit[asr,tts] @ git+https://github.com/NVIDIA/NeMo.git\"\n\n\n\nå¦‚ä½•ä½¿ç”¨æ­¤æ¨¡å‹\nè¯¥æ¨¡å‹å¯åœ¨ NVIDIA NeMo å·¥å…·åŒ… [6] ä¸­ä½¿ç”¨ï¼Œå¯ç”¨ä½œé¢„è®­ç»ƒæ£€æŸ¥ç‚¹è¿›è¡Œæ¨ç†ï¼Œæˆ–åœ¨å…¶ä»–æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n\n\nåŠ è½½æ¨¡å‹\nfrom nemo.collections.speechlm2.models import SALM\n\nmodel = SALM.from_pretrained('nvidia/canary-qwen-2.5b')\n\n\n\nè¾“å…¥ï¼š\nè¾“å…¥ç±»å‹ï¼š éŸ³é¢‘ã€æ–‡æœ¬æç¤º \nè¾“å…¥æ ¼å¼ï¼š éŸ³é¢‘ï¼š.wav æˆ– .flac æ–‡ä»¶ã€‚ASR æ¨¡å¼ä¸‹çš„æ–‡æœ¬æç¤ºå­—ç¬¦ä¸²ï¼šTranscribe the following: <|audioplaceholder|> \nè¾“å…¥å‚æ•°ï¼š éŸ³é¢‘ï¼šäºŒç»´ï¼ˆbatchï¼Œaudio-samplesï¼‰ï¼›æ–‡æœ¬ï¼šä¸€ç»´ï¼ˆå­—ç¬¦ä¸²ï¼‰ \nä¸è¾“å…¥ç›¸å…³çš„å…¶ä»–å±æ€§ï¼š 16000 Hz å•å£°é“éŸ³é¢‘ï¼Œæ— éœ€é¢„å¤„ç† \nCanary-Qwen-2.5B çš„è¾“å…¥æ˜¯ä¸€æ‰¹åŒ…å«éŸ³é¢‘çš„æç¤ºã€‚\nASR æ¨¡å¼ï¼ˆè¯­éŸ³è½¬æ–‡æœ¬ï¼‰ä½¿ç”¨ç¤ºä¾‹ï¼š\nanswer_ids = model.generate(\n    prompts=[\n        [{\"role\": \"user\", \"content\": f\"Transcribe the following: {model.audio_locator_tag}\", \"audio\": [\"speech.wav\"]}]\n    ],\n    max_new_tokens=128,\n)\nprint(model.tokenizer.ids_to_text(answer_ids[0].cpu()))\n\nLLM æ¨¡å¼ï¼ˆçº¯æ–‡æœ¬ï¼‰ä¸‹çš„ä½¿ç”¨ç¤ºä¾‹ï¼š\nprompt = \"...\"\ntranscript = \"...\"\nwith model.llm.disable_adapter():\n    answer_ids = model.generate(\n        prompts=[[{\"role\": \"user\", \"content\": f\"{prompt}\\n\\n{transcript}\"}]],\n        max_new_tokens=2048,\n    )\n\nè¦è½¬å½•å½•éŸ³æ•°æ®é›†ï¼Œè¯·å°†è¾“å…¥æŒ‡å®šä¸º jsonl æ¸…å•æ–‡ä»¶ï¼Œå…¶ä¸­æ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n# Example of a line in input_manifest.json\n{\n    \"audio_filepath\": \"/path/to/audio.wav\",  # path to the audio file\n    \"duration\": 30.0,  # duration of the audio\n}\n\nç„¶åä½¿ç”¨ï¼š\ncd NeMo\npython examples/speechlm2/salm_generate.py \\\n  pretrained_name=nvidia/canary-qwen-2.5b \\\n  inputs=input_manifest.json \\\n  output_manifest=generations.jsonl \\\n  batch_size=128 \\\n  user_prompt=\"Transcribe the following:\"  # audio locator is added automatically at the end if not present\n\n\n\nè¾“å‡ºï¼š\nè¾“å‡ºç±»å‹ï¼š æ–‡æœ¬ \nè¾“å‡ºæ ¼å¼ï¼š æ–‡æœ¬è½¬å½•æœ¬ï¼Œå½¢å¼ä¸ºä¸€ç³»åˆ—ä»¤ç‰Œ ID æˆ–å­—ç¬¦ä¸² \nè¾“å‡ºå‚æ•°ï¼š ä¸€ç»´æ–‡æœ¬å­—ç¬¦ä¸² \nä¸è¾“å‡ºç›¸å…³çš„å…¶ä»–å±æ€§ï¼š å¯èƒ½éœ€è¦é€†æ–‡æœ¬è§„èŒƒåŒ– \næˆ‘ä»¬çš„ AI æ¨¡å‹ç»è¿‡ä¸“é—¨è®¾è®¡å’Œ/æˆ–ä¼˜åŒ–ï¼Œå¯åœ¨ NVIDIA GPU åŠ é€Ÿç³»ç»Ÿä¸Šè¿è¡Œã€‚é€šè¿‡åˆ©ç”¨ NVIDIA çš„ç¡¬ä»¶ï¼ˆå¦‚ GPU æ ¸å¿ƒï¼‰å’Œè½¯ä»¶æ¡†æ¶ï¼ˆå¦‚ CUDA åº“ï¼‰ï¼Œä¸ä»…ä½¿ç”¨ CPU çš„è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œè¯¥æ¨¡å‹å®ç°äº†æ›´å¿«çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´ã€‚\n\n\nè½¯ä»¶é›†æˆï¼š\nè¿è¡Œæ—¶å¼•æ“ï¼š\n\nNeMo - 2.5.0 æˆ–æ›´é«˜ç‰ˆæœ¬ \n\næ”¯æŒçš„ç¡¬ä»¶å¾®æ¶æ„å…¼å®¹æ€§ï¼š \n\n[NVIDIA Ampere] \n[NVIDIA Blackwell] \n[NVIDIA Jetson]  \n[NVIDIA Hopper] \n[NVIDIA Lovelace] \n[NVIDIA Pascal] \n[NVIDIA Turing] \n[NVIDIA Volta] \n\n[é¦–é€‰/æ”¯æŒçš„] æ“ä½œç³»ç»Ÿï¼š \n\n[Linux] \n[Linux 4 Tegra] \n[Windows] \n\n\n\næ¨¡å‹ç‰ˆæœ¬ï¼š\nCanary-Qwen-2.5B \n\n\nè®­ç»ƒ\nCanary-Qwen-2.5B ä½¿ç”¨ NVIDIA NeMo å·¥å…·åŒ… [6] è¿›è¡Œè®­ç»ƒï¼Œåœ¨ 32 å— NVIDIA A100 80GB GPU ä¸Šå…±è®­ç»ƒäº† 90k æ­¥ã€‚LLM å‚æ•°ä¿æŒå†»ç»“çŠ¶æ€ã€‚è¯­éŸ³ç¼–ç å™¨ã€æŠ•å½±å±‚å’Œ LoRA å‚æ•°ä¸ºå¯è®­ç»ƒå‚æ•°ã€‚ç¼–ç å™¨çš„è¾“å‡ºå¸§ç‡ä¸º 80msï¼Œå³æ¯ç§’ 12.5 ä¸ªä»¤ç‰Œã€‚æ¨¡å‹è®­ç»ƒæ€»å…±ä½¿ç”¨äº†çº¦ 13 äº¿ä¸ªä»¤ç‰Œï¼ˆæ­¤æ•°å­—åŒ…æ‹¬è¯­éŸ³ç¼–ç å™¨è¾“å‡ºå¸§ã€æ–‡æœ¬å“åº”ä»¤ç‰Œã€æç¤ºä»¤ç‰Œå’ŒèŠå¤©æ¨¡æ¿ä»¤ç‰Œï¼‰ã€‚\nå¯ä½¿ç”¨æ­¤ç¤ºä¾‹è„šæœ¬å’ŒåŸºç¡€é…ç½®å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚\nåˆ†è¯å™¨ç»§æ‰¿è‡ª Qwen/Qwen3-1.7Bã€‚\n\n\nè®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ï¼š\n\n\nè®­ç»ƒæ•°æ®é›†ï¼š\n** æ€»æ•°æ®é‡ï¼ˆæ•°æ®ç‚¹æ•°é‡ï¼‰ï¼šçº¦ 4000 ä¸‡ï¼ˆè¯­éŸ³ã€æ–‡æœ¬ï¼‰å¯¹\n** æ•°æ®é›†æ€»æ•°ï¼š26 ä¸ªï¼Œå…¶ä¸­ 18 ä¸ªç”¨äºè®­ç»ƒï¼Œ8 ä¸ªç”¨äºæµ‹è¯•\n** æ•°æ®é›†åˆ’åˆ†ï¼šè®­ç»ƒé›† 99.6%ï¼Œæµ‹è¯•é›† 0.04%ï¼ŒéªŒè¯é›† 0%\n** è®­ç»ƒæ•°æ®æ”¶é›†æ—¶é—´æ®µï¼š1990-2025\n** æµ‹è¯•æ•°æ®æ”¶é›†æ—¶é—´æ®µï¼š2005-2022\n** éªŒè¯æ•°æ®æ”¶é›†æ—¶é—´æ®µï¼šä¸é€‚ç”¨ï¼ˆæœªä½¿ç”¨ï¼‰\nCanary-Qwen-2.5B æ¨¡å‹æ˜¯åœ¨æ€»è®¡ 23.4 ä¸‡å°æ—¶çš„å…¬å¼€å¯ç”¨è¯­éŸ³æ•°æ®ä¸Šè®­ç»ƒçš„ã€‚\nä»¥ä¸‹æ•°æ®é›†åŒ…æ‹¬å¯¹è¯ã€ç½‘ç»œè§†é¢‘å’Œæœ‰å£°è¯»ç‰©å½•éŸ³ã€‚\næ•°æ®æ”¶é›†æ–¹æ³•ï¼š\n\näººå·¥ \n\næ ‡æ³¨æ–¹æ³•ï¼š\n\næ··åˆï¼šäººå·¥ã€è‡ªåŠ¨åŒ– \n\n\n\nå±æ€§\n\n\nè‹±è¯­ï¼ˆ234.5k å°æ—¶ï¼‰\nå¤§éƒ¨åˆ†è®­ç»ƒæ•°æ®æ¥è‡ª Granary æ•°æ®é›†çš„è‹±æ–‡éƒ¨åˆ† [7]ï¼š\n\nYouTube-Commons (YTC)ï¼ˆ109.5k å°æ—¶ï¼‰\nYODAS2ï¼ˆ77k å°æ—¶ï¼‰\nLibriLightï¼ˆ13.6k å°æ—¶ï¼‰\n\næ­¤å¤–ï¼Œè¿˜ä½¿ç”¨äº†ä»¥ä¸‹æ•°æ®é›†ï¼š\n\nLibrispeech 960 å°æ—¶\nFisher è¯­æ–™åº“\nSwitchboard-1 æ•°æ®é›†\nWSJ-0 å’Œ WSJ-1\nNational Speech Corpusï¼ˆç¬¬ 1 éƒ¨åˆ†ã€ç¬¬ 6 éƒ¨åˆ†ï¼‰\nVCTK\nVoxPopuliï¼ˆENï¼‰\nEuroparl-ASRï¼ˆENï¼‰\nMultilingual Librispeechï¼ˆMLS ENï¼‰\nMozilla Common Voiceï¼ˆv11.0ï¼‰\nMozilla Common Voiceï¼ˆv7.0ï¼‰\nMozilla Common Voiceï¼ˆv4.0ï¼‰\nAMI\nFLEURS\n\nåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯¹ AMI è¿›è¡Œäº†è¿‡é‡‡æ ·ï¼Œä½¿å…¶çº¦å è§‚å¯Ÿåˆ°çš„æ€»æ•°æ®çš„ 15%ã€‚è¿™ä½¿å¾—æ¨¡å‹å€¾å‘äºç”Ÿæˆé€å­—è®°å½•ï¼Œå…¶ä¸­åŒ…æ‹¬è¯¸å¦‚é‡å¤ç­‰ä¼šè¯è¯­éŸ³ä¸æµç•…ç°è±¡ã€‚\nè®­ç»ƒè½¬å½•æ–‡æœ¬åŒ…å«æ ‡ç‚¹ç¬¦å·å’Œå¤§å°å†™ã€‚\n\n\nè¯„ä¼°æ•°æ®é›†ï¼š\næ•°æ®æ”¶é›†æ–¹æ³•ï¼š \n\näººå·¥ \n\næ ‡æ³¨æ–¹æ³•ï¼š \n\näººå·¥ \n\nè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼š\n\nHuggingFace OpenASR æ’è¡Œæ¦œè¯„ä¼°é›†\n\nå¹»è§‰é²æ£’æ€§ï¼š\n\nMUSAN 48 å°æ—¶è¯„ä¼°é›†\n\nå™ªå£°é²æ£’æ€§ï¼š\n\nLibrispeech\n\næ¨¡å‹å…¬å¹³æ€§ï¼š\n\nCasual Conversations Dataset\n\n\n\næ€§èƒ½\nASR é¢„æµ‹ä½¿ç”¨è´ªå©ªè§£ç ç”Ÿæˆã€‚\n\n\nASR æ€§èƒ½ï¼ˆæ—  PnCï¼‰\nASR æ€§èƒ½é€šè¿‡è¯é”™è¯¯ç‡ï¼ˆWERï¼‰è¡¡é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨ whisper-normalizer ç‰ˆæœ¬ 0.1.12 å¤„ç†çœŸå®æ–‡æœ¬å’Œé¢„æµ‹æ–‡æœ¬ã€‚\nHuggingFace OpenASR æ’è¡Œæ¦œ ä¸Šçš„ WERï¼š\n\n\n\nç‰ˆæœ¬\næ¨¡å‹\nRTFx\nå¹³å‡å€¼\nAMI\nGigaSpeech\nLS Clean\nLS Other\nEarnings22\nSPGISpech\nTedlium\nVoxpopuli\n\n\n\n\n2.5.0\nCanary-Qwen-2.5B\n418\n5.63\n10.18\n9.41\n1.60\n3.10\n10.42\n1.90\n2.72\n5.66\n\n\n\næœ‰å…³è¯„ä¼°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ HuggingFace ASR æ’è¡Œæ¦œ\n\n\nå¹»è§‰é²æ£’æ€§\nåœ¨ MUSAN 48å°æ—¶è¯„ä¼°é›†ä¸Šçš„æ¯åˆ†é’Ÿå­—ç¬¦æ•°ï¼ˆéµå¾ª nvidia/canary-1b-flash è¯„ä¼°æ–¹æ³•ï¼Œmax_new_tokens=50ï¼‰\n\n\n\nç‰ˆæœ¬\næ¨¡å‹\næ¯åˆ†é’Ÿå­—ç¬¦æ•°\n\n\n\n\n2.5.0\nCanary-Qwen-2.5B\n138.1\n\n\n\n\n\nå™ªå£°é²æ£’æ€§\nåœ¨ Librispeech Test Clean æ•°æ®é›†ä¸Šï¼Œä¸åŒä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ°´å¹³çš„åŠ æ€§ç™½å™ªå£°ä¸‹çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰\n\n\n\nç‰ˆæœ¬\næ¨¡å‹\nä¿¡å™ªæ¯” 10\nä¿¡å™ªæ¯” 5\nä¿¡å™ªæ¯” 0\nä¿¡å™ªæ¯” -5\n\n\n\n\n2.5.0\nCanary-Qwen-2.5B\n2.41%\n4.08%\n9.83%\n30.60%\n\n\n\n\n\næ¨¡å‹å…¬å¹³æ€§è¯„ä¼°\nå¦‚è®ºæ–‡ã€ŠTowards Measuring Fairness in AI: the Casual Conversations Datasetã€‹[8] ä¸­æ‰€è¿°ï¼Œæˆ‘ä»¬å¯¹ Canary-Qwen-2.5B æ¨¡å‹çš„å…¬å¹³æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚è¯¥æ¨¡å‹åœ¨ CasualConversations-v1 æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ¨ç†åœ¨éé‡å çš„ 40 ç§’éŸ³é¢‘ç‰‡æ®µä¸Šè¿›è¡Œï¼Œç»“æœæŠ¥å‘Šå¦‚ä¸‹ï¼š\n\n\næ€§åˆ«åè§ï¼š\n\n\n\næ€§åˆ«\nç”·æ€§\nå¥³æ€§\nä¸é€‚ç”¨\nå…¶ä»–\n\n\n\n\nutterance æ•°é‡\n18471\n23378\n880\n18\n\n\nè¯é”™è¯¯ç‡ï¼ˆ%ï¼‰\n16.71\n13.85\n17.71\n29.46\n\n\n\n\n\nå¹´é¾„åè§ï¼š\n\n\n\nå¹´é¾„ç»„\n(18-30)\n(31-45)\n(46-85)\n(1-100)\n\n\n\n\nutterance æ•°é‡\n15058\n13984\n12810\n41852\n\n\nè¯é”™è¯¯ç‡ï¼ˆ%ï¼‰\n15.73\n15.3\n14.14\n15.11\n\n\n\nï¼ˆå…¬å¹³æ€§è¯„ä¼°çš„é”™è¯¯ç‡æ˜¯é€šè¿‡å¯¹å‚è€ƒæ–‡æœ¬å’Œé¢„æµ‹æ–‡æœ¬è¿›è¡Œæ ‡å‡†åŒ–æ¥ç¡®å®šçš„ï¼Œç±»ä¼¼äº https://github.com/huggingface/open_asr_leaderboard ä¸­è¯„ä¼°æ‰€ä½¿ç”¨çš„æ–¹æ³•ã€‚ï¼‰\n\n\næ¨ç†ï¼š\nå¼•æ“ï¼š NVIDIA NeMo \næµ‹è¯•ç¡¬ä»¶ï¼š \n\nA6000 \nA100 \nRTX 5090 \n\n\n\nä¼¦ç†è€ƒé‡ï¼š\nNVIDIA è®¤ä¸ºå¯ä¿¡ AI æ˜¯ä¸€é¡¹å…±åŒçš„è´£ä»»ï¼Œæˆ‘ä»¬å·²åˆ¶å®šæ”¿ç­–å’Œå®è·µï¼Œä»¥æ”¯æŒå¹¿æ³›çš„ AI åº”ç”¨å¼€å‘ã€‚å½“æŒ‰ç…§æˆ‘ä»¬çš„æœåŠ¡æ¡æ¬¾ä¸‹è½½æˆ–ä½¿ç”¨æ—¶ï¼Œå¼€å‘äººå‘˜åº”ä¸å…¶å†…éƒ¨æ¨¡å‹å›¢é˜Ÿåˆä½œï¼Œç¡®ä¿æ­¤æ¨¡å‹æ»¡è¶³ç›¸å…³è¡Œä¸šå’Œç”¨ä¾‹çš„è¦æ±‚ï¼Œå¹¶è§£å†³æœªé¢„è§çš„äº§å“æ»¥ç”¨é—®é¢˜ã€‚\næœ‰å…³æ­¤æ¨¡å‹ä¼¦ç†è€ƒé‡çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… Model Card++ çš„å¯è§£é‡Šæ€§ã€åè§ã€å®‰å…¨ä¸å®‰ä¿ä»¥åŠéšç§å­å¡ã€‚è¯·é€šè¿‡ æ­¤å¤„ æŠ¥å‘Šå®‰å…¨æ¼æ´æˆ– NVIDIA AI ç›¸å…³é—®é¢˜ã€‚\n",
    "tags": [
      "PyTorch",
      "Transformers",
      "NeMo",
      "Safetensors",
      "English",
      "Creative Commons Attribution 4.0",
      "18 datasets",
      "FastConformer",
      "Transformer",
      "audio",
      "speech",
      "Qwen",
      "NeMo",
      "hf-asr-leaderboard",
      "Conformer",
      "automatic-speech-recognition",
      "arxiv:4 papers"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/LiquidAI/LFM2-700M-GGUF",
    "project_name": "LiquidAI/LFM2-700M-GGUF",
    "readme": "\n\n\n\n\n\n  Liquid: Playground\n  \n    \n    \n  \n  \n    \n\n\n  \n  \n    Liquid\n    Liquid\n    Playground\n    Playground\n  \n  \n    \n    \n  \n\n\n\n\n\nLFM2-700M-GGUF\nLFM2 æ˜¯ç”± Liquid AI å¼€å‘çš„æ–°ä¸€ä»£æ··åˆæ¨¡å‹ï¼Œä¸“ä¸ºè¾¹ç¼˜äººå·¥æ™ºèƒ½å’Œè®¾å¤‡ç«¯éƒ¨ç½²è€Œè®¾è®¡ã€‚å®ƒåœ¨è´¨é‡ã€é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡æ–¹é¢æ ‘ç«‹äº†æ–°çš„æ ‡å‡†ã€‚\næ›´å¤šè¯¦æƒ…è¯·å‚è§åŸå§‹æ¨¡å‹å¡ç‰‡ï¼šhttps://huggingface.co/LiquidAI/LFM2-700M\n\n\nğŸƒ å¦‚ä½•è¿è¡Œ LFM2\nä½¿ç”¨ llama.cpp çš„ç¤ºä¾‹ç”¨æ³•ï¼š\nllama-cli -hf LiquidAI/LFM2-700M-GGUF\n\n",
    "tags": [
      "Text Generation",
      "GGUF",
      "8 languages",
      "Other",
      "edge",
      "lfm2",
      "liquid",
      "llama.cpp"
    ]
  },
  {
    "url": "https://gitcode.com/hf_mirrors/LiquidAI/LFM2-1.2B",
    "project_name": "LiquidAI/LFM2-1.2B",
    "readme": "\n\n\n\n\n\n  Liquid: Playground\n  \n    \n    \n  \n  \n    \n\n\n  \n  \n    Liquid\n    Liquid\n    Playground\n    Playground\n  \n  \n    \n    \n  \n\n\n\n\n\nLFM2-1.2B\nLFM2 æ˜¯ç”± Liquid AI å¼€å‘çš„æ–°ä¸€ä»£æ··åˆæ¨¡å‹ï¼Œä¸“ä¸ºè¾¹ç¼˜äººå·¥æ™ºèƒ½å’Œç«¯ä¾§éƒ¨ç½²è®¾è®¡ã€‚å®ƒåœ¨è´¨é‡ã€é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡æ–¹é¢æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚\næˆ‘ä»¬å‘å¸ƒäº†ä¸‰ä¸ªç»è¿‡åè®­ç»ƒçš„æƒé‡æ£€æŸ¥ç‚¹ï¼Œå‚æ•°é‡åˆ†åˆ«ä¸º 3.5 äº¿ã€7 äº¿å’Œ 12 äº¿ã€‚å®ƒä»¬ä¸ºæ„å»ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„è¾¹ç¼˜åº”ç”¨æä¾›ä»¥ä¸‹æ ¸å¿ƒç‰¹æ€§ï¼š\n\nå¿«é€Ÿè®­ç»ƒä¸æ¨ç† â€“ LFM2 çš„è®­ç»ƒé€Ÿåº¦è¾ƒå‰ä»£æå‡ 3 å€ã€‚åœ¨ CPU ä¸Šç›¸æ¯” Qwen3 å®ç° 2 å€è§£ç é€Ÿåº¦å’Œé¢„å¡«å……åŠ é€Ÿã€‚\nå“è¶Šæ€§èƒ½è¡¨ç° â€“ åœ¨çŸ¥è¯†ç†è§£ã€æ•°å­¦æ¨ç†ã€æŒ‡ä»¤éµå¾ªå’Œå¤šè¯­è¨€èƒ½åŠ›ç­‰å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLFM2 å‡ä¼˜äºåŒè§„æ¨¡æ¨¡å‹ã€‚\nåˆ›æ–°æ¶æ„è®¾è®¡ â€“ é‡‡ç”¨å…·å¤‡ä¹˜æ³•é—¨æ§æœºåˆ¶å’ŒçŸ­å·ç§¯ç»“æ„çš„å…¨æ–°æ··åˆæ¶²æ€æ¨¡å‹æ¶æ„ã€‚\nçµæ´»éƒ¨ç½²æ–¹æ¡ˆ â€“ å¯é«˜æ•ˆè¿è¡Œäº CPUã€GPU å’Œ NPU ç¡¬ä»¶å¹³å°ï¼Œæ”¯æŒæ™ºèƒ½æ‰‹æœºã€ç¬”è®°æœ¬ç”µè„‘åŠè½¦è½½è®¾å¤‡çš„çµæ´»éƒ¨ç½²ã€‚\n\næ›´å¤šå…³äº LFM2 çš„è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…æˆ‘ä»¬çš„åšå®¢æ–‡ç« ã€‚\n\n\nğŸ“„ æ¨¡å‹è¯¦æƒ…\nåŸºäºå…¶ç´§å‡‘ä½“é‡ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ç‰¹å®šå‚ç›´åœºæ™¯ä¸­å¯¹ LFM2 æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æœ€å¤§åŒ–æ€§èƒ½è¡¨ç°ã€‚\nè¯¥ç³»åˆ—æ¨¡å‹ç‰¹åˆ«é€‚ç”¨äºæ™ºèƒ½ä½“ä»»åŠ¡ã€æ•°æ®æå–ã€RAG ç³»ç»Ÿã€åˆ›æ„å†™ä½œå’Œå¤šè½®å¯¹è¯åœºæ™¯ã€‚\nä½†ä¸æ¨èç”¨äºçŸ¥è¯†å¯†é›†å‹æˆ–éœ€è¦ç¼–ç¨‹èƒ½åŠ›çš„ä»»åŠ¡åœºæ™¯ã€‚\n\n\n\nå±æ€§\næ•°å€¼\n\n\n\n\nå‚æ•°é‡\n1,170,340,608\n\n\nå±‚æ•°\n16 (10 å·ç§¯å±‚ + 6 æ³¨æ„åŠ›å±‚)\n\n\nä¸Šä¸‹æ–‡é•¿åº¦\n32,768 ä¸ª token\n\n\nè¯è¡¨å¤§å°\n65,536\n\n\nç²¾åº¦æ ¼å¼\nbfloat16\n\n\nè®­ç»ƒæ•°æ®é‡\n10 ä¸‡äº¿ token\n\n\nè®¸å¯åè®®\nLFM å¼€æ”¾è®¸å¯ v1.0\n\n\n\næ”¯æŒè¯­è¨€ï¼šè‹±è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€ä¸­æ–‡ã€æ³•è¯­ã€å¾·è¯­ã€æ—¥è¯­ã€éŸ©è¯­å’Œè¥¿ç­ç‰™è¯­ã€‚\nç”Ÿæˆå‚æ•°æ¨èï¼š\n\ntemperature=0.3\nmin_p=0.15\nrepetition_penalty=1.05\n\nå¯¹è¯æ¨¡æ¿ï¼šLFM2 é‡‡ç”¨ç±» ChatML æ ¼å¼çš„å¯¹è¯æ¨¡æ¿å¦‚ä¸‹ï¼š\n<|startoftext|><|im_start|>system\nYou are a helpful assistant trained by Liquid AI.<|im_end|>\n<|im_start|>user\nWhat is C. elegans?<|im_end|>\n<|im_start|>assistant\nIt's a tiny nematode that lives in temperate soil environments.<|im_end|>\n\næ‚¨å¯ä»¥é€šè¿‡Hugging Face transformersåº“ä¸“ç”¨çš„.apply_chat_template()å‡½æ•°æ¥å®ç°æ­¤åŠŸèƒ½ã€‚\nå·¥å…·è°ƒç”¨ï¼šè¯¥è¿‡ç¨‹åŒ…å«å››ä¸ªä¸»è¦æ­¥éª¤ï¼š\n\nå‡½æ•°å®šä¹‰ï¼šLFM2æ¥æ”¶JSONæ ¼å¼çš„å‡½æ•°å®šä¹‰ä½œä¸ºè¾“å…¥ï¼ˆä½äºç‰¹æ®Šæ ‡è®°<|tool_list_start|>å’Œ<|tool_list_end|>ä¹‹é—´çš„JSONå¯¹è±¡ï¼‰ï¼Œé€šå¸¸ç½®äºç³»ç»Ÿæç¤ºä¸­\nå‡½æ•°è°ƒç”¨ï¼šLFM2ç”ŸæˆPythoné£æ ¼çš„å‡½æ•°è°ƒç”¨ï¼ˆä½äºç‰¹æ®Šæ ‡è®°<|tool_call_start|>å’Œ<|tool_call_end|>ä¹‹é—´çš„Pythonåˆ—è¡¨ï¼‰ï¼Œä½œä¸ºåŠ©æ‰‹å›ç­”\nå‡½æ•°æ‰§è¡Œï¼šæ‰§è¡Œå‡½æ•°è°ƒç”¨å¹¶è¿”å›ç»“æœï¼ˆä½äºç‰¹æ®Šæ ‡è®°<|tool_response_start|>å’Œ<|tool_response_end|>ä¹‹é—´çš„å­—ç¬¦ä¸²ï¼‰ï¼Œä»¥\"tool\"è§’è‰²å‘ˆç°\næœ€ç»ˆå›ç­”ï¼šLFM2è§£æå‡½æ•°è°ƒç”¨çš„æ‰§è¡Œç»“æœï¼Œä»¥çº¯æ–‡æœ¬å½¢å¼å›åº”ç”¨æˆ·çš„åŸå§‹æé—®\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨å·¥å…·è°ƒç”¨çš„ç®€å•å¯¹è¯ç¤ºä¾‹ï¼š\n<|startoftext|><|im_start|>system\nList of tools: <|tool_list_start|>[{\"name\": \"get_candidate_status\", \"description\": \"Retrieves the current status of a candidate in the recruitment process\", \"parameters\": {\"type\": \"object\", \"properties\": {\"candidate_id\": {\"type\": \"string\", \"description\": \"Unique identifier for the candidate\"}}, \"required\": [\"candidate_id\"]}}]<|tool_list_end|><|im_end|>\n<|im_start|>user\nWhat is the current status of candidate ID 12345?<|im_end|>\n<|im_start|>assistant\n<|tool_call_start|>[get_candidate_status(candidate_id=\"12345\")]<|tool_call_end|>Checking the current status of candidate ID 12345.<|im_end|>\n<|im_start|>tool\n<|tool_response_start|>{\"candidate_id\": \"12345\", \"status\": \"Interview Scheduled\", \"position\": \"Clinical Research Associate\", \"date\": \"2023-11-20\"}<|tool_response_end|><|im_end|>\n<|im_start|>assistant\nThe candidate with ID 12345 is currently in the \"Interview Scheduled\" stage for the position of Clinical Research Associate, with an interview date set for 2023-11-20.<|im_end|>\n\næ¶æ„ï¼šé‡‡ç”¨ä¹˜æ³•é—¨æ§ä¸çŸ­å·ç§¯çš„æ··åˆæ¨¡å‹ï¼šåŒ…å«10ä¸ªåŒé—¨æ§çŸ­ç¨‹LIVå·ç§¯å—å’Œ6ä¸ªåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰å—ã€‚\né¢„è®­ç»ƒæ•°æ®æ··åˆæ¯”ä¾‹ï¼šçº¦75%è‹±è¯­ã€20%å¤šè¯­è¨€åŠ5%ä»£ç æ•°æ®ï¼Œæ¥æºåŒ…æ‹¬ç½‘ç»œè·å–ä¸æˆæƒææ–™ã€‚\nè®­ç»ƒæ–¹æ³•ï¼š\n\nä½¿ç”¨LFM1-7Bä½œä¸ºæ•™å¸ˆæ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦\nè¶…å¤§è§„æ¨¡SFTè®­ç»ƒï¼š50%ä¸‹æ¸¸ä»»åŠ¡æ•°æ® + 50%é€šç”¨é¢†åŸŸæ•°æ®\né‡‡ç”¨å¸¦é•¿åº¦å½’ä¸€åŒ–çš„å®šåˆ¶DPOæ–¹æ³•ä¸åŠåœ¨çº¿æ•°æ®é›†\nè¿­ä»£å¼æ¨¡å‹èåˆ\n\n\n\nğŸƒ å¦‚ä½•è¿è¡ŒLFM2\næ‚¨å¯é€šè¿‡transformersä¸llama.cppè¿è¡ŒLFM2ï¼ŒvLLMæ”¯æŒå³å°†æ¨å‡ºã€‚\n\n\n1. Transformers\nè¿è¡ŒLFM2éœ€ä»æºç å®‰è£…Hugging Face transformersï¼ˆv4.54.0.dev0ç‰ˆæœ¬ï¼‰ã€‚å¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ›´æ–°æˆ–å®‰è£…ï¼špip install \"transformers @ git+https://github.com/huggingface/transformers.git@main\"\nä»¥ä¸‹æ˜¯é€šè¿‡Pythonä½¿ç”¨transformersç”Ÿæˆç­”æ¡ˆçš„ç¤ºä¾‹ï¼š\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load model and tokenizer\nmodel_id = \"LiquidAI/LFM2-1.2B\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=\"bfloat16\",\n#    attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Generate answer\nprompt = \"What is C. elegans?\"\ninput_ids = tokenizer.apply_chat_template(\n    [{\"role\": \"user\", \"content\": prompt}],\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    tokenize=True,\n).to(model.device)\n\noutput = model.generate(\n    input_ids,\n    do_sample=True,\n    temperature=0.3,\n    min_p=0.15,\n    repetition_penalty=1.05,\n    max_new_tokens=512,\n)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=False))\n\n# <|startoftext|><|im_start|>user\n# What is C. elegans?<|im_end|>\n# <|im_start|>assistant\n# C. elegans, also known as Caenorhabditis elegans, is a small, free-living\n# nematode worm (roundworm) that belongs to the phylum Nematoda.\n\næ‚¨å¯ä»¥ç›´æ¥é€šè¿‡è¿™ä¸ª Colab notebook è¿è¡Œå’Œæµ‹è¯•æ¨¡å‹ã€‚\n\n\n2. Llama.cpp\næ‚¨å¯ä»¥ä½¿ç”¨ llama.cpp é€šè¿‡å…¶ GGUF æ£€æŸ¥ç‚¹è¿è¡Œ LFM2ã€‚æ›´å¤šä¿¡æ¯è¯·å‚é˜…æ¨¡å‹å¡ç‰‡ã€‚\n\n\nğŸ”§ å¦‚ä½•å¾®è°ƒ LFM2\næˆ‘ä»¬å»ºè®®é’ˆå¯¹æ‚¨çš„å…·ä½“ç”¨ä¾‹å¯¹ LFM2 æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æœ€å¤§åŒ–æ€§èƒ½ã€‚\n\n\n\nç¬”è®°æœ¬\næè¿°\né“¾æ¥\n\n\n\n\nSFT (Unsloth)\nä½¿ç”¨ Unsloth é€šè¿‡ LoRA é€‚é…å™¨è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„ç¬”è®°æœ¬ã€‚\n\n\n\nSFT (Axolotl)\nä½¿ç”¨ Axolotl é€šè¿‡ LoRA é€‚é…å™¨è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„ç¬”è®°æœ¬ã€‚\n\n\n\nSFT (TRL)\nä½¿ç”¨ TRL é€šè¿‡ LoRA é€‚é…å™¨è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„ç¬”è®°æœ¬ã€‚\n\n\n\nDPO (TRL)\nä½¿ç”¨ TRL é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œåå¥½å¯¹é½ã€‚\n\n\n\n\n\n\nğŸ“ˆ æ€§èƒ½è¡¨ç°\nLFM2 åœ¨ä¸åŒè¯„ä¼°ç±»åˆ«ä¸­å‡ä¼˜äºåŒè§„æ¨¡æ¨¡å‹ã€‚\n\n\n1. è‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•\n\n\n\n\næ¨¡å‹\nMMLU\nGPQA\nIFEval\nIFBench\nGSM8K\nMGSM\nMMMLU\n\n\n\n\nLFM2-350M\n43.43\n27.46\n65.12\n16.41\n30.1\n29.52\n37.99\n\n\nLFM2-700M\n49.9\n28.48\n72.23\n20.56\n46.4\n45.36\n43.28\n\n\nLFM2-1.2B\n55.23\n31.47\n74.89\n20.7\n58.3\n55.04\n46.73\n\n\nQwen3-0.6B\n44.93\n22.14\n64.24\n19.75\n36.47\n41.28\n30.84\n\n\nQwen3-1.7B\n59.11\n27.72\n73.98\n21.27\n51.4\n66.56\n46.51\n\n\nLlama-3.2-1B-Instruct\n46.6\n28.84\n52.39\n16.86\n35.71\n29.12\n38.15\n\n\ngemma-3-1b-it\n40.08\n21.07\n62.9\n17.72\n59.59\n43.6\n34.43\n\n\n\n\n\n2. å¤§è¯­è¨€æ¨¡å‹å³è¯„åˆ¤å‘˜ï¼ˆLLM-as-a-Judgeï¼‰\n\n\n\n\n3. æ¨ç†æ€§èƒ½\n\n\nExecuTorchæ¡†æ¶ä¸‹CPUååé‡å¯¹æ¯”\n\n\n\nLlama.cppæ¡†æ¶ä¸‹CPUååé‡å¯¹æ¯”\n\n\n\nğŸ“¬ è”ç³»æˆ‘ä»¬\nå¦‚æœæ‚¨å¯¹è¾¹ç¼˜ç«¯å®šåˆ¶åŒ–è§£å†³æ–¹æ¡ˆæ„Ÿå…´è¶£ï¼Œè¯·è”ç³»æˆ‘ä»¬çš„é”€å”®å›¢é˜Ÿã€‚\n",
    "tags": [
      "Text Generation",
      "Transformers",
      "Safetensors",
      "8 languages",
      "Other",
      "edge",
      "lfm2",
      "liquid"
    ]
  }
]