[
  {
    "url": "https://gitcode.com/ModelEngine/Model-OpenSource-images",
    "project_name": "Model-OpenSource-images",
    "readme": "äº§å“ä»‹ç» ModelEngineæä¾›ä»æ•°æ®å¤„ç†ã€çŸ¥è¯†ç”Ÿæˆï¼Œåˆ°æ¨¡å‹å¾®è°ƒå’Œéƒ¨ç½²ï¼Œä»¥åŠRAGï¼ˆRetrieval Augmented Generationï¼‰åº”ç”¨å¼€å‘çš„AIè®­æ¨å…¨æµç¨‹å·¥å…·é“¾ï¼Œç”¨äºç¼©çŸ­ä»æ•°æ®åˆ°æ¨¡å‹ã€ æ•°æ®åˆ°AIåº”ç”¨çš„è½åœ°å‘¨æœŸã€‚ModelEngineæä¾›ä½ä»£ç ç¼–æ’ã€çµæ´»çš„æ‰§è¡Œè°ƒåº¦ã€é«˜æ€§èƒ½ æ•°æ®æ€»çº¿ç­‰æŠ€æœ¯ï¼Œç»“åˆå†…ç½®çš„æ•°æ®å¤„ç†ç®—å­ã€RAGæ¡†æ¶ä»¥åŠå¹¿æ³›çš„ç”Ÿæ€èƒ½åŠ›ï¼Œä¸ºæ•°æ® å¼€å‘å·¥ç¨‹å¸ˆã€æ¨¡å‹å¼€å‘å·¥ç¨‹å¸ˆã€åº”ç”¨å¼€å‘å·¥ç¨‹å¸ˆæä¾›é«˜æ•ˆæ˜“ç”¨ã€å¼€æ”¾çµæ´»ã€å¼€ç®±å³ ç”¨ã€è½»é‡çš„å…¨æµç¨‹AIå¼€å‘ä½“éªŒã€‚\n\nModelEngineäº§å“åŒ…æ‹¬æ•°æ®ä½¿èƒ½ã€æ¨¡å‹ä½¿èƒ½ã€åº”ç”¨ä½¿èƒ½æ¨¡å—ï¼Œå„æ¨¡å—åŠŸèƒ½å®šä½å¦‚ä¸‹ï¼š\n\næ•°æ®ä½¿èƒ½ï¼šæ˜¯ç”¨äºæ–‡æœ¬ã€å›¾åƒã€æ–‡æ¡£ç­‰å„ç±»å‹æ•°æ®å¤„ç†çš„å·¥å…·é“¾ï¼Œæä¾›å¦‚æ•°æ®æ¸…æ´—ã€æ•°æ®è¯„ä¼°ã€QAå¯¹ç”Ÿæˆã€çŸ¥è¯†ç”Ÿæˆå…³é”®èƒ½åŠ›ï¼Œä¸ºå¤§æ¨¡å‹è®­ç»ƒå’ŒRAGåº”ç”¨æä¾›è¯­æ–™å’ŒçŸ¥è¯†ï¼Œåº”ç”¨åœºæ™¯å¦‚ä¸‹ï¼š â— é¢å‘å¤§æ¨¡å‹è®­ç»ƒåœºæ™¯ï¼šæä¾›æ•°æ®å¤„ç†å·¥å…·é“¾ï¼Œè§£å†³ç”¨æˆ·é«˜è´¨é‡æ•°æ®å¤„ç†å›°éš¾çš„é—® é¢˜ã€ç¼“è§£æ•°æ®é‡ä¸è¶³çš„ç—›ç‚¹ï¼Œè¾…åŠ©æ¨¡å‹è®­ç»ƒææ•ˆã€‚\n\nâ— é¢å‘å¤§æ¨¡å‹æ¨ç†åœºæ™¯ï¼šæä¾›çŸ¥è¯†åº“ç®¡ç†èƒ½åŠ›ï¼Œè§£å†³è¡Œä¸šç”¨æˆ·é¢†åŸŸçŸ¥è¯†ç”Ÿæˆä¸æ›´æ–° çš„éœ€æ±‚ï¼ŒçŸ¥è¯†ç”Ÿæˆèƒ½åŠ›ï¼Œå¸®åŠ©ç”¨æˆ·æ›´åŠ æœ‰æ•ˆè½åœ°å¤§æ¨¡å‹.\n\næ¨¡å‹ä½¿èƒ½ï¼šæ˜¯é¢å‘æ¨¡å‹å¾®è°ƒå’Œæ¨¡å‹æ¨ç†çš„å·¥å…·é“¾ï¼Œé™ä½æ¨¡å‹è®­ç»ƒå’Œæ¨ç†é—¨æ§›ï¼Œä¸»è¦åº”ç”¨åœºæ™¯å¦‚ä¸‹ï¼š â— æ¨¡å‹è®­ç»ƒï¼šæä¾›æ¨¡å‹è®­ç»ƒå·¥å…·é“¾ï¼ŒåŸºäºç•Œé¢å†…é…ç½®æ–¹å¼ï¼Œé™ä½æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒéš¾ åº¦ã€‚\n\nâ— æ¨¡å‹éƒ¨ç½²ï¼šæä¾›æ¨¡å‹è¯„æµ‹ã€æ¨¡å‹ä»“åº“ã€æ¨¡å‹æœåŠ¡ç­‰åŠŸèƒ½ï¼Œå®ç°å¤§æ¨¡å‹éƒ¨ç½²ã€ä¸Šçº¿ å’Œç‰ˆæœ¬ç®¡ç†ã€‚\n\nâ— æ¨¡å‹ç®¡ç†ï¼šæä¾›æ¨¡å‹æƒé‡ç®¡ç†ã€æ¨¡å‹é‡åŒ–ï¼Œè®­ç»ƒåçš„checkpointså½’æ¡£åŠŸèƒ½ã€‚\n\nâ— æ¨¡å‹è¯„æµ‹ï¼šæä¾›åˆ©ç”¨ç¬¬ä¸‰æ–¹æˆ–è€…è‡ªå®šä¹‰è¯„æµ‹æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œç²¾åº¦å’Œæ€§èƒ½è¯„æµ‹çš„èƒ½ åŠ›ã€‚\n\næ¨¡å‹ä½¿èƒ½å…³é”®èƒ½åŠ›åŒ…æ‹¬ï¼š\n\nâ— æä¾›è®­ç»ƒå¼•æ“å’Œæ¨ç†å¼•æ“ï¼Œæ”¯æŒå¼€å‘è€…æ¨¡å‹é€‰å‹/è¯„æµ‹ã€æ¨¡å‹è®­ç»ƒã€æ¨¡å‹ç®¡ç†å’Œæ¨¡ å‹æ¨ç†ï¼›æä¾›æ— ä»£ç æ“ä½œã€ä¸€é”®ç²¾è°ƒèƒ½åŠ›ã€‚\n\nâ— æä¾›OpenAIæ ‡å‡†æ¨ç†æ¥å£ï¼Œä¸€é”®éƒ¨ç½²æ¨¡å‹ã€‚\n\nâ— å¼€æ”¾ç¡¬ä»¶ç”Ÿæ€ï¼Œæ”¯æŒæ˜‡è…¾NPUï¼›å¼€æ”¾æ¨¡å‹ç”Ÿæ€ï¼Œæ”¯æŒsafetensorsæ ¼å¼æ¨¡å‹æƒé‡ã€‚\n\nå¦‚ä½•å‚ä¸è´¡çŒ® https://gitcode.com/ModelEngine/ModelEngineCommunity/blob/main/å¦‚ä½•å‚ä¸è´¡çŒ®.md\n\næœ¬ä»“åº“æ˜¯Modelengine24.1.0ç‰ˆæœ¬Modelliteé…å¥—æ¨ç†é•œåƒ",
    "tags": "[\"Transformers\"]"
  },
  {
    "url": "https://gitcode.com/openMind/Wan2.1-T2V-14B-Diffusers",
    "project_name": "Wan2.1-T2V-14B-Diffusers",
    "readme": "Wan2.1\n\nğŸ’œ Wan Â Â  ï½œ Â Â  ğŸ–¥ï¸ GitHub Â Â  | Â Â ğŸ¤— Hugging FaceÂ Â  | Â Â ğŸ¤– ModelScopeÂ Â  | Â Â  ğŸ“‘ Paper (Coming soon) Â Â  | Â Â  ğŸ“‘ Blog Â Â  | Â Â ğŸ’¬ WeChat GroupÂ Â  | Â Â  ğŸ“– DiscordÂ Â \n\n\nWan: Open and Advanced Large-Scale Video Generative Models\n\nIn this repository, we present Wan2.1, a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. Wan2.1 offers these key features:\n\nğŸ‘ SOTA Performance: Wan2.1 consistently outperforms existing open-source models and state-of-the-art commercial solutions across multiple benchmarks.\nğŸ‘ Supports Consumer-grade GPUs: The T2V-1.3B model requires only 8.19 GB VRAM, making it compatible with almost all consumer-grade GPUs. It can generate a 5-second 480P video on an RTX 4090 in about 4 minutes (without optimization techniques like quantization). Its performance is even comparable to some closed-source models.\nğŸ‘ Multiple Tasks: Wan2.1 excels in Text-to-Video, Image-to-Video, Video Editing, Text-to-Image, and Video-to-Audio, advancing the field of video generation.\nğŸ‘ Visual Text Generation: Wan2.1 is the first video model capable of generating both Chinese and English text, featuring robust text generation that enhances its practical applications.\nğŸ‘ Powerful Video VAE: Wan-VAE delivers exceptional efficiency and performance, encoding and decoding 1080P videos of any length while preserving temporal information, making it an ideal foundation for video and image generation.\n\nThis repository features our T2V-14B model, which establishes a new SOTA performance benchmark among both open-source and closed-source models. It demonstrates exceptional capabilities in generating high-quality visuals with significant motion dynamics. It is also the only video model capable of producing both Chinese and English text and supports video generation at both 480P and 720P resolutions.\n\nVideo Demos\nğŸ”¥ Latest News!!\nFeb 22, 2025: ğŸ‘‹ We've released the inference code and weights of Wan2.1.\nğŸ“‘ Todo List\nWan2.1 Text-to-Video\nMulti-GPU Inference code of the 14B and 1.3B models\nCheckpoints of the 14B and 1.3B models\nGradio demo\nDiffusers integration\nComfyUI integration\nWan2.1 Image-to-Video\nMulti-GPU Inference code of the 14B model\nCheckpoints of the 14B model\nGradio demo\nDiffusers integration\nComfyUI integration\nQuickstart\nInstallation\n\nClone the repo:\n\ngit clone https://github.com/Wan-Video/Wan2.1.git\ncd Wan2.1\n\n\nInstall dependencies:\n\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\n\nModel Download\nModels\tDownload Link\tNotes\nT2V-14B\tğŸ¤— Huggingface ğŸ¤– ModelScope\tSupports both 480P and 720P\nI2V-14B-720P\tğŸ¤— Huggingface ğŸ¤– ModelScope\tSupports 720P\nI2V-14B-480P\tğŸ¤— Huggingface ğŸ¤– ModelScope\tSupports 480P\nT2V-1.3B\tğŸ¤— Huggingface ğŸ¤– ModelScope\tSupports 480P\n\nğŸ’¡Note: The 1.3B model is capable of generating videos at 720P resolution. However, due to limited training at this resolution, the results are generally less stable compared to 480P. For optimal performance, we recommend using 480P resolution.\n\nDownload models using ğŸ¤— huggingface-cli:\n\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.1-T2V-14B-Diffusers --local-dir ./Wan2.1-T2V-14B-Diffusers\n\n\nDownload models using ğŸ¤– modelscope-cli:\n\npip install modelscope\nmodelscope download Wan-AI/Wan2.1-T2V-14B-Diffusers --local_dir ./Wan2.1-T2V-14B-Diffusers\n\nRun Text-to-Video Generation\n\nThis repository supports two Text-to-Video models (1.3B and 14B) and two resolutions (480P and 720P). The parameters and configurations for these models are as follows:\n\nTask\tResolution\tModel\n480P\t720P\nt2v-14B\tâœ”ï¸\tâœ”ï¸\tWan2.1-T2V-14B\nt2v-1.3B\tâœ”ï¸\tâŒ\tWan2.1-T2V-1.3B\n(1) Without Prompt Extention\n\nTo facilitate implementation, we will start with a basic version of the inference process that skips the prompt extension step.\n\nSingle-GPU inference\npython generate.py  --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\n\n\nIf you encounter OOM (Out-of-Memory) issues, you can use the --offload_model True and --t5_cpu options to reduce GPU memory usage. For example, on an RTX 4090 GPU:\n\npython generate.py  --task t2v-1.3B --size 832*480 --ckpt_dir ./Wan2.1-T2V-1.3B --offload_model True --t5_cpu --sample_shift 8 --sample_guide_scale 6 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\n\n\nğŸ’¡Note: If you are using the T2V-1.3B model, we recommend setting the parameter --sample_guide_scale 6. The --sample_shift parameter can be adjusted within the range of 8 to 12 based on the performance.\n\nMulti-GPU inference using FSDP + xDiT USP\npip install \"xfuser>=0.4.1\"\ntorchrun --nproc_per_node=8 generate.py --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\n\n\nWan can also be run directly using ğŸ¤— Diffusers!\n\nimport torch\nfrom diffusers import AutoencoderKLWan, WanPipeline\nfrom diffusers.utils import export_to_video\n\n# Available models: Wan-AI/Wan2.1-T2V-14B-Diffusers, Wan-AI/Wan2.1-T2V-1.3B-Diffusers\nmodel_id = \"Wan-AI/Wan2.1-T2V-14B-Diffusers\"\nvae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\npipe = WanPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\n\nprompt = \"A cat walks on the grass, realistic\"\nnegative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n\noutput = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    height=480,\n    width=832,\n    num_frames=81,\n    guidance_scale=5.0\n).frames[0]\nexport_to_video(output, \"output.mp4\", fps=15)\n\n(2) Using Prompt Extention\n\nExtending the prompts can effectively enrich the details in the generated videos, further enhancing the video quality. Therefore, we recommend enabling prompt extension. We provide the following two methods for prompt extension:\n\nUse the Dashscope API for extension.\nApply for a dashscope.api_key in advance (EN | CN).\nConfigure the environment variable DASH_API_KEY to specify the Dashscope API key. For users of Alibaba Cloud's international site, you also need to set the environment variable DASH_API_URL to 'https://dashscope-intl.aliyuncs.com/api/v1'. For more detailed instructions, please refer to the dashscope document.\nUse the qwen-plus model for text-to-video tasks and qwen-vl-max for image-to-video tasks.\nYou can modify the model used for extension with the parameter --prompt_extend_model. For example:\nDASH_API_KEY=your_key python generate.py  --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'dashscope' --prompt_extend_target_lang 'ch'\n\n\nUsing a local model for extension.\n\nBy default, the Qwen model on HuggingFace is used for this extension. Users can choose based on the available GPU memory size.\nFor text-to-video tasks, you can use models like Qwen/Qwen2.5-14B-Instruct, Qwen/Qwen2.5-7B-Instruct and Qwen/Qwen2.5-3B-Instruct\nFor image-to-video tasks, you can use models like Qwen/Qwen2.5-VL-7B-Instruct and Qwen/Qwen2.5-VL-3B-Instruct.\nLarger models generally provide better extension results but require more GPU memory.\nYou can modify the model used for extension with the parameter --prompt_extend_model , allowing you to specify either a local model path or a Hugging Face model. For example:\npython generate.py  --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'local_qwen' --prompt_extend_target_lang 'ch'\n\n(3) Runing local gradio\ncd gradio\n# if one uses dashscopeâ€™s API for prompt extension\nDASH_API_KEY=your_key python t2v_14B_singleGPU.py --prompt_extend_method 'dashscope' --ckpt_dir ./Wan2.1-T2V-14B\n\n# if one uses a local model for prompt extension\npython t2v_14B_singleGPU.py --prompt_extend_method 'local_qwen' --ckpt_dir ./Wan2.1-T2V-14B\n\nManual Evaluation\n\nThrough manual evaluation, the results generated after prompt extension are superior to those from both closed-source and open-source models.\n\nComputational Efficiency on Different GPUs\n\nWe test the computational efficiency of different Wan2.1 models on different GPUs in the following table. The results are presented in the format: Total time (s) / peak GPU memory (GB).\n\nThe parameter settings for the tests presented in this table are as follows: (1) For the 1.3B model on 8 GPUs, set --ring_size 8 and --ulysses_size 1; (2) For the 14B model on 1 GPU, use --offload_model True; (3) For the 1.3B model on a single 4090 GPU, set --offload_model True --t5_cpu; (4) For all testings, no prompt extension was applied, meaning --use_prompt_extend was not enabled.\n\nCommunity Contributions\nDiffSynth-Studio provides more support for Wan, including video-to-video, FP8 quantization, VRAM optimization, LoRA training, and more. Please refer to their examples.\nIntroduction of Wan2.1\n\nWan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. These include our novel spatio-temporal variational autoencoder (VAE), scalable training strategies, large-scale data construction, and automated evaluation metrics. Collectively, these contributions enhance the modelâ€™s performance and versatility.\n\n(1) 3D Variational Autoencoders\n\nWe propose a novel 3D causal VAE architecture, termed Wan-VAE specifically designed for video generation. By combining multiple strategies, we improve spatio-temporal compression, reduce memory usage, and ensure temporal causality. Wan-VAE demonstrates significant advantages in performance efficiency compared to other open-source VAEs. Furthermore, our Wan-VAE can encode and decode unlimited-length 1080P videos without losing historical temporal information, making it particularly well-suited for video generation tasks.\n\n(2) Video Diffusion DiT\n\nWan2.1 is designed using the Flow Matching framework within the paradigm of mainstream Diffusion Transformers. Our model's architecture uses the T5 Encoder to encode multilingual text input, with cross-attention in each transformer block embedding the text into the model structure. Additionally, we employ an MLP with a Linear layer and a SiLU layer to process the input time embeddings and predict six modulation parameters individually. This MLP is shared across all transformer blocks, with each block learning a distinct set of biases. Our experimental findings reveal a significant performance improvement with this approach at the same parameter scale.\n\nModel\tDimension\tInput Dimension\tOutput Dimension\tFeedforward Dimension\tFrequency Dimension\tNumber of Heads\tNumber of Layers\n1.3B\t1536\t16\t16\t8960\t256\t12\t30\n14B\t5120\t16\t16\t13824\t256\t40\t40\nData\n\nWe curated and deduplicated a candidate dataset comprising a vast amount of image and video data. During the data curation process, we designed a four-step data cleaning process, focusing on fundamental dimensions, visual quality and motion quality. Through the robust data processing pipeline, we can easily obtain high-quality, diverse, and large-scale training sets of images and videos.\n\nComparisons to SOTA\n\nWe compared Wan2.1 with leading open-source and closed-source models to evaluate the performace. Using our carefully designed set of 1,035 internal prompts, we tested across 14 major dimensions and 26 sub-dimensions. We then compute the total score by performing a weighted calculation on the scores of each dimension, utilizing weights derived from human preferences in the matching process. The detailed results are shown in the table below. These results demonstrate our model's superior performance compared to both open-source and closed-source models.\n\nCitation\n\nIf you find our work helpful, please cite us.\n\n@article{wan2.1,\n    title   = {Wan: Open and Advanced Large-Scale Video Generative Models},\n    author  = {Wan Team},\n    journal = {},\n    year    = {2025}\n}\n\nLicense Agreement\n\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generate contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\n\nAcknowledgements\n\nWe would like to thank the contributors to the SD3, Qwen, umt5-xxl, diffusers and HuggingFace repositories, for their open research.\n\nContact Us\n\nIf you would like to leave a message to our research or product teams, feel free to join our Discord or WeChat groups!",
    "tags": "[\"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/yolov10x",
    "project_name": "yolov10x",
    "readme": "Model Description\n\nYOLOv10: Real-Time End-to-End Object Detection\n\narXiv: https://arxiv.org/abs/2405.14458v1\ngithub: https://github.com/THU-MIG/yolov10\nInstallation\npip install git+https://github.com/THU-MIG/yolov10.git\n\nTraining and validation\nfrom ultralytics import YOLOv10\n\nmodel = YOLOv10.from_pretrained('jameslahm/yolov10x')\n# Training\nmodel.train(...)\n# after training, one can push to the hub\nmodel.push_to_hub(\"your-hf-username/yolov10-finetuned\")\n\n# Validation\nmodel.val(...)\n\nInference\n\nHere's an end-to-end example showcasing inference on a cats image:\n\nfrom ultralytics import YOLOv10\n\nmodel = YOLOv10.from_pretrained('jameslahm/yolov10x')\nsource = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nmodel.predict(source=source, save=True)\n\n\nwhich shows:\n\nBibTeX Entry and Citation Info\n@article{wang2024yolov10,\n title={YOLOv10: Real-Time End-to-End Object Detection},\n author={Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang},\n journal={arXiv preprint arXiv:2405.14458},\n year={2024}\n}\n",
    "tags": "[\"GNU Affero General Public License v3.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/GOT-OCR2_0",
    "project_name": "GOT-OCR2_0",
    "readme": "é€šç”¨OCRç†è®ºï¼šé€šè¿‡ç»Ÿä¸€çš„ç«¯åˆ°ç«¯æ¨¡å‹å®ç°OCR-2.0\n\nğŸ”‹åœ¨çº¿æ¼”ç¤º | ğŸŒŸGitHub | ğŸ“œè®ºæ–‡\n\nHaoran Wei*, Chenglong Liu*, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang\n\nä½¿ç”¨æ–¹æ³•\n\nä½¿ç”¨ Huggingface transformers åœ¨ NVIDIA GPU ä¸Šè¿›è¡Œæ¨ç†ã€‚è¦æ±‚å·²åœ¨ Python 3.10 ä¸Šè¿›è¡Œæµ‹è¯•ï¼š\n\ntorch==2.0.1\ntorchvision==0.15.2\ntransformers==4.37.2\ntiktoken==0.6.0\nverovio==4.3.1\naccelerate==0.28.0\n\nfrom transformers import AutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True)\nmodel = AutoModel.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True, low_cpu_mem_usage=True, device_map='cuda', use_safetensors=True, pad_token_id=tokenizer.eos_token_id)\nmodel = model.eval().cuda()\n\n\n# è¾“å…¥æµ‹è¯•å›¾åƒ\nimage_file = 'xxx.jpg'\n\n# æ™®é€šæ–‡æœ¬OCR\nres = model.chat(tokenizer, image_file, ocr_type='ocr')\n\n# æ ¼å¼åŒ–æ–‡æœ¬OCRï¼š\n# res = model.chat(tokenizer, image_file, ocr_type='format')\n\n# ç²¾ç»†åŒ–OCRï¼š\n# res = model.chat(tokenizer, image_file, ocr_type='ocr', ocr_box='') \n# res = model.chat(tokenizer, image_file, ocr_type='format', ocr_box='')\n# res = model.chat(tokenizer, image_file, ocr_type='ocr', ocr_color='') \n# res = model.chat(tokenizer, image_file, ocr_type='format', ocr_color='')\n\n# å¤šè£å‰ªOCRï¼š\n# res = model.chat_crop(tokenizer, image_file, ocr_type='ocr') \n# res = model.chat_crop(tokenizer, image_file, ocr_type='format')\n\n# æ¸²æŸ“æ ¼å¼åŒ–çš„OCRç»“æœï¼š\n# res = model.chat(tokenizer, image_file, ocr_type='format', render=True, save_render_file = './demo.html')\n\nprint(res)\n\n\nNPU ç¯å¢ƒä¸‹çš„æ¨ç†\n\nåœ¨ NPU ç¯å¢ƒä¸‹è¿›è¡Œæ¨ç†æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç¡®ä¿ PyTorch ä½¿ç”¨ npu ä½œä¸ºè®¾å¤‡è¿›è¡Œè®¡ç®—ã€‚ä¸‹é¢æ˜¯å¦‚ä½•åœ¨ NPU ä¸Šè¿›è¡Œæ¨ç†çš„ä»£ç ã€‚\n\nç¯å¢ƒè¦æ±‚ï¼š\nç¡®ä¿æ‚¨å·²ç»é…ç½®äº†æ”¯æŒ NPU çš„ PyTorch ç¯å¢ƒï¼Œå¹¶å®‰è£…äº† torch_npu æˆ–åä¸º Ascend çš„ç‰¹å®šç‰ˆæœ¬ã€‚\nNPU æ¨ç†ä»£ç ï¼š\nimport sys\nimport os\nimport torch\nfrom PIL import Image\nfrom transformers import AutoTokenizer\nfrom torchvision import transforms  # ç”¨äºå›¾åƒè½¬æ¢\n\n# è®¾ç½®è·¯å¾„\nmodel_path = '/GOT-OCR2_0'  # æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è·¯å¾„\n\n# ç¡®ä¿è·¯å¾„æ·»åŠ åˆ°sys.path\nsys.path.append(os.path.abspath(model_path))\n\n# è®¾ç½®æ¨¡å‹è·¯å¾„\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# åˆå§‹åŒ–ImageEncoderViTï¼ˆæ ¹æ®æä¾›çš„æ¨¡å‹å‚æ•°ï¼‰\nimage_encoder = ImageEncoderViT(\n    depth=12,\n    embed_dim=768,\n    img_size=1024,\n    mlp_ratio=4,\n    norm_layer=torch.nn.LayerNorm,\n    num_heads=12,\n    patch_size=16,\n    qkv_bias=True,\n    use_rel_pos=True,\n    global_attn_indexes=[2, 5, 8, 11],\n    window_size=14,\n    out_chans=256,\n)\n\n# å¼ºåˆ¶ä½¿ç”¨NPUè®¾å¤‡\ndevice = torch.device(\"npu\")  # ç¡®ä¿ä½¿ç”¨NPUè®¾å¤‡\n\n# æ‰“å°å½“å‰ç¯å¢ƒï¼ˆNPUï¼‰\nprint(f\"ä½¿ç”¨ {device.type.upper()} è¿›è¡Œæ¨ç†ã€‚\")  \n\n# å°†æ¨¡å‹åŠ è½½åˆ° NPU ä¸Š\nimage_encoder.to(device)\n\n# åˆ›å»ºä¸€ä¸ªç©ºç™½å›¾åƒ\nimage = Image.new('RGB', (1024, 1024), color=(255, 255, 255))\n\n# å®šä¹‰è½¬æ¢æ“ä½œï¼šå°†PILå›¾åƒè½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–\ntransform = transforms.ToTensor()\n\n# å°†å›¾åƒè½¬æ¢ä¸ºtensor\nimage_tensor = transform(image).unsqueeze(0).to(device)  # å¢åŠ æ‰¹æ¬¡ç»´åº¦å¹¶ç§»è‡³è®¾å¤‡\n\n# ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†\noutput = image_encoder(image_tensor)  # é€šè¿‡ImageEncoderViTè¿›è¡Œæ¨ç†\n\n# è¾“å‡ºç»“æœ\nprint(\"OCR ç»“æœ:\", output)\n\nè¯´æ˜ï¼š\nè®¾å¤‡è®¾ç½®ï¼šæˆ‘ä»¬å¼ºåˆ¶å°†è®¾å¤‡è®¾ç½®ä¸º NPU (torch.device(\"npu\"))ï¼Œå¹¶å°†æ¨¡å‹å’Œè¾“å…¥æ•°æ®ç§»åˆ° NPU ä¸Šè¿›è¡Œæ¨ç†ã€‚\nè·¯å¾„è®¾ç½®ï¼šè¯·ç¡®ä¿æ‚¨å°† model_path è®¾ç½®ä¸ºå®é™…çš„æ¨¡å‹è·¯å¾„ï¼Œå¹¶æ ¹æ®éœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚\nå›¾åƒè¾“å…¥ï¼šé€šè¿‡ PIL åˆ›å»ºä¸€ä¸ªç©ºç™½å›¾åƒï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º Tensorï¼Œéšåä¼ è¾“åˆ° NPU è®¾å¤‡ä¸Šã€‚\næ›´å¤šå¤šæ¨¡æ€é¡¹ç›®\n\nğŸ‘ æ¬¢è¿æ¢ç´¢æˆ‘ä»¬å›¢é˜Ÿçš„æ›´å¤šå¤šæ¨¡æ€é¡¹ç›®ï¼š\n\nVary | Fox | OneChart\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ ğŸ“ å¹¶ä¸ºè¿™ä¸ªé¡¹ç›®ç‚¹èµ â¤ï¸ï¼\n\n@article{wei2024general,\n  title={General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model},\n  author={Wei, Haoran and Liu, Chenglong and Chen, Jinyue and Wang, Jia and Kong, Lingyu and Xu, Yanming and Ge, Zheng and Zhao, Liang and Sun, Jianjian and Peng, Yuang and others},\n  journal={arXiv preprint arXiv:2409.01704},\n  year={2024}\n}\n@article{liu2024focus,\n  title={Focus Anywhere for Fine-grained Multi-page Document Understanding},\n  author={Liu, Chenglong and Wei, Haoran and Chen, Jinyue and Kong, Lingyu and Ge, Zheng and Zhu, Zining and Zhao, Liang and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},\n  journal={arXiv preprint arXiv:2405.14295},\n  year={2024}\n}\n@article{wei2023vary,\n  title={Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models},\n  author={Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yang, Jinrong and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},\n  journal={arXiv preprint arXiv:2312.06109},\n  year={2023}\n}\n",
    "tags": "[\"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/yanfan/yuxp1",
    "project_name": "yuxp1",
    "readme": "Original Text\nQwen2.5-Coder-7B-Instruct\nç®€ä»‹\n\nQwen2.5-Coder æ˜¯æ–°ä¸€ä»£ä»£ç ä¸“ç”¨å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼ˆå‰èº«ä¸º CodeQwenï¼‰ã€‚æœ¬æ¬¡å‘å¸ƒçš„ Qwen2.5-Coder åŒ…å«ä¸‰ä¸ªåŸºç¡€è¯­è¨€æ¨¡å‹ä¸æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼Œå‚æ•°é‡åˆ†åˆ«ä¸º 1.5Bã€7B åŠå³å°†æ¨å‡ºçš„ 32Bã€‚ç›¸è¾ƒäº CodeQwen1.5ï¼ŒQwen2.5-Coder å®ç°äº†ä»¥ä¸‹çªç ´ï¼š\n\nä»£ç ç”Ÿæˆã€ä»£ç æ¨ç†ä¸ä»£ç ä¿®å¤èƒ½åŠ›æ˜¾è‘—æå‡ã€‚åŸºäºå¼ºå¤§çš„ Qwen2.5 æ¶æ„ï¼Œæˆ‘ä»¬å°†è®­ç»ƒæ•°æ®è§„æ¨¡æ‰©å±•è‡³ 5.5 ä¸‡äº¿ tokenï¼Œæ¶µç›–æºä»£ç ã€æ–‡æœ¬-ä»£ç å¯¹é½æ•°æ®ã€åˆæˆæ•°æ®ç­‰å¤šç±»å‹è¯­æ–™\nä¸ºæ™ºèƒ½ä½“ç¼–ç¨‹ç­‰å®é™…åº”ç”¨åœºæ™¯å¥ å®šæ›´åšå®åŸºç¡€ã€‚åœ¨å¼ºåŒ–ç¼–ç¨‹èƒ½åŠ›çš„åŒæ—¶ï¼Œä¿æŒå…¶åœ¨æ•°å­¦ä¸é€šç”¨ä»»åŠ¡é¢†åŸŸçš„ä¼˜åŠ¿\næ”¯æŒæœ€é•¿ 128K token çš„ä¸Šä¸‹æ–‡é•¿åº¦\n\næœ¬ä»“åº“åŒ…å«ç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„ 7B å‚æ•° Qwen2.5-Coder æ¨¡å‹ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š\n\næ¨¡å‹ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹\nè®­ç»ƒé˜¶æ®µï¼šé¢„è®­ç»ƒä¸åè®­ç»ƒ\næ¶æ„ï¼šé‡‡ç”¨ RoPE ä½ç½®ç¼–ç ã€SwiGLU æ¿€æ´»å‡½æ•°ã€RMSNorm å½’ä¸€åŒ–åŠå¸¦æ³¨æ„åŠ› QKV åç½®çš„ Transformer ç»“æ„\nå‚æ•°é‡ï¼š7.61B\néåµŒå…¥å‚æ•°é‡ï¼š6.53B\nå±‚æ•°ï¼š28\næ³¨æ„åŠ›å¤´æ•°ï¼ˆGQAï¼‰ï¼šæŸ¥è¯¢å¤´ 28 ä¸ªï¼Œé”®å€¼å¤´ 4 ä¸ª\nä¸Šä¸‹æ–‡é•¿åº¦ï¼šå®Œæ•´æ”¯æŒ 131,072 token\nå…·ä½“é•¿æ–‡æœ¬å¤„ç†æ–¹æ³•è¯·å‚é˜…æœ¬ç« èŠ‚çš„è¯¦ç»†è¯´æ˜\n\næ›´å¤šæŠ€æœ¯ç»†èŠ‚è¯·å‚é˜…æˆ‘ä»¬çš„åšå®¢ã€GitHub ä»“åº“åŠæ–‡æ¡£ã€‚\n\nç¯å¢ƒè¦æ±‚\n\nQwen2.5-Coder çš„ä»£ç å·²é›†æˆè‡³æœ€æ–°ç‰ˆ Hugging Face transformersï¼Œå»ºè®®ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ transformers åº“ã€‚\n\nè‹¥ä½¿ç”¨ transformers<4.37.0 ç‰ˆæœ¬ï¼Œå°†ä¼šå‡ºç°ä»¥ä¸‹é”™è¯¯ï¼š\n\nKeyError: 'qwen2'\n\nå¿«é€Ÿå¼€å§‹\n\nè¿™é‡Œæä¾›ä¸€ä¸ªä½¿ç”¨ apply_chat_template çš„ä»£ç ç‰‡æ®µï¼Œå‘æ‚¨å±•ç¤ºå¦‚ä½•åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•ç”Ÿæˆå†…å®¹ã€‚\n\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"qwen/Qwen2.5-Coder-7B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"write a quick sort algorithm.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nå¤„ç†é•¿æ–‡æœ¬\n\nå½“å‰ config.json çš„ä¸Šä¸‹æ–‡é•¿åº¦è®¾ç½®ä¸ºæœ€é«˜ 32,768 ä¸ª tokenã€‚\nä¸ºå¤„ç†è¶…è¿‡ 32,768 ä¸ª token çš„è¶…é•¿è¾“å…¥ï¼Œæˆ‘ä»¬é‡‡ç”¨ YaRN æŠ€æœ¯æ¥å¢å¼ºæ¨¡å‹çš„é•¿æ–‡æœ¬å¤–æ¨èƒ½åŠ›ï¼Œç¡®ä¿åœ¨é•¿æ–‡æœ¬ä¸Šè·å¾—æœ€ä½³æ€§èƒ½ã€‚\n\nå¯¹äºæ”¯æŒçš„æ¡†æ¶ï¼Œæ‚¨å¯ä»¥åœ¨ config.json ä¸­æ·»åŠ ä»¥ä¸‹é…ç½®ä»¥å¯ç”¨ YaRNï¼š\n\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n\n\nå…³äºéƒ¨ç½²ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ vLLMã€‚\nå¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ vLLMï¼Œè¯·æŸ¥é˜…æˆ‘ä»¬çš„æ–‡æ¡£äº†è§£ä½¿ç”¨æ–¹æ³•ã€‚\nç›®å‰ï¼ŒvLLM ä»…æ”¯æŒé™æ€ YARNï¼Œè¿™æ„å‘³ç€æ— è®ºè¾“å…¥é•¿åº¦å¦‚ä½•ï¼Œç¼©æ”¾å› å­éƒ½ä¿æŒä¸å˜ï¼Œå¯èƒ½ä¼šå½±å“çŸ­æ–‡æœ¬çš„æ€§èƒ½ã€‚\næˆ‘ä»¬å»ºè®®ä»…åœ¨éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶æ‰æ·»åŠ  rope_scaling é…ç½®ã€‚\n\nè¯„ä¼°ä¸æ€§èƒ½\n\nè¯¦ç»†çš„è¯„ä¼°ç»“æœè¯·å‚è§è¿™ç¯‡ğŸ“‘ åšå®¢ã€‚\n\nå…³äº GPU æ˜¾å­˜éœ€æ±‚åŠç›¸åº”çš„ååé‡ï¼Œè¯·æŸ¥çœ‹æ­¤å¤„çš„ç»“æœã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚\n\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Apache License 2.0\", \"6 datasets\", \"qwen-coder\", \"codeqwen\", \"chat\", \"code\", \"qwen\"]"
  },
  {
    "url": "https://gitcode.com/yanfan/yuijh",
    "project_name": "yuijh",
    "readme": "Qwen2.5-Coder-7B-Instruct\nIntroduction\n\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). For Qwen2.5-Coder, we release three base language models and instruction-tuned language models, 1.5, 7 and 32 (coming soon) billion parameters. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nLong-context Support up to 128K tokens.\n\nThis repo contains the instruction-tuned 7B Qwen2.5-Coder model, which has the following features:\n\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 7.61B\nNumber of Paramaters (Non-Embedding): 6.53B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 28 for Q and 4 for KV\nContext Length: Full 131,072 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our blog, GitHub, and Documentation.\n\nRequirements\n\nThe code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\n\nWith transformers<4.37.0, you will encounter the following error:\n\nKeyError: 'qwen2'\n\nQuickstart\n\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\n\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"qwen/Qwen2.5-Coder-7B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"write a quick sort algorithm.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nProcessing Long Texts\n\nThe current config.json is set for context length up to 32,768 tokens. To handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n\n\nFor deployment, we recommend using vLLM. Please refer to our Documentation for usage if you are not familar with vLLM. Presently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts. We advise adding the rope_scaling configuration only when processing long contexts is required.\n\nEvaluation & Performance\n\nDetailed evaluation results are reported in this ğŸ“‘ blog.\n\nFor requirements on GPU memory and the respective throughput, see results here.\n\nCitation\n\nIf you find our work helpful, feel free to give us a cite.\n\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Apache License 2.0\", \"redshade/data-set01\", \"code\", \"chat\", \"codeqwen\", \"qwen\", \"qwen-coder\"]"
  },
  {
    "url": "https://gitcode.com/yanfan/tx467",
    "project_name": "tx467",
    "readme": "Qwen2.5-32B-Instruct-GPTQ-Int4\nIntroduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nThis repo contains the GPTQ-quantized 4-bit instruction-tuned 32B Qwen2.5 model, which has the following features:\n\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 32.5B\nNumber of Paramaters (Non-Embedding): 31.0B\nNumber of Layers: 64\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: Full 131,072 tokens and generation 8192 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nQuantization: GPTQ 4-bit\n\nFor more details, please refer to our blog, GitHub, and Documentation.\n\nRequirements\n\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\n\nWith transformers<4.37.0, you will encounter the following error:\n\nKeyError: 'qwen2'\n\n\nAlso check out our GPTQ documentation for more usage guide.\n\nQuickstart\n\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\n\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\n\nProcessing Long Texts\n\nThe current config.json is set for context length up to 32,768 tokens. To handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n\n\nFor deployment, we recommend using vLLM. Please refer to our Documentation for usage if you are not familar with vLLM. Presently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts. We advise adding the rope_scaling configuration only when processing long contexts is required.\n\nEvaluation & Performance\n\nDetailed evaluation results are reported in this ğŸ“‘ blog.\n\nFor quantized models, the benchmark results against the original bfloat16 models can be found here\n\nFor requirements on GPU memory and the respective throughput, see results here.\n\nCitation\n\nIf you find our work helpful, feel free to give us a cite.\n\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Apache License 2.0\", \"redshade/data-set01\", \"chat\"]"
  },
  {
    "url": "https://gitcode.com/yanfan/uytyb",
    "project_name": "uytyb",
    "readme": "Original Text\nMeissonicï¼šé‡å¡‘æ©ç ç”Ÿæˆå¼Transformerï¼Œå®ç°é«˜æ•ˆé«˜åˆ†è¾¨ç‡æ–‡ç”Ÿå›¾åˆæˆ\n\nè®ºæ–‡ | æ¨¡å‹ | ä»£ç  | æ¼”ç¤º\n\nç®€ä»‹\n\nMeissonic æ˜¯ä¸€æ¬¾åŸºäºéè‡ªå›å½’æ©ç å›¾åƒå»ºæ¨¡æŠ€æœ¯çš„æ–‡ç”Ÿå›¾åˆæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒã€‚è¯¥æ¨¡å‹ä¸“ä¸ºæ¶ˆè´¹çº§æ˜¾å¡è®¾è®¡ï¼Œå¯åœ¨å¸¸è§ç¡¬ä»¶ç¯å¢ƒä¸‹æµç•…è¿è¡Œã€‚\n\nä½¿ç”¨æŒ‡å—\n\nè¯¦è§ GitHub é“¾æ¥ã€‚\n\nå¼•ç”¨\n\nè‹¥è®¤ä¸ºæœ¬ç ”ç©¶æˆæœå¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨ï¼š\n\n@article{bai2024meissonic,\n  title={Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis},\n  author={Bai, Jinbin and Ye, Tian and Chow, Wei and Song, Enxin and Chen, Qing-Guo and Li, Xiangtai and Dong, Zhen and Zhu, Lei and Yan, Shuicheng},\n  journal={arXiv preprint arXiv:2410.08261},\n  year={2024}\n}\n",
    "tags": "[\"Text-to-Image\", \"PyTorch\", \"Transformers\", \"Diffusers\", \"Safetensors\", \"Apache License 2.0\", \"redshade/data-set01\", \"Non-Autoregressive\"]"
  },
  {
    "url": "https://gitcode.com/yanfan/colpali",
    "project_name": "colpali",
    "readme": "ColPali: Visual Retriever based on PaliGemma-3B with ColBERT strategy\n\nColPali is a model based on a novel model architecture and training strategy based on Vision Language Models (VLMs) to efficiently index documents from their visual features. It is a PaliGemma-3B extension that generates ColBERT- style multi-vector representations of text and images. It was introduced in the paper ColPali: Efficient Document Retrieval with Vision Language Models and first released in this repository\n\nModel Description\n\nThis model is built iteratively starting from an off-the-shelf SigLIP model. We finetuned it to create BiSigLIP and fed the patch-embeddings output by SigLIP to an LLM, PaliGemma-3B to create BiPali.\n\nOne benefit of inputting image patch embeddings through a language model is that they are natively mapped to a latent space similar to textual input (query). This enables leveraging the ColBERT strategy to compute interactions between text tokens and image patches, which enables a step-change improvement in performance compared to BiPali.\n\nModel Training\nDataset\n\nOur training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages. We explicitly verify no multi-page PDF document is used both ViDoRe and in the train set to prevent evaluation contamination. A validation set is created with 2% of the samples to tune hyperparameters.\n\nNote: Multilingual data is present in the pretraining corpus of the language model (Gemma-2B) and potentially occurs during PaliGemma-3B's multimodal training.\n\nParameters\n\nAll models are trained for 1 epoch on the train set. Unless specified otherwise, we train models in bfloat16 format, use low-rank adapters (LoRA) with alpha=32 and r=32 on the transformer layers from the language model, as well as the final randomly initialized projection layer, and use a paged_adamw_8bit optimizer. We train on an 8 GPU setup with data parallelism, a learning rate of 5e-5 with linear decay with 2.5% warmup steps, and a batch size of 32.\n\nUsage\nFor best performance, newer models are available (vidore/colpali-v1.2)\n# This model checkpoint is compatible with version 0.1.1, but not more recent versions of the inference lib\npip install colpali_engine==0.1.1\n\nimport torch\nimport typer\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoProcessor\nfrom PIL import Image\n\nfrom colpali_engine.models.paligemma_colbert_architecture import ColPali\nfrom colpali_engine.trainer.retrieval_evaluator import CustomEvaluator\nfrom colpali_engine.utils.colpali_processing_utils import process_images, process_queries\nfrom colpali_engine.utils.image_from_page_utils import load_from_dataset\n\n\ndef main() -> None:\n    \"\"\"Example script to run inference with ColPali\"\"\"\n\n    # Load model\n    model_name = \"vidore/colpali\"\n    model = ColPali.from_pretrained(\"vidore/colpaligemma-3b-mix-448-base\", torch_dtype=torch.bfloat16, device_map=\"cuda\").eval()\n    model.load_adapter(model_name)\n    processor = AutoProcessor.from_pretrained(model_name)\n\n    # select images -> load_from_pdf(<pdf_path>),  load_from_image_urls([\"<url_1>\"]), load_from_dataset(<path>)\n    images = load_from_dataset(\"vidore/docvqa_test_subsampled\")\n    queries = [\"From which university does James V. Fiorca come ?\", \"Who is the japanese prime minister?\"]\n\n    # run inference - docs\n    dataloader = DataLoader(\n        images,\n        batch_size=4,\n        shuffle=False,\n        collate_fn=lambda x: process_images(processor, x),\n    )\n    ds = []\n    for batch_doc in tqdm(dataloader):\n        with torch.no_grad():\n            batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n            embeddings_doc = model(**batch_doc)\n        ds.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n\n    # run inference - queries\n    dataloader = DataLoader(\n        queries,\n        batch_size=4,\n        shuffle=False,\n        collate_fn=lambda x: process_queries(processor, x, Image.new(\"RGB\", (448, 448), (255, 255, 255))),\n    )\n\n    qs = []\n    for batch_query in dataloader:\n        with torch.no_grad():\n            batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n            embeddings_query = model(**batch_query)\n        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n\n    # run evaluation\n    retriever_evaluator = CustomEvaluator(is_multi_vector=True)\n    scores = retriever_evaluator.evaluate(qs, ds)\n    print(scores.argmax(axis=1))\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n\n\nLimitations\nFocus: The model primarily focuses on PDF-type documents and high-ressources languages, potentially limiting its generalization to other document types or less represented languages.\nSupport: The model relies on multi-vector retreiving derived from the ColBERT late interaction mechanism, which may require engineering efforts to adapt to widely used vector retrieval frameworks that lack native multi-vector support.\nLicense\n\nColPali's vision language backbone model (PaliGemma) is under gemma license as specified in its model card. The adapters attached to the model are under MIT license.\n\nContact\nManuel Faysse: manuel.faysse@illuin.tech\nHugues Sibille: hugues.sibille@illuin.tech\nTony Wu: tony.wu@illuin.tech\nCitation\n\nIf you use any datasets or models from this organization in your research, please cite the original dataset as follows:\n\n@misc{faysse2024colpaliefficientdocumentretrieval,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models}, \n  author={Manuel Faysse and Hugues Sibille and Tony Wu and Bilel Omrani and Gautier Viaud and CÃ©line Hudelot and Pierre Colombo},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR},\n  url={https://arxiv.org/abs/2407.01449}, \n}\n",
    "tags": "[\"Audio-Text-to-Text\", \"Transformers\", \"PEFT\", \"Safetensors\", \"ColPali\", \"English\", \"Chinese\", \"French\", \"MIT\", \"5 datasets\", \"hf_mirrors/deepseek-ai/DeepSeek-R1\", \"vidore\"]"
  },
  {
    "url": "https://gitcode.com/tencent_hunyuan/Tencent-Hunyuan-Large",
    "project_name": "Tencent-Hunyuan-Large",
    "readme": "\n\n\nÂ GITHUBÂ Â  | Â Â ğŸ–¥ï¸Â Â official websiteÂ Â ï½œÂ Â ğŸ•–Â Â  HunyuanAPIï½œÂ Â ğŸ³Â Â  Gitee\n\nTechnical ReportÂ Â ï½œÂ Â  DemoÂ Â Â ï½œÂ Â  Tencent Cloud TIÂ Â Â \n\nDownload Models\nModels\tHuggingface Download URL\tTencent Cloud Download URL\nHunyuan-A52B-Instruct-FP8\tHunyuan-A52B-Instruct-FP8\tHunyuan-A52B-Instruct-FP8\nHunyuan-A52B-Instruct\tHunyuan-A52B-Instruct\tHunyuan-A52B-Instruct\nHunyuan-A52B-Pretrain\tHunyuan-A52B-Pretrain\tHunyuan-A52B-Pretrain\n\nModel Introduction\n\nWith the rapid development of artificial intelligence technology, large language models (LLMs) have made significant progress in fields such as natural language processing, computer vision, and scientific tasks. However, as the scale of these models increases, optimizing resource consumption while maintaining high performance has become a key challenge. To address this challenge, we have explored Mixture of Experts (MoE) models. The currently unveiled Hunyuan-Large (Hunyuan-MoE-A52B) model is the largest open-source Transformer-based MoE model in the industry, featuring a total of 389 billion parameters and 52 billion active parameters. This is currently the largest open-source Transformer-based MoE model in the industry, featuring a total of 389 billion parameters and 52 billion active parameters.\n\nBy open-sourcing the Hunyuan-Large model and revealing related technical details, we hope to inspire more researchers with innovative ideas and collectively advance the progress and application of AI technology. We welcome you to join our open-source community to explore and optimize future AI models together!\n\nIntroduction to Model Technical Advantages\nModel\n\nHigh-Quality Synthetic Data: By enhancing training with synthetic data, Hunyuan-Large can learn richer representations, handle long-context inputs, and generalize better to unseen data.\n\nKV Cache Compression: Utilizes Grouped Query Attention (GQA) and Cross-Layer Attention (CLA) strategies to significantly reduce memory usage and computational overhead of KV caches, improving inference throughput.\n\nExpert-Specific Learning Rate Scaling: Sets different learning rates for different experts to ensure each sub-model effectively learns from the data and contributes to overall performance.\n\nLong-Context Processing Capability: The pre-trained model supports text sequences up to 256K, and the Instruct model supports up to 128K, significantly enhancing the ability to handle long-context tasks.\n\nExtensive Benchmarking: Conducts extensive experiments across various languages and tasks to validate the practical effectiveness and safety of Hunyuan-Large.\n\nÂ \n\nBenchmark Evaluation\n\nHunyuan-Large pre-trained model achieves the best overall performance compared to both Dense and MoE based competitors having similar activated parameter sizes. For aggregated benchmarks such as MMLU, MMLU-Pro, and CMMLU, Hunyuan-Large consistently achieves the best performance, confirming its comprehensive abilities on aggregated tasks. Hunyuan-Large also shows superior performance in commonsense understanding and reasoning, and classical NLP tasks such as QA and reading comprehension tasks (e.g., CommonsenseQA, PIQA and TriviaQA).\nFor the mathematics capability, Hunyuan-Large outperforms all baselines in math datasets of GSM8K and MATH, and also gains the best results on CMATH in Chinese.We also observe that Hunyuan-Large achieves the overall best performance in all Chinese tasks (e.g., CMMLU, C-Eval).\n\nModel\tLLama3.1-405B\tLLama3.1-70B\tMixtral-8x22B\tDeepSeek-V2\tHunyuan-Large\nMMLU\t85.2\t79.3\t77.8\t78.5\t88.4\nMMLU-Pro\t61.6\t53.8\t49.5\t-\t60.2\nBBH\t85.9\t81.6\t78.9\t78.9\t86.3\nHellaSwag\t-\t-\t88.7\t87.8\t86.8\nCommonsenseQA\t85.8\t84.1\t82.4\t-\t92.9\nWinoGrande\t86.7\t85.3\t85.0\t84.9\t88.7\nPIQA\t-\t-\t83.6\t83.7\t88.3\nNaturalQuestions\t-\t-\t39.6\t38.7\t52.8\nDROP\t84.8\t79.6\t80.4\t80.1\t88.9\nARC-C\t96.1\t92.9\t91.2\t92.4\t95.0\nTriviaQA\t-\t-\t82.1\t79.9\t89.2\nCMMLU\t-\t-\t60.0\t84.0\t90.2\nC-Eval\t-\t-\t59.6\t81.7\t91.9\nC3\t-\t-\t71.4\t77.4\t82.3\nGSM8K\t89.0\t83.7\t83.7\t79.2\t92.8\nMATH\t53.8\t41.4\t42.5\t43.6\t69.8\nCMATH\t-\t-\t72.3\t78.7\t91.3\nHumanEval\t61.0\t58.5\t53.1\t48.8\t71.4\nMBPP\t73.4\t68.6\t64.2\t66.6\t72.6\n\nHunyuan-Large-Instruct achieves consistent improvements on most types of tasks compared to LLMs having similar activated parameters, indicating the effectiveness of our post-training. Delving into the model performance in different categories of benchmarks, we find that our instruct model achieves the best performance on MMLU and MATH dataset.\nNotably, on the MMLU dataset, our model demonstrates a significant improvement, outperforming the LLama3.1-405B model by 2.6%.\nThis enhancement is not just marginal but indicative of the Hunyuan-Large-Instructâ€™s superior understanding and reasoning capabilities across a wide array of language understanding tasks. The modelâ€™s prowess is further underscored in its performance on the MATH dataset, where it surpasses the LLama3.1-405B by a notable margin of 3.6%.\nRemarkably, this leap in accuracy is achieved with only 52 billion activated parameters, underscoring the efficiency of our model.\n\nModel\tLLama3.1 405B Inst.\tLLama3.1 70B Inst.\tMixtral 8x22B Inst.\tDeepSeekV2.5 Chat\tHunyuan-Large Inst.\nMMLU\t87.3\t83.6\t77.8\t80.4\t89.9\nCMMLU\t-\t-\t61.0\t-\t90.4\nC-Eval\t-\t-\t60.0\t-\t88.6\nBBH\t-\t-\t78.4\t84.3\t89.5\nHellaSwag\t-\t-\t86.0\t90.3\t88.5\nARC-C\t96.9\t94.8\t90.0\t-\t94.6\nGPQA_diamond\t51.1\t46.7\t-\t-\t42.4\nMATH\t73.8\t68.0\t49.8\t74.7\t77.4\nHumanEval\t89.0\t80.5\t75.0\t89.0\t90.0\nAlignBench\t6.0\t5.9\t6.2\t8.0\t8.3\nMT-Bench\t9.1\t8.8\t8.1\t9.0\t9.4\nIFEval strict-prompt\t86.0\t83.6\t71.2\t-\t85.0\nArena-Hard\t69.3\t55.7\t-\t76.2\t81.8\nAlpacaEval-2.0\t39.3\t34.3\t30.9\t50.5\t51.8\nQuick Start\n\nYou can quickly get started by referring to the content in the Quick Start Guide.\n\nInference and Deployment\n\nHunyuanLLM uses TRT-LLM and vLLM for deployment. We are open sourcing the vLLM deployment (see Reasoning with vLLM), and the TRT-LLM deployment (see Reasoning with TRT-LLM) will be available in the near future.\n\nLearn More at Tencent-Hunyuan-Large.\n\nCitation\n\nIf you find our work helpful, feel free to give us a cite.\n\n@misc{sun2024hunyuanlargeopensourcemoemodel,\n      title={Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent}, \n      author={Xingwu Sun and Yanfeng Chen and Yiqing Huang and Ruobing Xie and Jiaqi Zhu and Kai Zhang and Shuaipeng Li and Zhen Yang and Jonny Han and Xiaobo Shu and Jiahao Bu and Zhongzhi Chen and Xuemeng Huang and Fengzong Lian and Saiyong Yang and Jianfeng Yan and Yuyuan Zeng and Xiaoqin Ren and Chao Yu and Lulu Wu and Yue Mao and Tao Yang and Suncong Zheng and Kan Wu and Dian Jiao and Jinbao Xue and Xipeng Zhang and Decheng Wu and Kai Liu and Dengpeng Wu and Guanghui Xu and Shaohua Chen and Shuang Chen and Xiao Feng and Yigeng Hong and Junqiang Zheng and Chengcheng Xu and Zongwei Li and Xiong Kuang and Jianglu Hu and Yiqi Chen and Yuchi Deng and Guiyang Li and Ao Liu and Chenchen Zhang and Shihui Hu and Zilong Zhao and Zifan Wu and Yao Ding and Weichao Wang and Han Liu and Roberts Wang and Hao Fei and Peijie She and Ze Zhao and Xun Cao and Hai Wang and Fusheng Xiang and Mengyuan Huang and Zhiyuan Xiong and Bin Hu and Xuebin Hou and Lei Jiang and Jiajia Wu and Yaping Deng and Yi Shen and Qian Wang and Weijie Liu and Jie Liu and Meng Chen and Liang Dong and Weiwen Jia and Hu Chen and Feifei Liu and Rui Yuan and Huilin Xu and Zhenxiang Yan and Tengfei Cao and Zhichao Hu and Xinhua Feng and Dong Du and Tinghao She and Yangyu Tao and Feng Zhang and Jianchen Zhu and Chengzhong Xu and Xirui Li and Chong Zha and Wen Ouyang and Yinben Xia and Xiang Li and Zekun He and Rongpeng Chen and Jiawei Song and Ruibin Chen and Fan Jiang and Chongqing Zhao and Bo Wang and Hao Gong and Rong Gan and Winston Hu and Zhanhui Kang and Yong Yang and Yuhong Liu and Di Wang and Jie Jiang},\n      year={2024},\n      eprint={2411.02265},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2411.02265}, \n}\n",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"Safetensors\", \"English\", \"Other\", \"arxiv:2411.02265\"]"
  },
  {
    "url": "https://gitode.com/tencent_hunyuan/HunyuanDiT",
    "project_name": "HunyuanDiT",
    "readme": "Error: Page.goto: net::ERR_CONNECTION_CLOSED at https://gitode.com/tencent_hunyuan/HunyuanDiT\nCall log:\n  - navigating to \"https://gitode.com/tencent_hunyuan/HunyuanDiT\", waiting until \"domcontentloaded\"\n",
    "tags": "[]"
  },
  {
    "url": "https://gitcode.com/tencent_hunyuan/HunyuanVideo-PromptRewrite",
    "project_name": "HunyuanVideo-PromptRewrite",
    "readme": "Original Text\n\nHunyuanVideo: å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹è®­ç»ƒçš„ç³»ç»ŸåŒ–æ¡†æ¶\n\næœ¬ä»“åº“åŒ…å«äº† HunyuanVideo-PromptRewrite æ¨¡å‹çš„æƒé‡ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ Hunyuan-Large åŸå§‹ä»£ç  è¿›è¡Œéƒ¨ç½²å’Œæ¨ç†ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢åœ¨ è¿™é‡Œã€‚\n\nHunyuanVideo: å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹è®­ç»ƒçš„ç³»ç»ŸåŒ–æ¡†æ¶\n\n\nç›®å½•\nHunyuanVideo: å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹è®­ç»ƒçš„ç³»ç»ŸåŒ–æ¡†æ¶\nç›®å½•\næ‘˜è¦\nHunyuanVideo æ•´ä½“æ¶æ„\nğŸ‰ HunyuanVideo å…³é”®ç‰¹æ€§\nç»Ÿä¸€çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¶æ„\nMLLM æ–‡æœ¬ç¼–ç å™¨\n3D VAE\næç¤ºè¯é‡å†™\nğŸ“ˆ å¯¹æ¯”\nğŸ”— BibTeX\nè‡´è°¢\næ‘˜è¦\n\næˆ‘ä»¬æå‡ºäº† HunyuanVideoï¼Œä¸€ä¸ªå…¨æ–°çš„å¼€æºè§†é¢‘åŸºç¡€æ¨¡å‹ï¼Œå…¶åœ¨è§†é¢‘ç”Ÿæˆæ–¹é¢çš„è¡¨ç°å¯ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹ç›¸åª²ç¾ï¼Œç”šè‡³æ›´ä¼˜ã€‚HunyuanVideo é‡‡ç”¨äº†ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œé›†æˆäº†å¤šé¡¹å…³é”®è´¡çŒ®ï¼ŒåŒ…æ‹¬æ•°æ®æ•´ç†ã€å›¾åƒ-è§†é¢‘è”åˆæ¨¡å‹è®­ç»ƒä»¥åŠä¸ºå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒå’Œæ¨ç†è®¾è®¡çš„é«˜æ•ˆåŸºç¡€è®¾æ–½ã€‚æ­¤å¤–ï¼Œé€šè¿‡æœ‰æ•ˆçš„æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†æ‰©å±•ç­–ç•¥ï¼Œæˆ‘ä»¬æˆåŠŸè®­ç»ƒäº†ä¸€ä¸ªæ‹¥æœ‰è¶…è¿‡ 130 äº¿å‚æ•°çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä½¿å…¶æˆä¸ºæ‰€æœ‰å¼€æºæ¨¡å‹ä¸­è§„æ¨¡æœ€å¤§çš„ã€‚\n\næˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶å®æ–½äº†ä¸€ç³»åˆ—é’ˆå¯¹æ€§çš„è®¾è®¡ï¼Œä»¥ç¡®ä¿é«˜è§†è§‰è´¨é‡ã€è¿åŠ¨å¤šæ ·æ€§ã€æ–‡æœ¬-è§†é¢‘å¯¹é½å’Œç”Ÿæˆç¨³å®šæ€§ã€‚æ ¹æ®ä¸“ä¸šçš„äººç±»è¯„ä¼°ç»“æœï¼ŒHunyuanVideo ä¼˜äºä¹‹å‰çš„æœ€æ–°æ¨¡å‹ï¼ŒåŒ…æ‹¬ Runway Gen-3ã€Luma 1.6 ä»¥åŠ 3 ä¸ªè¡¨ç°æœ€ä½³çš„ä¸­æ–‡è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡å‘å¸ƒåŸºç¡€æ¨¡å‹åŠå…¶åº”ç”¨çš„ä»£ç å’Œæƒé‡ï¼Œæˆ‘ä»¬æ—¨åœ¨å¼¥åˆé—­æºå’Œå¼€æºè§†é¢‘åŸºç¡€æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚è¿™ä¸€ä¸¾æªå°†ä½¿ç¤¾åŒºä¸­çš„æ¯ä¸ªäººéƒ½èƒ½å®éªŒä»–ä»¬çš„æƒ³æ³•ï¼Œä¿ƒè¿›ä¸€ä¸ªæ›´åŠ åŠ¨æ€å’Œå……æ»¡æ´»åŠ›çš„è§†é¢‘ç”Ÿæˆç”Ÿæ€ç³»ç»Ÿã€‚\n\nHunyuanVideo æ•´ä½“æ¶æ„\n\nHunyuanVideo åœ¨æ—¶ç©ºå‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥ç©ºé—´é€šè¿‡ Causal 3D VAE è¿›è¡Œå‹ç¼©ã€‚æ–‡æœ¬æç¤ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¼–ç ï¼Œå¹¶ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚é«˜æ–¯å™ªå£°å’Œæ¡ä»¶ä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹ç”Ÿæˆä¸€ä¸ªè¾“å‡ºæ½œåœ¨è¡¨ç¤ºï¼Œé€šè¿‡ 3D VAE è§£ç å™¨è§£ç ä¸ºå›¾åƒæˆ–è§†é¢‘ã€‚\n\nğŸ‰ HunyuanVideo å…³é”®ç‰¹æ€§\nç»Ÿä¸€çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¶æ„\n\nHunyuanVideo å¼•å…¥äº† Transformer è®¾è®¡ï¼Œå¹¶é‡‡ç”¨å…¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç»Ÿä¸€çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ä½¿ç”¨â€œåŒæµåˆ°å•æµâ€çš„æ··åˆæ¨¡å‹è®¾è®¡è¿›è¡Œè§†é¢‘ç”Ÿæˆã€‚åœ¨åŒæµé˜¶æ®µï¼Œè§†é¢‘å’Œæ–‡æœ¬ä»¤ç‰Œé€šè¿‡å¤šä¸ª Transformer å—ç‹¬ç«‹å¤„ç†ï¼Œä½¿æ¯ç§æ¨¡æ€èƒ½å¤Ÿå­¦ä¹ å…¶é€‚å½“çš„è°ƒåˆ¶æœºåˆ¶è€Œä¸ä¼šç›¸äº’å¹²æ‰°ã€‚åœ¨å•æµé˜¶æ®µï¼Œæˆ‘ä»¬å°†è§†é¢‘å’Œæ–‡æœ¬ä»¤ç‰Œè¿æ¥èµ·æ¥ï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°åç»­çš„ Transformer å—ä¸­ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€ä¿¡æ¯èåˆã€‚è¿™ç§è®¾è®¡æ•æ‰äº†è§†è§‰å’Œè¯­ä¹‰ä¿¡æ¯ä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œå¢å¼ºäº†æ•´ä½“æ¨¡å‹æ€§èƒ½ã€‚\n\nMLLM æ–‡æœ¬ç¼–ç å™¨\n\nä¸€äº›ä¹‹å‰çš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹é€šå¸¸ä½¿ç”¨é¢„è®­ç»ƒçš„ CLIP å’Œ T5-XXL ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œå…¶ä¸­ CLIP ä½¿ç”¨ Transformer ç¼–ç å™¨ï¼ŒT5 ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨ç»“æ„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé‡‡ç”¨ä»…è§£ç å™¨ç»“æ„ä½œä¸ºæˆ‘ä»¬çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼šï¼ˆiï¼‰ä¸ T5 ç›¸æ¯”ï¼ŒMLLM åœ¨è§†è§‰æŒ‡ä»¤å¾®è°ƒååœ¨ç‰¹å¾ç©ºé—´ä¸­å…·æœ‰æ›´å¥½çš„å›¾åƒ-æ–‡æœ¬å¯¹é½ï¼Œè¿™ç¼“è§£äº†æ‰©æ•£æ¨¡å‹ä¸­æŒ‡ä»¤è·Ÿéšçš„éš¾åº¦ï¼›ï¼ˆiiï¼‰ä¸ CLIP ç›¸æ¯”ï¼ŒMLLM åœ¨å›¾åƒç»†èŠ‚æè¿°å’Œå¤æ‚æ¨ç†æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„èƒ½åŠ›ï¼›ï¼ˆiiiï¼‰MLLM å¯ä»¥é€šè¿‡éµå¾ªç³»ç»ŸæŒ‡ä»¤ä½œä¸ºé›¶æ ·æœ¬å­¦ä¹ è€…ï¼Œå¸®åŠ©æ–‡æœ¬ç‰¹å¾æ›´å¤šåœ°å…³æ³¨å…³é”®ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒMLLM åŸºäºå› æœæ³¨æ„åŠ›ï¼Œè€Œ T5-XXL ä½¿ç”¨åŒå‘æ³¨æ„åŠ›ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›æ›´å¥½çš„æ–‡æœ¬æŒ‡å¯¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„åŒå‘ä»¤ç‰Œç²¾ç‚¼å™¨æ¥å¢å¼ºæ–‡æœ¬ç‰¹å¾ã€‚\n\n3D VAE\n\nHunyuanVideo è®­ç»ƒäº†ä¸€ä¸ªå¸¦æœ‰ CausalConv3D çš„ 3D VAEï¼Œå°†åƒç´ ç©ºé—´çš„è§†é¢‘å’Œå›¾åƒå‹ç¼©åˆ°ä¸€ä¸ªç´§å‡‘çš„æ½œåœ¨ç©ºé—´ä¸­ã€‚æˆ‘ä»¬å°†è§†é¢‘é•¿åº¦ã€ç©ºé—´å’Œé€šé“çš„å‹ç¼©æ¯”åˆ†åˆ«è®¾ç½®ä¸º 4ã€8 å’Œ 16ã€‚è¿™å¯ä»¥æ˜¾è‘—å‡å°‘åç»­æ‰©æ•£ Transformer æ¨¡å‹çš„ä»¤ç‰Œæ•°é‡ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨åŸå§‹åˆ†è¾¨ç‡å’Œå¸§ç‡ä¸‹è®­ç»ƒè§†é¢‘ã€‚\n\næç¤ºè¯é‡å†™\n\nä¸ºäº†è§£å†³ç”¨æˆ·æä¾›çš„æç¤ºè¯åœ¨è¯­è¨€é£æ ¼å’Œé•¿åº¦ä¸Šçš„å¯å˜æ€§ï¼Œæˆ‘ä»¬å¾®è°ƒäº† Hunyuan-Large æ¨¡å‹ ä½œä¸ºæˆ‘ä»¬çš„æç¤ºè¯é‡å†™æ¨¡å‹ï¼Œä»¥å°†åŸå§‹ç”¨æˆ·æç¤ºè¯é€‚é…ä¸ºæ¨¡å‹åå¥½çš„æç¤ºè¯ã€‚\n\næˆ‘ä»¬æä¾›äº†ä¸¤ç§é‡å†™æ¨¡å¼ï¼šæ™®é€šæ¨¡å¼å’Œå¤§å¸ˆæ¨¡å¼ï¼Œå¯ä»¥é€šè¿‡ä¸åŒçš„æç¤ºè¯è°ƒç”¨ã€‚æ™®é€šæ¨¡å¼æ—¨åœ¨å¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹å¯¹ç”¨æˆ·æ„å›¾çš„ç†è§£ï¼Œä¿ƒè¿›å¯¹æ‰€æä¾›æŒ‡ä»¤çš„æ›´å‡†ç¡®è§£é‡Šã€‚å¤§å¸ˆæ¨¡å¼å¢å¼ºäº†æ„å›¾ã€å…‰ç…§å’Œæ‘„åƒæœºè¿åŠ¨ç­‰æ–¹é¢çš„æè¿°ï¼Œå€¾å‘äºç”Ÿæˆè§†è§‰è´¨é‡æ›´é«˜çš„è§†é¢‘ã€‚ç„¶è€Œï¼Œè¿™ç§å¼ºè°ƒæœ‰æ—¶å¯èƒ½ä¼šå¯¼è‡´ä¸€äº›è¯­ä¹‰ç»†èŠ‚çš„ä¸¢å¤±ã€‚\n\næç¤ºè¯é‡å†™æ¨¡å‹å¯ä»¥ç›´æ¥ä½¿ç”¨ Hunyuan-Large åŸå§‹ä»£ç  è¿›è¡Œéƒ¨ç½²å’Œæ¨ç†ã€‚æˆ‘ä»¬åœ¨æ­¤å‘å¸ƒäº†æç¤ºè¯é‡å†™æ¨¡å‹çš„æƒé‡ è¿™é‡Œã€‚\n\nğŸ“ˆ å¯¹æ¯”åˆ†æ\n\nä¸ºäº†è¯„ä¼°HunyuanVideoçš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä»é—­æºè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ç²¾é€‰äº†äº”ç§å¼ºæœ‰åŠ›çš„åŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬æ€»å…±ä½¿ç”¨äº†1,533ä¸ªæ–‡æœ¬æç¤ºï¼Œé€šè¿‡ä¸€æ¬¡è¿è¡Œç”Ÿæˆäº†ä¸HunyuanVideoæ•°é‡ç›¸ç­‰çš„è§†é¢‘æ ·æœ¬ã€‚ä¸ºäº†ç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬ä»…è¿›è¡Œäº†ä¸€æ¬¡æ¨ç†ï¼Œé¿å…äº†å¯¹ç»“æœçš„æŒ‘é€‰ã€‚åœ¨å¯¹æ¯”åŸºçº¿æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬ä¿æŒäº†æ‰€æœ‰é€‰å®šæ¨¡å‹çš„é»˜è®¤è®¾ç½®ï¼Œç¡®ä¿è§†é¢‘åˆ†è¾¨ç‡ä¸€è‡´ã€‚è§†é¢‘çš„è¯„ä¼°åŸºäºä¸‰ä¸ªæ ‡å‡†ï¼šæ–‡æœ¬å¯¹é½ã€è¿åŠ¨è´¨é‡å’Œè§†è§‰è´¨é‡ã€‚è¶…è¿‡60åä¸“ä¸šè¯„ä¼°äººå‘˜å‚ä¸äº†æ­¤æ¬¡è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒHunyuanVideoåœ¨æ•´ä½“è¡¨ç°ä¸Šæœ€ä¸ºå‡ºè‰²ï¼Œå°¤å…¶åœ¨è¿åŠ¨è´¨é‡æ–¹é¢è¡¨ç°çªå‡ºã€‚\n\næ¨¡å‹\tå¼€æº\tæ—¶é•¿\tæ–‡æœ¬å¯¹é½\tè¿åŠ¨è´¨é‡\tè§†è§‰è´¨é‡\tæ•´ä½“è¡¨ç°\tæ’å\nHunyuanVideo (æˆ‘ä»¬çš„)\tâœ”\t5ç§’\t61.8%\t66.5%\t95.7%\t41.3%\t1\nCNTopA (API)\tâœ˜\t5ç§’\t62.6%\t61.7%\t95.6%\t37.7%\t2\nCNTopB (Web)\tâœ˜\t5ç§’\t60.1%\t62.9%\t97.7%\t37.5%\t3\nGEN-3 alpha (Web)\tâœ˜\t6ç§’\t47.7%\t54.7%\t97.5%\t27.4%\t4\nLuma1.6 (API)\tâœ˜\t5ç§’\t57.6%\t44.2%\t94.1%\t24.8%\t6\nCNTopC (Web)\tâœ˜\t5ç§’\t48.4%\t47.2%\t96.3%\t24.6%\t5\n\nğŸ”— BibTeX\n\nå¦‚æœæ‚¨å‘ç°HunyuanVideoå¯¹æ‚¨çš„ç ”ç©¶æˆ–åº”ç”¨æœ‰å¸®åŠ©ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹BibTeXè¿›è¡Œå¼•ç”¨ï¼š\n\n@misc{kong2024hunyuanvideo,\n      title={HunyuanVideo: A Systematic Framework For Large Video Generative Models}, \n      author={Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Junkun Yuan, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yanxin Long, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, and Jie Jiang, along with Caesar Zhong},\n      year={2024},\n      archivePrefix={arXiv preprint arXiv:2412.03603},\n      primaryClass={cs.CV}\n}\n\nè‡´è°¢\n\næˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢å¯¹SD3ã€FLUXã€Llamaã€LLaVAã€Xtunerã€diffusersä»¥åŠHuggingFaceä»“åº“åšå‡ºè´¡çŒ®çš„ç ”ç©¶è€…å’Œå¼€å‘è€…ä»¬ï¼Œæ„Ÿè°¢ä»–ä»¬å¼€æ”¾çš„ç ”ç©¶ç²¾ç¥å’Œæ¢ç´¢ç²¾ç¥ã€‚\n\næ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¦æ„Ÿè°¢è…¾è®¯æ··å…ƒå¤šæ¨¡æ€å›¢é˜Ÿåœ¨æ–‡æœ¬ç¼–ç å™¨æ–¹é¢ç»™äºˆçš„å¸®åŠ©ã€‚",
    "tags": "[\"PyTorch\", \"Transformers\", \"Other\"]"
  },
  {
    "url": "https://gitcode.com/tencent_hunyuan/HunyuanVideo",
    "project_name": "HunyuanVideo",
    "readme": "Original Text\n\nHunyuanVideo: å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹è®­ç»ƒçš„ç³»ç»Ÿæ¡†æ¶\n\næœ¬ä»“åº“åŒ…å«äº†PyTorchæ¨¡å‹å®šä¹‰ã€é¢„è®­ç»ƒæƒé‡ä»¥åŠæˆ‘ä»¬è®ºæ–‡ä¸­æ¢ç´¢HunyuanVideoçš„æ¨ç†/é‡‡æ ·ä»£ç ã€‚æ‚¨å¯ä»¥åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ä¸Šæ‰¾åˆ°æ›´å¤šå¯è§†åŒ–å†…å®¹ã€‚\n\nHunyuanVideo: å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹è®­ç»ƒçš„ç³»ç»Ÿæ¡†æ¶\n\n\nğŸ”¥ğŸ”¥ğŸ”¥ æœ€æ–°åŠ¨æ€!!\n2024å¹´12æœˆ3æ—¥: ğŸ¤— æˆ‘ä»¬å‘å¸ƒäº†HunyuanVideoçš„æ¨ç†ä»£ç å’Œæ¨¡å‹æƒé‡ã€‚\nğŸ“‘ å¼€æºè®¡åˆ’\nHunyuanVideo (æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹)\næ¨ç†\næ£€æŸ¥ç‚¹\nPenguinè§†é¢‘åŸºå‡†\nWebæ¼”ç¤º (Gradio)\nComfyUI\nDiffusers\nHunyuanVideo (å›¾åƒåˆ°è§†é¢‘æ¨¡å‹)\næ¨ç†\næ£€æŸ¥ç‚¹\nç›®å½•\nHunyuanVideo: å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹è®­ç»ƒçš„ç³»ç»Ÿæ¡†æ¶\nğŸ”¥ğŸ”¥ğŸ”¥ æœ€æ–°åŠ¨æ€!!\nğŸ“‘ å¼€æºè®¡åˆ’\nç›®å½•\næ‘˜è¦\nHunyuanVideoæ•´ä½“æ¶æ„\nğŸ‰ HunyuanVideoå…³é”®ç‰¹æ€§\nç»Ÿä¸€çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¶æ„\nMLLMæ–‡æœ¬ç¼–ç å™¨\n3D VAE\næç¤ºé‡å†™\nğŸ“ˆ å¯¹æ¯”\nğŸ“œ è¦æ±‚\nğŸ› ï¸ ä¾èµ–ä¸å®‰è£…\nLinuxå®‰è£…æŒ‡å—\nğŸ§± ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹\nğŸ”‘ æ¨ç†\nä½¿ç”¨å‘½ä»¤è¡Œ\næ›´å¤šé…ç½®\nğŸ”— BibTeX\nè‡´è°¢\næ‘˜è¦\n\næˆ‘ä»¬æå‡ºäº†HunyuanVideoï¼Œä¸€ä¸ªæ–°é¢–çš„å¼€æºè§†é¢‘åŸºç¡€æ¨¡å‹ï¼Œå…¶åœ¨è§†é¢‘ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½å¯ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹ç›¸åª²ç¾ï¼Œç”šè‡³æ›´ä¼˜ã€‚HunyuanVideoé‡‡ç”¨äº†ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œé›†æˆäº†å¤šé¡¹å…³é”®è´¡çŒ®ï¼ŒåŒ…æ‹¬æ•°æ®æ•´ç†ã€å›¾åƒ-è§†é¢‘è”åˆæ¨¡å‹è®­ç»ƒä»¥åŠä¸ºå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒå’Œæ¨ç†è®¾è®¡çš„é«˜æ•ˆåŸºç¡€è®¾æ–½ã€‚æ­¤å¤–ï¼Œé€šè¿‡æœ‰æ•ˆçš„æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†æ‰©å±•ç­–ç•¥ï¼Œæˆ‘ä»¬æˆåŠŸè®­ç»ƒäº†ä¸€ä¸ªæ‹¥æœ‰è¶…è¿‡130äº¿å‚æ•°çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä½¿å…¶æˆä¸ºæ‰€æœ‰å¼€æºæ¨¡å‹ä¸­æœ€å¤§çš„ã€‚\n\næˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶å®æ–½äº†ä¸€ç³»åˆ—é’ˆå¯¹æ€§çš„è®¾è®¡ï¼Œä»¥ç¡®ä¿é«˜è§†è§‰è´¨é‡ã€è¿åŠ¨å¤šæ ·æ€§ã€æ–‡æœ¬-è§†é¢‘å¯¹é½å’Œç”Ÿæˆç¨³å®šæ€§ã€‚æ ¹æ®ä¸“ä¸šçš„äººç±»è¯„ä¼°ç»“æœï¼ŒHunyuanVideoä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼ŒåŒ…æ‹¬Runway Gen-3ã€Luma 1.6ä»¥åŠä¸‰æ¬¾è¡¨ç°æœ€ä½³çš„ä¸­æ–‡è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡å‘å¸ƒåŸºç¡€æ¨¡å‹åŠå…¶åº”ç”¨çš„ä»£ç å’Œæƒé‡ï¼Œæˆ‘ä»¬æ—¨åœ¨å¼¥åˆé—­æºä¸å¼€æºè§†é¢‘åŸºç¡€æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚è¿™ä¸€ä¸¾æªå°†ä½¿ç¤¾åŒºä¸­çš„æ¯ä¸ªäººéƒ½èƒ½å®éªŒä»–ä»¬çš„æƒ³æ³•ï¼Œä¿ƒè¿›ä¸€ä¸ªæ›´åŠ åŠ¨æ€å’Œå……æ»¡æ´»åŠ›çš„è§†é¢‘ç”Ÿæˆç”Ÿæ€ç³»ç»Ÿã€‚\n\nHunyuanVideoæ•´ä½“æ¶æ„\n\nHunyuanVideoåœ¨æ—¶ç©ºå‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥ç©ºé—´é€šè¿‡å› æœ3D VAEè¿›è¡Œå‹ç¼©ã€‚æ–‡æœ¬æç¤ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¼–ç ï¼Œå¹¶ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚é«˜æ–¯å™ªå£°å’Œæ¡ä»¶ä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹ç”Ÿæˆä¸€ä¸ªè¾“å‡ºæ½œåœ¨å˜é‡ï¼Œé€šè¿‡3D VAEè§£ç å™¨è§£ç ä¸ºå›¾åƒæˆ–è§†é¢‘ã€‚\n\nğŸ‰ HunyuanVideoå…³é”®ç‰¹æ€§\nç»Ÿä¸€çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¶æ„\n\nHunyuanVideoå¼•å…¥äº†Transformerè®¾è®¡ï¼Œå¹¶é‡‡ç”¨å…¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç»Ÿä¸€çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨â€œåŒæµåˆ°å•æµâ€çš„æ··åˆæ¨¡å‹è®¾è®¡è¿›è¡Œè§†é¢‘ç”Ÿæˆã€‚åœ¨åŒæµé˜¶æ®µï¼Œè§†é¢‘å’Œæ–‡æœ¬ä»¤ç‰Œé€šè¿‡å¤šä¸ªTransformerå—ç‹¬ç«‹å¤„ç†ï¼Œä½¿æ¯ç§æ¨¡æ€èƒ½å¤Ÿå­¦ä¹ å…¶é€‚å½“çš„è°ƒåˆ¶æœºåˆ¶è€Œä¸å—å¹²æ‰°ã€‚åœ¨å•æµé˜¶æ®µï¼Œæˆ‘ä»¬å°†è§†é¢‘å’Œæ–‡æœ¬ä»¤ç‰Œè¿æ¥èµ·æ¥ï¼Œå¹¶å°†å…¶è¾“å…¥åç»­çš„Transformerå—ä¸­ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€ä¿¡æ¯èåˆã€‚è¿™ç§è®¾è®¡æ•æ‰äº†è§†è§‰å’Œè¯­ä¹‰ä¿¡æ¯ä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œå¢å¼ºäº†æ•´ä½“æ¨¡å‹æ€§èƒ½ã€‚\n\nMLLMæ–‡æœ¬ç¼–ç å™¨\n\nä¸€äº›ä¹‹å‰çš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹é€šå¸¸ä½¿ç”¨é¢„è®­ç»ƒçš„CLIPå’ŒT5-XXLä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œå…¶ä¸­CLIPä½¿ç”¨Transformerç¼–ç å™¨ï¼ŒT5ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨ç»“æ„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé‡‡ç”¨ä»…è§£ç å™¨ç»“æ„ä½œä¸ºæˆ‘ä»¬çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼šï¼ˆiï¼‰ä¸T5ç›¸æ¯”ï¼ŒMLLMåœ¨è§†è§‰æŒ‡ä»¤å¾®è°ƒååœ¨ç‰¹å¾ç©ºé—´ä¸­å…·æœ‰æ›´å¥½çš„å›¾åƒ-æ–‡æœ¬å¯¹é½ï¼Œç¼“è§£äº†æ‰©æ•£æ¨¡å‹ä¸­æŒ‡ä»¤è·Ÿéšçš„éš¾åº¦ï¼›ï¼ˆiiï¼‰ä¸CLIPç›¸æ¯”ï¼ŒMLLMåœ¨å›¾åƒç»†èŠ‚æè¿°å’Œå¤æ‚æ¨ç†æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„èƒ½åŠ›ï¼›ï¼ˆiiiï¼‰MLLMå¯ä»¥é€šè¿‡éµå¾ªç³»ç»ŸæŒ‡ä»¤ä½œä¸ºé›¶æ ·æœ¬å­¦ä¹ è€…ï¼Œå¸®åŠ©æ–‡æœ¬ç‰¹å¾æ›´å¤šåœ°å…³æ³¨å…³é”®ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒMLLMåŸºäºå› æœæ³¨æ„åŠ›ï¼Œè€ŒT5-XXLä½¿ç”¨åŒå‘æ³¨æ„åŠ›ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›æ›´å¥½çš„æ–‡æœ¬æŒ‡å¯¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„åŒå‘ä»¤ç‰Œç²¾ç‚¼å™¨æ¥å¢å¼ºæ–‡æœ¬ç‰¹å¾ã€‚\n\n3D VAE\n\nHunyuanVideoè®­ç»ƒäº†ä¸€ä¸ªå¸¦æœ‰CausalConv3Dçš„3D VAEï¼Œå°†åƒç´ ç©ºé—´çš„è§†é¢‘å’Œå›¾åƒå‹ç¼©åˆ°ä¸€ä¸ªç´§å‡‘çš„æ½œåœ¨ç©ºé—´ä¸­ã€‚æˆ‘ä»¬å°†è§†é¢‘é•¿åº¦ã€ç©ºé—´å’Œé€šé“çš„å‹ç¼©æ¯”åˆ†åˆ«è®¾ç½®ä¸º4ã€8å’Œ16ã€‚è¿™å¯ä»¥æ˜¾è‘—å‡å°‘åç»­æ‰©æ•£Transformeræ¨¡å‹çš„ä»¤ç‰Œæ•°é‡ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨åŸå§‹åˆ†è¾¨ç‡å’Œå¸§ç‡ä¸‹è®­ç»ƒè§†é¢‘ã€‚\n\næç¤ºé‡å†™\n\nä¸ºäº†è§£å†³ç”¨æˆ·æä¾›çš„æç¤ºåœ¨è¯­è¨€é£æ ¼å’Œé•¿åº¦ä¸Šçš„å¯å˜æ€§ï¼Œæˆ‘ä»¬å¾®è°ƒäº†Hunyuan-Largeæ¨¡å‹ä½œä¸ºæˆ‘ä»¬çš„æç¤ºé‡å†™æ¨¡å‹ï¼Œä»¥å°†åŸå§‹ç”¨æˆ·æç¤ºé€‚é…ä¸ºæ¨¡å‹åå¥½çš„æç¤ºã€‚\n\næˆ‘ä»¬æä¾›äº†ä¸¤ç§é‡å†™æ¨¡å¼ï¼šæ™®é€šæ¨¡å¼å’Œå¤§å¸ˆæ¨¡å¼ï¼Œå¯ä»¥é€šè¿‡ä¸åŒçš„æç¤ºè°ƒç”¨ã€‚æ™®é€šæ¨¡å¼æ—¨åœ¨å¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹å¯¹ç”¨æˆ·æ„å›¾çš„ç†è§£ï¼Œä¿ƒè¿›å¯¹æ‰€æä¾›æŒ‡ä»¤çš„æ›´å‡†ç¡®è§£é‡Šã€‚å¤§å¸ˆæ¨¡å¼å¢å¼ºäº†æ„å›¾ã€å…‰çº¿å’Œæ‘„åƒæœºè¿åŠ¨ç­‰æ–¹é¢çš„æè¿°ï¼Œå€¾å‘äºç”Ÿæˆè§†è§‰è´¨é‡æ›´é«˜çš„è§†é¢‘ã€‚ç„¶è€Œï¼Œè¿™ç§å¼ºè°ƒæœ‰æ—¶å¯èƒ½ä¼šå¯¼è‡´ä¸€äº›è¯­ä¹‰ç»†èŠ‚çš„ä¸¢å¤±ã€‚\n\næç¤ºé‡å†™æ¨¡å‹å¯ä»¥ç›´æ¥ä½¿ç”¨Hunyuan-LargeåŸå§‹ä»£ç è¿›è¡Œéƒ¨ç½²å’Œæ¨ç†ã€‚æˆ‘ä»¬åœ¨æ­¤å¤„å‘å¸ƒäº†æç¤ºé‡å†™æ¨¡å‹çš„æƒé‡è¿™é‡Œã€‚\n\nğŸ“ˆ å¯¹æ¯”åˆ†æ\n\nä¸ºäº†è¯„ä¼°HunyuanVideoçš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä»é—­æºè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­é€‰å–äº†äº”ä¸ªæ€§èƒ½å¼ºåŠ²çš„åŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬å…±ä½¿ç”¨äº†1,533ä¸ªæ–‡æœ¬æç¤ºï¼Œä½¿ç”¨HunyuanVideoåœ¨ä¸€æ¬¡è¿è¡Œä¸­ç”Ÿæˆç›¸åŒæ•°é‡çš„è§†é¢‘æ ·æœ¬ã€‚ä¸ºäº†ç¡®ä¿æ¯”è¾ƒçš„å…¬æ­£æ€§ï¼Œæˆ‘ä»¬åªè¿›è¡Œäº†ä¸€æ¬¡æ¨æ–­ï¼Œé¿å…ä»»ä½•ç»“æœçš„æŒ‘é€‰ã€‚åœ¨å¯¹æ¯”åŸºçº¿æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬ç»´æŒäº†æ‰€æœ‰é€‰å®šæ¨¡å‹çš„é»˜è®¤è®¾ç½®ï¼Œç¡®ä¿è§†é¢‘åˆ†è¾¨ç‡çš„ä¸€è‡´æ€§ã€‚è§†é¢‘è¯„ä¼°åŸºäºä¸‰ä¸ªæ ‡å‡†ï¼šæ–‡æœ¬å¯¹é½ã€åŠ¨ä½œè´¨é‡å’Œè§†è§‰è´¨é‡ã€‚è¶…è¿‡60ä½ä¸“ä¸šè¯„ä¼°äººå‘˜è¿›è¡Œäº†è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒHunyuanVideoåœ¨æ•´ä½“æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨åŠ¨ä½œè´¨é‡æ–¹é¢ç‰¹åˆ«çªå‡ºã€‚\n\næ¨¡å‹\tå¼€æº\tæ—¶é•¿\tæ–‡æœ¬å¯¹é½\tåŠ¨ä½œè´¨é‡\tè§†è§‰è´¨é‡\tæ€»åˆ†\tæ’å\nHunyuanVideo (æˆ‘ä»¬çš„)\tâœ”\t5ç§’\t61.8%\t66.5%\t95.7%\t41.3%\t1\nCNTopA (API)\tâœ˜\t5ç§’\t62.6%\t61.7%\t95.6%\t37.7%\t2\nCNTopB (Web)\tâœ˜\t5ç§’\t60.1%\t62.9%\t97.7%\t37.5%\t3\nGEN-3 alpha (Web)\tâœ˜\t6ç§’\t47.7%\t54.7%\t97.5%\t27.4%\t4\nLuma1.6 (API)\tâœ˜\t5ç§’\t57.6%\t44.2%\t94.1%\t24.8%\t6\nCNTopC (Web)\tâœ˜\t5ç§’\t48.4%\t47.2%\t96.3%\t24.6%\t5\n\nğŸ“œ ç³»ç»Ÿè¦æ±‚\n\nä¸‹è¡¨æ˜¾ç¤ºäº†è¿è¡ŒHunyuanVideoæ¨¡å‹ï¼ˆæ‰¹é‡å¤§å°=1ï¼‰ç”Ÿæˆè§†é¢‘çš„è¦æ±‚ï¼š\n\næ¨¡å‹\tè®¾ç½®\n(é«˜åº¦/å®½åº¦/å¸§ç‡)\tGPUå³°å€¼å†…å­˜\nHunyuanVideo\t720px1280px129f\t60GB\nHunyuanVideo\t544px960px129f\t45GB\néœ€è¦æ”¯æŒCUDAçš„NVIDIA GPUã€‚\næ¨¡å‹åœ¨å•ä¸ª80G GPUä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚\næœ€å°å€¼ï¼šå¯¹äº720px1280px129fï¼Œæ‰€éœ€çš„æœ€ä½GPUå†…å­˜ä¸º60GBï¼Œå¯¹äº544px960px129fä¸º45GBã€‚\næ¨èï¼šæˆ‘ä»¬æ¨èä½¿ç”¨80GBå†…å­˜çš„GPUä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚\næµ‹è¯•æ“ä½œç³»ç»Ÿï¼šLinux\nğŸ› ï¸ ä¾èµ–å…³ç³»ä¸å®‰è£…\n\né¦–å…ˆï¼Œå…‹éš†ä»“åº“ï¼š\n\ngit clone https://github.com/tencent/HunyuanVideo\ncd HunyuanVideo\n\nLinux å®‰è£…æŒ‡å—\n\næˆ‘ä»¬æä¾›äº†ä¸€ä¸ª environment.yml æ–‡ä»¶ï¼Œç”¨äºè®¾ç½® Conda ç¯å¢ƒã€‚ Conda çš„å®‰è£…è¯´æ˜å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ã€‚\n\næˆ‘ä»¬æ¨èä½¿ç”¨ CUDA 11.8 åŠ 12.0+ ç‰ˆæœ¬ã€‚\n\n# 1. Prepare conda environment\nconda env create -f environment.yml\n\n# 2. Activate the environment\nconda activate HunyuanVideo\n\n# 3. Install pip dependencies\npython -m pip install -r requirements.txt\n\n# 4. Install flash attention v2 for acceleration (requires CUDA 11.8 or above)\npython -m pip install git+https://github.com/Dao-AILab/flash-attention.git@v2.5.9.post1\n\n\næ­¤å¤–ï¼ŒHunyuanVideo è¿˜æä¾›äº†ä¸€ä¸ªé¢„å…ˆæ„å»ºçš„ Docker é•œåƒï¼š docker_hunyuanvideoã€‚\n\n# 1. Use the following link to download the docker image tar file (For CUDA 12).\nwget https://aivideo.hunyuan.tencent.com/download/HunyuanVideo/hunyuan_video_cu12.tar\n\n# 2. Import the docker tar file and show the image meta information (For CUDA 12).\ndocker load -i hunyuan_video.tar\n\ndocker image ls\n\n# 3. Run the container based on the image\ndocker run -itd --gpus all --init --net=host --uts=host --ipc=host --name hunyuanvideo --security-opt=seccomp=unconfined --ulimit=stack=67108864 --ulimit=memlock=-1 --privileged  docker_image_tag\n\nğŸ§± ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹\n\né¢„è®­ç»ƒæ¨¡å‹çš„ä¸‹è½½è¯¦æƒ…è¯·å‚é˜…è¿™é‡Œã€‚\n\nğŸ”‘ æ¨ç†\n\nä»¥ä¸‹è¡¨æ ¼åˆ—å‡ºäº†æˆ‘ä»¬æ”¯æŒçš„åˆ†è¾¨ç‡é«˜åº¦/å®½åº¦/å¸§è®¾ç½®ã€‚\n\nåˆ†è¾¨ç‡\tå®½é«˜æ¯” 9:16\tå®½é«˜æ¯” 16:9\tå®½é«˜æ¯” 4:3\tå®½é«˜æ¯” 3:4\tå®½é«˜æ¯” 1:1\n540p\t544px Ã— 960px Ã— 129f\t960px Ã— 544px Ã— 129f\t624px Ã— 832px Ã— 129f\t832px Ã— 624px Ã— 129f\t720px Ã— 720px Ã— 129f\n720p (æ¨è)\t720px Ã— 1280px Ã— 129f\t1280px Ã— 720px Ã— 129f\t1104px Ã— 832px Ã— 129f\t832px Ã— 1104px Ã— 129f\t960px Ã— 960px Ã— 129f\nä½¿ç”¨å‘½ä»¤è¡Œ\ncd HunyuanVideo\n\npython3 sample_video.py \\\n    --video-size 720 1280 \\\n    --video-length 129 \\\n    --infer-steps 30 \\\n    --prompt \"a cat is running, realistic.\" \\\n    --flow-reverse \\\n    --seed 0 \\\n    --use-cpu-offload \\\n    --save-path ./results\n\næ›´å¤šé…ç½®\n\næˆ‘ä»¬åˆ—å‡ºäº†ä¸€äº›æ›´å®ç”¨çš„é…ç½®ï¼Œä»¥ä¾¿äºä½¿ç”¨ï¼š\n\nå‚æ•°\té»˜è®¤å€¼\tæè¿°\n--prompt\tNone\tç”¨äºè§†é¢‘ç”Ÿæˆçš„æ–‡æœ¬æç¤º\n--video-size\t720 1280\tç”Ÿæˆè§†é¢‘çš„å°ºå¯¸\n--video-length\t129\tç”Ÿæˆè§†é¢‘çš„é•¿åº¦\n--infer-steps\t30\té‡‡æ ·æ­¥éª¤çš„æ•°é‡\n--embedded-cfg-scale\t6.0\tåµŒå…¥å¼æ— åˆ†ç±»å™¨å¼•å¯¼æ¯”ä¾‹\n--flow-shift\t9.0\tæµåŒ¹é…è°ƒåº¦å™¨çš„åç§»å› å­\n--flow-reverse\tFalse\tå¦‚æœä¸ºåå‘ï¼Œåˆ™ä» t=1 -> t=0 è¿›è¡Œå­¦ä¹ /é‡‡æ ·\n--neg-prompt\tNone\tç”¨äºè§†é¢‘ç”Ÿæˆçš„è´Ÿå‘æç¤º\n--seed\t0\tç”¨äºç”Ÿæˆè§†é¢‘çš„éšæœºç§å­\n--use-cpu-offload\tFalse\tä½¿ç”¨ CPU å¸è½½æ¨¡å‹åŠ è½½ä»¥èŠ‚çœæ›´å¤šå†…å­˜ï¼Œç”Ÿæˆé«˜åˆ†è¾¨ç‡è§†é¢‘æ—¶å¿…è¦\n--save-path\t./results\tä¿å­˜ç”Ÿæˆè§†é¢‘çš„è·¯å¾„\nğŸ”— BibTeX\n\nå¦‚æœæ‚¨å‘ç° HunyuanVideo å¯¹æ‚¨çš„ç ”ç©¶æˆ–åº”ç”¨æœ‰å¸®åŠ©ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ BibTeX è¿›è¡Œå¼•ç”¨ï¼š\n\n@misc{kong2024hunyuanvideo,\n      title={HunyuanVideo: A Systematic Framework For Large Video Generative Models}, \n      author={Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Junkun Yuan, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yanxin Long, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, and Jie Jiang, along with Caesar Zhong},\n      year={2024},\n      archivePrefix={arXiv preprint arXiv:2412.03603},\n      primaryClass={cs.CV}\n}\n\nè‡´è°¢\n\næˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢ä»¥ä¸‹è´¡çŒ®è€…ï¼šSD3ã€FLUXã€Llamaã€LLaVAã€Xtunerã€diffusers ä»¥åŠ HuggingFace ä»“åº“çš„å¼€æ”¾ç ”ç©¶è€…å’Œæ¢ç´¢è€…ã€‚\n\næ­¤å¤–ï¼Œæˆ‘ä»¬ä¹Ÿæ„Ÿè°¢è…¾è®¯æ··å…ƒå¤šæ¨¡æ€å›¢é˜Ÿåœ¨æ–‡æœ¬ç¼–ç å™¨æ–¹é¢çš„ååŠ©ã€‚",
    "tags": "[\"Text-to-Video\", \"PyTorch\", \"Transformers\", \"Other\"]"
  },
  {
    "url": "https://gitcode.com/tencent_hunyuan/Hunyuan3D-1",
    "project_name": "Hunyuan3D-1",
    "readme": "Original Text\n\nè…¾è®¯æ··å…ƒ3D-1.0ï¼šç»Ÿä¸€æ–‡æœ¬å’Œå›¾åƒè½¬3Dç”Ÿæˆçš„æ¡†æ¶\n â€‚  â€‚  â€‚  â€‚  â€‚\nğŸ”¥ğŸ”¥ğŸ”¥ æ–°é—»!!\n2024å¹´11æœˆ5æ—¥ï¼šğŸ’¬ ç°åœ¨æ”¯æŒå›¾åƒè½¬3Dç”Ÿæˆçš„æ¼”ç¤ºã€‚è¯·æŸ¥çœ‹ä¸‹é¢çš„è„šæœ¬ã€‚\n2024å¹´11æœˆ5æ—¥ï¼šğŸ’¬ ç°åœ¨æ”¯æŒæ–‡æœ¬è½¬3Dç”Ÿæˆçš„æ¼”ç¤ºã€‚è¯·æŸ¥çœ‹ä¸‹é¢çš„è„šæœ¬ã€‚\nğŸ“‘ å¼€æºè®¡åˆ’\næ¨æ–­\næ£€æŸ¥ç‚¹\nçƒ˜ç„™ç›¸å…³\nè®­ç»ƒ\nComfyUI\nè’¸é¦ç‰ˆæœ¬\nTensorRT ç‰ˆæœ¬\næ‘˜è¦\n\nè™½ç„¶3Dç”Ÿæˆæ¨¡å‹æå¤§åœ°æ”¹å–„äº†è‰ºæœ¯å®¶çš„å·¥ä½œæµç¨‹ï¼Œä½†ç°æœ‰çš„ç”¨äº3Dç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé€Ÿåº¦å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ–¹æ¡ˆï¼Œåä¸ºæ··å…ƒ3D-1.0ï¼ŒåŒ…æ‹¬è½»é‡ç‰ˆå’Œæ ‡å‡†ç‰ˆï¼Œä¸¤è€…éƒ½æ”¯æŒåŸºäºæ–‡æœ¬å’Œå›¾åƒçš„ç”Ÿæˆã€‚\n\nåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªå¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œåœ¨å¤§çº¦4ç§’å†…é«˜æ•ˆåœ°ç”Ÿæˆå¤šè§†è§’RGBã€‚è¿™äº›å¤šè§†è§’å›¾åƒä»ä¸åŒçš„è§†è§’æ•æ‰äº†3Dèµ„æºçš„ä¸°å¯Œç»†èŠ‚ï¼Œå°†ä»»åŠ¡ä»å•è§†è§’é‡å»ºæ‰©å±•åˆ°å¤šè§†è§’é‡å»ºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå‰é¦ˆé‡å»ºæ¨¡å‹ï¼Œåœ¨çº¦7ç§’å†…å¿«é€Ÿã€å¿ å®åœ°æ ¹æ®ç”Ÿæˆçš„å¤šè§†è§’å›¾åƒé‡å»º3Dèµ„æºã€‚é‡å»ºç½‘ç»œå­¦ä¹ å¤„ç†ç”±å¤šè§†è§’æ‰©æ•£å¼•å…¥çš„å™ªå£°å’Œä¸ä¸€è‡´æ€§ï¼Œå¹¶åˆ©ç”¨æ¡ä»¶å›¾åƒä¸­çš„å¯ç”¨ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°æ¢å¤3Dç»“æ„ã€‚\n\næˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå³æ··å…ƒ-DiTï¼Œä½¿å…¶æˆä¸ºæ”¯æŒåŸºäºæ–‡æœ¬å’Œå›¾åƒçš„3Dç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ ‡å‡†ç‰ˆæ¨¡å‹å‚æ•°æ˜¯è½»é‡ç‰ˆçš„3å€ï¼Œä¹Ÿæ¯”ç°æœ‰çš„å…¶ä»–æ¨¡å‹å‚æ•°å¤šã€‚æˆ‘ä»¬çš„æ··å…ƒ3D-1.0åœ¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„å¹³è¡¡ï¼Œå¤§å¹…é™ä½äº†ç”Ÿæˆæ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆçš„èµ„æºçš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚\n\nğŸ‰ æ··å…ƒ3D-1 æ¶æ„\n\nğŸ“ˆ å¯¹æ¯”\n\næˆ‘ä»¬ä½¿ç”¨å…¶ä»–å¼€æºçš„3Dç”Ÿæˆæ–¹æ³•è¯„ä¼°äº†æ··å…ƒ3D-1.0ï¼Œæˆ‘ä»¬çš„æ··å…ƒ3D-1.0åœ¨5é¡¹æŒ‡æ ‡ä¸­è·å¾—äº†æœ€é«˜çš„ç”¨æˆ·åå¥½ã€‚è¯¦ç»†ä¿¡æ¯è§å·¦ä¸‹è§’çš„å›¾ç‰‡ã€‚\n\nè½»é‡æ¨¡å‹åœ¨NVIDIA A100 GPUä¸Šä»å•å¼ å›¾åƒç”Ÿæˆ3Dç½‘æ ¼å¤§çº¦éœ€è¦10ç§’ï¼Œè€Œæ ‡å‡†æ¨¡å‹å¤§çº¦éœ€è¦25ç§’ã€‚å³ä¸‹è§’çš„å›¾è¡¨å±•ç¤ºäº†æ··å…ƒ3D-1.0åœ¨è´¨é‡å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚\n\n \n\nå¼€å§‹ä½¿ç”¨\né¦–å…ˆå…‹éš†ä»“åº“ï¼š\ngit clone https://github.com/tencent/Hunyuan3D-1\ncd Hunyuan3D-1\n\nLinux å®‰è£…æŒ‡å—\n\næˆ‘ä»¬æä¾›äº†ä¸€ä¸ª env_install.sh è„šæœ¬æ–‡ä»¶ï¼Œç”¨äºç¯å¢ƒé…ç½®ã€‚\n\n# step 1, create conda env\nconda create -n hunyuan3d-1 python=3.9 or 3.10 or 3.11 or 3.12\nconda activate hunyuan3d-1\n\n# step 2. install torch realated package\nwhich pip # check pip corresponds to python\n\n# modify the cuda version according to your machine (recommended)\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\n# step 3. install other packages\nbash env_install.sh\n\nğŸ’¡ç¯å¢ƒå®‰è£…çš„å…¶ä»–æç¤º\nä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹\n\næ¨¡å‹å¯ä» https://huggingface.co/tencent/Hunyuan3D-1 è·å–ï¼š\n\nHunyuan3D-1/liteï¼Œè½»é‡çº§æ¨¡å‹ï¼Œç”¨äºå¤šè§†è§’ç”Ÿæˆã€‚\nHunyuan3D-1/stdï¼Œæ ‡å‡†æ¨¡å‹ï¼Œç”¨äºå¤šè§†è§’ç”Ÿæˆã€‚\nHunyuan3D-1/svrmï¼Œç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹ã€‚\n\nä¸‹è½½æ¨¡å‹å‰ï¼Œè¯·å…ˆå®‰è£… huggingface-cliã€‚ï¼ˆè¯¦ç»†å®‰è£…è¯´æ˜è¯·è§æ­¤å¤„ã€‚ï¼‰\n\npython3 -m pip install \"huggingface_hub[cli]\"\n\n\nç„¶åä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ¨¡å‹ï¼š\n\nmkdir weights\nhuggingface-cli download tencent/Hunyuan3D-1 --local-dir ./weights\n\nmkdir weights/hunyuanDiT\nhuggingface-cli download Tencent-Hunyuan/HunyuanDiT-v1.1-Diffusers-Distilled --local-dir ./weights/hunyuanDiT\n\næ¨ç†\n\né’ˆå¯¹æ–‡æœ¬åˆ°3Dç”Ÿæˆï¼Œæˆ‘ä»¬æ”¯æŒä¸­è‹±åŒè¯­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†ã€‚\n\npython3 main.py \\\n    --text_prompt \"a lovely rabbit\" \\\n    --save_folder ./outputs/test/ \\\n    --max_faces_num 90000 \\\n    --do_texture_mapping \\\n    --do_render\n\n\nå¯¹äºå›¾åƒåˆ°3Dç”Ÿæˆï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥è¿›è¡Œæ¨æ–­ã€‚\n\npython3 main.py \\\n    --image_prompt \"/path/to/your/image\" \\\n    --save_folder ./outputs/test/ \\\n    --max_faces_num 90000 \\\n    --do_texture_mapping \\\n    --do_render\n\n\nä»¥ä¸‹æ˜¯ä¸€äº›ä¾¿äºä½¿ç”¨çš„é‡è¦é…ç½®é¡¹åˆ—è¡¨ï¼š\n\nå‚æ•°å\té»˜è®¤å€¼\tæè¿°\n--text_prompt\tæ— \tç”¨äº3Dç”Ÿæˆçš„æ–‡æœ¬æç¤ºä¿¡æ¯\n--image_prompt\tæ— \tç”¨äº3Dç”Ÿæˆçš„å›¾åƒæç¤ºä¿¡æ¯\n--t2i_seed\t0\tç”Ÿæˆçš„å›¾åƒçš„éšæœºç§å­\n--t2i_steps\t25\tæ–‡æœ¬åˆ°å›¾åƒé‡‡æ ·æ­¥éª¤æ•°\n--gen_seed\t0\tç”Ÿæˆ3Dæ¨¡å‹æ—¶çš„éšæœºç§å­\n--gen_steps\t50\t3Dç”Ÿæˆé‡‡æ ·çš„æ­¥éª¤æ•°\n--max_faces_num\t90000\t3Dç½‘æ ¼é¢æ•°ä¸Šé™\n--save_memory\tFalse\tæ¨¡å—å°†è‡ªåŠ¨è¿ç§»è‡³CPUä»¥èŠ‚çœæ˜¾å­˜\n--do_texture_mapping\tFalse\tå°†é¡¶ç‚¹ç€è‰²æ”¹ä¸ºçº¹ç†ç€è‰²\n--do_render\tFalse\tæ¸²æŸ“åŠ¨å›¾ (GIF)\n\næˆ‘ä»¬è¿˜å‡†å¤‡äº†å…·æœ‰ä¸åŒé…ç½®çš„è„šæœ¬ä¾›å‚è€ƒï¼š\n\næ ‡å‡†æ¨ç†æµç¨‹éœ€è¦30GBæ˜¾å­˜ï¼ˆä½¿ç”¨--save_memoryæ—¶éœ€24Gæ˜¾å­˜ï¼‰ã€‚\nç²¾ç®€æ¨ç†æµç¨‹éœ€è¦22GBæ˜¾å­˜ï¼ˆä½¿ç”¨--save_memoryæ—¶éœ€18Gæ˜¾å­˜ï¼‰ã€‚\næ³¨æ„ï¼šä½¿ç”¨--save_memoryå°†å¢åŠ æ¨ç†æ—¶é—´ã€‚\nbash scripts/text_to_3d_std.sh \nbash scripts/text_to_3d_lite.sh \nbash scripts/image_to_3d_std.sh \nbash scripts/image_to_3d_lite.sh \n\n\nå¦‚æœæ‚¨æ˜¾å¡çš„å†…å­˜æ˜¯16Gï¼Œæ‚¨å¯ä»¥å°è¯•åˆ†åˆ«ç‹¬ç«‹è¿è¡Œæµæ°´çº¿ä¸­çš„æ¨¡å—ï¼š\n\nbash scripts/text_to_3d_std_separately.sh 'a lovely rabbit' ./outputs/test # >= 16G\nbash scripts/text_to_3d_lite_separately.sh 'a lovely rabbit' ./outputs/test # >= 14G\nbash scripts/image_to_3d_std_separately.sh ./demos/example_000.png ./outputs/test  # >= 16G\nbash scripts/image_to_3d_lite_separately.sh ./demos/example_000.png ./outputs/test # >= 10G\n\nä½¿ç”¨ Gradio\n\næˆ‘ä»¬å‡†å¤‡äº†ä¸¤ä¸ªç‰ˆæœ¬çš„å¤šè§†è§’ç”Ÿæˆï¼Œåˆ†åˆ«æ˜¯æ ‡å‡†ç‰ˆ(std)å’Œè½»é‡ç‰ˆ(lite)ã€‚\n\n# std \npython3 app.py\npython3 app.py --save_memory\n\n# lite\npython3 app.py --use_lite\npython3 app.py --use_lite --save_memory\n\n\nç„¶åå¯ä»¥é€šè¿‡ http://0.0.0.0:8080 è®¿é—®æ¼”ç¤ºã€‚è¯·æ³¨æ„ï¼Œè¿™é‡Œçš„ 0.0.0.0 éœ€è¦æ›¿æ¢ä¸ºä½ çš„æœåŠ¡å™¨ IP åœ°å€ X.X.X.Xã€‚\n\nç›¸æœºå‚æ•°\n\nè¾“å‡ºè§†å›¾æ˜¯ä¸€ç»„å›ºå®šçš„ç›¸æœºå§¿æ€ï¼š\n\nèµ¤çº¬ï¼ˆç›¸å¯¹äºè¾“å…¥è§†å›¾ï¼‰ï¼š+0, +60, +120, +180, +240, +300ã€‚\nå¼•ç”¨\n\nå¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªä»“åº“æœ‰ç”¨ï¼Œè¯·åœ¨æ‚¨çš„æŠ¥å‘Šä¸­å¼•ç”¨æˆ‘ä»¬çš„ç ”ç©¶æŠ¥å‘Šï¼š\n\n@misc{yang2024tencent,\n    title={Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},\n    author={Xianghui Yang and Huiwen Shi and Bowen Zhang and Fan Yang and Jiacheng Wang and Hongxu Zhao and Xinhai Liu and Xinzhou Wang and Qingxiang Lin and Jiaao Yu and Lifu Wang and Zhuo Chen and Sicong Liu and Yuhong Liu and Yong Yang and Di Wang and Jie Jiang and Chunchao Guo},\n    year={2024},\n    eprint={2411.02293},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\n\nè¯·æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬å’Œç›¸åº”çš„ Markdown æ ¼å¼ï¼Œæˆ‘å°†ä¼šä¸ºæ‚¨ç¿»è¯‘æˆä¸­æ–‡ï¼Œå¹¶ä¿æŒåŸæ–‡çš„ Markdown æ ¼å¼ã€‚",
    "tags": "[\"Text-to-3D\", \"PyTorch\", \"Transformers\", \"Diffusers\", \"Safetensors\", \"English\", \"Chinese\", \"Other\", \"image-to-3d\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-21B-A3B-Base-Paddle",
    "project_name": "ERNIE-4.5-21B-A3B-Base-Paddle",
    "readme": "Original Text\n   \nERNIE-4.5-21B-A3B-Base\nERNIE 4.5 çš„äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„å…ˆè¿›èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—ï¼Œä¾èµ–äºä»¥ä¸‹å‡ é¡¹å…³é”®æŠ€æœ¯é©æ–°ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶æé«˜æ¶‰åŠæ–‡æœ¬ç†è§£ä¸ç”Ÿæˆã€å›¾åƒç†è§£å’Œè·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡æ€§èƒ½ã€‚ä¸ºäº†å®ç°ä¸åŒæ¨¡æ€ä¹‹é—´äº’ä¸å¹²æ‰°çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº† å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥äº† æ¨¡æ€éš”ç¦»è·¯ç”±ï¼Œå¹¶ä½¿ç”¨äº† è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€ä»¤ç‰Œå¹³è¡¡æŸå¤±ã€‚è¿™äº›ç»“æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€éƒ½èƒ½æœ‰æ•ˆè¡¨ç¤ºï¼Œä½¿å¾—åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿç›¸äº’å¢å¼ºã€‚\n\næ‰©å±•æ•ˆç‡åŒ–çš„åŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬æå‡ºäº†æ–°é¢–çš„å¼‚æ„æ··åˆå¹¶è¡Œæ€§å’Œåˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œç”¨äº ERNIE 4.5 æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œæ€§ã€å†…å­˜æ•ˆç‡åŒ–çš„ç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†å“è¶Šçš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•å’Œ å·ç§¯ä»£ç é‡åŒ– ç®—æ³•ï¼Œå®ç°äº† 4 ä½/2 ä½æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºåŠ¨æ€è§’è‰²åˆ‡æ¢çš„ PD åˆ†è§£ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œå¢å¼º ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚å»ºç«‹åœ¨ PaddlePaddle ä¹‹ä¸Šï¼ŒERNIE 4.5 åœ¨å¤šç§ç¡¬ä»¶å¹³å°ä¸Šæä¾›äº†é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºäº†æ»¡è¶³å®é™…åº”ç”¨å¤šæ ·åŒ–çš„éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒæ¨¡æ€è¿›è¡Œäº†å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLMs ä¼˜åŒ–äº†é€šç”¨è¯­è¨€ç†è§£å’Œç”Ÿæˆï¼ŒVLMs ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹éƒ½é‡‡ç”¨äº† ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ æˆ–ä¸€ç§åä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰ çš„ä¿®æ”¹åå¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œåè®­ç»ƒã€‚\n\nä¸ºäº†ç¡®ä¿å¤šæ¨¡æ€è”åˆè®­ç»ƒçš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚åœ¨å‰ä¸¤ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬ä»…è®­ç»ƒæ–‡æœ¬ç›¸å…³å‚æ•°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå»ºç«‹å¼ºå¤§çš„åŸºæœ¬è¯­è¨€ç†è§£èƒ½åŠ›ä»¥åŠé•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚æœ€åçš„ å¤šæ¨¡æ€é˜¶æ®µé€šè¿‡å¼•å…¥é¢å¤–çš„å‚æ•°ï¼ŒåŒ…æ‹¬ç”¨äºå›¾åƒç‰¹å¾æå–çš„ ViTã€ç”¨äºç‰¹å¾è½¬æ¢çš„é€‚é…å™¨ä»¥åŠç”¨äºå¤šæ¨¡æ€ç†è§£çš„è§†è§‰ä¸“å®¶ï¼Œæ‰©å±•äº†å›¾åƒå’Œè§†é¢‘çš„èƒ½åŠ›ã€‚åœ¨è¿™ä¸€é˜¶æ®µï¼Œæ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ç›¸äº’å¢å¼ºã€‚åœ¨é¢„è®­ç»ƒäº†æ•°ä¸‡äº¿ä¸ªä»¤ç‰Œä¹‹åï¼Œæˆ‘ä»¬æå–äº†æ–‡æœ¬ç›¸å…³å‚æ•°ï¼Œæœ€ç»ˆè·å¾—äº† ERNIE-4.5-21B-A3B-Baseã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-21B-A3B-Base æ˜¯ä¸€ä¸ªæ–‡æœ¬ MoE åŸºç¡€æ¨¡å‹ï¼Œæ€»å‚æ•°é‡ä¸º 21Bï¼Œæ¯ä¸ªä»¤ç‰Œæ¿€æ´»å‚æ•°é‡ä¸º 3Bã€‚ä»¥ä¸‹æ˜¯æ¨¡å‹é…ç½®çš„è¯¦ç»†ä¿¡æ¯ï¼š\n\nå…³é”®å­—\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°é‡ï¼ˆæ€»/æ¿€æ´»ï¼‰\t21B / 3B\nå±‚æ•°\t28\nå¤´æ•°ï¼ˆQ/KVï¼‰\t20 / 4\næ–‡æœ¬ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 6\nè§†è§‰ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 6\nå…±äº«ä¸“å®¶\t2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ ERNIEKit è¿›è¡Œæ¨¡å‹å¾®è°ƒ\n\nERNIEKit æ˜¯åŸºäº PaddlePaddle çš„è®­ç»ƒå·¥å…·åŒ…ï¼Œä¸“é—¨ä¸º ERNIE ç³»åˆ—å¼€æºå¤§å‹æ¨¡å‹è®¾è®¡ã€‚å®ƒä¸ºæŒ‡ä»¤å¾®è°ƒï¼ˆSFTã€LoRAï¼‰å’Œæ’é½è®­ç»ƒï¼ˆDPOï¼‰ç­‰åœºæ™¯æä¾›äº†å…¨é¢æ”¯æŒï¼Œç¡®ä¿æœ€ä¼˜æ€§èƒ½ã€‚\n\nä½¿ç”¨ç¤ºä¾‹ï¼š\n\n# Download model\nhuggingface-cli download baidu/ERNIE-4.5-21B-A3B-Base-Paddle --local-dir baidu/ERNIE-4.5-21B-A3B-Base-Paddle\n# SFT\nerniekit train examples/configs/ERNIE-4.5-21B-A3B/sft/run_sft_lora_8k.yaml model_name_or_path=baidu/ERNIE-4.5-21B-A3B-Base-Paddle\n# DPO\nerniekit train examples/configs/ERNIE-4.5-21B-A3B/dpo/run_dpo_lora_8k.yaml model_name_or_path=baidu/ERNIE-4.5-21B-A3B-Base-Paddle\n\n\nä»¥ä¸‹æ˜¯ç¿»è¯‘åçš„å†…å®¹ï¼š\n\nè‹¥éœ€æ›´å¤šè¯¦ç»†ç¤ºä¾‹ï¼ŒåŒ…æ‹¬æ­é… LoRA çš„ SFTã€å¤š GPU é…ç½®ä»¥åŠé«˜çº§è„šæœ¬ï¼Œè¯·å‚é˜… ERNIEKit å­˜å‚¨åº“ä¸­çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ã€‚\n\nFastDeploy ï¿½ infer æ¨ç†\n\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ FastDeploy å‘½ä»¤å¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚å…³äºæ›´å¤šè¯¦ç»†çš„ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚è€ƒ FastDeploy å­˜å‚¨åº“ã€‚\n\næ³¨æ„ï¼šå¯¹äºå•å¡éƒ¨ç½²ï¼Œè‡³å°‘éœ€è¦ 80G çš„ GPU å†…å­˜èµ„æºã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-21B-A3B-Base-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768\n       --max-num-seqs 32\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç‰‡æ®µç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•æ ¹æ®ç»™å®šè¾“å…¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Base-PT\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\nprompt = \"Large language model is\"\nmodel_inputs = tokenizer([prompt], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\nresult = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\nprint(\"result:\", result)\n\nvLLM æ¨æ–­\n\nvLLMç›®å‰æ­£åœ¨é€‚é…ä¸­ï¼Œå¯ä¼˜å…ˆä½¿ç”¨æˆ‘ä»¬çš„åˆ†å‰ä»“åº“ vllmã€‚æˆ‘ä»¬æ­£åœ¨ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢æ”¯æŒERNIE4.5æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nvllm serve baidu/ERNIE-4.5-21B-A3B-Base-PT --trust-remote-code\n\nè®¸å¯è¯\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache License 2.0 æä¾›æœåŠ¡ã€‚è¯¥è®¸å¯è¯åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶çš„å‰æä¸‹ï¼Œå…è®¸å•†ä¸šç”¨é€”ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨å‘ç° ERNIE 4.5 æœ‰ç”¨ï¼Œæˆ–å¸Œæœ›åœ¨æ‚¨çš„é¡¹ç›®ä¸­ä½¿ç”¨å®ƒï¼Œè¯·åŠ¡å¿…å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-W4A8C8-TP4-Paddle",
    "project_name": "ERNIE-4.5-300B-A47B-W4A8C8-TP4-Paddle",
    "readme": "Original Text\n   \nERNIE-4.5-300B-A47B\nERNIE 4.5 äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„å…ˆè¿›èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—ï¼Œå¾—ç›Šäºä»¥ä¸‹å‡ é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰ä¸¤ç§æ¨¡æ€ä¸Šè”åˆè®­ç»ƒï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®å¼‚ï¼Œå¹¶æé«˜æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆã€å›¾åƒç†è§£ä»¥åŠè·¨æ¨¡æ€æ¨ç†ç­‰ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ºäº†é˜²æ­¢ä¸€ç§æ¨¡æ€é˜»ç¢å¦ä¸€ç§æ¨¡æ€çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº† å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥äº† æ¨¡æ€éš”ç¦»è·¯ç”±ï¼Œå¹¶ä½¿ç”¨äº† è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€ä»¤ç‰Œå¹³è¡¡æŸå¤±ã€‚è¿™äº›ç»“æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€çš„æœ‰æ•ˆè¡¨ç¤ºï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°ç›¸äº’å¼ºåŒ–ã€‚\n\nè§„æ¨¡æ•ˆç‡åŒ–åŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼‚æ„æ··åˆå¹¶è¡Œæ€§å’Œå±‚çº§è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œç”¨äº ERNIE 4.5 æ¨¡å‹çš„æ•ˆç‡åŒ–è®­ç»ƒã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œæ€§ã€å†…å­˜æ•ˆç‡åŒ–ç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†æ˜¾è‘—çš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•å’Œ å·ç§¯ä»£ç é‡åŒ– ç®—æ³•ï¼Œä»¥å®ç° 4-bit/2-bit æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºåŠ¨æ€è§’è‰²åˆ‡æ¢çš„ PD è§£èšåˆï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œå¢å¼º ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚åŸºäº PaddlePaddle æ„å»ºè€Œæˆï¼ŒERNIE 4.5 åœ¨å¹¿æ³›çš„ç¡¬ä»¶å¹³å°ä¸Šæä¾›é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºäº†æ»¡è¶³ç°å®ä¸–ç•Œåº”ç”¨çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œäº†é’ˆå¯¹ç‰¹å®šæ¨¡æ€çš„å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLMs ä¼˜åŒ–äº†ä¸€èˆ¬æ€§è¯­è¨€ç†è§£å’Œç”Ÿæˆï¼Œè€Œ VLMs åˆ™ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹éƒ½é‡‡ç”¨äº†ä¸€ç§ç»“åˆ ç›‘ç£å¾®è°ƒ (SFT)ã€ç›´æ¥åå¥½ä¼˜åŒ– (DPO) æˆ–ä¸€ç§åä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ– (UPO) çš„ä¿®æ”¹åçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œåè®­ç»ƒã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-300B-A47B æ˜¯ä¸€ä¸ªæ–‡æœ¬ MoE åè®­ç»ƒæ¨¡å‹ï¼Œæ‹¥æœ‰ 300B æ€»å‚æ•°å’Œæ¯ä¸ªæ ‡è®°æ¿€æ´»çš„ 47B å‚æ•°ã€‚ä»¥ä¸‹æ˜¯æ¨¡å‹é…ç½®çš„è¯¦ç»†ä¿¡æ¯ï¼š\n\nå…³é”®å­—\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°ï¼ˆæ€»/æ¿€æ´»ï¼‰\t300B / 47B\nå±‚æ•°\t54\nå¤´éƒ¨ï¼ˆQ/KVï¼‰\t64 / 8\næ–‡æœ¬ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 8\nè§†è§‰ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 8\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ FastDeploy\n\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ FastDeploy å‘½ä»¤å¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚æœ‰å…³æ›´è¯¦ç»†çš„ç”¨æ³•è¯´æ˜ï¼Œè¯·å‚è€ƒ FastDeploy ä»“åº“ã€‚\n\næ³¨æ„ï¼š è‹¥è¦åœ¨æ¯ä¸ªæ˜¾å­˜è‡³å°‘ 80G çš„ 4 å— GPU é…ç½®ä¸Šéƒ¨ç½²ï¼Œè¯·æŒ‡å®š --quantization wint4ã€‚å¦‚æœæ‚¨æŒ‡å®š --quantization wint8ï¼Œåˆ™éœ€è¦ 8 å— GPU çš„èµ„æºã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-300B-A47B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --quantization wint4 \\\n       --tensor-parallel-size 8 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\n\nè¦ä½¿ç”¨ FastDeploy éƒ¨ç½² W4A8C8 é‡åŒ–ç‰ˆæœ¬ï¼Œæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-300B-A47B-W4A8C8-TP4-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --tensor-parallel-size 4 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\n\nè¦åœ¨å•ä¸ª141G GPUä¸Šä½¿ç”¨FastDeployéƒ¨ç½²WINT2é‡åŒ–ç‰ˆæœ¬ï¼Œæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model \"baidu/ERNIE-4.5-300B-A47B-2Bits-Paddle\" \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --tensor-parallel-size 1 \\\n       --max-model-len  32768 \\\n       --max-num-seqs 128\n\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ ERNIE-4.5-300B-A47B-FP8 æ ¹æ®ç»™å®šè¾“å…¥ç”Ÿæˆå†…å®¹ã€‚\n\nfrom fastdeploy import LLM, SamplingParams\n\nprompts = [\n    \"Hello, my name is\",\n]\n\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=128)\n\nmodel = \"baidu/ERNIE-4.5-300B-A47B-FP8-Paddle\"\nllm = LLM(model=model, tensor_parallel_size=8, max_model_len=8192, num_gpu_blocks_override=1024, engine_worker_queue_port=9981)\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs.text\n    print(\"generated_text\", generated_text)\n\næœ€ä½³å®è·µ\né‡‡æ ·å‚æ•°\n\nä¸ºäº†è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ Temperature=0.8ï¼ŒTopP=0.8ã€‚\n\nç½‘ç»œæœç´¢æç¤º\n\nå¯¹äºç½‘ç»œæœç´¢ï¼Œ{references}ã€{date} å’Œ {question} æ˜¯å‚æ•°ã€‚\n\nå¯¹äºä¸­æ–‡é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æç¤ºï¼š\n\nernie_search_zh_prompt = \\\n'''ä¸‹é¢ä½ ä¼šæ”¶åˆ°å½“å‰æ—¶é—´ã€å¤šä¸ªä¸åŒæ¥æºçš„å‚è€ƒæ–‡ç« å’Œä¸€æ®µå¯¹è¯ã€‚ä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»å¤šä¸ªå‚è€ƒæ–‡ç« ï¼Œå¹¶æ ¹æ®å‚è€ƒæ–‡ç« ä¸­çš„ä¿¡æ¯å›ç­”å¯¹è¯ä¸­çš„é—®é¢˜ã€‚\nä»¥ä¸‹æ˜¯å½“å‰æ—¶é—´å’Œå‚è€ƒæ–‡ç« ï¼š\n---------\n#å½“å‰æ—¶é—´\n{date}\n\n#å‚è€ƒæ–‡ç« \n{references}\n\n---------\nè¯·æ³¨æ„ï¼š\n1. å›ç­”å¿…é¡»ç»“åˆé—®é¢˜éœ€æ±‚å’Œå½“å‰æ—¶é—´ï¼Œå¯¹å‚è€ƒæ–‡ç« çš„å¯ç”¨æ€§è¿›è¡Œåˆ¤æ–­ï¼Œé¿å…åœ¨å›ç­”ä¸­ä½¿ç”¨é”™è¯¯æˆ–è¿‡æ—¶çš„ä¿¡æ¯ã€‚\n2. å½“å‚è€ƒæ–‡ç« ä¸­çš„ä¿¡æ¯æ— æ³•å‡†ç¡®åœ°å›ç­”é—®é¢˜æ—¶ï¼Œä½ éœ€è¦åœ¨å›ç­”ä¸­æä¾›è·å–ç›¸åº”ä¿¡æ¯çš„å»ºè®®ï¼Œæˆ–æ‰¿è®¤æ— æ³•æä¾›ç›¸åº”ä¿¡æ¯ã€‚\n3. ä½ éœ€è¦ä¼˜å…ˆæ ¹æ®ç™¾ç§‘ã€å®˜ç½‘ã€æƒå¨æœºæ„ã€ä¸“ä¸šç½‘ç«™ç­‰é«˜æƒå¨æ€§æ¥æºçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚\n4. å›å¤éœ€è¦ç»¼åˆå‚è€ƒæ–‡ç« ä¸­çš„ç›¸å…³æ•°å­—ã€æ¡ˆä¾‹ã€æ³•å¾‹æ¡æ–‡ã€å…¬å¼ç­‰ä¿¡æ¯ï¼Œä½¿ä½ çš„ç­”æ¡ˆæ›´ä¸“ä¸šã€‚\n5. å½“é—®é¢˜å±äºåˆ›ä½œç±»ä»»åŠ¡æ—¶ï¼Œéœ€æ³¨æ„ä»¥ä¸‹ç»´åº¦ï¼š\n   - æ€åº¦é²œæ˜ï¼šè§‚ç‚¹ã€ç«‹åœºæ¸…æ™°æ˜ç¡®ï¼Œé¿å…æ¨¡æ£±ä¸¤å¯ï¼Œè¯­è¨€æœæ–­ç›´æ¥\n   - æ–‡é‡‡é£æ‰¬ï¼šç”¨è¯ç²¾å‡†ç”ŸåŠ¨ï¼Œå–„ç”¨ä¿®è¾æ‰‹æ³•ï¼Œå¢å¼ºæ„ŸæŸ“åŠ›\n   - æœ‰ç†æœ‰æ®ï¼šé€»è¾‘ä¸¥å¯†é€’è¿›ï¼Œç»“åˆæƒå¨æ•°æ®/äº‹å®æ”¯æ’‘è®ºç‚¹\n---------\nä¸‹é¢è¯·ç»“åˆä»¥ä¸Šä¿¡æ¯ï¼Œå›ç­”é—®é¢˜ï¼Œè¡¥å…¨å¯¹è¯\n{question}'''\n\n\nå¯¹äºè‹±æ–‡é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨å¦‚ä¸‹æç¤ºè¯­ï¼š\n\nernie_search_en_prompt = \\\n'''\nBelow you will be given the current time, multiple references from different sources, and a conversation. Your task is to read the references and use the information in them to answer the question in the conversation.\nHere are the current time and the references:\n---------\n#Current Time\n{date}\n\n#References\n{references}\n\n---------\nPlease note:\n1. Based on the questionâ€™s requirements and the current time, assess the usefulness of the references to avoid using inaccurate or outdated information in the answer.  \n2. If the references do not provide enough information to accurately answer the question, you should suggest how to obtain the relevant information or acknowledge that you are unable to provide it.  \n3. Prioritize using information from highly authoritative sources such as encyclopedias, official websites, authoritative institutions, and professional websites when answering questions.\n4. Incorporate relevant numbers, cases, legal provisions, formulas, and other details from the references to make your answer more professional.\n5. For creative tasks, keep these dimensions in mind:\n   - Clear attitude: Clear views and positions, avoid ambiguity, and use decisive and direct language\n   - Brilliant writing: Precise and vivid words, good use of rhetoric, and enhance the appeal\n   - Well-reasoned: Rigorous logic and progressive, combined with authoritative data/facts to support the argument\n\n---------\nNow, using the information above, answer the question and complete the conversation:  \n{question}'''\n\n\nå‚æ•°è¯´æ˜ï¼š\n\n{question} æ˜¯ç”¨æˆ·æå‡ºçš„é—®é¢˜\n{date} ä»£è¡¨å½“å‰æ—¶é—´ï¼Œæ¨èæ ¼å¼ä¸ºâ€œYYYY-MM-DD HH:MM:SSï¼Œæ˜ŸæœŸï¼ŒåŒ—äº¬/ä¸­å›½ã€‚â€\n{references} ä¸ºå‚è€ƒæ–‡çŒ®ï¼Œæ¨èæ ¼å¼ä¸ºï¼š\n##å‚è€ƒæ–‡ç« 1\næ ‡é¢˜ï¼šå‘¨æ°ä¼¦\næ–‡ç« å‘å¸ƒæ—¶é—´ï¼š2025-04-20\nå†…å®¹ï¼šå‘¨æ°ä¼¦(Jay Chou),1979å¹´1æœˆ18æ—¥å‡ºç”Ÿäºå°æ¹¾çœæ–°åŒ—å¸‚,ç¥–ç±ç¦å»ºçœæ°¸æ˜¥å¿,åè¯­æµè¡Œä¹ç”·æ­Œæ‰‹ã€éŸ³ä¹äººã€æ¼”å‘˜ã€å¯¼æ¼”ã€ç¼–å‰§,æ¯•ä¸šäºæ·¡æ±Ÿä¸­å­¦ã€‚2000å¹´,å‘è¡Œä¸ªäººé¦–å¼ éŸ³ä¹ä¸“è¾‘ã€ŠJayã€‹ã€‚...\næ¥æºç½‘ç«™ç½‘å€ï¼šbaike.baidu.com\næ¥æºç½‘ç«™çš„ç½‘ç«™åï¼šç™¾åº¦ç™¾ç§‘\n\n##å‚è€ƒæ–‡ç« 2\n...\n\nè®¸å¯è¯\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache License 2.0 æä¾›ã€‚è¯¥è®¸å¯è¯åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶çš„å‰æä¸‹ï¼Œå…è®¸å•†ä¸šç”¨é€”ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨è®¤ä¸º ERNIE 4.5 æœ‰ç”¨ï¼Œæˆ–å¸Œæœ›å°†å…¶åº”ç”¨äºæ‚¨çš„é¡¹ç›®ä¸­ï¼Œè¯·åŠ¡å¿…å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n\n\nå½“ç„¶ï¼Œæˆ‘ä¼šæŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚è¯·æ‚¨æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬ä»¥åŠæœŸæœ›çš„ä¸­æ–‡é£æ ¼ï¼ˆé€šä¿—ã€ä¸“ä¸šã€ä¼˜é›…æˆ–æµç•…ï¼‰ï¼Œæˆ‘å°†ä¼šä¸ºæ‚¨ç¿»è¯‘ã€‚",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-VL-424B-A47B-Paddle",
    "project_name": "ERNIE-4.5-VL-424B-A47B-Paddle",
    "readme": "Original Text\n   \nERNIE-4.5-VL-424B-A47B\næ–‡å¿ƒ4.5æŠ€æœ¯äº®ç‚¹\n\næ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯åŸºäºMoEæ¶æ„çš„A47Bå’ŒA3Bç‰ˆæœ¬ï¼‰çš„å“è¶Šèƒ½åŠ›æºè‡ªä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯çªç ´ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„MoEé¢„è®­ç»ƒï¼šé€šè¿‡æ–‡æœ¬ä¸è§†è§‰æ¨¡æ€çš„è”åˆè®­ç»ƒï¼Œæ¨¡å‹èƒ½æ›´ç²¾å‡†æ•æ‰è·¨æ¨¡æ€ä¿¡æ¯ç‰¹å¾ï¼Œåœ¨æ–‡æœ¬ç†è§£ç”Ÿæˆã€å›¾åƒç†è§£å’Œè·¨æ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ä¸ºé¿å…æ¨¡æ€é—´ç›¸äº’å¹²æ‰°ï¼Œæˆ‘ä»¬åˆ›æ–°æ€§åœ°è®¾è®¡äº†å¼‚æ„MoEç»“æ„ï¼Œé‡‡ç”¨æ¨¡æ€éš”ç¦»è·¯ç”±æœºåˆ¶ï¼Œå¹¶ç»“åˆè·¯ç”±å™¨æ­£äº¤æŸå¤±ä¸å¤šæ¨¡æ€ä»¤ç‰Œå‡è¡¡æŸå¤±ï¼Œç¡®ä¿åŒæ¨¡æ€ç‰¹å¾çš„é«˜æ•ˆè¡¨å¾ä¸ååŒä¼˜åŒ–ã€‚\n\né«˜æ•ˆæ‰©å±•åŸºç¡€è®¾æ–½ï¼šæˆ‘ä»¬æå‡ºåˆ›æ–°çš„å¼‚æ„æ··åˆå¹¶è¡Œä¸åˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œç»“åˆèŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜ä¼˜åŒ–æµæ°´çº¿è°ƒåº¦ã€FP8æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡é¢„è®­ç»ƒååæ•ˆç‡ã€‚æ¨ç†é˜¶æ®µé‡‡ç”¨å¤šä¸“å®¶å¹¶è¡Œåä½œæ–¹æ³•ä¸å·ç§¯ç¼–ç é‡åŒ–ç®—æ³•ï¼Œå®ç°4æ¯”ç‰¹/2æ¯”ç‰¹æ— æŸé‡åŒ–ã€‚åŸºäºé£æ¡¨æ¡†æ¶çš„åŠ¨æ€è§’è‰²åˆ‡æ¢PDåˆ†ç‰‡æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†MoEæ¨¡å‹çš„æ¨ç†èµ„æºåˆ©ç”¨ç‡ï¼Œç¡®ä¿è·¨ç¡¬ä»¶å¹³å°çš„é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ä¸“é¡¹åè®­ç»ƒï¼šé’ˆå¯¹å®é™…åº”ç”¨åœºæ™¯çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨¡æ€ä¸“é¡¹å¾®è°ƒã€‚é€šç”¨è¯­è¨€æ¨¡å‹ä¸“æ³¨äºæ–‡æœ¬ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ä¼˜åŒ–ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åˆ™å¼ºåŒ–å›¾æ–‡ç†è§£èƒ½åŠ›å¹¶æ”¯æŒæ€ç»´é“¾ä¸éæ€ç»´é“¾åŒæ¨¡å¼ã€‚å„æ¨¡å‹é‡‡ç”¨ç›‘ç£å¾®è°ƒ(SFT)ã€*ç›´æ¥åå¥½ä¼˜åŒ–(DPO)æˆ–æ”¹è¿›å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç»Ÿä¸€åå¥½ä¼˜åŒ–(UPO)*è¿›è¡Œåè®­ç»ƒã€‚\n\nåœ¨è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒé˜¶æ®µï¼Œè§†è§‰ä¸è¯­è¨€çš„æ·±åº¦èåˆå¯¹æ¨¡å‹åœ¨ç†è§£ã€æ¨ç†ã€ç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡çš„è¡¨ç°å…·æœ‰å†³å®šæ€§å½±å“ã€‚ä¸ºå¢å¼ºæ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æ³›åŒ–é€‚åº”èƒ½åŠ›ï¼Œæˆ‘ä»¬å›´ç»•å›¾åƒç†è§£ã€ä»»åŠ¡ä¸“é¡¹å¾®è°ƒå’Œå¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†ä¸‰å¤§æ ¸å¿ƒèƒ½åŠ›ï¼Œç³»ç»ŸåŒ–æ„å»ºè®­ç»ƒæ•°æ®å¹¶ä¼˜åŒ–è®­ç»ƒç­–ç•¥ã€‚é€šè¿‡RLVRï¼ˆå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼‰æŠ€æœ¯è¿›ä¸€æ­¥ä¼˜åŒ–å¯¹é½æ•ˆæœï¼Œæœ€ç»ˆè·å¾—ERNIE-4.5-VL-424B-A47Bæ¨¡å‹ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-VL-424B-A47Bæ˜¯åŸºäºERNIE-4.5-VL-424B-A47B-Baseçš„å¤šæ¨¡æ€MoEå¯¹è¯æ¨¡å‹ï¼Œæ€»å‚æ•°é‡424Bï¼Œå•tokenæ¿€æ´»å‚æ•°é‡47Bã€‚å…·ä½“é…ç½®å¦‚ä¸‹ï¼š\n\nå…³é”®å‚æ•°\tå€¼\næ¨¡æ€\tæ–‡æœ¬ & è§†è§‰\nè®­ç»ƒé˜¶æ®µ\tåè®­ç»ƒ\nå‚æ•°é‡(æ€»/æ¿€æ´»)\t424B / 47B\nå±‚æ•°\t54\næ³¨æ„åŠ›å¤´æ•°(Q/KV)\t64 / 8\næ–‡æœ¬ä¸“å®¶(æ€»/æ¿€æ´»)\t64 / 8\nè§†è§‰ä¸“å®¶(æ€»/æ¿€æ´»)\t64 / 8\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå¼€å§‹\nFastDeployæ¨ç†\n\nä½¿ç”¨FastDeployå¿«é€Ÿéƒ¨ç½²æœåŠ¡å¦‚ä¸‹ï¼Œæ›´è¯¦ç»†ç”¨æ³•è¯·å‚è€ƒFastDeploy GitHubä»“åº“ã€‚\n\næ³¨æ„ï¼šéœ€è¦80GBæ˜¾å­˜GPU x 8ã€‚--quantizationå‚æ•°æ”¯æŒæŒ‡å®šwint4æˆ–wint8åˆ†åˆ«è¿›è¡Œ4æ¯”ç‰¹/8æ¯”ç‰¹é‡åŒ–éƒ¨ç½²ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-VL-424B-A47B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --tensor-parallel-size 8 \\\n       --quantization wint4 \\\n       --max-model-len 32768 \\\n       --enable-mm \\\n       --reasoning-parser ernie-45-vl \\\n       --max-num-seqs 32\n\n\nERNIE-4.5-VL æ¨¡å‹æ”¯æŒé€šè¿‡è¯·æ±‚å‚æ•°å¼€å¯æˆ–å…³é—­æ€è€ƒæ¨¡å¼ã€‚\n\nå¯ç”¨æ€è€ƒæ¨¡å¼\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": [\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://paddlenlp.bj.bcebos.com/datasets/paddlemix/demo_images/example2.jpg\"}},\n      {\"type\": \"text\", \"text\": \"Descript this image\"}\n    ]}\n  ],\n  \"metadata\": {\"enable_thinking\": true}\n}'\n\nå…³é—­æ€è€ƒæ¨¡å¼\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": [\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://paddlenlp.bj.bcebos.com/datasets/paddlemix/demo_images/example2.jpg\"}},\n      {\"type\": \"text\", \"text\": \"Descript this image\"}\n    ]}\n  ],\n  \"metadata\": {\"enable_thinking\": false}\n}'\n\nvLLM æ¨ç†æ”¯æŒ\n\næˆ‘ä»¬æ­£ä¸ç¤¾åŒºç´§å¯†åˆä½œï¼Œå…¨åŠ›å®ç°å¯¹ ERNIE4.5 æ¨¡å‹çš„å®Œæ•´æ”¯æŒï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nè®¸å¯åè®®\n\nERNIE 4.5 æ¨¡å‹åŸºäº Apache License 2.0 æä¾›ã€‚è¯¥è®¸å¯è¯å…è®¸å•†ä¸šç”¨é€”ï¼Œä½†éœ€éµå®ˆå…¶æ¡æ¬¾ä¸æ¡ä»¶ã€‚ç‰ˆæƒæ‰€æœ‰ (c) 2025 ç™¾åº¦å…¬å¸ï¼Œä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨è¯´æ˜\n\nå¦‚æœæ‚¨å‘ç° ERNIE 4.5 å¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¸®åŠ©ï¼Œæˆ–å¸Œæœ›åœ¨é¡¹ç›®ä¸­ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Image-Text-to-Text\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-PT",
    "project_name": "ERNIE-4.5-300B-A47B-PT",
    "readme": "Original Text\n   \nERNIE-4.5-300B-A47B\nERNIE 4.5 äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„å…ˆè¿›èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—ï¼Œå¾—ç›Šäºä»¥ä¸‹å…³é”®æŠ€æœ¯é©æ–°ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®å¼‚ï¼Œå¹¶æé«˜æ¶‰åŠæ–‡æœ¬ç†è§£ä¸ç”Ÿæˆã€å›¾åƒç†è§£ä»¥åŠè·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡æ€§èƒ½ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œä¸è®©ä¸€ä¸ªæ¨¡æ€é˜»ç¢å¦ä¸€ä¸ªæ¨¡æ€çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº† å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥äº† æ¨¡æ€éš”ç¦»è·¯ç”±ï¼Œå¹¶é‡‡ç”¨äº† è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€ Token å¹³è¡¡æŸå¤±ã€‚è¿™äº›æ¶æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€çš„æœ‰æ•ˆè¡¨ç¤ºï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°ç›¸äº’å¼ºåŒ–ã€‚\n\nç¼©æ”¾æ•ˆç‡åŒ–åŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬ä¸º ERNIE 4.5 æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒæå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼‚æ„æ··åˆå¹¶è¡Œæ€§å’Œåˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œæ€§ã€å†…å­˜é«˜æ•ˆçš„ç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†å“è¶Šçš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•å’Œ å·ç§¯ä»£ç é‡åŒ– ç®—æ³•ï¼Œå®ç°äº† 4 ä½/2 ä½æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸¦æœ‰åŠ¨æ€è§’è‰²åˆ‡æ¢çš„ PD è§£è€¦ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œæå‡ ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚åŸºäº PaddlePaddleï¼ŒERNIE 4.5 åœ¨å¤šç§ç¡¬ä»¶å¹³å°ä¸Šæä¾›é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºäº†æ»¡è¶³ç°å®ä¸–ç•Œåº”ç”¨çš„å„ç§éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒæ¨¡æ€å˜ä½“è¿›è¡Œäº†å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLMs ä¼˜åŒ–äº†é€šç”¨ç›®çš„çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆï¼Œè€Œ VLMs åˆ™ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ç§æ¨¡å‹éƒ½é‡‡ç”¨äº† ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ æˆ–ä¸€ç§åä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰ çš„ä¿®æ”¹åå¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œåè®­ç»ƒã€‚\n\næ¨¡å‹æ¦‚è¿°\n\nERNIE-4.5-300B-A47B æ˜¯ä¸€ä¸ªæ–‡æœ¬ MoE åè®­ç»ƒæ¨¡å‹ï¼Œæ€»å‚æ•°é‡ä¸º 300Bï¼Œæ¯ä¸ª Token æ¿€æ´»çš„å‚æ•°é‡ä¸º 47Bã€‚ä»¥ä¸‹ä¸ºæ¨¡å‹çš„é…ç½®ç»†èŠ‚ï¼š\n\nå…³é”®å­—\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°é‡ï¼ˆæ€»/æ¿€æ´»ï¼‰\t300B / 47B\nå±‚æ•°\t54\nå¤´æ•°ï¼ˆQ/KVï¼‰\t64 / 8\næ–‡æœ¬ä¸“å®¶æ•°ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 8\nè§†è§‰ä¸“å®¶æ•°ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 8\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ ERNIEKit è¿›è¡Œæ¨¡å‹å¾®è°ƒ\n\nERNIEKit æ˜¯åŸºäº PaddlePaddle çš„è®­ç»ƒå·¥å…·åŒ…ï¼Œä¸“é—¨ä¸º ERNIE ç³»åˆ—å¼€æºå¤§å‹æ¨¡å‹è®¾è®¡ã€‚å®ƒä¸ºæŒ‡ä»¤å¾®è°ƒï¼ˆSFTã€LoRAï¼‰å’Œæ’åˆ—è®­ç»ƒï¼ˆDPOï¼‰ç­‰åœºæ™¯æä¾›å…¨é¢æ”¯æŒï¼Œç¡®ä¿æœ€ä¼˜æ€§èƒ½ã€‚\n\nä½¿ç”¨ç¤ºä¾‹ï¼š\n\n# Download model\nhuggingface-cli download baidu/ERNIE-4.5-300B-A47B-Paddle --local-dir baidu/ERNIE-4.5-300B-A47B-Paddle\n# SFT\nerniekit train examples/configs/ERNIE-4.5-300B-A47B/sft/run_sft_wint8mix_lora_8k.yaml\n# DPO\nerniekit train examples/configs/ERNIE-4.5-300B-A47B/dpo/run_dpo_wint8mix_lora_8k.yaml\n\n\nä»¥ä¸‹æ˜¯æ›´ä¸ºè¯¦ç»†çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬å¸¦æœ‰ LoRA çš„ SFTã€å¤š GPU é…ç½®ä»¥åŠé«˜çº§è„šæœ¬ï¼Œè¯·å‚è€ƒ ERNIEKit å­˜å‚¨åº“ä¸­çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ã€‚\n\nä½¿ç”¨ FastDeploy\n\né€šè¿‡ä»¥ä¸‹å‘½ä»¤ä½¿ç”¨ FastDeploy å¯ä»¥å¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚å…³äºæ›´è¯¦ç»†çš„ç”¨æ³•è¯´æ˜ï¼Œè¯·å‚é˜… FastDeploy å­˜å‚¨åº“ã€‚\n\næ³¨æ„ï¼šè‹¥è¦åœ¨é…å¤‡è‡³å°‘ 80G å†…å­˜ã€å…± 4 å— GPU çš„é…ç½®ä¸Šè¿›è¡Œéƒ¨ç½²ï¼Œè¯·æŒ‡å®š --quantization wint4ã€‚å¦‚æœæ‚¨æŒ‡å®š --quantization wint8ï¼Œåˆ™éœ€è¦ 8 å— GPU çš„èµ„æºã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-300B-A47B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --quantization wint4 \\\n       --tensor-parallel-size 8 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\n\nè¦ä½¿ç”¨ FastDeploy éƒ¨ç½² W4A8C8 é‡åŒ–ç‰ˆæœ¬ï¼Œæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-300B-A47B-W4A8C8-TP4-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --tensor-parallel-size 4 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\n\nè¦åœ¨å•å—141G GPUä¸Šä½¿ç”¨FastDeployéƒ¨ç½²WINT2é‡åŒ–ç‰ˆæœ¬ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model \"baidu/ERNIE-4.5-300B-A47B-2Bits-Paddle\" \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --tensor-parallel-size 1 \\\n       --max-model-len  32768 \\\n       --max-num-seqs 128\n\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ ERNIE-4.5-300B-A47B-FP8 æ ¹æ®ç»™å®šè¾“å…¥ç”Ÿæˆå†…å®¹ã€‚\n\nfrom fastdeploy import LLM, SamplingParams\n\nprompts = [\n    \"Hello, my name is\",\n]\n\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=128)\n\nmodel = \"baidu/ERNIE-4.5-300B-A47B-FP8-Paddle\"\nllm = LLM(model=model, tensor_parallel_size=8, max_model_len=8192, num_gpu_blocks_override=1024, engine_worker_queue_port=9981)\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs.text\n    print(\"generated_text\", generated_text)\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•æ ¹æ®ç»™å®šçš„è¾“å…¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-300B-A47B-PT\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n\n# decode the generated ids\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\nprint(\"generate_text:\", generate_text)\n\nä½¿ç”¨ vLLM\n\nvLLM ç›®å‰æ­£åœ¨é€‚é…ä¸­ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨æˆ‘ä»¬åˆ†å‰çš„ä»“åº“ vllmã€‚æˆ‘ä»¬æ­£åœ¨ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢æ”¯æŒ ERNIE4.5 æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\n# 80G * 16 GPU\nvllm serve baidu/ERNIE-4.5-300B-A47B-PT --trust-remote-code\n\n# FP8 online quantification 80G * 8 GPU\nvllm serve baidu/ERNIE-4.5-300B-A47B-PT --trust-remote-code --quantization fp8\n\næœ€ä½³å®è·µ\né‡‡æ ·å‚æ•°\n\nä¸ºäº†è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ Temperature=0.8ï¼ŒTopP=0.8ã€‚\n\nç½‘ç»œæœç´¢æç¤º\n\nå¯¹äºç½‘ç»œæœç´¢ï¼Œ{references}ã€{date} å’Œ {question} æ˜¯å‚æ•°ã€‚\n\nå¯¹äºä¸­æ–‡é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æç¤ºï¼š\n\nernie_search_zh_prompt = \\\n'''ä¸‹é¢ä½ ä¼šæ”¶åˆ°å½“å‰æ—¶é—´ã€å¤šä¸ªä¸åŒæ¥æºçš„å‚è€ƒæ–‡ç« å’Œä¸€æ®µå¯¹è¯ã€‚ä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»å¤šä¸ªå‚è€ƒæ–‡ç« ï¼Œå¹¶æ ¹æ®å‚è€ƒæ–‡ç« ä¸­çš„ä¿¡æ¯å›ç­”å¯¹è¯ä¸­çš„é—®é¢˜ã€‚\nä»¥ä¸‹æ˜¯å½“å‰æ—¶é—´å’Œå‚è€ƒæ–‡ç« ï¼š\n---------\n#å½“å‰æ—¶é—´\n{date}\n\n#å‚è€ƒæ–‡ç« \n{references}\n\n---------\nè¯·æ³¨æ„ï¼š\n1. å›ç­”å¿…é¡»ç»“åˆé—®é¢˜éœ€æ±‚å’Œå½“å‰æ—¶é—´ï¼Œå¯¹å‚è€ƒæ–‡ç« çš„å¯ç”¨æ€§è¿›è¡Œåˆ¤æ–­ï¼Œé¿å…åœ¨å›ç­”ä¸­ä½¿ç”¨é”™è¯¯æˆ–è¿‡æ—¶çš„ä¿¡æ¯ã€‚\n2. å½“å‚è€ƒæ–‡ç« ä¸­çš„ä¿¡æ¯æ— æ³•å‡†ç¡®åœ°å›ç­”é—®é¢˜æ—¶ï¼Œä½ éœ€è¦åœ¨å›ç­”ä¸­æä¾›è·å–ç›¸åº”ä¿¡æ¯çš„å»ºè®®ï¼Œæˆ–æ‰¿è®¤æ— æ³•æä¾›ç›¸åº”ä¿¡æ¯ã€‚\n3. ä½ éœ€è¦ä¼˜å…ˆæ ¹æ®ç™¾ç§‘ã€å®˜ç½‘ã€æƒå¨æœºæ„ã€ä¸“ä¸šç½‘ç«™ç­‰é«˜æƒå¨æ€§æ¥æºçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚\n4. å›å¤éœ€è¦ç»¼åˆå‚è€ƒæ–‡ç« ä¸­çš„ç›¸å…³æ•°å­—ã€æ¡ˆä¾‹ã€æ³•å¾‹æ¡æ–‡ã€å…¬å¼ç­‰ä¿¡æ¯ï¼Œä½¿ä½ çš„ç­”æ¡ˆæ›´ä¸“ä¸šã€‚\n5. å½“é—®é¢˜å±äºåˆ›ä½œç±»ä»»åŠ¡æ—¶ï¼Œéœ€æ³¨æ„ä»¥ä¸‹ç»´åº¦ï¼š\n   - æ€åº¦é²œæ˜ï¼šè§‚ç‚¹ã€ç«‹åœºæ¸…æ™°æ˜ç¡®ï¼Œé¿å…æ¨¡æ£±ä¸¤å¯ï¼Œè¯­è¨€æœæ–­ç›´æ¥\n   - æ–‡é‡‡é£æ‰¬ï¼šç”¨è¯ç²¾å‡†ç”ŸåŠ¨ï¼Œå–„ç”¨ä¿®è¾æ‰‹æ³•ï¼Œå¢å¼ºæ„ŸæŸ“åŠ›\n   - æœ‰ç†æœ‰æ®ï¼šé€»è¾‘ä¸¥å¯†é€’è¿›ï¼Œç»“åˆæƒå¨æ•°æ®/äº‹å®æ”¯æ’‘è®ºç‚¹\n---------\nä¸‹é¢è¯·ç»“åˆä»¥ä¸Šä¿¡æ¯ï¼Œå›ç­”é—®é¢˜ï¼Œè¡¥å…¨å¯¹è¯\n{question}'''\n\n\nå¯¹äºè‹±æ–‡é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨å¦‚ä¸‹æç¤ºè¯­ï¼š\n\nernie_search_en_prompt = \\\n'''\nBelow you will be given the current time, multiple references from different sources, and a conversation. Your task is to read the references and use the information in them to answer the question in the conversation.\nHere are the current time and the references:\n---------\n#Current Time\n{date}\n\n#References\n{references}\n\n---------\nPlease note:\n1. Based on the questionâ€™s requirements and the current time, assess the usefulness of the references to avoid using inaccurate or outdated information in the answer.  \n2. If the references do not provide enough information to accurately answer the question, you should suggest how to obtain the relevant information or acknowledge that you are unable to provide it.  \n3. Prioritize using information from highly authoritative sources such as encyclopedias, official websites, authoritative institutions, and professional websites when answering questions.\n4. Incorporate relevant numbers, cases, legal provisions, formulas, and other details from the references to make your answer more professional.\n5. For creative tasks, keep these dimensions in mind:\n   - Clear attitude: Clear views and positions, avoid ambiguity, and use decisive and direct language\n   - Brilliant writing: Precise and vivid words, good use of rhetoric, and enhance the appeal\n   - Well-reasoned: Rigorous logic and progressive, combined with authoritative data/facts to support the argument\n\n---------\nNow, using the information above, answer the question and complete the conversation:  \n{question}'''\n\n\nå‚æ•°è¯´æ˜ï¼š\n\n{question} æ˜¯ç”¨æˆ·æå‡ºçš„é—®é¢˜\n{date} æ˜¯å½“å‰æ—¶é—´ï¼Œå»ºè®®æ ¼å¼ä¸ºâ€œYYYY-MM-DD HH:MM:SSï¼Œæ˜ŸæœŸï¼ŒåŒ—äº¬/ä¸­å›½ã€‚â€\n{references} æ˜¯å‚è€ƒæ–‡çŒ®ï¼Œå»ºè®®æ ¼å¼ä¸ºï¼š\n##å‚è€ƒæ–‡ç« 1\næ ‡é¢˜ï¼šå‘¨æ°ä¼¦\næ–‡ç« å‘å¸ƒæ—¶é—´ï¼š2025-04-20\nå†…å®¹ï¼šå‘¨æ°ä¼¦(Jay Chou),1979å¹´1æœˆ18æ—¥å‡ºç”Ÿäºå°æ¹¾çœæ–°åŒ—å¸‚,ç¥–ç±ç¦å»ºçœæ°¸æ˜¥å¿,åè¯­æµè¡Œä¹ç”·æ­Œæ‰‹ã€éŸ³ä¹äººã€æ¼”å‘˜ã€å¯¼æ¼”ã€ç¼–å‰§,æ¯•ä¸šäºæ·¡æ±Ÿä¸­å­¦ã€‚2000å¹´,å‘è¡Œä¸ªäººé¦–å¼ éŸ³ä¹ä¸“è¾‘ã€ŠJayã€‹ã€‚...\næ¥æºç½‘ç«™ç½‘å€ï¼šbaike.baidu.com\næ¥æºç½‘ç«™çš„ç½‘ç«™åï¼šç™¾åº¦ç™¾ç§‘\n\n##å‚è€ƒæ–‡ç« 2\n...\n\nè®¸å¯åè®®\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache è®¸å¯åè®® 2.0 æä¾›ä½¿ç”¨ã€‚è¯¥è®¸å¯åè®®å…è®¸åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶çš„å‰æä¸‹è¿›è¡Œå•†ä¸šç”¨é€”ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨è®¤ä¸º ERNIE 4.5 æœ‰ç”¨ï¼Œæˆ–å¸Œæœ›åœ¨æ‚¨çš„é¡¹ç›®ä¸­ä½¿ç”¨å®ƒï¼Œè¯·å‹å¥½åœ°å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-Base-PT",
    "project_name": "ERNIE-4.5-300B-A47B-Base-PT",
    "readme": "Original Text\n   \nERNIE-4.5-300B-A47B-åŸºç¡€ç‰ˆ\n\n[!æ³¨æ„] æ³¨ï¼š\"-Paddle\"æ¨¡å‹ä½¿ç”¨PaddlePaddleæƒé‡ï¼Œè€Œ\"-PT\"æ¨¡å‹é‡‡ç”¨Transformeré£æ ¼çš„PyTorchæƒé‡ã€‚\n\n[!æ³¨æ„] æ³¨ï¼šåŸºç¡€ç‰ˆæ¨¡å‹ä»…æ”¯æŒæ–‡æœ¬è¡¥å…¨åŠŸèƒ½ã€‚è¯„ä¼°æ—¶è¯·ä½¿ç”¨vLLM/FastDeployä¸­çš„completionæ¥å£ï¼ˆéchat_completionï¼‰ã€‚\n\nERNIE 4.5 æ ¸å¿ƒäº®ç‚¹\n\nERNIE 4.5ç³»åˆ—æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯åŸºäºMoEæ¶æ„çš„A47Bå’ŒA3Bç‰ˆæœ¬ï¼‰çš„å“è¶Šèƒ½åŠ›æºäºå¤šé¡¹å…³é”®æŠ€æœ¯çªç ´ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„MoEé¢„è®­ç»ƒï¼šé€šè¿‡æ–‡æœ¬ä¸è§†è§‰æ¨¡æ€çš„è”åˆè®­ç»ƒï¼Œæ¨¡å‹èƒ½æ›´ç²¾å‡†æ•æ‰è·¨æ¨¡æ€ä¿¡æ¯ç‰¹å¾ï¼Œæ˜¾è‘—æå‡æ–‡æœ¬ç†è§£ç”Ÿæˆã€å›¾åƒç†è§£åŠè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ä¸ºé¿å…æ¨¡æ€é—´ç›¸äº’å¹²æ‰°ï¼Œæˆ‘ä»¬åˆ›æ–°è®¾è®¡äº†å¼‚æ„MoEç»“æ„ï¼Œé‡‡ç”¨æ¨¡æ€éš”ç¦»è·¯ç”±æœºåˆ¶ï¼Œå¹¶å¼•å…¥è·¯ç”±å™¨æ­£äº¤æŸå¤±ä¸å¤šæ¨¡æ€ä»¤ç‰Œå‡è¡¡æŸå¤±ï¼Œç¡®ä¿åŒæ¨¡æ€è¡¨å¾æ—¢ç‹¬ç«‹åˆäº’è¡¥ï¼Œå®ç°ååŒä¼˜åŒ–ã€‚\n\né«˜æ•ˆæ‰©å±•åŸºç¡€è®¾æ–½ï¼šæˆ‘ä»¬æå‡ºå¼‚æ„æ··åˆå¹¶è¡Œä¸åˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œç»“åˆèŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜ä¼˜åŒ–æµæ°´çº¿è°ƒåº¦ã€FP8æ··åˆç²¾åº¦è®­ç»ƒåŠç»†ç²’åº¦é‡è®¡ç®—æŠ€æœ¯ï¼Œå¤§å¹…æå‡é¢„è®­ç»ƒååé‡ã€‚æ¨ç†é˜¶æ®µé‡‡ç”¨å¤šä¸“å®¶å¹¶è¡Œåä½œæ–¹æ¡ˆä¸å·ç§¯ç¼–ç é‡åŒ–ç®—æ³•ï¼Œå®ç°4æ¯”ç‰¹/2æ¯”ç‰¹æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡PDè§£è€¦æ¶æ„ä¸åŠ¨æ€è§’è‰²åˆ‡æ¢æœºåˆ¶ï¼Œæ˜¾è‘—æå‡MoEæ¨¡å‹æ¨ç†æ•ˆç‡ã€‚åŸºäºPaddlePaddleæ¡†æ¶ï¼ŒERNIE 4.5å¯åœ¨å¤šå…ƒç¡¬ä»¶å¹³å°ä¸Šå®ç°é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ä¸“é¡¹åè®­ç»ƒï¼šé’ˆå¯¹å®é™…åº”ç”¨åœºæ™¯éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨¡æ€ä¸“é¡¹å¾®è°ƒã€‚å…¶ä¸­é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸“æ³¨æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼ºåŒ–å›¾æ–‡äº¤äº’èƒ½åŠ›å¹¶æ”¯æŒæ€ç»´/éæ€ç»´åŒæ¨¡å¼ã€‚å„æ¨¡å‹é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€*ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æˆ–åˆ›æ–°è®¾è®¡çš„ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰*å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œåè®­ç»ƒã€‚\n\nä¸ºç¡®ä¿å¤šæ¨¡æ€è”åˆè®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚å‰ä¸¤é˜¶æ®µä»…è®­ç»ƒæ–‡æœ¬ç›¸å…³å‚æ•°ï¼Œæ„å»ºå¼ºå¤§çš„åŸºç¡€è¯­è¨€ç†è§£ä¸é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚æœ€ç»ˆå¤šæ¨¡æ€é˜¶æ®µé€šè¿‡å¼•å…¥ViTå›¾åƒç‰¹å¾æå–å™¨ã€ç‰¹å¾è½¬æ¢é€‚é…å™¨åŠè§†è§‰ä¸“å®¶æ¨¡å—ï¼Œæ‰©å±•å›¾åƒ/è§†é¢‘ç†è§£èƒ½åŠ›ã€‚æ­¤é˜¶æ®µæ–‡æœ¬ä¸è§†è§‰æ¨¡æ€ç›¸äº’å¢å¼ºã€‚åœ¨å®Œæˆæ•°ä¸‡äº¿tokenè®­ç»ƒåï¼Œæˆ‘ä»¬æå–æ–‡æœ¬ç›¸å…³å‚æ•°æœ€ç»ˆè·å¾—ERNIE-4.5-300B-A47B-åŸºç¡€ç‰ˆã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-300B-A47B-åŸºç¡€ç‰ˆæ˜¯çº¯æ–‡æœ¬MoEåŸºç¡€æ¨¡å‹ï¼Œæ€»å‚æ•°é‡300Bï¼Œå•tokenæ¿€æ´»å‚æ•°é‡47Bã€‚å…·ä½“é…ç½®å¦‚ä¸‹ï¼š\n\nå…³é”®é¡¹\tå‚æ•°å€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°é‡(æ€»/æ¿€æ´»)\t300B / 47B\nå±‚æ•°\t54\næ³¨æ„åŠ›å¤´æ•°(Q/KV)\t64 / 8\næ–‡æœ¬ä¸“å®¶æ•°(æ€»/æ¿€æ´»)\t64 / 8\nè§†è§‰ä¸“å®¶æ•°(æ€»/æ¿€æ´»)\t64 / 8\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå¼€å§‹\nä½¿ç”¨transformersåº“\n\næ³¨æ„ï¼šä½¿ç”¨å‰è¯·ç¡®ä¿å·²å®‰è£…transformersåº“ï¼ˆç‰ˆæœ¬éœ€â‰¥4.50.0ï¼‰\n\nä»¥ä¸‹ä»£ç ç¤ºä¾‹å±•ç¤ºå¦‚ä½•åŸºäºç»™å®šè¾“å…¥ç”Ÿæˆæ–‡æœ¬å†…å®¹ï¼š\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-300B-A47B-Base-PT\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\nprompt = \"Large language model is\"\nmodel_inputs = tokenizer([prompt], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\nresult = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\nprint(\"result:\", result)\n\nä½¿ç”¨ vLLM\n\nvllm GitHub åº“ã€‚çº¯ Python æ„å»ºã€‚\n\n# 80G * 16 GPU\nvllm serve baidu/ERNIE-4.5-300B-A47B-Base-PT --trust-remote-code\n\n# FP8 online quantification 80G * 16 GPU\nvllm serve baidu/ERNIE-4.5-300B-A47B-Base-PT --trust-remote-code --quantization fp8\n\nè®¸å¯åè®®\n\nERNIE 4.5 æ¨¡å‹åŸºäº Apache 2.0 è®¸å¯è¯æä¾›ã€‚è¯¥è®¸å¯è¯å…è®¸å•†ä¸šç”¨é€”ï¼Œä½†éœ€éµå®ˆå…¶æ¡æ¬¾ä¸æ¡ä»¶ã€‚ç‰ˆæƒæ‰€æœ‰ (c) 2025 ç™¾åº¦å…¬å¸ï¼Œä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨è¯´æ˜\n\nè‹¥æ‚¨è®¤ä¸º ERNIE 4.5 å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©æˆ–å¸Œæœ›åœ¨é¡¹ç›®ä¸­ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\", \"ERNIE4.5\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-0.3B-Paddle",
    "project_name": "ERNIE-4.5-0.3B-Paddle",
    "readme": "Original Text\n   \nERNIE-4.5-0.3B\nERNIE 4.5 æŠ€æœ¯äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—ï¼Œå…¶å…ˆè¿›èƒ½åŠ›å¾—ç›Šäºä»¥ä¸‹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰ä¸¤ç§æ¨¡æ€ä¸Šè”åˆè®­ç»ƒï¼Œæ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶æé«˜åœ¨æ¶‰åŠæ–‡æœ¬ç†è§£ä¸ç”Ÿæˆã€å›¾åƒç†è§£ä»¥åŠè·¨æ¨¡æ€æ¨ç†ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼ŒåŒæ—¶é¿å…ä¸€ç§æ¨¡æ€é˜»ç¢å¦ä¸€ç§æ¨¡æ€çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº† å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥ æ¨¡æ€éš”ç¦»è·¯ç”±ï¼Œå¹¶é‡‡ç”¨ è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€ Token å¹³è¡¡æŸå¤±ã€‚è¿™äº›æ¶æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€çš„æœ‰æ•ˆè¡¨ç¤ºï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°ç›¸äº’å¼ºåŒ–ã€‚\n\næ‰©å±•æ•ˆç‡åŒ–åŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬ä¸º ERNIE 4.5 æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒæå‡ºäº†æ–°é¢–çš„å¼‚æ„æ··åˆå¹¶è¡Œæ€§å’Œåˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œæ€§ã€å†…å­˜é«˜æ•ˆçš„ç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒä»¥åŠç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†å“è¶Šçš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•å’Œ å·ç§¯ç é‡åŒ– ç®—æ³•ï¼Œå®ç°äº† 4 ä½/2 ä½æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºåŠ¨æ€è§’è‰²åˆ‡æ¢çš„ PD è§£èšï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œå¢å¼º ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚åŸºäº PaddlePaddleï¼ŒERNIE 4.5 åœ¨å¤šç§ç¡¬ä»¶å¹³å°ä¸Šæä¾›é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºæ»¡è¶³å®é™…åº”ç”¨çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒæ¨¡æ€å˜ä½“è¿›è¡Œäº†å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLMs ä¼˜åŒ–äº†é€šç”¨ç›®çš„çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆï¼Œè€Œ VLMs åˆ™ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹åœ¨åç»­è®­ç»ƒä¸­éƒ½é‡‡ç”¨äº† ç›‘ç£å¾®è°ƒ (SFT)ã€ç›´æ¥åå¥½ä¼˜åŒ– (DPO) æˆ–ä¸€ç§åä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ– (UPO) çš„ä¿®æ”¹åçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-0.3B æ˜¯ä¸€ä¸ªæ–‡æœ¬å¯†é›†å‹åè®­ç»ƒæ¨¡å‹ã€‚ä»¥ä¸‹ä¸ºæ¨¡å‹çš„é…ç½®è¯¦æƒ…ï¼š\n\nå…³é”®å­—\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\tåè®­ç»ƒ\nå‚æ•°é‡\t0.36B\nå±‚æ•°\t18\nå¤´æ•°(Q/KV)\t16 / 2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ ERNIEKit è¿›è¡Œæ¨¡å‹å¾®è°ƒ\n\nERNIEKit æ˜¯åŸºäº PaddlePaddle å¼€å‘çš„ä¸€æ¬¾è®­ç»ƒå·¥å…·åŒ…ï¼Œä¸“é—¨ä¸º ERNIE ç³»åˆ—å¼€æºå¤§å‹æ¨¡å‹è®¾è®¡ã€‚å®ƒä¸ºæŒ‡ä»¤å¾®è°ƒï¼ˆSFTã€LoRAï¼‰å’Œ alignment è®­ç»ƒï¼ˆDPOï¼‰ç­‰åœºæ™¯æä¾›äº†å…¨é¢çš„æ”¯æŒï¼Œç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚\n\nä½¿ç”¨ç¤ºä¾‹ï¼š\n\n# Download Model\nhuggingface-cli download baidu/ERNIE-4.5-0.3B-Paddle --local-dir baidu/ERNIE-4.5-0.3B-Paddle\n# SFT\nerniekit train examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml\n# DPO\nerniekit train examples/configs/ERNIE-4.5-0.3B/dpo/run_dpo_8k.yaml\n\n\nä»¥ä¸‹æ˜¯æ‚¨æ‰€éœ€çš„å†…å®¹ï¼š\n\næ›´å¤šè¯¦ç»†çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬å¸¦æœ‰LoRAçš„SFTã€å¤šGPUé…ç½®ä»¥åŠé«˜çº§è„šæœ¬ï¼Œè¯·å‚è€ƒ ERNIEKit ä»“åº“ä¸­çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ã€‚\n\nFastDeploy æ¨ç†\n\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼Œé€šè¿‡FastDeployå¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚æ›´å¤šè¯¦ç»†çš„ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚è€ƒ FastDeploy ä»“åº“ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-0.3B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•æ ¹æ®ç»™å®šçš„è¾“å…¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-0.3B-PT\"\nmodel_name = \"baidu/ERNIE-4.5-0.3B-PT\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n\n# decode the generated ids\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\nprint(\"generate_text:\", generate_text)\n\nvLLM æ¨æ–­\n\nvLLM æ­£åœ¨é€‚é…ä¸­ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨æˆ‘ä»¬åˆ†å‰çš„ä»£ç åº“ vllmã€‚æˆ‘ä»¬æ­£åœ¨ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢æ”¯æŒ ERNIE4.5 æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nvllm serve baidu/ERNIE-4.5-0.3B-PT --trust-remote-code\n\nè®¸å¯è¯\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache License 2.0 æä¾›æœåŠ¡ã€‚è¯¥è®¸å¯è¯åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶çš„å‰æä¸‹ï¼Œå…è®¸å•†ä¸šä½¿ç”¨ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨å‘ç° ERNIE 4.5 æœ‰ç”¨æˆ–å¸Œæœ›åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­ä½¿ç”¨ï¼Œæ•¬è¯·å‹å¥½åœ°å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-VL-28B-A3B-Base-Paddle",
    "project_name": "ERNIE-4.5-VL-28B-A3B-Base-Paddle",
    "readme": "Original Text\n   \næ–‡å¿ƒå¤§æ¨¡å‹4.5-å¤šæ¨¡æ€-28B-A3B-åŸºç¡€ç‰ˆ\næ–‡å¿ƒ4.5æ ¸å¿ƒæŠ€æœ¯äº®ç‚¹\n\næ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯åŸºäºæ··åˆä¸“å®¶ç³»ç»Ÿçš„A47Bå’ŒA3Bç‰ˆæœ¬ï¼‰çš„å“è¶Šèƒ½åŠ›æºäºä»¥ä¸‹å…³é”®æŠ€æœ¯çªç ´ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„æ··åˆä¸“å®¶é¢„è®­ç»ƒï¼šé€šè¿‡æ–‡æœ¬ä¸è§†è§‰æ¨¡æ€çš„è”åˆè®­ç»ƒï¼Œæ¨¡å‹èƒ½æ›´ç²¾å‡†æ•æ‰è·¨æ¨¡æ€ä¿¡æ¯ç‰¹å¾ï¼Œæ˜¾è‘—æå‡æ–‡æœ¬ç†è§£ç”Ÿæˆã€å›¾åƒè§£æåŠè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ä¸ºé¿å…æ¨¡æ€é—´å­¦ä¹ å¹²æ‰°ï¼Œæˆ‘ä»¬åˆ›æ–°è®¾è®¡äº†å¼‚æ„MoEæ¶æ„ï¼Œé‡‡ç”¨æ¨¡æ€éš”ç¦»è·¯ç”±æœºåˆ¶ï¼Œå¹¶å¼•å…¥è·¯ç”±å™¨æ­£äº¤æŸå¤±ä¸å¤šæ¨¡æ€ä»¤ç‰Œå‡è¡¡æŸå¤±ï¼Œç¡®ä¿åŒæ¨¡æ€è¡¨å¾æ—¢ç‹¬ç«‹åˆäº’è¡¥ï¼Œå®ç°è®­ç»ƒè¿‡ç¨‹ä¸­çš„ååŒè¿›åŒ–ã€‚\n\né«˜æ•ˆå¯æ‰©å±•çš„åº•å±‚æ¶æ„ï¼šæˆ‘ä»¬æå‡ºå¼‚æ„æ··åˆå¹¶è¡Œä¸åˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œç»“åˆèŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜ä¼˜åŒ–æµæ°´çº¿è°ƒåº¦ã€FP8æ··åˆç²¾åº¦è®­ç»ƒåŠç»†ç²’åº¦é‡è®¡ç®—æŠ€æœ¯ï¼Œå¤§å¹…æå‡é¢„è®­ç»ƒååæ•ˆç‡ã€‚æ¨ç†é˜¶æ®µé‡‡ç”¨å¤šä¸“å®¶å¹¶è¡Œåä½œæ–¹æ³•ä¸å·ç§¯ç¼–ç é‡åŒ–ç®—æ³•ï¼Œå®ç°4æ¯”ç‰¹/2æ¯”ç‰¹æ— æŸé‡åŒ–ã€‚é€šè¿‡åŠ¨æ€è§’è‰²åˆ‡æ¢çš„PDè§£è€¦æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡MoEæ¨¡å‹æ¨ç†èµ„æºåˆ©ç”¨ç‡ã€‚åŸºäºé£æ¡¨æ¡†æ¶çš„æ–‡å¿ƒ4.5å¯åœ¨å¤šç§ç¡¬ä»¶å¹³å°ä¸Šå®ç°é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ä¸“é¡¹åè®­ç»ƒï¼šé’ˆå¯¹å®é™…åº”ç”¨åœºæ™¯çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨¡æ€ä¸“é¡¹å¾®è°ƒã€‚é€šç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸“æ³¨æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ä¼˜åŒ–ï¼›è§†è§‰è¯­è¨€æ¨¡å‹(VLM)å¼ºåŒ–å›¾æ–‡äº¤äº’ç†è§£ï¼Œæ”¯æŒæ€ç»´é“¾ä¸éæ€ç»´é“¾åŒæ¨¡å¼ã€‚å„æ¨¡å‹é‡‡ç”¨ç›‘ç£å¾®è°ƒ(SFT)ã€*ç›´æ¥åå¥½ä¼˜åŒ–(DPO)åŠæˆ‘ä»¬åˆ›æ–°çš„ç»Ÿä¸€åå¥½ä¼˜åŒ–(UPO)*å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œåè®­ç»ƒã€‚\n\nä¸ºç¡®ä¿å¤šæ¨¡æ€è”åˆè®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šå‰ä¸¤é˜¶æ®µä»…è®­ç»ƒæ–‡æœ¬ç›¸å…³å‚æ•°ï¼Œå¤¯å®åŸºç¡€è¯­è¨€ç†è§£ä¸é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼›æœ€ç»ˆé˜¶æ®µå¼•å…¥ViTå›¾åƒç‰¹å¾æå–å™¨ã€ç‰¹å¾è½¬æ¢é€‚é…å™¨åŠè§†è§‰ä¸“å®¶æ¨¡å—ï¼Œæ‰©å±•å›¾åƒè§†é¢‘ç†è§£èƒ½åŠ›ã€‚æ­¤é˜¶æ®µæ–‡æœ¬ä¸è§†è§‰æ¨¡æ€ç›¸äº’å¢å¼ºï¼Œç»è¿‡ä¸‡äº¿çº§tokenè®­ç»ƒåï¼Œæœ€ç»ˆäº§å‡ºæ–‡å¿ƒ4.5-å¤šæ¨¡æ€-28B-A3B-åŸºç¡€ç‰ˆæ¨¡å‹ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\næ–‡å¿ƒ4.5-å¤šæ¨¡æ€-28B-A3B-åŸºç¡€ç‰ˆæ˜¯å¤šæ¨¡æ€æ··åˆä¸“å®¶åŸºç¡€æ¨¡å‹ï¼Œæ€»å‚æ•°é‡280äº¿ï¼Œå•tokenæ¿€æ´»å‚æ•°é‡30äº¿ã€‚æ ¸å¿ƒé…ç½®å¦‚ä¸‹ï¼š\n\nå…³é”®å‚æ•°\tæ•°å€¼\næ¨¡æ€æ”¯æŒ\tæ–‡æœ¬ & è§†è§‰\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°é‡(æ€»/æ¿€æ´»)\t280äº¿/30äº¿\nå±‚æ•°\t28\næ³¨æ„åŠ›å¤´æ•°(Q/KV)\t20 / 4\næ–‡æœ¬ä¸“å®¶(æ€»/æ¿€æ´»)\t64 / 6\nè§†è§‰ä¸“å®¶(æ€»/æ¿€æ´»)\t64 / 6\nå…±äº«ä¸“å®¶\t2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå¼€å§‹\nvLLMæ¨ç†æ”¯æŒ\n\næˆ‘ä»¬æ­£ä¸å¼€æºç¤¾åŒºç´§å¯†åˆä½œå®Œå–„å¯¹æ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹çš„æ”¯æŒï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nè®¸å¯åè®®\n\næ–‡å¿ƒ4.5æ¨¡å‹åŸºäºApache 2.0è®¸å¯è¯å¼€æºï¼Œå…è®¸ç¬¦åˆè®¸å¯æ¡æ¬¾çš„å•†ä¸šä½¿ç”¨ã€‚ç‰ˆæƒæ‰€æœ‰ (c) 2025 ç™¾åº¦ï¼Œä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨å£°æ˜\n\nå¦‚æœæ‚¨åœ¨é¡¹ç›®ä¸­ä½¿ç”¨æ–‡å¿ƒ4.5æ¨¡å‹ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Image-Text-to-Text\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-VL-28B-A3B-Paddle",
    "project_name": "ERNIE-4.5-VL-28B-A3B-Paddle",
    "readme": "Original Text\n   \nERNIE-4.5-VL-28B-A3B\nERNIE 4.5 æ ¸å¿ƒäº®ç‚¹\n\nERNIE 4.5 ç³»åˆ—æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯åŸºäº MoE æ¶æ„çš„ A47B å’Œ A3B ç‰ˆæœ¬ï¼‰çš„å“è¶Šèƒ½åŠ›æºäºå¤šé¡¹å…³é”®æŠ€æœ¯çªç ´ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„æ··åˆä¸“å®¶é¢„è®­ç»ƒï¼šæ¨¡å‹é€šè¿‡è”åˆè®­ç»ƒæ–‡æœ¬ä¸è§†è§‰æ¨¡æ€ï¼Œæ·±åº¦æ•æ‰è·¨æ¨¡æ€ä¿¡æ¯å…³è”ï¼Œæ˜¾è‘—æå‡æ–‡æœ¬ç†è§£ç”Ÿæˆã€å›¾åƒè§£æåŠè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ä¸ºé¿å…æ¨¡æ€é—´å­¦ä¹ å¹²æ‰°ï¼Œæˆ‘ä»¬åˆ›æ–°è®¾è®¡äº†å¼‚æ„MoEç»“æ„ï¼Œé‡‡ç”¨æ¨¡æ€éš”ç¦»è·¯ç”±æœºåˆ¶ï¼Œå¹¶å¼•å…¥è·¯ç”±å™¨æ­£äº¤æŸå¤±ä¸å¤šæ¨¡æ€ä»¤ç‰Œå‡è¡¡æŸå¤±ï¼Œç¡®ä¿åŒæ¨¡æ€è¡¨å¾æ—¢ç‹¬ç«‹åˆäº’è¡¥ï¼Œå®ç°ååŒä¼˜åŒ–ã€‚\n\né«˜æ•ˆå¯æ‰©å±•çš„åŸºç¡€æ¶æ„ï¼šæˆ‘ä»¬æå‡ºå¼‚æ„æ··åˆå¹¶è¡Œä¸åˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œç»“åˆèŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜ä¼˜åŒ–æµæ°´çº¿è°ƒåº¦ã€FP8æ··åˆç²¾åº¦è®­ç»ƒåŠç»†ç²’åº¦é‡è®¡ç®—æŠ€æœ¯ï¼Œå¤§å¹…æå‡é¢„è®­ç»ƒååé‡ã€‚æ¨ç†é˜¶æ®µé‡‡ç”¨å¤šä¸“å®¶å¹¶è¡Œåä½œæ–¹æ³•ä¸å·ç§¯ç¼–ç é‡åŒ–ç®—æ³•ï¼Œå®ç°4æ¯”ç‰¹/2æ¯”ç‰¹æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŠ¨æ€è§’è‰²åˆ‡æ¢çš„PDè§£è€¦æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡MoEæ¨¡å‹æ¨ç†æ•ˆç‡ã€‚åŸºäºPaddlePaddleæ¡†æ¶ï¼ŒERNIE 4.5 å¯åœ¨å¤šæ ·åŒ–ç¡¬ä»¶å¹³å°ä¸Šå®ç°é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€å®šå‘å¾®è°ƒä¼˜åŒ–ï¼šä¸ºæ»¡è¶³å®é™…åœºæ™¯éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨¡æ€ä¸“é¡¹å¾®è°ƒã€‚é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸“æ³¨äºæ–‡æœ¬ç†è§£ä¸ç”Ÿæˆï¼›è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼ºåŒ–å›¾æ–‡äº¤äº’èƒ½åŠ›ï¼Œæ”¯æŒæ€ç»´é“¾ä¸éæ€ç»´é“¾åŒæ¨¡å¼ã€‚é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€*ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åŠæ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰*è¿›è¡Œæ¨¡å‹å¯¹é½ã€‚\n\nåœ¨è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒé˜¶æ®µï¼Œå›¾æ–‡æ·±åº¦èåˆå¯¹å¤æ‚ä»»åŠ¡ï¼ˆå¦‚ç†è§£ã€æ¨ç†ã€ç”Ÿæˆï¼‰çš„æ€§èƒ½èµ·å†³å®šæ€§ä½œç”¨ã€‚æˆ‘ä»¬å›´ç»•å›¾åƒç†è§£ã€ä»»åŠ¡é€‚é…å¾®è°ƒå’Œå¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†ä¸‰å¤§æ ¸å¿ƒèƒ½åŠ›ï¼Œç³»ç»Ÿæ„å»ºè®­ç»ƒæ•°æ®å¹¶ä¼˜åŒ–ç­–ç•¥ã€‚è¿›ä¸€æ­¥é€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æå‡å¯¹é½è´¨é‡ã€‚ç»SFTä¸RLé˜¶æ®µä¼˜åŒ–åï¼Œæœ€ç»ˆè·å¾—ERNIE-4.5-VL-28B-A3Bæ¨¡å‹ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-VL-28B-A3B æ˜¯ä¸€æ¬¾å¤šæ¨¡æ€æ··åˆä¸“å®¶å¯¹è¯æ¨¡å‹ï¼Œæ€»å‚æ•°é‡280äº¿ï¼Œå•tokenæ¿€æ´»å‚æ•°é‡30äº¿ã€‚å…³é”®é…ç½®å¦‚ä¸‹ï¼š\n\nå‚æ•°é¡¹\tå€¼\næ¨¡æ€æ”¯æŒ\tæ–‡æœ¬ & è§†è§‰\nè®­ç»ƒé˜¶æ®µ\tå¾®è°ƒé˜¶æ®µ\nå‚æ•°é‡ï¼ˆæ€»é‡/æ¿€æ´»é‡ï¼‰\t280äº¿/30äº¿\nç½‘ç»œå±‚æ•°\t28\næ³¨æ„åŠ›å¤´æ•°ï¼ˆQ/KVï¼‰\t20 / 4\næ–‡æœ¬ä¸“å®¶æ•°ï¼ˆæ€»é‡/æ¿€æ´»é‡ï¼‰\t64 / 6\nè§†è§‰ä¸“å®¶æ•°ï¼ˆæ€»é‡/æ¿€æ´»é‡ï¼‰\t64 / 6\nå…±äº«ä¸“å®¶æ•°\t2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå¼€å§‹\nFastDeploy æ¨ç†éƒ¨ç½²\n\nä½¿ç”¨FastDeployå¿«é€Ÿéƒ¨ç½²æœåŠ¡ï¼Œç¤ºä¾‹å¦‚ä¸‹ã€‚å®Œæ•´ç”¨æ³•è¯·å‚è€ƒFastDeploy GitHubä»“åº“ã€‚\n\næ³¨æ„ï¼šå•å¡éƒ¨ç½²éœ€è‡³å°‘80GBæ˜¾å­˜ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-VL-28B-A3B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --enable-mm \\\n       --reasoning-parser ernie-45-vl \\\n       --max-num-seqs 32\n\n\nERNIE-4.5-VL æ¨¡å‹æ”¯æŒé€šè¿‡è¯·æ±‚å‚æ•°å¼€å¯æˆ–å…³é—­æ€è€ƒæ¨¡å¼ã€‚\n\nå¼€å¯æ€è€ƒæ¨¡å¼\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": [\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://paddlenlp.bj.bcebos.com/datasets/paddlemix/demo_images/example2.jpg\"}},\n      {\"type\": \"text\", \"text\": \"Descript this image\"}\n    ]}\n  ],\n  \"metadata\": {\"enable_thinking\": true}\n}'\n\nå…³é—­æ€è€ƒæ¨¡å¼\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": [\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://paddlenlp.bj.bcebos.com/datasets/paddlemix/demo_images/example2.jpg\"}},\n      {\"type\": \"text\", \"text\": \"Descript this image\"}\n    ]}\n  ],\n  \"metadata\": {\"enable_thinking\": false}\n}'\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªåˆ©ç”¨ transformers åº“è¿›è¡Œæ¨ç†çš„ç¤ºä¾‹ï¼š\n\nimport torch\nfrom transformers import AutoProcessor, AutoTokenizer, AutoModelForCausalLM\n\nmodel_path = 'baidu/ERNIE-4.5-VL-28B-A3B-PT'\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True\n)\n\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\nprocessor.eval()\nmodel.add_image_preprocess(processor)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the image.\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://paddlenlp.bj.bcebos.com/datasets/paddlemix/demo_images/example1.jpg\"}},\n        ]\n    },\n]\n\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nimage_inputs, video_inputs = processor.process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\n\ndevice = next(model.parameters()).device\ninputs = inputs.to(device)\n\ngenerated_ids = model.generate(\n    inputs=inputs['input_ids'].to(device),\n    **inputs,\n    max_new_tokens=128\n    )\noutput_text = processor.decode(generated_ids[0])\nprint(output_text)\n\nvLLM æ¨ç†æ”¯æŒ\n\næˆ‘ä»¬æ­£ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢é€‚é…ERNIE4.5æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nè®¸å¯åè®®\n\nERNIE 4.5æ¨¡å‹åŸºäºApache License 2.0å¼€æºåè®®æä¾›ã€‚è¯¥åè®®å…è®¸å•†ä¸šç”¨é€”ï¼Œä½†éœ€éµå®ˆç›¸å…³æ¡æ¬¾ä¸æ¡ä»¶ã€‚ç‰ˆæƒæ‰€æœ‰ (c) 2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨è¯´æ˜\n\nå¦‚æœæ‚¨è®¤ä¸ºERNIE 4.5å¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¸®åŠ©ï¼Œæˆ–è®¡åˆ’åœ¨é¡¹ç›®ä¸­ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Image-Text-to-Text\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-Paddle",
    "project_name": "ERNIE-4.5-300B-A47B-Paddle",
    "readme": "Original Text\n   \nERNIE-4.5-300B-A47B\nERNIE 4.5 äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„å…ˆè¿›èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—æ¨¡å‹ï¼Œå¾—ç›Šäºä»¥ä¸‹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰ä¸¤ç§æ¨¡æ€ä¸Šè”åˆè®­ç»ƒï¼Œæ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®å¼‚ï¼Œå¹¶æå‡åœ¨æ¶‰åŠæ–‡æœ¬ç†è§£ä¸ç”Ÿæˆã€å›¾åƒç†è§£ä»¥åŠè·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº† å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥äº† æ¨¡æ€éš”ç¦»è·¯ç”± å¹¶åº”ç”¨äº† è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€ Token å¹³è¡¡æŸå¤±ã€‚è¿™äº›æ¶æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€éƒ½å¾—åˆ°æœ‰æ•ˆè¡¨å¾ï¼Œä½¿å¾—åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿç›¸äº’å¢å¼ºã€‚\n\næ‰©å±•æ•ˆç‡åŒ–åŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬æå‡ºäº†ç”¨äº ERNIE 4.5 æ¨¡å‹é«˜æ•ˆè®­ç»ƒçš„æ–°å‹å¼‚æ„æ··åˆå¹¶è¡Œå’Œåˆ†å±‚è´Ÿè½½å¹³è¡¡ç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜é«˜æ•ˆç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†æ˜¾è‘—çš„é¢„è®­ç»ƒååé‡ã€‚åœ¨æ¨ç†æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•å’Œ å·ç§¯ç é‡åŒ– ç®—æ³•ä»¥å®ç° 4 ä½/2 ä½æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºåŠ¨æ€è§’è‰²åˆ‡æ¢çš„ PD è§£èšï¼Œæœ‰æ•ˆåˆ©ç”¨èµ„æºæ¥å¢å¼º ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚å»ºç«‹åœ¨ PaddlePaddle ä¹‹ä¸Šçš„ ERNIE 4.5 åœ¨å¤šç§ç¡¬ä»¶å¹³å°ä¸Šæä¾›é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºäº†æ»¡è¶³å®é™…åº”ç”¨çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒæ¨¡æ€å˜ä½“è¿›è¡Œäº†å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLM æ¨¡å‹é’ˆå¯¹é€šç”¨è¯­è¨€ç†è§£å’Œç”Ÿæˆè¿›è¡Œäº†ä¼˜åŒ–ï¼Œè€Œ VLM æ¨¡å‹åˆ™ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹åœ¨åç»­è®­ç»ƒä¸­é‡‡ç”¨äº† ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ æˆ–åä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰ çš„æ”¹è¿›å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-300B-A47B æ˜¯ä¸€ä¸ªæ–‡æœ¬ MoE åè®­ç»ƒæ¨¡å‹ï¼Œæ€»å‚æ•°é‡ä¸º 300Bï¼Œæ¯ä¸ª Token æ¿€æ´»çš„å‚æ•°é‡ä¸º 47Bã€‚ä»¥ä¸‹ä¸ºæ¨¡å‹é…ç½®è¯¦æƒ…ï¼š\n\nå‚æ•°\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°é‡ï¼ˆæ€»/æ¿€æ´»ï¼‰\t300B / 47B\nå±‚æ•°\t54\nå¤´æ•°ï¼ˆQ/KVï¼‰\t64 / 8\næ–‡æœ¬ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 8\nè§†è§‰ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 8\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ ERNIEKit è¿›è¡Œæ¨¡å‹å¾®è°ƒ\n\nERNIEKit æ˜¯åŸºäº PaddlePaddle å¼€å‘çš„è®­ç»ƒå·¥å…·åŒ…ï¼Œä¸“é—¨ä¸º ERNIE ç³»åˆ—å¼€æºå¤§æ¨¡å‹è®¾è®¡ã€‚å®ƒä¸ºæŒ‡ä»¤å¾®è°ƒï¼ˆSFTã€LoRAï¼‰å’Œå¯¹é½è®­ç»ƒï¼ˆDPOï¼‰ç­‰åœºæ™¯æä¾›å…¨é¢æ”¯æŒï¼Œç¡®ä¿æœ€ä¼˜æ€§èƒ½ã€‚\n\nä½¿ç”¨ç¤ºä¾‹ï¼š\n\n# Download model\nhuggingface-cli download baidu/ERNIE-4.5-300B-A47B-Paddle --local-dir baidu/ERNIE-4.5-300B-A47B-Paddle\n# SFT\nerniekit train examples/configs/ERNIE-4.5-300B-A47B/sft/run_sft_wint8mix_lora_8k.yaml\n# DPO\nerniekit train examples/configs/ERNIE-4.5-300B-A47B/dpo/run_dpo_wint8mix_lora_8k.yaml\n\n\nä»¥ä¸‹æ˜¯è¯¦ç»†ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨LoRAçš„SFTã€å¤šGPUé…ç½®ä»¥åŠé«˜çº§è„šæœ¬ï¼Œè¯·å‚è€ƒ ERNIEKit å­˜å‚¨åº“ä¸­çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ã€‚\n\nä½¿ç”¨ FastDeploy\n\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é€šè¿‡ FastDeploy å¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚å…³äºæ›´è¯¦ç»†çš„ç”¨æ³•è¯´æ˜ï¼Œè¯·å‚è€ƒ FastDeploy å­˜å‚¨åº“ã€‚\n\næ³¨æ„ï¼šè¦åœ¨é…å¤‡è‡³å°‘80Gå†…å­˜çš„4ä¸ªGPUä¸Šéƒ¨ç½²ï¼Œè¯·æŒ‡å®š --quantization wint4ã€‚å¦‚æœæ‚¨æŒ‡å®š --quantization wint8ï¼Œåˆ™éœ€è¦8ä¸ªGPUçš„èµ„æºã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-300B-A47B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --quantization wint4 \\\n       --tensor-parallel-size 8 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\n\nè¦ä½¿ç”¨ FastDeploy éƒ¨ç½² W4A8C8 é‡åŒ–ç‰ˆæœ¬ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-300B-A47B-W4A8C8-TP4-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --tensor-parallel-size 4 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\n\nè¦åœ¨å•ä¸ª141G GPUä¸Šä½¿ç”¨FastDeployéƒ¨ç½²WINT2é‡åŒ–ç‰ˆæœ¬ï¼Œæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model \"baidu/ERNIE-4.5-300B-A47B-2Bits-Paddle\" \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --tensor-parallel-size 1 \\\n       --max-model-len  32768 \\\n       --max-num-seqs 128\n\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ ERNIE-4.5-300B-A47B-FP8 æ ¹æ®ç»™å®šè¾“å…¥ç”Ÿæˆå†…å®¹ã€‚\n\nfrom fastdeploy import LLM, SamplingParams\n\nprompts = [\n    \"Hello, my name is\",\n]\n\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=128)\n\nmodel = \"baidu/ERNIE-4.5-300B-A47B-FP8-Paddle\"\nllm = LLM(model=model, tensor_parallel_size=8, max_model_len=8192, num_gpu_blocks_override=1024, engine_worker_queue_port=9981)\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs.text\n    print(\"generated_text\", generated_text)\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ¨¡å‹æ ¹æ®ç»™å®šè¾“å…¥ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-300B-A47B-PT\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n\n# decode the generated ids\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\nprint(\"generate_text:\", generate_text)\n\nä½¿ç”¨ vLLM\n\nvLLM æ­£åœ¨é€‚é…ä¸­ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨æˆ‘ä»¬åˆ†å‰çš„ä»“åº“ vllmã€‚æˆ‘ä»¬æ­£åœ¨ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢æ”¯æŒ ERNIE4.5 æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\n# 80G * 16 GPU\nvllm serve baidu/ERNIE-4.5-300B-A47B-PT --trust-remote-code\n\n# FP8 online quantification 80G * 8 GPU\nvllm serve baidu/ERNIE-4.5-300B-A47B-PT --trust-remote-code --quantization fp8\n\næœ€ä½³å®è·µ\né‡‡æ ·å‚æ•°\n\nä¸ºäº†å®ç°æœ€ä¼˜æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ Temperature=0.8ï¼ŒTopP=0.8ã€‚\n\nç½‘ç»œæœç´¢æç¤º\n\nå¯¹äºç½‘ç»œæœç´¢ï¼Œ{references}ã€{date} å’Œ {question} æ˜¯å‚æ•°ã€‚\n\nå¯¹äºä¸­æ–‡é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æç¤ºï¼š\n\nernie_search_zh_prompt = \\\n'''ä¸‹é¢ä½ ä¼šæ”¶åˆ°å½“å‰æ—¶é—´ã€å¤šä¸ªä¸åŒæ¥æºçš„å‚è€ƒæ–‡ç« å’Œä¸€æ®µå¯¹è¯ã€‚ä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»å¤šä¸ªå‚è€ƒæ–‡ç« ï¼Œå¹¶æ ¹æ®å‚è€ƒæ–‡ç« ä¸­çš„ä¿¡æ¯å›ç­”å¯¹è¯ä¸­çš„é—®é¢˜ã€‚\nä»¥ä¸‹æ˜¯å½“å‰æ—¶é—´å’Œå‚è€ƒæ–‡ç« ï¼š\n---------\n#å½“å‰æ—¶é—´\n{date}\n\n#å‚è€ƒæ–‡ç« \n{references}\n\n---------\nè¯·æ³¨æ„ï¼š\n1. å›ç­”å¿…é¡»ç»“åˆé—®é¢˜éœ€æ±‚å’Œå½“å‰æ—¶é—´ï¼Œå¯¹å‚è€ƒæ–‡ç« çš„å¯ç”¨æ€§è¿›è¡Œåˆ¤æ–­ï¼Œé¿å…åœ¨å›ç­”ä¸­ä½¿ç”¨é”™è¯¯æˆ–è¿‡æ—¶çš„ä¿¡æ¯ã€‚\n2. å½“å‚è€ƒæ–‡ç« ä¸­çš„ä¿¡æ¯æ— æ³•å‡†ç¡®åœ°å›ç­”é—®é¢˜æ—¶ï¼Œä½ éœ€è¦åœ¨å›ç­”ä¸­æä¾›è·å–ç›¸åº”ä¿¡æ¯çš„å»ºè®®ï¼Œæˆ–æ‰¿è®¤æ— æ³•æä¾›ç›¸åº”ä¿¡æ¯ã€‚\n3. ä½ éœ€è¦ä¼˜å…ˆæ ¹æ®ç™¾ç§‘ã€å®˜ç½‘ã€æƒå¨æœºæ„ã€ä¸“ä¸šç½‘ç«™ç­‰é«˜æƒå¨æ€§æ¥æºçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚\n4. å›å¤éœ€è¦ç»¼åˆå‚è€ƒæ–‡ç« ä¸­çš„ç›¸å…³æ•°å­—ã€æ¡ˆä¾‹ã€æ³•å¾‹æ¡æ–‡ã€å…¬å¼ç­‰ä¿¡æ¯ï¼Œä½¿ä½ çš„ç­”æ¡ˆæ›´ä¸“ä¸šã€‚\n5. å½“é—®é¢˜å±äºåˆ›ä½œç±»ä»»åŠ¡æ—¶ï¼Œéœ€æ³¨æ„ä»¥ä¸‹ç»´åº¦ï¼š\n   - æ€åº¦é²œæ˜ï¼šè§‚ç‚¹ã€ç«‹åœºæ¸…æ™°æ˜ç¡®ï¼Œé¿å…æ¨¡æ£±ä¸¤å¯ï¼Œè¯­è¨€æœæ–­ç›´æ¥\n   - æ–‡é‡‡é£æ‰¬ï¼šç”¨è¯ç²¾å‡†ç”ŸåŠ¨ï¼Œå–„ç”¨ä¿®è¾æ‰‹æ³•ï¼Œå¢å¼ºæ„ŸæŸ“åŠ›\n   - æœ‰ç†æœ‰æ®ï¼šé€»è¾‘ä¸¥å¯†é€’è¿›ï¼Œç»“åˆæƒå¨æ•°æ®/äº‹å®æ”¯æ’‘è®ºç‚¹\n---------\nä¸‹é¢è¯·ç»“åˆä»¥ä¸Šä¿¡æ¯ï¼Œå›ç­”é—®é¢˜ï¼Œè¡¥å…¨å¯¹è¯\n{question}'''\n\n\nå¯¹äºè‹±æ–‡é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨å¦‚ä¸‹æç¤ºè¯­ï¼š\n\nernie_search_en_prompt = \\\n'''\nBelow you will be given the current time, multiple references from different sources, and a conversation. Your task is to read the references and use the information in them to answer the question in the conversation.\nHere are the current time and the references:\n---------\n#Current Time\n{date}\n\n#References\n{references}\n\n---------\nPlease note:\n1. Based on the questionâ€™s requirements and the current time, assess the usefulness of the references to avoid using inaccurate or outdated information in the answer.  \n2. If the references do not provide enough information to accurately answer the question, you should suggest how to obtain the relevant information or acknowledge that you are unable to provide it.  \n3. Prioritize using information from highly authoritative sources such as encyclopedias, official websites, authoritative institutions, and professional websites when answering questions.\n4. Incorporate relevant numbers, cases, legal provisions, formulas, and other details from the references to make your answer more professional.\n5. For creative tasks, keep these dimensions in mind:\n   - Clear attitude: Clear views and positions, avoid ambiguity, and use decisive and direct language\n   - Brilliant writing: Precise and vivid words, good use of rhetoric, and enhance the appeal\n   - Well-reasoned: Rigorous logic and progressive, combined with authoritative data/facts to support the argument\n\n---------\nNow, using the information above, answer the question and complete the conversation:  \n{question}'''\n\n\nå‚æ•°è¯´æ˜ï¼š\n\n{question} æ˜¯ç”¨æˆ·çš„æé—®\n{date} ä¸ºå½“å‰æ—¶é—´ï¼Œå»ºè®®æ ¼å¼ä¸ºâ€œYYYY-MM-DD HH:MM:SSï¼Œæ˜ŸæœŸå‡ ï¼ŒåŒ—äº¬/ä¸­å›½ã€‚â€\n{references} æ˜¯å‚è€ƒæ–‡çŒ®ï¼Œå»ºè®®æ ¼å¼ä¸ºï¼š\n##å‚è€ƒæ–‡ç« 1\næ ‡é¢˜ï¼šå‘¨æ°ä¼¦\næ–‡ç« å‘å¸ƒæ—¶é—´ï¼š2025-04-20\nå†…å®¹ï¼šå‘¨æ°ä¼¦(Jay Chou),1979å¹´1æœˆ18æ—¥å‡ºç”Ÿäºå°æ¹¾çœæ–°åŒ—å¸‚,ç¥–ç±ç¦å»ºçœæ°¸æ˜¥å¿,åè¯­æµè¡Œä¹ç”·æ­Œæ‰‹ã€éŸ³ä¹äººã€æ¼”å‘˜ã€å¯¼æ¼”ã€ç¼–å‰§,æ¯•ä¸šäºæ·¡æ±Ÿä¸­å­¦ã€‚2000å¹´,å‘è¡Œä¸ªäººé¦–å¼ éŸ³ä¹ä¸“è¾‘ã€ŠJayã€‹ã€‚...\næ¥æºç½‘ç«™ç½‘å€ï¼šbaike.baidu.com\næ¥æºç½‘ç«™çš„ç½‘ç«™åï¼šç™¾åº¦ç™¾ç§‘\n\n##å‚è€ƒæ–‡ç« 2\n...\n\nè®¸å¯è¯\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache è®¸å¯è¯ 2.0 æä¾›ã€‚è¯¥è®¸å¯è¯åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶çš„å‰æä¸‹ï¼Œå…è®¸å•†ä¸šä½¿ç”¨ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨è®¤ä¸º ERNIE 4.5 æœ‰ç”¨ï¼Œæˆ–å¸Œæœ›åœ¨æ‚¨çš„é¡¹ç›®ä¸­ä½¿ç”¨å®ƒï¼Œè¯·åŠ¡å¿…å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-VL-28B-A3B-Base-PT",
    "project_name": "ERNIE-4.5-VL-28B-A3B-Base-PT",
    "readme": "Original Text\n   \næ–‡å¿ƒ4.5å¤šæ¨¡æ€å¤§æ¨¡å‹-28B-A3BåŸºç¡€ç‰ˆ\næ–‡å¿ƒ4.5æŠ€æœ¯äº®ç‚¹\n\næ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯åŸºäºMoEæ¶æ„çš„A47Bå’ŒA3Bç³»åˆ—ï¼‰çš„å“è¶Šèƒ½åŠ›æºäºå¤šé¡¹æ ¸å¿ƒæŠ€æœ¯çªç ´ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„MoEé¢„è®­ç»ƒï¼šé€šè¿‡æ–‡æœ¬ä¸è§†è§‰æ¨¡æ€çš„è”åˆè®­ç»ƒï¼Œæ¨¡å‹èƒ½æ›´ç²¾å‡†æ•æ‰è·¨æ¨¡æ€ä¿¡æ¯å…³è”ï¼Œæ˜¾è‘—æå‡æ–‡æœ¬ç†è§£ç”Ÿæˆã€å›¾åƒç†è§£åŠè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ›æ–°æ€§åœ°é‡‡ç”¨å¼‚æ„MoEæ¶æ„ï¼Œç»“åˆæ¨¡æ€éš”ç¦»è·¯ç”±æœºåˆ¶ï¼Œå¹¶å¼•å…¥è·¯ç”±å™¨æ­£äº¤æŸå¤±ä¸å¤šæ¨¡æ€ä»¤ç‰Œå‡è¡¡æŸå¤±ï¼Œç¡®ä¿å„æ¨¡æ€ç‰¹å¾ç‹¬ç«‹å­¦ä¹ çš„åŒæ—¶å®ç°ååŒä¼˜åŒ–ã€‚\n\né«˜æ•ˆæ‰©å±•åŸºç¡€è®¾æ–½ï¼šæå‡ºå¼‚æ„æ··åˆå¹¶è¡Œä¸åˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œé€šè¿‡èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜ä¼˜åŒ–æµæ°´çº¿è°ƒåº¦ã€FP8æ··åˆç²¾åº¦è®­ç»ƒåŠç»†ç²’åº¦é‡è®¡ç®—æŠ€æœ¯ï¼Œå®ç°æƒŠäººçš„è®­ç»ƒååé‡ã€‚æ¨ç†é˜¶æ®µé‡‡ç”¨å¤šä¸“å®¶å¹¶è¡Œåä½œæ–¹æ³•ä¸å·ç§¯ç¼–ç é‡åŒ–ç®—æ³•ï¼Œè¾¾æˆ4æ¯”ç‰¹/2æ¯”ç‰¹æ— æŸé‡åŒ–ã€‚åŸºäºé£æ¡¨æ¡†æ¶çš„åŠ¨æ€è§’è‰²åˆ‡æ¢PDè§£è€¦æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡MoEæ¨¡å‹æ¨ç†æ•ˆç‡ã€‚\n\næ¨¡æ€ä¸“é¡¹è°ƒä¼˜ï¼šé’ˆå¯¹å®é™…åº”ç”¨åœºæ™¯éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œæ¨¡æ€ä¸“é¡¹ä¼˜åŒ–ã€‚é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸“æ³¨äºæ–‡æœ¬ç†è§£ä¸ç”Ÿæˆï¼›è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼ºåŒ–å›¾æ–‡äº¤äº’èƒ½åŠ›ï¼Œæ”¯æŒæ€ç»´é“¾ä¸éæ€ç»´é“¾åŒæ¨¡å¼ã€‚é‡‡ç”¨ç›‘ç£å¾®è°ƒ(SFT)ã€*ç›´æ¥åå¥½ä¼˜åŒ–(DPO)åŠåˆ›æ–°çš„ç»Ÿä¸€åå¥½ä¼˜åŒ–(UPO)*æ–¹æ³•è¿›è¡Œè®­ç»ƒåä¼˜åŒ–ã€‚\n\nä¸ºç¡®ä¿å¤šæ¨¡æ€è”åˆè®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šå‰ä¸¤é˜¶æ®µä¸“æ³¨æ–‡æœ¬å‚æ•°è®­ç»ƒï¼Œæ„å»ºå¼ºå¤§çš„è¯­è¨€ç†è§£ä¸é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼›æœ€ç»ˆé˜¶æ®µå¼•å…¥ViTå›¾åƒç‰¹å¾æå–å™¨ã€ç‰¹å¾è½¬æ¢é€‚é…å™¨åŠè§†è§‰ä¸“å®¶æ¨¡å—ï¼Œå®ç°å›¾æ–‡æ¨¡æ€çš„ç›¸äº’å¢å¼ºã€‚ç»è¿‡ä¸‡äº¿çº§tokené¢„è®­ç»ƒï¼Œæœ€ç»ˆäº§å‡ºæ–‡å¿ƒ4.5å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\næ–‡å¿ƒ4.5å¤šæ¨¡æ€åŸºç¡€ç‰ˆé‡‡ç”¨MoEæ¶æ„ï¼Œæ€»å‚æ•°é‡280äº¿ï¼Œå•tokenæ¿€æ´»å‚æ•°é‡30äº¿ã€‚æ ¸å¿ƒé…ç½®å¦‚ä¸‹ï¼š\n\nå…³é”®å‚æ•°\tæ•°å€¼\næ¨¡æ€æ”¯æŒ\tæ–‡æœ¬ & è§†è§‰\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°é‡(æ€»/æ¿€æ´»)\t280äº¿/30äº¿\nå±‚æ•°\t28\næ³¨æ„åŠ›å¤´æ•°(Q/KV)\t20 / 4\næ–‡æœ¬ä¸“å®¶(æ€»/æ¿€æ´»)\t64 / 6\nè§†è§‰ä¸“å®¶(æ€»/æ¿€æ´»)\t64 / 6\nå…±äº«ä¸“å®¶\t2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå¼€å§‹\nvLLMæ¨ç†æ”¯æŒ\n\næˆ‘ä»¬æ­£ä¸ç¤¾åŒºåˆä½œå®Œå–„å¯¹æ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹çš„å…¨é¢æ”¯æŒï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nè®¸å¯åè®®\n\næ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹éµå¾ªApache 2.0å¼€æºåè®®ï¼Œå…è®¸ç¬¦åˆæ¡æ¬¾çš„å•†ä¸šä½¿ç”¨ã€‚ç‰ˆæƒæ‰€æœ‰ (c) 2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨å£°æ˜\n\nå¦‚æœæ‚¨åœ¨é¡¹ç›®ä¸­ä½¿ç”¨æ–‡å¿ƒ4.5æ¨¡å‹ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Image-Text-to-Text\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-21B-A3B-PT",
    "project_name": "ERNIE-4.5-21B-A3B-PT",
    "readme": "Original Text\n   \nERNIE-4.5-21B\nERNIE 4.5 äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„å…ˆè¿›èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—ï¼Œå¾—ç›Šäºå‡ é¡¹å…³é”®æŠ€æœ¯é©æ–°ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¸Šè”åˆè®­ç»ƒï¼Œä»¥æ›´å¥½åœ°æ•è·å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶æé«˜æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆã€å›¾åƒç†è§£ä»¥åŠè·¨æ¨¡æ€æ¨ç†ç­‰ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ºäº†é˜²æ­¢ä¸€ä¸ªæ¨¡æ€é˜»ç¢å¦ä¸€ä¸ªæ¨¡æ€çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº† å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥äº† æ¨¡æ€éš”ç¦»è·¯ç”±ï¼Œå¹¶ä½¿ç”¨äº† è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€æ ‡è®°å¹³è¡¡æŸå¤±ã€‚è¿™äº›æ¶æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€çš„æœ‰æ•ˆè¡¨å¾ï¼Œä½¿å¾—åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿç›¸äº’å¢å¼ºã€‚\n\nå¯æ‰©å±•é«˜æ•ˆåŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬æå‡ºäº†æ–°é¢–çš„å¼‚æ„æ··åˆå¹¶è¡Œå’Œåˆ†å±‚è´Ÿè½½å¹³è¡¡ç­–ç•¥ï¼Œç”¨äº ERNIE 4.5 æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜é«˜æ•ˆç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†æ˜¾è‘—çš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•å’Œ å·ç§¯ä»£ç é‡åŒ– ç®—æ³•ï¼Œå®ç°äº† 4 ä½/2 ä½æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºåŠ¨æ€è§’è‰²åˆ‡æ¢çš„ PD è§£è€¦ï¼Œæœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œæé«˜ ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚åŸºäº PaddlePaddleï¼ŒERNIE 4.5 åœ¨å¹¿æ³›çš„ç¡¬ä»¶å¹³å°ä¸Šæä¾›é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºäº†æ»¡è¶³ç°å®ä¸–ç•Œåº”ç”¨çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œäº†ç‰¹å®šæ¨¡æ€çš„å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLMs ä¼˜åŒ–äº†ä¸€èˆ¬æ€§çš„è¯­è¨€ç†è§£ä¸ç”Ÿæˆï¼Œè€Œ VLMs ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹éƒ½é‡‡ç”¨äº† ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ æˆ–åä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰ çš„ä¿®æ”¹åå¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œåè®­ç»ƒã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-21B-A3B æ˜¯ä¸€ä¸ªæ–‡æœ¬ MoE åè®­ç»ƒæ¨¡å‹ï¼Œå…·æœ‰ 21B æ€»å‚æ•°å’Œæ¯ä¸ªæ ‡è®°æ¿€æ´»çš„ 3B å‚æ•°ã€‚ä»¥ä¸‹æ˜¯æ¨¡å‹çš„é…ç½®ç»†èŠ‚ï¼š\n\nå…³é”®å­—\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\tåè®­ç»ƒ\nå‚æ•°ï¼ˆæ€»/æ¿€æ´»ï¼‰\t21B / 3B\nå±‚æ•°\t28\nå¤´æ•°ï¼ˆQ/KVï¼‰\t20 / 4\næ–‡æœ¬ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 6\nè§†è§‰ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 6\nå…±äº«ä¸“å®¶\t2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ ERNIEKit è¿›è¡Œæ¨¡å‹å¾®è°ƒ\n\nERNIEKit æ˜¯åŸºäº PaddlePaddle çš„è®­ç»ƒå·¥å…·åŒ…ï¼Œä¸“é—¨ä¸º ERNIE ç³»åˆ—çš„å¼€æºå¤§å‹æ¨¡å‹è®¾è®¡ã€‚å®ƒä¸ºæŒ‡ä»¤å¾®è°ƒï¼ˆSFTã€LoRAï¼‰å’Œå¯¹é½è®­ç»ƒï¼ˆDPOï¼‰ç­‰åœºæ™¯æä¾›äº†å…¨é¢æ”¯æŒï¼Œç¡®ä¿æœ€ä¼˜æ€§èƒ½ã€‚\n\nä½¿ç”¨ç¤ºä¾‹ï¼š\n\n# Download model\nhuggingface-cli download baidu/ERNIE-4.5-21B-A3B-Paddle --local-dir baidu/ERNIE-4.5-21B-A3B-Paddle\n# SFT\nerniekit train examples/configs/ERNIE-4.5-21B-A3B/sft/run_sft_lora_8k.yaml\n# DPO\nerniekit train examples/configs/ERNIE-4.5-21B-A3B/dpo/run_dpo_lora_8k.yaml\n\n\nä»¥ä¸‹æ˜¯æ›´åŠ è¯¦ç»†ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨LoRAçš„SFTã€å¤šGPUé…ç½®ä»¥åŠé«˜çº§è„šæœ¬ï¼Œè¯·å‚è€ƒ ERNIEKit ä»“åº“ä¸­çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ã€‚\n\nFastDeploy ï¿½ infer æ¨ç†\n\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é€šè¿‡ FastDeploy å¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚å…³äºæ›´è¯¦ç»†çš„ç”¨æ³•æŒ‡å¯¼ï¼Œè¯·å‚è€ƒ FastDeploy ä»“åº“ã€‚\n\næ³¨æ„ï¼šå¯¹äºå•å¡éƒ¨ç½²ï¼Œè‡³å°‘éœ€è¦80Gçš„GPUå†…å­˜èµ„æºã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-21B-A3B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•æ ¹æ®ç»™å®šè¾“å…¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-PT\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n\n# decode the generated ids\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\nprint(\"generate_text:\", generate_text)\n\nvLLM æ¨æ–­\n\nvLLM æ­£åœ¨é€‚é…ä¸­ï¼Œä¼˜å…ˆè€ƒè™‘ä½¿ç”¨æˆ‘ä»¬åˆ†å‰çš„ä»“åº“ vllmã€‚æˆ‘ä»¬æ­£åœ¨ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢æ”¯æŒ ERNIE4.5 æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nvllm serve baidu/ERNIE-4.5-21B-A3B-PT --trust-remote-code\n\nè®¸å¯è¯\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache è®¸å¯è¯ 2.0 æä¾›ã€‚è¯¥è®¸å¯è¯åœ¨å…¶æ¡æ¬¾å’Œæ¡ä»¶èŒƒå›´å†…å…è®¸å•†ä¸šç”¨é€”ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨å‘ç° ERNIE 4.5 æœ‰ç”¨æˆ–å¸Œæœ›åœ¨æ‚¨çš„é¡¹ç›®ä¸­ä½¿ç”¨å®ƒï¼Œè¯·åŠ¡å¿…å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-0.3B-Base-Paddle",
    "project_name": "ERNIE-4.5-0.3B-Base-Paddle",
    "readme": "Original Text\n   \nERNIE-4.5-0.3B-Base\nERNIE 4.5 æŠ€æœ¯äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„é«˜çº§åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—ï¼Œå¾—åˆ°äº†å‡ é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°çš„æ”¯æ’‘ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¸Šè”åˆè®­ç»ƒï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶æé«˜æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆã€å›¾åƒç†è§£åŠè·¨æ¨¡æ€æ¨ç†ç­‰ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œä¸ä½¿ä¸€ç§æ¨¡æ€é˜»ç¢å¦ä¸€ç§æ¨¡æ€çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº† å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥äº† æ¨¡æ€éš”ç¦»è·¯ç”±ï¼Œå¹¶é‡‡ç”¨äº† è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€ Token å¹³è¡¡æŸå¤±ã€‚è¿™äº›ç»“æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€çš„æœ‰æ•ˆè¡¨ç¤ºï¼Œä½¿å¾—å®ƒä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿç›¸äº’å¢å¼ºã€‚\n\næ‰©å±•æ•ˆç‡åŒ–çš„åŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬ä¸º ERNIE 4.5 æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒæå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼‚æ„æ··åˆå¹¶è¡Œä¸»ä¹‰å’Œåˆ†å±‚è´Ÿè½½å¹³è¡¡ç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œä¸»ä¹‰ã€å†…å­˜é«˜æ•ˆçš„ç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†å“è¶Šçš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•å’Œ å·ç§¯ç é‡åŒ– ç®—æ³•ï¼Œä»¥å®ç° 4 ä½/2 ä½æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºåŠ¨æ€è§’è‰²åˆ‡æ¢çš„ PD è§£è€¦ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œå¢å¼º ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚åŸºäº PaddlePaddleï¼ŒERNIE 4.5 åœ¨å¹¿æ³›çš„ç¡¬ä»¶å¹³å°ä¸Šå®ç°äº†é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºäº†æ»¡è¶³ç°å®ä¸–ç•Œåº”ç”¨çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„å˜ä½“è¿›è¡Œäº†ç‰¹å®šæ¨¡æ€çš„å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLMs ä¼˜åŒ–äº†ä¸€èˆ¬æ€§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆã€‚VLMs åˆ™ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹åœ¨åç»­è®­ç»ƒä¸­é‡‡ç”¨äº† ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ æˆ–åä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰ çš„æ”¹è¿›å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚\n\næ¨¡å‹æ¦‚è¿°\n\nERNIE-4.5-0.3B-Base æ˜¯ä¸€ä¸ªæ–‡æœ¬å¯†é›†å‹åŸºç¡€æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯æ¨¡å‹é…ç½®çš„è¯¦ç»†ä¿¡æ¯ï¼š\n\nå…³é”®å­—\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°é‡\t0.36B\nå±‚æ•°\t18\nå¤´æ•°ï¼ˆQ/KVï¼‰\t16 / 2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ ERNIEKit è¿›è¡Œæ¨¡å‹å¾®è°ƒ\n\nERNIEKit æ˜¯åŸºäº PaddlePaddle å¼€å‘çš„ä¸€ä¸ªè®­ç»ƒå·¥å…·åŒ…ï¼Œä¸“é—¨ä¸º ERNIE ç³»åˆ—å¼€æºå¤§å‹æ¨¡å‹è®¾è®¡ã€‚å®ƒä¸ºæŒ‡ä»¤å¾®è°ƒï¼ˆSFTã€LoRAï¼‰å’Œå¯¹é½è®­ç»ƒï¼ˆDPOï¼‰ç­‰åœºæ™¯æä¾›äº†å…¨é¢æ”¯æŒï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚\n\nä½¿ç”¨ç¤ºä¾‹ï¼š\n\n# Download Model\nhuggingface-cli download baidu/ERNIE-4.5-0.3B-Base-Paddle --local-dir baidu/ERNIE-4.5-0.3B-Base-Paddle\n# SFT\nerniekit train examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml model_name_or_path=baidu/ERNIE-4.5-0.3B-Base-Paddle\n# DPO\nerniekit train examples/configs/ERNIE-4.5-0.3B/dpo/run_dpo_8k.yaml model_name_or_path=baidu/ERNIE-4.5-0.3B-Base-Paddle\n\n\nå¯¹äºæ›´è¯¦ç»†çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬å¸¦æœ‰ LoRA çš„ SFTã€å¤š GPU é…ç½®ä»¥åŠé«˜çº§è„šæœ¬ï¼Œè¯·å‚è€ƒ ERNIEKit å­˜å‚¨åº“ä¸­çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ã€‚\n\nFastDeploy æ¨ç†\n\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é€šè¿‡ FastDeploy å¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚æœ‰å…³æ›´è¯¦ç»†çš„ç”¨æ³•è¯´æ˜ï¼Œè¯·å‚è€ƒ FastDeploy å­˜å‚¨åº“ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-0.3B-Base-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•æ ¹æ®ç»™å®šçš„è¾“å…¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-0.3B-Base-PT\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\nprompt = \"Large language model is\"\nmodel_inputs = tokenizer([prompt], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\nresult = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\nprint(\"result:\", result)\n\nvLLM æ¨æ–­\n\nvLLM æ­£åœ¨é€‚é…ä¸­ï¼Œä¼˜å…ˆè€ƒè™‘ä½¿ç”¨æˆ‘ä»¬åˆ†å‰çš„ä»£ç ä»“åº“ vllmã€‚æˆ‘ä»¬æ­£åœ¨ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢æ”¯æŒ ERNIE4.5 æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nvllm serve baidu/ERNIE-4.5-0.3B-Base-PT --trust-remote-code\n\nè®¸å¯è¯\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache License 2.0 æä¾›ã€‚è¯¥è®¸å¯è¯å…è®¸åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶çš„å‰æä¸‹è¿›è¡Œå•†ä¸šä½¿ç”¨ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nè‹¥æ‚¨è®¤ä¸º ERNIE 4.5 æœ‰ç”¨æˆ–å¸Œæœ›å°†å…¶åº”ç”¨äºæ‚¨çš„é¡¹ç›®ä¸­ï¼Œè¯·åŠ¡å¿…å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-21B-A3B-Paddle",
    "project_name": "ERNIE-4.5-21B-A3B-Paddle",
    "readme": "Original Text\n   \nERNIE-4.5-21B\nERNIE 4.5 äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„å…ˆè¿›èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—ï¼Œä¾èµ–äºä»¥ä¸‹å‡ é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰ä¸¤ç§æ¨¡æ€ä¸Šå…±åŒè®­ç»ƒï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶æé«˜æ¶‰åŠæ–‡æœ¬ç†è§£å’Œç”Ÿæˆã€å›¾åƒç†è§£å’Œè·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡æ€§èƒ½ã€‚ä¸ºäº†é˜²æ­¢ä¸€ç§æ¨¡æ€é˜»ç¢å¦ä¸€ç§æ¨¡æ€çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº† å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥äº† æ¨¡æ€éš”ç¦»è·¯ç”±ï¼Œå¹¶åº”ç”¨äº† è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€æ ‡è®°å¹³è¡¡æŸå¤±ã€‚è¿™äº›ç»“æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€éƒ½èƒ½æœ‰æ•ˆè¡¨ç¤ºï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°ç›¸äº’å¼ºåŒ–ã€‚\n\næ‰©å±•æ•ˆç‡åŒ–åŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬æå‡ºäº†ç”¨äº ERNIE 4.5 æ¨¡å‹é«˜æ•ˆè®­ç»ƒçš„å¼‚æ„æ··åˆå¹¶è¡Œå’Œåˆ†å±‚è´Ÿè½½å¹³è¡¡ç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜é«˜æ•ˆç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†æ˜¾è‘—çš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•å’Œ å·ç§¯ä»£ç é‡åŒ– ç®—æ³•ï¼Œä»¥å®ç° 4 ä½/2 ä½æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºè§’è‰²çš„åŠ¨æ€åˆ‡æ¢çš„ PD è§£èšï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œå¢å¼º ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚å»ºç«‹åœ¨ PaddlePaddle ä¹‹ä¸Šçš„ ERNIE 4.5 åœ¨å¤šç§ç¡¬ä»¶å¹³å°ä¸Šæä¾›äº†é«˜æ€§èƒ½æ¨æ–­ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºäº†æ»¡è¶³ç°å®ä¸–ç•Œåº”ç”¨çš„å„ç§éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒæ¨¡æ€å˜ä½“è¿›è¡Œäº†å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLM ä¼˜åŒ–äº†é€šç”¨è¯­è¨€ç†è§£å’Œç”Ÿæˆï¼Œè€Œ VLM ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹åœ¨è®­ç»ƒåé‡‡ç”¨äº† ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ æˆ–åä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰ çš„ä¿®æ”¹åçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-21B-A3B æ˜¯ä¸€ä¸ªæ–‡æœ¬ MoE åè®­ç»ƒæ¨¡å‹ï¼Œæ‹¥æœ‰ 21B æ€»å‚æ•°å’Œæ¯ä¸ªæ ‡è®° 3B æ¿€æ´»å‚æ•°ã€‚ä»¥ä¸‹ä¸ºæ¨¡å‹çš„é…ç½®è¯¦æƒ…ï¼š\n\nå…³é”®å‚æ•°\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\tåè®­ç»ƒ\nå‚æ•°ï¼ˆæ€»/æ¿€æ´»ï¼‰\t21B / 3B\nå±‚æ•°\t28\nå¤´æ•°ï¼ˆQ/KVï¼‰\t20 / 4\næ–‡æœ¬ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 6\nè§†è§‰ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 6\nå…±äº«ä¸“å®¶\t2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ ERNIEKit è¿›è¡Œæ¨¡å‹å¾®è°ƒ\n\nERNIEKit æ˜¯åŸºäº PaddlePaddle å¼€å‘çš„è®­ç»ƒå·¥å…·åŒ…ï¼Œä¸“ä¸º ERNIE ç³»åˆ—å¼€æºå¤§æ¨¡å‹è®¾è®¡ã€‚å®ƒä¸ºæŒ‡ä»¤å¾®è°ƒï¼ˆSFT, LoRAï¼‰å’Œå¯¹é½è®­ç»ƒï¼ˆDPOï¼‰ç­‰åœºæ™¯æä¾›äº†å…¨é¢æ”¯æŒï¼Œç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚\n\nä½¿ç”¨ç¤ºä¾‹ï¼š\n\n# Download model\nhuggingface-cli download baidu/ERNIE-4.5-21B-A3B-Paddle --local-dir baidu/ERNIE-4.5-21B-A3B-Paddle\n# SFT\nerniekit train examples/configs/ERNIE-4.5-21B-A3B/sft/run_sft_lora_8k.yaml\n# DPO\nerniekit train examples/configs/ERNIE-4.5-21B-A3B/dpo/run_dpo_lora_8k.yaml\n\n\nä»¥ä¸‹æ˜¯æ›´ä¸ºè¯¦ç»†çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬æ­é…LoRAçš„SFTã€å¤šGPUé…ç½®ä»¥åŠé«˜çº§è„šæœ¬ï¼Œè¯·å‚è€ƒ ERNIEKit ä»“åº“ä¸­çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ã€‚\n\nFastDeploy æ¨ç†éƒ¨ç½²\n\nä½¿ç”¨ä»¥ä¸‹ FastDeploy å‘½ä»¤ï¼Œå¯ä»¥å¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚æœ‰å…³æ›´è¯¦ç»†çš„ç”¨æ³•æŒ‡å—ï¼Œè¯·æŸ¥é˜… FastDeploy ä»“åº“ã€‚\n\næ³¨æ„ï¼šå•å¡éƒ¨ç½²æ—¶ï¼Œè‡³å°‘éœ€è¦80Gçš„GPUå†…å­˜èµ„æºã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-21B-A3B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•æ ¹æ®ç»™å®šè¾“å…¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-PT\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n\n# decode the generated ids\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\nprint(\"generate_text:\", generate_text)\n\nvLLM æ¨ç†\n\nvLLM ç›®å‰æ­£åœ¨é€‚é…ä¸­ï¼Œå¯ä¼˜å…ˆä½¿ç”¨æˆ‘ä»¬åˆ†å‰çš„ä»£ç ä»“åº“ vllmã€‚æˆ‘ä»¬æ­£åœ¨ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢æ”¯æŒ ERNIE4.5 æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nvllm serve baidu/ERNIE-4.5-21B-A3B-PT --trust-remote-code\n\nè®¸å¯è¯\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache è®¸å¯è¯ 2.0 æä¾›ä½¿ç”¨ã€‚è¯¥è®¸å¯è¯åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶çš„å‰æä¸‹ï¼Œå…è®¸å•†ä¸šç”¨é€”ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨è®¤ä¸º ERNIE 4.5 æœ‰ç”¨ï¼Œæˆ–å¸Œæœ›å°†å…¶åº”ç”¨äºæ‚¨çš„é¡¹ç›®ä¸­ï¼Œæ•¬è¯·å‹å¥½å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-0.3B-Base-PT",
    "project_name": "ERNIE-4.5-0.3B-Base-PT",
    "readme": "Original Text\n   \nERNIE-4.5-0.3B-Base\nERNIE 4.5 çš„äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„å…ˆè¿›èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—ï¼Œä¾èµ–äºå‡ é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¸Šå…±åŒè®­ç»ƒï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶æé«˜åœ¨æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆã€å›¾åƒç†è§£ä»¥åŠè·¨æ¨¡æ€æ¨ç†ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸ºäº†é¿å…ä¸€ä¸ªæ¨¡æ€é˜»ç¢å¦ä¸€ä¸ªæ¨¡æ€çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥äº† æ¨¡æ€éš”ç¦»è·¯ç”±ï¼Œå¹¶åº”ç”¨äº† è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€æ ‡è®°å¹³è¡¡æŸå¤±ã€‚è¿™äº›ç»“æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€éƒ½èƒ½æœ‰æ•ˆè¡¨ç¤ºï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°ç›¸äº’å¼ºåŒ–ã€‚\n\næ‰©å±•æ•ˆç‡åŒ–çš„åŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬æå‡ºäº†æ–°é¢–çš„å¼‚æ„æ··åˆå¹¶è¡Œä¸»ä¹‰å’Œåˆ†å±‚è´Ÿè½½å¹³è¡¡ç­–ç•¥ï¼Œä»¥é«˜æ•ˆè®­ç»ƒ ERNIE 4.5 æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œä¸»ä¹‰ã€å†…å­˜æ•ˆç‡åŒ–çš„ç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†å“è¶Šçš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•åŠ å·ç§¯ä»£ç é‡åŒ– ç®—æ³•ï¼Œå®ç°äº† 4 ä½/2 ä½æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸¦æœ‰åŠ¨æ€è§’è‰²åˆ‡æ¢çš„ PD è§£è€¦ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œå¢å¼º ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚åŸºäº PaddlePaddleï¼ŒERNIE 4.5 åœ¨å¤šç§ç¡¬ä»¶å¹³å°ä¸Šæä¾›é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºæ»¡è¶³ç°å®ä¸–ç•Œåº”ç”¨çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒæ¨¡æ€å˜ç§è¿›è¡Œäº†å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLM ä¼˜åŒ–äº†é€šç”¨è¯­è¨€ç†è§£ä¸ç”Ÿæˆï¼›VLM åˆ™ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹éƒ½é‡‡ç”¨äº† ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ æˆ–ä¸€ç§ä¿®æ”¹åçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰ï¼Œè¿›è¡Œåè®­ç»ƒã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-0.3B-Base æ˜¯ä¸€ä¸ªæ–‡æœ¬å¯†é›†å‹åŸºç¡€æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯å¯¹æ¨¡å‹é…ç½®ç»†èŠ‚çš„è¯´æ˜ï¼š\n\nå…³é”®å­—\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°é‡\t0.36B\nå±‚æ•°\t18\nå¤´æ•°ï¼ˆQ/KVï¼‰\t16 / 2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ ERNIEKit è¿›è¡Œæ¨¡å‹å¾®è°ƒ\n\nERNIEKit æ˜¯åŸºäº PaddlePaddle çš„è®­ç»ƒå·¥å…·åŒ…ï¼Œä¸“é—¨ä¸º ERNIE ç³»åˆ—å¼€æºå¤§å‹æ¨¡å‹è®¾è®¡ã€‚å®ƒä¸ºæŒ‡ä»¤å¾®è°ƒï¼ˆSFTã€LoRAï¼‰å’Œå¯¹æ¯”è®­ç»ƒï¼ˆDPOï¼‰ç­‰åœºæ™¯æä¾›å…¨é¢æ”¯æŒï¼Œç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚\n\nä½¿ç”¨ç¤ºä¾‹ï¼š\n\n# Download Model\nhuggingface-cli download baidu/ERNIE-4.5-0.3B-Base-Paddle --local-dir baidu/ERNIE-4.5-0.3B-Base-Paddle\n# SFT\nerniekit train examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml model_name_or_path=baidu/ERNIE-4.5-0.3B-Base-Paddle\n# DPO\nerniekit train examples/configs/ERNIE-4.5-0.3B/dpo/run_dpo_8k.yaml model_name_or_path=baidu/ERNIE-4.5-0.3B-Base-Paddle\n\n\nå¯¹äºæ›´è¯¦ç»†çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬å¸¦æœ‰ LoRA çš„ SFTã€å¤š GPU é…ç½®ä»¥åŠé«˜çº§è„šæœ¬ï¼Œè¯·å‚è€ƒ ERNIEKit å­˜å‚¨åº“ä¸­çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ã€‚\n\nFastDeploy æ¨ç†\n\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é€šè¿‡ FastDeploy å¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚å…³äºæ›´è¯¦ç»†çš„ç”¨æ³•æŒ‡å¯¼ï¼Œè¯·å‚è€ƒ FastDeploy å­˜å‚¨åº“ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-0.3B-Base-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œå±•ç¤ºäº†å¦‚ä½•æ ¹æ®ç»™å®šè¾“å…¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-0.3B-Base-PT\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\nprompt = \"Large language model is\"\nmodel_inputs = tokenizer([prompt], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\nresult = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\nprint(\"result:\", result)\n\nvLLM æ¨æ–­\n\nvLLMç›®å‰æ­£åœ¨é€‚é…ä¸­ï¼Œå¯ä»¥ä¼˜å…ˆä½¿ç”¨æˆ‘ä»¬åˆ†å‰çš„ä»£ç ä»“åº“ vllmã€‚æˆ‘ä»¬æ­£åœ¨ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢æ”¯æŒERNIE4.5æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nvllm serve baidu/ERNIE-4.5-0.3B-Base-PT --trust-remote-code\n\nè®¸å¯è¯\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache License 2.0 æä¾›æœåŠ¡ã€‚è¯¥è®¸å¯è¯åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶çš„å‰æä¸‹ï¼Œå…è®¸å•†ä¸šç”¨é€”ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨å‘ç° ERNIE 4.5 æœ‰ç”¨æˆ–å¸Œæœ›åœ¨æ‚¨çš„é¡¹ç›®ä¸­ä½¿ç”¨å®ƒï¼Œè¯·åŠ¡å¿…å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-21B-A3B-Base-PT",
    "project_name": "ERNIE-4.5-21B-A3B-Base-PT",
    "readme": "Original Text\n   \nERNIE-4.5-21B-A3B-Base\nERNIE 4.5 äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„å…ˆè¿›èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—ï¼Œå¾—ç›Šäºä»¥ä¸‹å‡ é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¸Šè”åˆè®­ç»ƒï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶æé«˜æ¶‰åŠæ–‡æœ¬ç†è§£å’Œç”Ÿæˆã€å›¾åƒç†è§£å’Œè·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡æ€§èƒ½ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œé¿å…ä¸€ç§æ¨¡æ€é˜»ç¢å¦ä¸€ç§æ¨¡æ€çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº† å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥äº† æ¨¡æ€éš”ç¦»è·¯ç”±ï¼Œå¹¶é‡‡ç”¨äº† è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€ Token å¹³è¡¡æŸå¤±ã€‚è¿™äº›ç»“æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€éƒ½å¾—åˆ°æœ‰æ•ˆè¡¨ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’å¼ºåŒ–ã€‚\n\næ‰©å±•æ•ˆç‡åŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬ä¸º ERNIE 4.5 æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒæå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼‚æ„æ··åˆå¹¶è¡Œæ€§å’Œå±‚æ¬¡è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œæ€§ã€å†…å­˜é«˜æ•ˆçš„ç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†æ˜¾è‘—çš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•å’Œ å·ç§¯ä»£ç é‡åŒ– ç®—æ³•ï¼Œä»¥å®ç° 4 ä½/2 ä½æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…·æœ‰åŠ¨æ€è§’è‰²åˆ‡æ¢çš„ PD è§£è€¦ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œå¢å¼º ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚åŸºäº PaddlePaddleï¼ŒERNIE 4.5 åœ¨å¹¿æ³›çš„ç¡¬ä»¶å¹³å°ä¸Šæä¾›é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºäº†æ»¡è¶³ç°å®ä¸–ç•Œåº”ç”¨çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„å˜ä½“è¿›è¡Œäº†ç‰¹å®šæ¨¡æ€çš„å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLM ä¼˜åŒ–ç”¨äºé€šç”¨ç›®çš„çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆã€‚VLM ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹éƒ½é‡‡ç”¨äº† ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ æˆ–ä¸€ç§åä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰ çš„ä¿®æ”¹åå¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œåè®­ç»ƒã€‚\n\nä¸ºç¡®ä¿å¤šæ¨¡æ€è”åˆè®­ç»ƒçš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä»…è®­ç»ƒæ–‡æœ¬ç›¸å…³å‚æ•°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå»ºç«‹å¼ºå¤§çš„åŸºç¡€è¯­è¨€ç†è§£èƒ½åŠ›å’Œé•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚æœ€ç»ˆçš„è·¨æ¨¡æ€é˜¶æ®µé€šè¿‡å¼•å…¥åŒ…æ‹¬ ViT å›¾åƒç‰¹å¾æå–ã€ç‰¹å¾è½¬æ¢é€‚é…å™¨å’Œè§†è§‰ä¸“å®¶ç­‰é¢å¤–å‚æ•°ï¼Œæ‰©å±•äº†å›¾åƒå’Œè§†é¢‘èƒ½åŠ›ã€‚åœ¨è¿™ä¸€é˜¶æ®µï¼Œæ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ç›¸äº’å¢å¼ºã€‚åœ¨é¢„è®­ç»ƒäº†æ•°ä¸‡äº¿ Token åï¼Œæˆ‘ä»¬æå–äº†æ–‡æœ¬ç›¸å…³å‚æ•°ï¼Œæœ€ç»ˆè·å¾—äº† ERNIE-4.5-21B-A3B-Baseã€‚\n\næ¨¡å‹æ¦‚è¿°\n\nERNIE-4.5-21B-A3B-Base æ˜¯ä¸€ä¸ªæ–‡æœ¬ MoE åŸºç¡€æ¨¡å‹ï¼Œæ€»å‚æ•°é‡ä¸º 21Bï¼Œæ¯ä¸ª Token æ¿€æ´»å‚æ•°é‡ä¸º 3Bã€‚ä»¥ä¸‹æ˜¯æ¨¡å‹é…ç½®çš„è¯¦ç»†ä¿¡æ¯ï¼š\n\nå…³é”®å­—\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°é‡ï¼ˆæ€»/æ¿€æ´»ï¼‰\t21B / 3B\nå±‚æ•°\t28\nå¤´æ•°ï¼ˆQ/KVï¼‰\t20 / 4\næ–‡æœ¬ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 6\nè§†è§‰ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 6\nå…±äº«ä¸“å®¶\t2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ ERNIEKit è¿›è¡Œæ¨¡å‹å¾®è°ƒ\n\nERNIEKit æ˜¯åŸºäº PaddlePaddle å¼€å‘çš„è®­ç»ƒå·¥å…·åŒ…ï¼Œä¸“ä¸º ERNIE ç³»åˆ—å¼€æºå¤§å‹æ¨¡å‹è®¾è®¡ã€‚å®ƒä¸ºæŒ‡ä»¤å¾®è°ƒï¼ˆSFTã€LoRAï¼‰å’Œå¯¹é½è®­ç»ƒï¼ˆDPOï¼‰ç­‰åœºæ™¯æä¾›äº†å…¨é¢æ”¯æŒï¼Œç¡®ä¿æœ€ä¼˜æ€§èƒ½ã€‚\n\nä½¿ç”¨ç¤ºä¾‹ï¼š\n\n# Download model\nhuggingface-cli download baidu/ERNIE-4.5-21B-A3B-Base-Paddle --local-dir baidu/ERNIE-4.5-21B-A3B-Base-Paddle\n# SFT\nerniekit train examples/configs/ERNIE-4.5-21B-A3B/sft/run_sft_lora_8k.yaml model_name_or_path=baidu/ERNIE-4.5-21B-A3B-Base-Paddle\n# DPO\nerniekit train examples/configs/ERNIE-4.5-21B-A3B/dpo/run_dpo_lora_8k.yaml model_name_or_path=baidu/ERNIE-4.5-21B-A3B-Base-Paddle\n\n\nä¸ºäº†è·å–æ›´è¯¦å°½çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬å¸¦æœ‰LoRAçš„SFTã€å¤šGPUé…ç½®ä»¥åŠé«˜çº§è„šæœ¬ï¼Œè¯·å‚è€ƒ ERNIEKit å­˜å‚¨åº“ä¸­çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ã€‚\n\nFastDeploy æ¨ç†\n\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é€šè¿‡FastDeployå¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚å…³äºæ›´è¯¦ç»†çš„ç”¨æ³•æŒ‡å—ï¼Œè¯·å‚è€ƒ FastDeploy å­˜å‚¨åº“ã€‚\n\næ³¨æ„ï¼šå¯¹äºå•å¡éƒ¨ç½²ï¼Œè‡³å°‘éœ€è¦80Gçš„GPUå†…å­˜èµ„æºã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-21B-A3B-Base-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768\n       --max-num-seqs 32\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç‰‡æ®µç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨æ¨¡å‹æ ¹æ®ç»™å®šè¾“å…¥ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Base-PT\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\nprompt = \"Large language model is\"\nmodel_inputs = tokenizer([prompt], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\nresult = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\nprint(\"result:\", result)\n\nvLLM æ¨ç†\n\nvLLMç›®å‰æ­£åœ¨é€‚é…ä¸­ï¼Œä¼˜å…ˆè€ƒè™‘ä½¿ç”¨æˆ‘ä»¬åˆ†å‰çš„ä»“åº“ vllmã€‚æˆ‘ä»¬æ­£åœ¨ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢æ”¯æŒERNIE4.5æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nvllm serve baidu/ERNIE-4.5-21B-A3B-Base-PT --trust-remote-code\n\nè®¸å¯è¯\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache License 2.0 æä¾›ã€‚è¯¥è®¸å¯è¯åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶çš„å‰æä¸‹ï¼Œå…è®¸å•†ä¸šç”¨é€”ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nè‹¥æ‚¨è®¤ä¸º ERNIE 4.5 æœ‰ç”¨æˆ–å¸Œæœ›åœ¨å…¶é¡¹ç›®ä¸­ä½¿ç”¨ï¼Œè¯·åŠ¡å¿…å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-VL-424B-A47B-PT",
    "project_name": "ERNIE-4.5-VL-424B-A47B-PT",
    "readme": "Original Text\n   \næ–‡å¿ƒå¤§æ¨¡å‹4.5-å¤šæ¨¡æ€-4240äº¿å‚æ•°-A47Bæ¿€æ´»\næ–‡å¿ƒ4.5æ ¸å¿ƒæŠ€æœ¯äº®ç‚¹\n\næ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯åŸºäºæ··åˆä¸“å®¶æ¶æ„çš„A47Bå’ŒA3Bç‰ˆæœ¬ï¼‰çš„å“è¶Šèƒ½åŠ›æºè‡ªå¤šé¡¹å…³é”®æŠ€æœ¯çªç ´ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„æ··åˆä¸“å®¶é¢„è®­ç»ƒï¼šé€šè¿‡æ–‡æœ¬ä¸è§†è§‰æ¨¡æ€çš„è”åˆè®­ç»ƒï¼Œæ¨¡å‹èƒ½æ›´ç²¾å‡†æ•æ‰è·¨æ¨¡æ€ä¿¡æ¯ç‰¹å¾ï¼Œæ˜¾è‘—æå‡æ–‡æœ¬ç†è§£ç”Ÿæˆã€å›¾åƒè§£æåŠè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ä¸ºé¿å…æ¨¡æ€é—´å­¦ä¹ å¹²æ‰°ï¼Œæˆ‘ä»¬åˆ›æ–°è®¾è®¡äº†å¼‚æ„MoEæ¶æ„ï¼Œé‡‡ç”¨æ¨¡æ€éš”ç¦»è·¯ç”±æœºåˆ¶ï¼Œå¹¶ç»“åˆè·¯ç”±å™¨æ­£äº¤æŸå¤±ä¸å¤šæ¨¡æ€ä»¤ç‰Œå‡è¡¡æŸå¤±ï¼Œç¡®ä¿åŒæ¨¡æ€è¡¨å¾æ—¢ç‹¬ç«‹åˆäº’è¡¥ï¼Œå®ç°ååŒä¼˜åŒ–ã€‚\n\né«˜æ•ˆå¯æ‰©å±•çš„åŸºç¡€è®¾æ–½ï¼šæˆ‘ä»¬æå‡ºå¼‚æ„æ··åˆå¹¶è¡Œä¸åˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œç»“åˆèŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜ä¼˜åŒ–æµæ°´çº¿è°ƒåº¦ã€FP8æ··åˆç²¾åº¦è®­ç»ƒåŠç»†ç²’åº¦é‡è®¡ç®—æŠ€æœ¯ï¼Œå¤§å¹…æå‡é¢„è®­ç»ƒååé‡ã€‚æ¨ç†é˜¶æ®µé‡‡ç”¨å¤šä¸“å®¶å¹¶è¡Œåä½œæ–¹æ³•ä¸å·ç§¯ç¼–ç é‡åŒ–ç®—æ³•ï¼Œå®ç°4æ¯”ç‰¹/2æ¯”ç‰¹æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡PDè§£è€¦æ¶æ„ä¸åŠ¨æ€è§’è‰²åˆ‡æ¢æœºåˆ¶ï¼Œæ˜¾è‘—æå‡MoEæ¨¡å‹æ¨ç†æ•ˆç‡ã€‚åŸºäºé£æ¡¨æ¡†æ¶çš„ä¼˜åŒ–ï¼Œæ–‡å¿ƒ4.5å¯åœ¨å¤šç§ç¡¬ä»¶å¹³å°ä¸Šå®ç°é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ä¸“é¡¹åè®­ç»ƒï¼šä¸ºæ»¡è¶³å®é™…åº”ç”¨åœºæ™¯çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨¡æ€ä¸“é¡¹å¾®è°ƒã€‚é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸“æ³¨äºæ–‡æœ¬ç†è§£ä¸ç”Ÿæˆï¼›å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆVLMï¼‰å¼ºåŒ–è§†è§‰è¯­è¨€ç†è§£ï¼Œæ”¯æŒæ€ç»´é“¾ä¸éæ€ç»´é“¾åŒæ¨¡å¼ã€‚å„æ¨¡å‹é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€*ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æˆ–æ”¹è¿›ç‰ˆå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰*è¿›è¡Œåè®­ç»ƒã€‚\n\nåœ¨è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒé˜¶æ®µï¼Œå›¾æ–‡æ·±åº¦èåˆå¯¹å¤æ‚ä»»åŠ¡ï¼ˆå¦‚ç†è§£ã€æ¨ç†ã€ç”Ÿæˆï¼‰çš„è¡¨ç°èµ·å†³å®šæ€§ä½œç”¨ã€‚ä¸ºå¢å¼ºæ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æ³›åŒ–ä¸é€‚åº”èƒ½åŠ›ï¼Œæˆ‘ä»¬å›´ç»•å›¾åƒç†è§£ã€ä»»åŠ¡é€‚é…å¾®è°ƒå’Œå¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†ä¸‰å¤§æ ¸å¿ƒèƒ½åŠ›ï¼Œç³»ç»ŸåŒ–æ„å»ºæ•°æ®å¹¶ä¼˜åŒ–è®­ç»ƒç­–ç•¥ã€‚åŒæ—¶å¼•å…¥å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è¿›ä¸€æ­¥æå‡å¯¹é½æ•ˆæœã€‚ç»è¿‡SFTä¸RLé˜¶æ®µåï¼Œæœ€ç»ˆè·å¾—ERNIE-4.5-VL-424B-A47Bæ¨¡å‹ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-VL-424B-A47Bæ˜¯åŸºäºERNIE-4.5-VL-424B-A47B-Baseçš„å¤šæ¨¡æ€æ··åˆä¸“å®¶å¯¹è¯æ¨¡å‹ï¼Œæ€»å‚æ•°é‡4240äº¿ï¼Œå•tokenæ¿€æ´»å‚æ•°470äº¿ã€‚å…³é”®é…ç½®å¦‚ä¸‹ï¼š\n\nå‚æ•°é¡¹\tå€¼\næ¨¡æ€æ”¯æŒ\tæ–‡æœ¬ & è§†è§‰\nè®­ç»ƒé˜¶æ®µ\tåè®­ç»ƒ\nå‚æ•°é‡ï¼ˆæ€»é‡/æ¿€æ´»é‡ï¼‰\t4240äº¿/470äº¿\nç½‘ç»œå±‚æ•°\t54\næ³¨æ„åŠ›å¤´æ•°ï¼ˆQ/KVï¼‰\t64 / 8\næ–‡æœ¬ä¸“å®¶æ•°ï¼ˆæ€»é‡/æ¿€æ´»é‡ï¼‰\t64 / 8\nè§†è§‰ä¸“å®¶æ•°ï¼ˆæ€»é‡/æ¿€æ´»é‡ï¼‰\t64 / 8\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå¼€å§‹\nFastDeployæ¨ç†éƒ¨ç½²\n\nä½¿ç”¨FastDeployå¿«é€Ÿéƒ¨ç½²æœåŠ¡å¦‚ä¸‹ï¼Œè¯¦ç»†ç”¨æ³•è¯·å‚è€ƒFastDeploy GitHubä»“åº“ã€‚\n\næ³¨æ„ï¼šéœ€80GBæ˜¾å­˜GPU x 8ã€‚--quantizationå‚æ•°æ”¯æŒæŒ‡å®šwint4æˆ–wint8åˆ†åˆ«è¿›è¡Œ4æ¯”ç‰¹/8æ¯”ç‰¹é‡åŒ–éƒ¨ç½²ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-VL-424B-A47B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --tensor-parallel-size 8 \\\n       --quantization wint4 \\\n       --max-model-len 32768 \\\n       --enable-mm \\\n       --reasoning-parser ernie-45-vl \\\n       --max-num-seqs 32\n\n\nERNIE-4.5-VL æ¨¡å‹æ”¯æŒé€šè¿‡è¯·æ±‚å‚æ•°å¼€å¯æˆ–å…³é—­æ€è€ƒæ¨¡å¼ã€‚\n\nå¯ç”¨æ€è€ƒæ¨¡å¼\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": [\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://paddlenlp.bj.bcebos.com/datasets/paddlemix/demo_images/example2.jpg\"}},\n      {\"type\": \"text\", \"text\": \"Descript this image\"}\n    ]}\n  ],\n  \"metadata\": {\"enable_thinking\": true}\n}'\n\nå…³é—­æ€è€ƒæ¨¡å¼\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": [\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://paddlenlp.bj.bcebos.com/datasets/paddlemix/demo_images/example2.jpg\"}},\n      {\"type\": \"text\", \"text\": \"Descript this image\"}\n    ]}\n  ],\n  \"metadata\": {\"enable_thinking\": false}\n}'\n\nvLLM æ¨ç†æ”¯æŒ\n\næˆ‘ä»¬æ­£ä¸ç¤¾åŒºåˆä½œï¼Œå…¨åŠ›å®ç°å¯¹ ERNIE4.5 æ¨¡å‹çš„å®Œæ•´æ”¯æŒï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nè®¸å¯åè®®\n\nERNIE 4.5 æ¨¡å‹åŸºäº Apache License 2.0 æä¾›ã€‚è¯¥è®¸å¯è¯å…è®¸å•†ä¸šç”¨é€”ï¼Œä½†éœ€éµå®ˆå…¶æ¡æ¬¾ä¸æ¡ä»¶ã€‚ç‰ˆæƒæ‰€æœ‰ (c) 2025 ç™¾åº¦å…¬å¸ï¼Œä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨è¯´æ˜\n\nå¦‚æœæ‚¨è®¤ä¸º ERNIE 4.5 å¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¸®åŠ©ï¼Œæˆ–å¸Œæœ›åœ¨é¡¹ç›®ä¸­ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Image-Text-to-Text\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\", \"Image-Text-to-Text\", \"Multimodal Models\", \"Large Language Models\", \"Large Reasoning Models\", \"ERNIE Large Models\", \"image-to-text\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-Base-Paddle",
    "project_name": "ERNIE-4.5-300B-A47B-Base-Paddle",
    "readme": "Original Text\n   \nERNIE-4.5-300B-A47B-Base\nERNIE 4.5 æŠ€æœ¯äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„å…ˆè¿›èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—ï¼Œä¾èµ–äºå‡ é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼š\n\n**å¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š**æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰ä¸¤ç§æ¨¡æ€ä¸Šè”åˆè®­ç»ƒï¼Œæ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶æé«˜æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆã€å›¾åƒç†è§£å’Œè·¨æ¨¡æ€æ¨ç†ç­‰ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ºäº†é˜²æ­¢ä¸€ç§æ¨¡æ€é˜»ç¢å¦ä¸€ç§æ¨¡æ€çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥æ¨¡æ€éš”ç¦»è·¯ç”±ï¼Œå¹¶é‡‡ç”¨è·¯ç”±æ­£äº¤æŸå¤±å’Œå¤šæ¨¡æ€æ ‡è®°å¹³è¡¡æŸå¤±ã€‚è¿™äº›æ¶æ„é€‰æ‹©ç¡®ä¿ä¸¤ç§æ¨¡æ€éƒ½å¾—åˆ°æœ‰æ•ˆè¡¨å¾ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’ä¿ƒè¿›ã€‚\n\n**æ‰©å±•æ•ˆç‡åŒ–çš„åŸºç¡€è®¾æ–½ï¼š**æˆ‘ä»¬æå‡ºäº†ç”¨äº ERNIE 4.5 æ¨¡å‹é«˜æ•ˆè®­ç»ƒçš„æ–°å‹å¼‚æ„æ··åˆå¹¶è¡Œä¸»ä¹‰å’Œåˆ†å±‚è´Ÿè½½å¹³è¡¡ç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œä¸»ä¹‰ã€å†…å­˜æ•ˆç‡åŒ–çš„ç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†æ˜¾è‘—çš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šä¸“å®¶å¹¶è¡Œåä½œæ–¹æ³•å’Œå·ç§¯ä»£ç é‡åŒ–ç®—æ³•ï¼Œå®ç°äº† 4 ä½/2 ä½æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€è§’è‰²åˆ‡æ¢çš„ PD è§£èšï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œå¢å¼º ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚åŸºäº PaddlePaddleï¼ŒERNIE 4.5 åœ¨å¹¿æ³›çš„ç¡¬ä»¶å¹³å°ä¸Šæä¾›é«˜æ€§èƒ½æ¨ç†ã€‚\n\n**æ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š**ä¸ºäº†æ»¡è¶³ç°å®ä¸–ç•Œåº”ç”¨çš„å„ç§éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒæ¨¡æ€å˜ä½“è¿›è¡Œäº†å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLMs ä¼˜åŒ–ç”¨äºé€šç”¨è¯­è¨€ç†è§£å’Œç”Ÿæˆï¼ŒVLMs åˆ™ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹åœ¨åå¤©è®­ç»ƒä¸­ä½¿ç”¨äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æˆ–ä¸€ç§ä¿®æ”¹åçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåä¸ºç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰ã€‚\n\nä¸ºäº†ç¡®ä¿å¤šæ¨¡æ€è”åˆè®­ç»ƒçš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚åœ¨å‰ä¸¤ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬ä»…è®­ç»ƒä¸æ–‡æœ¬ç›¸å…³çš„å‚æ•°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå‘å±•å‡ºå¼ºå¤§çš„åŸºç¡€è¯­è¨€ç†è§£å’Œé•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚æœ€åçš„è·¨æ¨¡æ€é˜¶æ®µé€šè¿‡å¼•å…¥åŒ…æ‹¬ ViT å›¾åƒç‰¹å¾æå–ã€ç‰¹å¾è½¬æ¢é€‚é…å™¨å’Œè§†è§‰ä¸“å®¶åœ¨å†…çš„é™„åŠ å‚æ•°ï¼Œå°†èƒ½åŠ›æ‰©å±•åˆ°å›¾åƒå’Œè§†é¢‘ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ç›¸äº’ä¿ƒè¿›ã€‚åœ¨é¢„è®­ç»ƒäº†æ•°ä¸‡äº¿æ ‡è®°åï¼Œæˆ‘ä»¬æå–äº†æ–‡æœ¬ç›¸å…³çš„å‚æ•°ï¼Œæœ€ç»ˆå¾—åˆ°äº† ERNIE-4.5-300B-A47B-Baseã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-300B-A47B-Base æ˜¯ä¸€ä¸ªæ–‡æœ¬ MoE åŸºç¡€æ¨¡å‹ï¼Œæ‹¥æœ‰ 3000 äº¿æ€»å‚æ•°å’Œæ¯ä¸ªæ ‡è®°æ¿€æ´»çš„ 470 äº¿å‚æ•°ã€‚ä»¥ä¸‹æ˜¯æ¨¡å‹é…ç½®çš„è¯¦ç»†ä¿¡æ¯ï¼š\n\nå…³é”®å­—\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°ï¼ˆæ€»/æ¿€æ´»ï¼‰\t3000äº¿ / 470äº¿\nå±‚æ•°\t54\nå¤´éƒ¨ï¼ˆQ/KVï¼‰\t64 / 8\næ–‡æœ¬ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 8\nè§†è§‰ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 8\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ ERNIEKit è¿›è¡Œæ¨¡å‹å¾®è°ƒ\n\nERNIEKit æ˜¯åŸºäº PaddlePaddle çš„è®­ç»ƒå·¥å…·åŒ…ï¼Œä¸“é—¨ä¸º ERNIE ç³»åˆ—çš„å¼€æºå¤§å‹æ¨¡å‹è®¾è®¡ã€‚å®ƒä¸ºæŒ‡ä»¤å¾®è°ƒï¼ˆSFT, LoRAï¼‰å’Œ alignment è®­ç»ƒï¼ˆDPOï¼‰ç­‰åœºæ™¯æä¾›äº†å…¨é¢æ”¯æŒï¼Œç¡®ä¿æœ€ä¼˜æ€§èƒ½ã€‚\n\nä½¿ç”¨ç¤ºä¾‹ï¼š\n\n# Download model\nhuggingface-cli download baidu/ERNIE-4.5-300B-A47B-Base-Paddle --local-dir baidu/ERNIE-4.5-300B-A47B-Base-Paddle\n# SFT\nerniekit train examples/configs/ERNIE-4.5-300B-A47B/sft/run_sft_wint8mix_lora_8k.yaml model_name_or_path=baidu/ERNIE-4.5-300B-A47B-Base-Paddle\n# DPO\nerniekit train examples/configs/ERNIE-4.5-300B-A47B/dpo/run_dpo_wint8mix_lora_8k.yaml model_name_or_path=baidu/ERNIE-4.5-300B-A47B-Base-Paddle\n\n\nä»¥ä¸‹æ˜¯æ›´ä¸ºè¯¦ç»†çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨LoRAçš„SFTã€å¤šGPUé…ç½®å’Œé«˜çº§è„šæœ¬ï¼Œè¯·å‚è€ƒERNIEKitä»“åº“ä¸­çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ã€‚\n\nä½¿ç”¨FastDeploy\n\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é€šè¿‡FastDeployå¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚æ›´å¤šè¯¦ç»†ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚è§FastDeployä»“åº“ã€‚\n\næ³¨æ„ï¼šè‹¥è¦åœ¨å…·å¤‡è‡³å°‘80Gå†…å­˜çš„4ä¸ªGPUä¸Šè¿›è¡Œéƒ¨ç½²ï¼Œè¯·æŒ‡å®š--quantization wint4ã€‚å¦‚æœæ‚¨æŒ‡å®š--quantization wint8ï¼Œåˆ™éœ€è¦8ä¸ªGPUçš„èµ„æºã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-300B-A47B-Base-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --quantization wint4 \\\n       --tensor-parallel-size 8 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç‰‡æ®µç¤ºä¾‹ï¼Œè¯´æ˜å¦‚ä½•æ ¹æ®ç»™å®šçš„è¾“å…¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-300B-A47B-Base-PT\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\nprompt = \"Large language model is\"\nmodel_inputs = tokenizer([prompt], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\nresult = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\nprint(\"result:\", result)\n\nä½¿ç”¨ vLLM\n\nvLLM æ­£åœ¨é€‚é…ä¸­ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨æˆ‘ä»¬åˆ†å‰çš„ä»£ç åº“ vllmã€‚æˆ‘ä»¬æ­£åœ¨ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢æ”¯æŒ ERNIE4.5 æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\n# 80G * 16 GPU\nvllm serve baidu/ERNIE-4.5-300B-A47B-Base-PT --trust-remote-code\n\n# FP8 online quantification 80G * 8 GPU\nvllm serve baidu/ERNIE-4.5-300B-A47B-Base-PT --trust-remote-code --quantization fp8\n\nè®¸å¯åè®®\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache è®¸å¯åè®® 2.0 æä¾›ã€‚è¯¥è®¸å¯åè®®å…è®¸åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶çš„çš„å‰æä¸‹è¿›è¡Œå•†ä¸šä½¿ç”¨ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨å‘ç° ERNIE 4.5 æœ‰ç”¨æˆ–å¸Œæœ›åœ¨æ‚¨çš„é¡¹ç›®ä¸­ä½¿ç”¨ï¼Œæ•¬è¯·å–„æ„å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-FP8-Paddle",
    "project_name": "ERNIE-4.5-300B-A47B-FP8-Paddle",
    "readme": "Original Text\n   \nERNIE-4.5-300B-A47B\nERNIE 4.5 çš„äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„é«˜çº§åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—æ¨¡å‹ï¼Œç”±ä»¥ä¸‹å‡ é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°æ”¯æ’‘ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¸Šè”åˆè®­ç»ƒï¼Œæ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶æé«˜æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆã€å›¾åƒç†è§£å’Œè·¨æ¨¡æ€æ¨ç†ç­‰ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œåˆä¸è®©ä¸€ä¸ªæ¨¡æ€é˜»ç¢å¦ä¸€ä¸ªæ¨¡æ€çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº† å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥äº† æ¨¡æ€éš”ç¦»è·¯ç”± å¹¶ä½¿ç”¨äº† è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€ Token å¹³è¡¡æŸå¤±ã€‚è¿™äº›æ¶æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€éƒ½å¾—åˆ°äº†æœ‰æ•ˆçš„è¡¨ç¤ºï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°ç›¸äº’å¢å¼ºã€‚\n\nè§„æ¨¡æ•ˆç‡åŒ–åŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬æå‡ºäº†æ–°é¢–çš„å¼‚æ„æ··åˆå¹¶è¡Œå’Œåˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œä»¥é«˜æ•ˆè®­ç»ƒ ERNIE 4.5 æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜é«˜æ•ˆçš„ç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒå’Œç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†å“è¶Šçš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•å’Œ å·ç§¯ä»£ç é‡åŒ– ç®—æ³•ï¼Œä»¥å®ç° 4-bit/2-bit æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºåŠ¨æ€è§’è‰²åˆ‡æ¢çš„ PD è§£è€¦ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œå¢å¼º ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚åŸºäº PaddlePaddle æ„å»ºè€Œæˆï¼ŒERNIE 4.5 åœ¨å¹¿æ³›çš„ç¡¬ä»¶å¹³å°ä¸Šæä¾›é«˜æ€§èƒ½çš„æ¨ç†ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºæ»¡è¶³ç°å®ä¸–ç•Œåº”ç”¨çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„å˜ä½“è¿›è¡Œäº†ç‰¹å®šæ¨¡æ€çš„å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLMs ä¼˜åŒ–ç”¨äºé€šç”¨è¯­è¨€ç†è§£å’Œç”Ÿæˆã€‚VLMs ç€é‡äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹åœ¨åæœŸè®­ç»ƒä¸­éƒ½é‡‡ç”¨äº† ç›‘ç£å¾®è°ƒ (SFT)ã€ç›´æ¥åå¥½ä¼˜åŒ– (DPO) æˆ–ä¸€ç§åä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ– (UPO) çš„æ”¹è¿›å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-300B-A47B æ˜¯ä¸€ä¸ªæ–‡æœ¬ MoE åè®­ç»ƒæ¨¡å‹ï¼Œæ€»å…±æ‹¥æœ‰ 3000 äº¿ä¸ªå‚æ•°ï¼Œæ¯ä¸ª Token æ¿€æ´» 470 äº¿ä¸ªå‚æ•°ã€‚ä»¥ä¸‹ä¸ºæ¨¡å‹é…ç½®ç»†èŠ‚ï¼š\n\nå…³é”®å­—\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°ï¼ˆæ€»è®¡/æ¿€æ´»ï¼‰\t300B / 47B\nå±‚æ•°\t54\nå¤´éƒ¨ï¼ˆQ/KVï¼‰\t64 / 8\næ–‡æœ¬ä¸“å®¶ï¼ˆæ€»è®¡/æ¿€æ´»ï¼‰\t64 / 8\nè§†è§‰ä¸“å®¶ï¼ˆæ€»è®¡/æ¿€æ´»ï¼‰\t64 / 8\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ FastDeploy\n\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ FastDeploy å‘½ä»¤å¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚æœ‰å…³æ›´è¯¦ç»†çš„ç”¨æ³•è¯´æ˜ï¼Œè¯·å‚è€ƒ FastDeploy ä»“åº“ã€‚\n\næ³¨æ„ï¼š åœ¨é…å¤‡è‡³å°‘ 80G å†…å­˜ã€4 å— GPU çš„é…ç½®ä¸Šéƒ¨ç½²æ—¶ï¼Œè¯·æŒ‡å®š --quantization wint4ã€‚å¦‚æœæ‚¨æŒ‡å®š --quantization wint8ï¼Œåˆ™éœ€è¦ 8 å— GPU çš„èµ„æºã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-300B-A47B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --quantization wint4 \\\n       --tensor-parallel-size 8 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\n\nè¦ä½¿ç”¨ FastDeploy éƒ¨ç½² W4A8C8 é‡åŒ–ç‰ˆæœ¬ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-300B-A47B-W4A8C8-TP4-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --tensor-parallel-size 4 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\n\nè¦åœ¨å•ä¸ª141G GPUä¸Šä½¿ç”¨FastDeployéƒ¨ç½²WINT2é‡åŒ–ç‰ˆæœ¬ï¼Œæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model \"baidu/ERNIE-4.5-300B-A47B-2Bits-Paddle\" \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --tensor-parallel-size 1 \\\n       --max-model-len  32768 \\\n       --max-num-seqs 128\n\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ ERNIE-4.5-300B-A47B-FP8 æ ¹æ®ç»™å®šè¾“å…¥ç”Ÿæˆå†…å®¹ã€‚\n\nfrom fastdeploy import LLM, SamplingParams\n\nprompts = [\n    \"Hello, my name is\",\n]\n\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=128)\n\nmodel = \"baidu/ERNIE-4.5-300B-A47B-FP8-Paddle\"\nllm = LLM(model=model, tensor_parallel_size=8, max_model_len=8192, num_gpu_blocks_override=1024, engine_worker_queue_port=9981)\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs.text\n    print(\"generated_text\", generated_text)\n\næœ€ä½³å®è·µ\né‡‡æ ·å‚æ•°\n\nä¸ºäº†è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ Temperature=0.8ï¼ŒTopP=0.8ã€‚\n\nç½‘ç»œæœç´¢æç¤º\n\nå¯¹äºç½‘ç»œæœç´¢ï¼Œ{references}ã€{date} å’Œ {question} æ˜¯å‚æ•°ã€‚\n\nå¯¹äºä¸­æ–‡é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æç¤ºï¼š\n\nernie_search_zh_prompt = \\\n'''ä¸‹é¢ä½ ä¼šæ”¶åˆ°å½“å‰æ—¶é—´ã€å¤šä¸ªä¸åŒæ¥æºçš„å‚è€ƒæ–‡ç« å’Œä¸€æ®µå¯¹è¯ã€‚ä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»å¤šä¸ªå‚è€ƒæ–‡ç« ï¼Œå¹¶æ ¹æ®å‚è€ƒæ–‡ç« ä¸­çš„ä¿¡æ¯å›ç­”å¯¹è¯ä¸­çš„é—®é¢˜ã€‚\nä»¥ä¸‹æ˜¯å½“å‰æ—¶é—´å’Œå‚è€ƒæ–‡ç« ï¼š\n---------\n#å½“å‰æ—¶é—´\n{date}\n\n#å‚è€ƒæ–‡ç« \n{references}\n\n---------\nè¯·æ³¨æ„ï¼š\n1. å›ç­”å¿…é¡»ç»“åˆé—®é¢˜éœ€æ±‚å’Œå½“å‰æ—¶é—´ï¼Œå¯¹å‚è€ƒæ–‡ç« çš„å¯ç”¨æ€§è¿›è¡Œåˆ¤æ–­ï¼Œé¿å…åœ¨å›ç­”ä¸­ä½¿ç”¨é”™è¯¯æˆ–è¿‡æ—¶çš„ä¿¡æ¯ã€‚\n2. å½“å‚è€ƒæ–‡ç« ä¸­çš„ä¿¡æ¯æ— æ³•å‡†ç¡®åœ°å›ç­”é—®é¢˜æ—¶ï¼Œä½ éœ€è¦åœ¨å›ç­”ä¸­æä¾›è·å–ç›¸åº”ä¿¡æ¯çš„å»ºè®®ï¼Œæˆ–æ‰¿è®¤æ— æ³•æä¾›ç›¸åº”ä¿¡æ¯ã€‚\n3. ä½ éœ€è¦ä¼˜å…ˆæ ¹æ®ç™¾ç§‘ã€å®˜ç½‘ã€æƒå¨æœºæ„ã€ä¸“ä¸šç½‘ç«™ç­‰é«˜æƒå¨æ€§æ¥æºçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚\n4. å›å¤éœ€è¦ç»¼åˆå‚è€ƒæ–‡ç« ä¸­çš„ç›¸å…³æ•°å­—ã€æ¡ˆä¾‹ã€æ³•å¾‹æ¡æ–‡ã€å…¬å¼ç­‰ä¿¡æ¯ï¼Œä½¿ä½ çš„ç­”æ¡ˆæ›´ä¸“ä¸šã€‚\n5. å½“é—®é¢˜å±äºåˆ›ä½œç±»ä»»åŠ¡æ—¶ï¼Œéœ€æ³¨æ„ä»¥ä¸‹ç»´åº¦ï¼š\n   - æ€åº¦é²œæ˜ï¼šè§‚ç‚¹ã€ç«‹åœºæ¸…æ™°æ˜ç¡®ï¼Œé¿å…æ¨¡æ£±ä¸¤å¯ï¼Œè¯­è¨€æœæ–­ç›´æ¥\n   - æ–‡é‡‡é£æ‰¬ï¼šç”¨è¯ç²¾å‡†ç”ŸåŠ¨ï¼Œå–„ç”¨ä¿®è¾æ‰‹æ³•ï¼Œå¢å¼ºæ„ŸæŸ“åŠ›\n   - æœ‰ç†æœ‰æ®ï¼šé€»è¾‘ä¸¥å¯†é€’è¿›ï¼Œç»“åˆæƒå¨æ•°æ®/äº‹å®æ”¯æ’‘è®ºç‚¹\n---------\nä¸‹é¢è¯·ç»“åˆä»¥ä¸Šä¿¡æ¯ï¼Œå›ç­”é—®é¢˜ï¼Œè¡¥å…¨å¯¹è¯\n{question}'''\n\n\nå¯¹äºè‹±æ–‡é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨å¦‚ä¸‹æç¤ºï¼š\n\nernie_search_en_prompt = \\\n'''\nBelow you will be given the current time, multiple references from different sources, and a conversation. Your task is to read the references and use the information in them to answer the question in the conversation.\nHere are the current time and the references:\n---------\n#Current Time\n{date}\n\n#References\n{references}\n\n---------\nPlease note:\n1. Based on the questionâ€™s requirements and the current time, assess the usefulness of the references to avoid using inaccurate or outdated information in the answer.  \n2. If the references do not provide enough information to accurately answer the question, you should suggest how to obtain the relevant information or acknowledge that you are unable to provide it.  \n3. Prioritize using information from highly authoritative sources such as encyclopedias, official websites, authoritative institutions, and professional websites when answering questions.\n4. Incorporate relevant numbers, cases, legal provisions, formulas, and other details from the references to make your answer more professional.\n5. For creative tasks, keep these dimensions in mind:\n   - Clear attitude: Clear views and positions, avoid ambiguity, and use decisive and direct language\n   - Brilliant writing: Precise and vivid words, good use of rhetoric, and enhance the appeal\n   - Well-reasoned: Rigorous logic and progressive, combined with authoritative data/facts to support the argument\n\n---------\nNow, using the information above, answer the question and complete the conversation:  \n{question}'''\n\n\nå‚æ•°è¯´æ˜ï¼š\n\n{question} æ˜¯ç”¨æˆ·æå‡ºçš„é—®é¢˜\n{date} è¡¨ç¤ºå½“å‰æ—¶é—´ï¼Œå»ºè®®æ ¼å¼ä¸ºâ€œYYYY-MM-DD HH:MM:SSï¼Œæ˜ŸæœŸï¼ŒåŒ—äº¬/ä¸­å›½ã€‚â€\n{references} æ˜¯å‚è€ƒæ–‡çŒ®ï¼Œå»ºè®®æ ¼å¼ä¸ºï¼š\n##å‚è€ƒæ–‡ç« 1\næ ‡é¢˜ï¼šå‘¨æ°ä¼¦\næ–‡ç« å‘å¸ƒæ—¶é—´ï¼š2025-04-20\nå†…å®¹ï¼šå‘¨æ°ä¼¦(Jay Chou),1979å¹´1æœˆ18æ—¥å‡ºç”Ÿäºå°æ¹¾çœæ–°åŒ—å¸‚,ç¥–ç±ç¦å»ºçœæ°¸æ˜¥å¿,åè¯­æµè¡Œä¹ç”·æ­Œæ‰‹ã€éŸ³ä¹äººã€æ¼”å‘˜ã€å¯¼æ¼”ã€ç¼–å‰§,æ¯•ä¸šäºæ·¡æ±Ÿä¸­å­¦ã€‚2000å¹´,å‘è¡Œä¸ªäººé¦–å¼ éŸ³ä¹ä¸“è¾‘ã€ŠJayã€‹ã€‚...\næ¥æºç½‘ç«™ç½‘å€ï¼šbaike.baidu.com\næ¥æºç½‘ç«™çš„ç½‘ç«™åï¼šç™¾åº¦ç™¾ç§‘\n\n##å‚è€ƒæ–‡ç« 2\n...\n\nè®¸å¯è¯\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache è®¸å¯è¯ 2.0 æä¾›æœåŠ¡ã€‚è¯¥è®¸å¯è¯åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶çš„å‰æä¸‹ï¼Œå…è®¸å•†ä¸šä½¿ç”¨ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 ç™¾åº¦å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨å‘ç° ERNIE 4.5 æœ‰ç”¨ï¼Œæˆ–å¸Œæœ›åœ¨æ‚¨çš„é¡¹ç›®ä¸­ä½¿ç”¨å®ƒï¼Œè¯·åŠ¡å¿…å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-VL-28B-A3B-PT",
    "project_name": "ERNIE-4.5-VL-28B-A3B-PT",
    "readme": "Original Text\n   \næ–‡å¿ƒ4.5-VL-28B-A3B\næ–‡å¿ƒ4.5æ ¸å¿ƒäº®ç‚¹\n\næ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯åŸºäºMoEæ¶æ„çš„A47Bä¸A3Bç³»åˆ—ï¼‰çš„å“è¶Šèƒ½åŠ›æºäºå¤šé¡¹å…³é”®æŠ€æœ¯çªç ´ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„MoEé¢„è®­ç»ƒï¼šé€šè¿‡æ–‡æœ¬ä¸è§†è§‰æ¨¡æ€çš„è”åˆè®­ç»ƒï¼Œæ¨¡å‹èƒ½æ›´ç²¾å‡†æ•æ‰è·¨æ¨¡æ€ä¿¡æ¯ç‰¹å¾ï¼Œæå‡æ–‡æœ¬ç†è§£ç”Ÿæˆã€å›¾åƒç†è§£åŠè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ä¸ºé¿å…æ¨¡æ€é—´å­¦ä¹ å¹²æ‰°ï¼Œæˆ‘ä»¬åˆ›æ–°è®¾è®¡äº†å¼‚æ„MoEç»“æ„ï¼Œé‡‡ç”¨æ¨¡æ€éš”ç¦»è·¯ç”±æœºåˆ¶ï¼Œå¹¶å¼•å…¥è·¯ç”±å™¨æ­£äº¤æŸå¤±ä¸å¤šæ¨¡æ€ä»¤ç‰Œå‡è¡¡æŸå¤±ï¼Œç¡®ä¿åŒæ¨¡æ€é«˜æ•ˆè¡¨å¾ä¸ååŒä¼˜åŒ–ã€‚\n\né«˜æ•ˆæ‰©å±•åŸºç¡€è®¾æ–½ï¼šæå‡ºå¼‚æ„æ··åˆå¹¶è¡Œä¸åˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œç»“åˆèŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜ä¼˜åŒ–æµæ°´çº¿è°ƒåº¦ã€FP8æ··åˆç²¾åº¦è®­ç»ƒåŠç»†ç²’åº¦é‡è®¡ç®—æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡é¢„è®­ç»ƒååé‡ã€‚æ¨ç†é˜¶æ®µé‡‡ç”¨å¤šä¸“å®¶å¹¶è¡Œåä½œæ–¹æ³•ä¸å·ç§¯ç¼–ç é‡åŒ–ç®—æ³•ï¼Œå®ç°4æ¯”ç‰¹/2æ¯”ç‰¹æ— æŸé‡åŒ–ã€‚åŸºäºé£æ¡¨æ¡†æ¶çš„åŠ¨æ€è§’è‰²åˆ‡æ¢PDè§£è€¦æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ–‡å¿ƒ4.5 MoEæ¨¡å‹çš„æ¨ç†èµ„æºåˆ©ç”¨ç‡ä¸è·¨å¹³å°æ€§èƒ½ã€‚\n\næ¨¡æ€ä¸“é¡¹åè®­ç»ƒï¼šé’ˆå¯¹å®é™…åº”ç”¨åœºæ™¯éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨¡æ€ä¸“é¡¹å¾®è°ƒã€‚é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸“æ³¨æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆï¼›è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼ºåŒ–å›¾æ–‡äº¤äº’èƒ½åŠ›ï¼Œæ”¯æŒæ€ç»´é“¾ä¸éæ€ç»´é“¾åŒæ¨¡å¼ã€‚å„æ¨¡å‹é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€*ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æˆ–æ”¹è¿›ç‰ˆå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰*è¿›è¡Œåè®­ç»ƒã€‚\n\nåœ¨è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒé˜¶æ®µï¼Œè§†è§‰ä¸è¯­è¨€çš„æ·±åº¦èåˆå¯¹æ¨¡å‹çš„ç†è§£ã€æ¨ç†ã€ç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡è¡¨ç°å…·æœ‰å†³å®šæ€§å½±å“ã€‚æˆ‘ä»¬å›´ç»•å›¾åƒç†è§£ã€ä»»åŠ¡é€‚é…å¾®è°ƒã€å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†ä¸‰å¤§æ ¸å¿ƒèƒ½åŠ›ï¼Œç³»ç»Ÿæ„å»ºè®­ç»ƒæ•°æ®å¹¶ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒæ—¶å¼•å…¥å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è¿›ä¸€æ­¥æå‡å¯¹é½æ•ˆæœã€‚ç»è¿‡SFTä¸RLé˜¶æ®µè®­ç»ƒï¼Œæœ€ç»ˆè·å¾—æ–‡å¿ƒ4.5-VL-28B-A3Bæ¨¡å‹ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\næ–‡å¿ƒ4.5-VL-28B-A3Bä¸ºå¤šæ¨¡æ€MoEå¯¹è¯æ¨¡å‹ï¼Œæ€»å‚æ•°é‡280äº¿ï¼Œå•tokenæ¿€æ´»å‚æ•°é‡30äº¿ã€‚å…³é”®é…ç½®å¦‚ä¸‹ï¼š\n\nå‚æ•°é¡¹\tå€¼\næ¨¡æ€\tæ–‡æœ¬ & è§†è§‰\nè®­ç»ƒé˜¶æ®µ\tåè®­ç»ƒ\nå‚æ•°é‡ï¼ˆæ€»/æ¿€æ´»ï¼‰\t280äº¿/30äº¿\nå±‚æ•°\t28\næ³¨æ„åŠ›å¤´æ•°ï¼ˆQ/KVï¼‰\t20 / 4\næ–‡æœ¬ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 6\nè§†è§‰ä¸“å®¶ï¼ˆæ€»/æ¿€æ´»ï¼‰\t64 / 6\nå…±äº«ä¸“å®¶\t2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå¼€å§‹\nFastDeployæ¨ç†\n\nä½¿ç”¨FastDeployå¿«é€Ÿéƒ¨ç½²æœåŠ¡ï¼Œç¤ºä¾‹ä»£ç å¦‚ä¸‹ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚è€ƒFastDeploy GitHubä»“åº“ã€‚\n\næ³¨æ„ï¼šå•å¡éƒ¨ç½²éœ€è‡³å°‘80GBæ˜¾å­˜ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-VL-28B-A3B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --enable-mm \\\n       --reasoning-parser ernie-45-vl \\\n       --max-num-seqs 32\n\n\nERNIE-4.5-VL æ¨¡å‹æ”¯æŒé€šè¿‡è¯·æ±‚å‚æ•°å¼€å¯æˆ–å…³é—­æ€è€ƒæ¨¡å¼ã€‚\n\nå¯ç”¨æ€è€ƒæ¨¡å¼\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": [\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://paddlenlp.bj.bcebos.com/datasets/paddlemix/demo_images/example2.jpg\"}},\n      {\"type\": \"text\", \"text\": \"Descript this image\"}\n    ]}\n  ],\n  \"metadata\": {\"enable_thinking\": true}\n}'\n\nç¦ç”¨æ€è€ƒæ¨¡å¼\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": [\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://paddlenlp.bj.bcebos.com/datasets/paddlemix/demo_images/example2.jpg\"}},\n      {\"type\": \"text\", \"text\": \"Descript this image\"}\n    ]}\n  ],\n  \"metadata\": {\"enable_thinking\": false}\n}'\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨ transformers åº“è¿›è¡Œæ¨ç†çš„ç¤ºä¾‹ï¼š\n\nimport torch\nfrom transformers import AutoProcessor, AutoTokenizer, AutoModelForCausalLM\n\nmodel_path = 'baidu/ERNIE-4.5-VL-28B-A3B-PT'\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True\n)\n\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\nprocessor.eval()\nmodel.add_image_preprocess(processor)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the image.\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://paddlenlp.bj.bcebos.com/datasets/paddlemix/demo_images/example1.jpg\"}},\n        ]\n    },\n]\n\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nimage_inputs, video_inputs = processor.process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\n\ndevice = next(model.parameters()).device\ninputs = inputs.to(device)\n\ngenerated_ids = model.generate(\n    inputs=inputs['input_ids'].to(device),\n    **inputs,\n    max_new_tokens=128\n    )\noutput_text = processor.decode(generated_ids[0])\nprint(output_text)\n\nvLLM æ¨ç†æ”¯æŒ\n\næˆ‘ä»¬æ­£ä¸ç¤¾åŒºåˆä½œï¼Œå…¨åŠ›å®ç°å¯¹ ERNIE4.5 æ¨¡å‹çš„å®Œæ•´æ”¯æŒï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nè®¸å¯åè®®\n\nERNIE 4.5 æ¨¡å‹åŸºäº Apache License 2.0 æä¾›ã€‚è¯¥è®¸å¯å…è®¸å•†ä¸šç”¨é€”ï¼Œä½†éœ€éµå®ˆå…¶æ¡æ¬¾ä¸æ¡ä»¶ã€‚ç‰ˆæƒæ‰€æœ‰ (c) 2025 ç™¾åº¦å…¬å¸ï¼Œä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨è¯´æ˜\n\nè‹¥ ERNIE 4.5 å¯¹æ‚¨çš„ç ”ç©¶æˆ–é¡¹ç›®æœ‰æ‰€å¸®åŠ©ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Image-Text-to-Text\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-0.3B-PT",
    "project_name": "ERNIE-4.5-0.3B-PT",
    "readme": "Original Text\n   \nERNIE-4.5-0.3B\nERNIE 4.5 äº®ç‚¹\n\nERNIE 4.5 æ¨¡å‹çš„å…ˆè¿›èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åŸºäº MoE çš„ A47B å’Œ A3B ç³»åˆ—ï¼Œå¾—ç›Šäºä»¥ä¸‹å‡ é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„ MoE é¢„è®­ç»ƒï¼š æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰ä¸¤ç§æ¨¡æ€ä¸Šè”åˆè®­ç»ƒï¼Œä»¥æ›´å¥½åœ°æ•è·å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶æé«˜æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆã€å›¾åƒç†è§£å’Œè·¨æ¨¡æ€æ¨ç†ç­‰ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡è€Œä¸è®©ä¸€ç§æ¨¡æ€é˜»ç¢å¦ä¸€ç§æ¨¡æ€çš„å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº† å¼‚æ„ MoE ç»“æ„ï¼Œå¼•å…¥äº† æ¨¡æ€éš”ç¦»è·¯ç”±ï¼Œå¹¶é‡‡ç”¨äº† è·¯ç”±æ­£äº¤æŸå¤± å’Œ å¤šæ¨¡æ€ Token å¹³è¡¡æŸå¤±ã€‚è¿™äº›æ¶æ„é€‰æ‹©ç¡®ä¿äº†ä¸¤ç§æ¨¡æ€éƒ½å¾—åˆ°æœ‰æ•ˆè¡¨ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°ç›¸äº’å¢å¼ºã€‚\n\nè§„æ¨¡æ•ˆç‡åŒ–åŸºç¡€è®¾æ–½ï¼š æˆ‘ä»¬ä¸º ERNIE 4.5 æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒæå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼‚æ„æ··åˆå¹¶è¡Œæ€§å’Œåˆ†å±‚è´Ÿè½½å¹³è¡¡ç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œæ€§ã€å†…å­˜æ•ˆç‡åŒ–çš„ç®¡é“è°ƒåº¦ã€FP8 æ··åˆç²¾åº¦è®­ç»ƒä»¥åŠç»†ç²’åº¦é‡è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†æ˜¾è‘—çš„é¢„è®­ç»ƒååé‡ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº† å¤šä¸“å®¶å¹¶è¡Œåä½œ æ–¹æ³•å’Œ å·ç§¯ä»£ç é‡åŒ– ç®—æ³•ï¼Œä»¥å®ç° 4 ä½/2 ä½æ— æŸé‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† PD è§£è€¦å’ŒåŠ¨æ€è§’è‰²åˆ‡æ¢ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨èµ„æºï¼Œå¢å¼º ERNIE 4.5 MoE æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚åŸºäº PaddlePaddleï¼ŒERNIE 4.5 åœ¨å¹¿æ³›çš„ç¡¬ä»¶å¹³å°ä¸Šæä¾›é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ç‰¹å®šåè®­ç»ƒï¼š ä¸ºäº†æ»¡è¶³ç°å®ä¸–ç•Œåº”ç”¨çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒæ¨¡æ€å˜ä½“è¿›è¡Œäº†å¾®è°ƒã€‚æˆ‘ä»¬çš„ LLMs ä¼˜åŒ–ç”¨äºé€šç”¨ç›®çš„çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆã€‚VLMs ä¸“æ³¨äºè§†è§‰è¯­è¨€ç†è§£ï¼Œå¹¶æ”¯æŒæ€è€ƒå’Œå’Œéæ€è€ƒæ¨¡å¼ã€‚æ¯ä¸ªæ¨¡å‹åœ¨åæœŸè®­ç»ƒä¸­é‡‡ç”¨äº† ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ æˆ–ä¸€ç§åä¸º ç»Ÿä¸€åå¥½ä¼˜åŒ–ï¼ˆUPOï¼‰ çš„ä¿®æ”¹åå¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nERNIE-4.5-0.3B æ˜¯ä¸€ä¸ªæ–‡æœ¬å¯†é›†å‹åè®­ç»ƒæ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯æ¨¡å‹çš„é…ç½®ç»†èŠ‚ï¼š\n\nå…³é”®å­—\tå€¼\næ¨¡æ€\tæ–‡æœ¬\nè®­ç»ƒé˜¶æ®µ\tåè®­ç»ƒ\nå‚æ•°é‡\t0.36B\nå±‚æ•°\t18\nå¤´æ•°ï¼ˆQ/KVï¼‰\t16 / 2\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå…¥é—¨\nä½¿ç”¨ ERNIEKit è¿›è¡Œæ¨¡å‹å¾®è°ƒ\n\nERNIEKit æ˜¯åŸºäº PaddlePaddle çš„è®­ç»ƒå·¥å…·åŒ…ï¼Œä¸“ä¸º ERNIE ç³»åˆ—å¼€æºå¤§å‹æ¨¡å‹è®¾è®¡ã€‚å®ƒä¸ºæŒ‡ä»¤å¾®è°ƒï¼ˆSFTã€LoRAï¼‰å’Œå¯¹é½è®­ç»ƒï¼ˆDPOï¼‰ç­‰åœºæ™¯æä¾›å…¨é¢æ”¯æŒï¼Œç¡®ä¿æœ€ä¼˜æ€§èƒ½ã€‚\n\nä½¿ç”¨ç¤ºä¾‹ï¼š\n\n# Download Model\nhuggingface-cli download baidu/ERNIE-4.5-0.3B-Paddle --local-dir baidu/ERNIE-4.5-0.3B-Paddle\n# SFT\nerniekit train examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml\n# DPO\nerniekit train examples/configs/ERNIE-4.5-0.3B/dpo/run_dpo_8k.yaml\n\n\nä¸ºäº†è·å–æ›´è¯¦å°½çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ç»“åˆLoRAçš„SFTã€å¤šGPUé…ç½®ä»¥åŠé«˜çº§è„šæœ¬ï¼Œè¯·å‚è€ƒ ERNIEKit ä»“åº“ä¸­çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ã€‚\n\nFastDeploy æ¨ç†éƒ¨ç½²\n\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é€šè¿‡FastDeployå¿«é€Ÿå®ŒæˆæœåŠ¡éƒ¨ç½²ã€‚å…³äºæ›´è¯¦ç»†çš„ç”¨æ³•è¯´æ˜ï¼Œè¯·å‚é˜… FastDeploy ä»“åº“ã€‚\n\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-0.3B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n\nä½¿ç”¨ transformers åº“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•æ ¹æ®ç»™å®šè¾“å…¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-0.3B-PT\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=1024\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n\n# decode the generated ids\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\nprint(\"generate_text:\", generate_text)\n\nvLLM æ¨ç†\n\nvLLMç›®å‰æ­£åœ¨é€‚é…ä¸­ï¼Œå¯ä»¥ä¼˜å…ˆä½¿ç”¨æˆ‘ä»¬åˆ†å‰çš„ä»£ç åº“ vllmã€‚æˆ‘ä»¬æ­£åœ¨ä¸ç¤¾åŒºåˆä½œï¼Œå…¨é¢æ”¯æŒERNIE4.5æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nvllm serve baidu/ERNIE-4.5-0.3B-PT --trust-remote-code\n\nè®¸å¯è¯\n\nERNIE 4.5 æ¨¡å‹éµå¾ª Apache License 2.0 æä¾›ä½¿ç”¨ã€‚è¯¥è®¸å¯è¯åœ¨éµå®ˆå…¶æ¡æ¬¾å’Œæ¡ä»¶ä¸‹ï¼Œå…è®¸å•†ä¸šç”¨é€”ã€‚ç‰ˆæƒæ‰€æœ‰ï¼ˆcï¼‰2025 Baidu, Incã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨å‘ç° ERNIE 4.5 æœ‰ç”¨æˆ–å¸Œæœ›åœ¨æ‚¨çš„é¡¹ç›®ä¸­ä½¿ç”¨ï¼Œæ•¬è¯·å‹å¥½åœ°å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-VL-424B-A47B-Base-Paddle",
    "project_name": "ERNIE-4.5-VL-424B-A47B-Base-Paddle",
    "readme": "Original Text\n   \næ–‡å¿ƒ4.5-VL-424B-A47B-åŸºåº§æ¨¡å‹\næ–‡å¿ƒ4.5æŠ€æœ¯äº®ç‚¹\n\næ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯åŸºäºæ··åˆä¸“å®¶çš„A47Bå’ŒA3Bç‰ˆæœ¬ï¼‰çš„å“è¶Šèƒ½åŠ›æºäºä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯çªç ´ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„æ··åˆä¸“å®¶é¢„è®­ç»ƒï¼šé€šè¿‡æ–‡æœ¬ä¸è§†è§‰æ¨¡æ€çš„è”åˆè®­ç»ƒï¼Œæ¨¡å‹èƒ½æ›´ç²¾å‡†æ•æ‰è·¨æ¨¡æ€ä¿¡æ¯çš„å…³è”æ€§ï¼Œæ˜¾è‘—æå‡æ–‡æœ¬ç†è§£ç”Ÿæˆã€å›¾åƒç†è§£å’Œè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ä¸ºé¿å…æ¨¡æ€é—´ç›¸äº’å¹²æ‰°ï¼Œæˆ‘ä»¬åˆ›æ–°æ€§åœ°è®¾è®¡äº†å¼‚æ„æ··åˆä¸“å®¶æ¶æ„ï¼Œé‡‡ç”¨æ¨¡æ€éš”ç¦»è·¯ç”±æœºåˆ¶ï¼Œå¹¶å¼•å…¥è·¯ç”±å™¨æ­£äº¤æŸå¤±ä¸å¤šæ¨¡æ€ä»¤ç‰Œå‡è¡¡æŸå¤±ï¼Œç¡®ä¿åŒæ¨¡æ€ç‰¹å¾çš„é«˜æ•ˆè¡¨å¾ä¸ååŒä¼˜åŒ–ã€‚\n\né«˜æ•ˆå¯æ‰©å±•çš„æ¶æ„è®¾è®¡ï¼šä¸ºæå‡è®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºå¼‚æ„æ··åˆå¹¶è¡Œä¸åˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œç»“åˆèŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜ä¼˜åŒ–æµæ°´çº¿è°ƒåº¦ã€FP8æ··åˆç²¾åº¦è®­ç»ƒåŠç»†ç²’åº¦é‡è®¡ç®—æŠ€æœ¯ï¼Œå®ç°äº†æƒŠäººçš„é¢„è®­ç»ƒååé‡ã€‚æ¨ç†é˜¶æ®µé‡‡ç”¨å¤šä¸“å®¶å¹¶è¡Œåä½œæ–¹æ³•ä¸å·ç§¯ç¼–ç é‡åŒ–ç®—æ³•ï¼Œè¾¾æˆ4æ¯”ç‰¹/2æ¯”ç‰¹æ— æŸé‡åŒ–ã€‚é€šè¿‡åŠ¨æ€è§’è‰²åˆ‡æ¢çš„PDè§£è€¦æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡æ–‡å¿ƒ4.5æ··åˆä¸“å®¶æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚åŸºäºé£æ¡¨æ¡†æ¶çš„æ·±åº¦ä¼˜åŒ–ï¼Œæ–‡å¿ƒ4.5å¯åœ¨å¤šç§ç¡¬ä»¶å¹³å°ä¸Šå®ç°é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ä¸“é¡¹åè®­ç»ƒï¼šé’ˆå¯¹å®é™…åº”ç”¨åœºæ™¯çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨¡æ€ä¸“é¡¹å¾®è°ƒã€‚é€šç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸“æ³¨äºæ–‡æœ¬ç†è§£ä¸ç”Ÿæˆï¼›å¤šæ¨¡æ€å¤§æ¨¡å‹(VLM)å¼ºåŒ–è§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œæ”¯æŒæ€ç»´é“¾ä¸éæ€ç»´é“¾åŒæ¨¡å¼ã€‚å„æ¨¡å‹é‡‡ç”¨ç›‘ç£å¾®è°ƒ(SFT)ã€*ç›´æ¥åå¥½ä¼˜åŒ–(DPO)æˆ–æ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç»Ÿä¸€åå¥½ä¼˜åŒ–(UPO)*è¿›è¡Œåè®­ç»ƒã€‚\n\nä¸ºç¡®ä¿å¤šæ¨¡æ€è”åˆè®­ç»ƒçš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šå‰ä¸¤é˜¶æ®µä»…è®­ç»ƒæ–‡æœ¬ç›¸å…³å‚æ•°ï¼Œå¤¯å®è¯­è¨€ç†è§£ä¸é•¿æ–‡æœ¬å¤„ç†åŸºç¡€ï¼›æœ€ç»ˆå¤šæ¨¡æ€é˜¶æ®µå¼•å…¥ViTå›¾åƒç‰¹å¾æå–å™¨ã€ç‰¹å¾è½¬æ¢é€‚é…å™¨å’Œè§†è§‰ä¸“å®¶æ¨¡å—ï¼Œå®ç°å›¾æ–‡æ¨¡æ€çš„ç›¸äº’å¢å¼ºã€‚ç»è¿‡ä¸‡äº¿çº§tokenè®­ç»ƒï¼Œæœ€ç»ˆè·å¾—æ–‡å¿ƒ4.5-VL-424B-A47B-åŸºåº§æ¨¡å‹ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\næ–‡å¿ƒ4.5-VL-424B-A47B-åŸºåº§æ˜¯å¤šæ¨¡æ€æ··åˆä¸“å®¶æ¨¡å‹ï¼Œæ€»å‚æ•°é‡4240äº¿ï¼Œå•tokenæ¿€æ´»å‚æ•°é‡470äº¿ã€‚å…³é”®é…ç½®å¦‚ä¸‹ï¼š\n\nå‚æ•°é¡¹\tæ•°å€¼\næ¨¡æ€æ”¯æŒ\tæ–‡æœ¬ & è§†è§‰\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°é‡(æ€»/æ¿€æ´»)\t424B / 47B\nç½‘ç»œå±‚æ•°\t54\næ³¨æ„åŠ›å¤´æ•°(Q/KV)\t64 / 8\næ–‡æœ¬ä¸“å®¶æ•°(æ€»/æ¿€æ´»)\t64 / 8\nè§†è§‰ä¸“å®¶æ•°(æ€»/æ¿€æ´»)\t64 / 8\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå¼€å§‹\nvLLMæ¨ç†æ”¯æŒ\n\næˆ‘ä»¬æ­£ä¸ç¤¾åŒºåˆä½œå®Œå–„å¯¹æ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹çš„æ”¯æŒï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nè®¸å¯åè®®\n\næ–‡å¿ƒ4.5æ¨¡å‹åŸºäºApache License 2.0å¼€æºï¼Œå…è®¸ç¬¦åˆè®¸å¯æ¡æ¬¾çš„å•†ä¸šä½¿ç”¨ã€‚ç‰ˆæƒæ‰€æœ‰ Â© 2025 ç™¾åº¦åœ¨çº¿ç½‘ç»œæŠ€æœ¯æœ‰é™å…¬å¸ã€‚\n\nå¼•ç”¨å£°æ˜\n\nå¦‚æ‚¨ä½¿ç”¨æ–‡å¿ƒ4.5æ¨¡å‹æˆ–ç›¸å…³æŠ€æœ¯ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Image-Text-to-Text\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/paddlepaddle/ERNIE-4.5-VL-424B-A47B-Base-PT",
    "project_name": "ERNIE-4.5-VL-424B-A47B-Base-PT",
    "readme": "Original Text\n   \næ–‡å¿ƒå¤§æ¨¡å‹4.5-VL-424B-A47B-åŸºåº§ç‰ˆ\næ–‡å¿ƒ4.5æŠ€æœ¯äº®ç‚¹\n\næ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯åŸºäºæ··åˆä¸“å®¶æ¶æ„çš„A47Bå’ŒA3Bç‰ˆæœ¬ï¼‰çš„å“è¶Šèƒ½åŠ›æºäºä»¥ä¸‹å…³é”®æŠ€æœ¯çªç ´ï¼š\n\nå¤šæ¨¡æ€å¼‚æ„æ··åˆä¸“å®¶é¢„è®­ç»ƒï¼šé€šè¿‡æ–‡æœ¬ä¸è§†è§‰æ¨¡æ€çš„è”åˆè®­ç»ƒï¼Œæ¨¡å‹èƒ½æ›´ç²¾å‡†æ•æ‰è·¨æ¨¡æ€ä¿¡æ¯å…³è”ï¼Œæ˜¾è‘—æå‡æ–‡æœ¬ç†è§£ç”Ÿæˆã€å›¾åƒç†è§£åŠè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ä¸ºå®ç°æ¨¡æ€é—´ååŒè€Œéå¹²æ‰°ï¼Œæˆ‘ä»¬åˆ›æ–°è®¾è®¡äº†å¼‚æ„MoEæ¶æ„ï¼Œé‡‡ç”¨æ¨¡æ€éš”ç¦»è·¯ç”±æœºåˆ¶ï¼Œå¹¶å¼•å…¥è·¯ç”±å™¨æ­£äº¤æŸå¤±ä¸å¤šæ¨¡æ€ä»¤ç‰Œå‡è¡¡æŸå¤±ã€‚è¿™äº›è®¾è®¡ç¡®ä¿åŒæ¨¡æ€ç‰¹å¾çš„é«˜æ•ˆè¡¨å¾ï¼Œå½¢æˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ­£å‘äº’å¢å¼ºæ•ˆåº”ã€‚\n\né«˜æ•ˆæ‰©å±•åŸºç¡€è®¾æ–½ï¼šæˆ‘ä»¬æå‡ºåˆ›æ–°çš„å¼‚æ„æ··åˆå¹¶è¡Œä¸åˆ†å±‚è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œç»“åˆèŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œã€å†…å­˜ä¼˜åŒ–æµæ°´çº¿è°ƒåº¦ã€FP8æ··åˆç²¾åº¦è®­ç»ƒåŠç»†ç²’åº¦é‡è®¡ç®—æŠ€æœ¯ï¼Œå®ç°æƒŠäººçš„é¢„è®­ç»ƒååé‡ã€‚æ¨ç†é˜¶æ®µé‡‡ç”¨å¤šä¸“å®¶å¹¶è¡ŒååŒæ–¹æ³•ä¸å·ç§¯ç¼–ç é‡åŒ–ç®—æ³•ï¼Œè¾¾æˆ4æ¯”ç‰¹/2æ¯”ç‰¹æ— æŸé‡åŒ–ã€‚åŸºäºé£æ¡¨æ¡†æ¶çš„åŠ¨æ€è§’è‰²åˆ‡æ¢PDè§£è€¦æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ–‡å¿ƒ4.5 MoEæ¨¡å‹çš„æ¨ç†èµ„æºåˆ©ç”¨ç‡ï¼Œç¡®ä¿å…¨ç¡¬ä»¶å¹³å°çš„é«˜æ€§èƒ½æ¨ç†ã€‚\n\næ¨¡æ€ä¸“é¡¹åè®­ç»ƒï¼šä¸ºæ»¡è¶³å®é™…åº”ç”¨åœºæ™¯çš„å¤šæ ·åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨¡æ€ä¸“é¡¹å¾®è°ƒã€‚é€šç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸“æ³¨äºæ–‡æœ¬ç†è§£ä¸ç”Ÿæˆï¼Œè§†è§‰è¯­è¨€æ¨¡å‹(VLM)å¼ºåŒ–å›¾æ–‡ç†è§£èƒ½åŠ›å¹¶æ”¯æŒæ€ç»´é“¾ä¸éæ€ç»´é“¾åŒæ¨¡å¼ã€‚å„æ¨¡å‹é‡‡ç”¨ç›‘ç£å¾®è°ƒ(SFT)ã€*ç›´æ¥åå¥½ä¼˜åŒ–(DPO)æˆ–æ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç»Ÿä¸€åå¥½ä¼˜åŒ–(UPO)*è¿›è¡Œåè®­ç»ƒã€‚\n\nä¸ºç¡®ä¿å¤šæ¨¡æ€è”åˆè®­ç»ƒçš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šå‰ä¸¤é˜¶æ®µä¸“æ³¨æ–‡æœ¬å‚æ•°è®­ç»ƒï¼Œæ„å»ºå¼ºå¤§çš„è¯­è¨€ç†è§£ä¸é•¿æ–‡æœ¬å¤„ç†åŸºç¡€ï¼›æœ€ç»ˆé˜¶æ®µé€šè¿‡å¼•å…¥ViTå›¾åƒç‰¹å¾æå–å™¨ã€ç‰¹å¾è½¬æ¢é€‚é…å™¨åŠè§†è§‰ä¸“å®¶æ¨¡å—ï¼Œå°†èƒ½åŠ›æ‰©å±•è‡³å›¾åƒè§†é¢‘é¢†åŸŸï¼Œå®ç°æ–‡æœ¬ä¸è§†è§‰æ¨¡æ€çš„ååŒè¿›åŒ–ã€‚ç»è¿‡ä¸‡äº¿çº§tokenè®­ç»ƒï¼Œæœ€ç»ˆè·å¾—æ–‡å¿ƒ4.5-VL-424B-A47B-åŸºåº§ç‰ˆæ¨¡å‹ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\næ–‡å¿ƒ4.5-VL-424B-A47B-åŸºåº§ç‰ˆæ˜¯å¤šæ¨¡æ€æ··åˆä¸“å®¶åŸºåº§æ¨¡å‹ï¼Œæ€»å‚æ•°é‡4240äº¿ï¼Œå•tokenæ¿€æ´»å‚æ•°é‡470äº¿ã€‚æ ¸å¿ƒé…ç½®å¦‚ä¸‹ï¼š\n\nå…³é”®å‚æ•°\tæ•°å€¼\næ¨¡æ€æ”¯æŒ\tæ–‡æœ¬ & è§†è§‰\nè®­ç»ƒé˜¶æ®µ\té¢„è®­ç»ƒ\nå‚æ•°é‡(æ€»é‡/æ¿€æ´»é‡)\t4240äº¿/470äº¿\nå±‚æ•°\t54\næ³¨æ„åŠ›å¤´æ•°(Q/KV)\t64 / 8\næ–‡æœ¬ä¸“å®¶(æ€»é‡/æ¿€æ´»é‡)\t64 / 8\nè§†è§‰ä¸“å®¶(æ€»é‡/æ¿€æ´»é‡)\t64 / 8\nä¸Šä¸‹æ–‡é•¿åº¦\t131072\nå¿«é€Ÿå¼€å§‹\nvLLMæ¨ç†æ”¯æŒ\n\næˆ‘ä»¬æ­£ä¸ç¤¾åŒºåˆä½œå®Œå–„å¯¹æ–‡å¿ƒ4.5ç³»åˆ—æ¨¡å‹çš„å…¨é¢æ”¯æŒï¼Œæ•¬è¯·æœŸå¾…ã€‚\n\nè®¸å¯åè®®\n\næ–‡å¿ƒ4.5æ¨¡å‹åŸºäºApache 2.0è®¸å¯è¯å¼€æºï¼Œå…è®¸ç¬¦åˆè®¸å¯æ¡æ¬¾çš„å•†ä¸šä½¿ç”¨ã€‚ç‰ˆæƒæ‰€æœ‰ Â© 2025 ç™¾åº¦åœ¨çº¿ç½‘ç»œæŠ€æœ¯æœ‰é™å…¬å¸ã€‚\n\nå¼•ç”¨å£°æ˜\n\nå¦‚æ‚¨ä½¿ç”¨æ–‡å¿ƒ4.5æ¨¡å‹æˆ–ç›¸å…³æŠ€æœ¯ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼š\n\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n",
    "tags": "[\"Image-Text-to-Text\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/saulcy/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
    "project_name": "punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
    "readme": "Controllable Time-delay Transformeræ¨¡å‹ä»‹ç»\nHighlights\nä¸­æ–‡æ ‡ç‚¹é€šç”¨æ¨¡å‹ï¼šå¯ç”¨äºè¯­éŸ³è¯†åˆ«æ¨¡å‹è¾“å‡ºæ–‡æœ¬çš„æ ‡ç‚¹é¢„æµ‹ã€‚\nåŸºäºParaformer-largeé•¿éŸ³é¢‘æ¨¡å‹åœºæ™¯çš„ä½¿ç”¨\nåŸºäºFunASRæ¡†æ¶ï¼Œå¯è¿›è¡ŒASRï¼ŒVADï¼Œæ ‡ç‚¹çš„è‡ªç”±ç»„åˆ\nåŸºäºçº¯æ–‡æœ¬è¾“å…¥çš„æ ‡ç‚¹é¢„æµ‹\nFunASRå¼€æºé¡¹ç›®ä»‹ç»\n\nFunASRå¸Œæœ›åœ¨è¯­éŸ³è¯†åˆ«çš„å­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ä¹‹é—´æ¶èµ·ä¸€åº§æ¡¥æ¢ã€‚é€šè¿‡å‘å¸ƒå·¥ä¸šçº§è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜å¯ä»¥æ›´æ–¹ä¾¿åœ°è¿›è¡Œè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç ”ç©¶å’Œç”Ÿäº§ï¼Œå¹¶æ¨åŠ¨è¯­éŸ³è¯†åˆ«ç”Ÿæ€çš„å‘å±•ã€‚è®©è¯­éŸ³è¯†åˆ«æ›´æœ‰è¶£ï¼\n\ngithubä»“åº“ | æœ€æ–°åŠ¨æ€ | ç¯å¢ƒå®‰è£… | æœåŠ¡éƒ¨ç½² | æ¨¡å‹åº“ | è”ç³»æˆ‘ä»¬\n\næ¨¡å‹åŸç†ä»‹ç»\n\nControllable Time-delay Transformeræ˜¯è¾¾æ‘©é™¢è¯­éŸ³å›¢é˜Ÿæå‡ºçš„é«˜æ•ˆåå¤„ç†æ¡†æ¶ä¸­çš„æ ‡ç‚¹æ¨¡å—ã€‚æœ¬é¡¹ç›®ä¸ºä¸­æ–‡é€šç”¨æ ‡ç‚¹æ¨¡å‹ï¼Œæ¨¡å‹å¯ä»¥è¢«åº”ç”¨äºæ–‡æœ¬ç±»è¾“å…¥çš„æ ‡ç‚¹é¢„æµ‹ï¼Œä¹Ÿå¯åº”ç”¨äºè¯­éŸ³è¯†åˆ«ç»“æœçš„åå¤„ç†æ­¥éª¤ï¼ŒååŠ©è¯­éŸ³è¯†åˆ«æ¨¡å—è¾“å‡ºå…·æœ‰å¯è¯»æ€§çš„æ–‡æœ¬ç»“æœã€‚\n\nControllable Time-delay Transformer æ¨¡å‹ç»“æ„å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç”± Embeddingã€Encoder å’Œ Predictor ä¸‰éƒ¨åˆ†ç»„æˆã€‚Embedding æ˜¯è¯å‘é‡å åŠ ä½ç½®å‘é‡ã€‚Encoderå¯ä»¥é‡‡ç”¨ä¸åŒçš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚self-attentionï¼Œconformerï¼ŒSAN-Mç­‰ã€‚Predictor é¢„æµ‹æ¯ä¸ªtokenåçš„æ ‡ç‚¹ç±»å‹ã€‚\n\nåœ¨æ¨¡å‹çš„é€‰æ‹©ä¸Šé‡‡ç”¨äº†æ€§èƒ½ä¼˜è¶Šçš„Transformeræ¨¡å‹ã€‚Transformeræ¨¡å‹åœ¨è·å¾—è‰¯å¥½æ€§èƒ½çš„åŒæ—¶ï¼Œç”±äºæ¨¡å‹è‡ªèº«åºåˆ—åŒ–è¾“å…¥ç­‰ç‰¹æ€§ï¼Œä¼šç»™ç³»ç»Ÿå¸¦æ¥è¾ƒå¤§æ—¶å»¶ã€‚å¸¸è§„çš„Transformerå¯ä»¥çœ‹åˆ°æœªæ¥çš„å…¨éƒ¨ä¿¡æ¯ï¼Œå¯¼è‡´æ ‡ç‚¹ä¼šä¾èµ–å¾ˆè¿œçš„æœªæ¥ä¿¡æ¯ã€‚è¿™ä¼šç»™ç”¨æˆ·å¸¦æ¥ä¸€ç§æ ‡ç‚¹ä¸€ç›´åœ¨å˜åŒ–åˆ·æ–°ï¼Œé•¿æ—¶é—´ç»“æœä¸å›ºå®šçš„ä¸è‰¯æ„Ÿå—ã€‚åŸºäºè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›æ–°æ€§çš„æå‡ºäº†å¯æ§æ—¶å»¶çš„Transformeræ¨¡å‹ï¼ˆControllable Time-Delay Transformer, CT-Transformerï¼‰ï¼Œåœ¨æ¨¡å‹æ€§èƒ½æ— æŸå¤±çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæ§åˆ¶æ ‡ç‚¹çš„å»¶æ—¶ã€‚\n\næ›´è¯¦ç»†çš„ç»†èŠ‚è§ï¼š\n\nè®ºæ–‡ï¼š CONTROLLABLE TIME-DELAY TRANSFORMER FOR REAL-TIME PUNCTUATION PREDICTION AND DISFLUENCY DETECTION\nåŸºäºModelScopeè¿›è¡Œæ¨ç†\n\nä»¥ä¸‹ä¸ºä¸‰ç§æ”¯æŒæ ¼å¼åŠapiè°ƒç”¨æ–¹å¼å‚è€ƒå¦‚ä¸‹èŒƒä¾‹ï¼š\n\ntext.scpæ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚example/punc_example.txtï¼Œæ ¼å¼ä¸ºï¼š key + \"\\t\" + value\ncat example/punc_example.txt\n1       è·¨å¢ƒæ²³æµæ˜¯å…»è‚²æ²¿å²¸äººæ°‘çš„ç”Ÿå‘½ä¹‹æº\n2       ä»å­˜å‚¨ä¸Šæ¥è¯´ä»…ä»…æ˜¯å…¨æ™¯å›¾ç‰‡å®ƒå°±ä¼šæ˜¯å›¾ç‰‡çš„å››å€çš„å®¹é‡\n3       é‚£ä»Šå¤©çš„ä¼šå°±åˆ°è¿™é‡Œå§happy new yearæ˜å¹´è§\n\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\n\ninference_pipline = pipeline(\n    task=Tasks.punctuation,\n    model='damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch',\n    model_revision=\"v2.0.4\")\n\nrec_result = inference_pipline(input='example/punc_example.txt')\nprint(rec_result)\n\ntextäºŒè¿›åˆ¶æ•°æ®ï¼Œä¾‹å¦‚ï¼šç”¨æˆ·ç›´æ¥ä»æ–‡ä»¶é‡Œè¯»å‡ºbytesæ•°æ®\nrec_result = inference_pipline(input='æˆ‘ä»¬éƒ½æ˜¯æœ¨å¤´äººä¸ä¼šè®²è¯ä¸ä¼šåŠ¨')\n\ntextæ–‡ä»¶urlï¼Œä¾‹å¦‚ï¼šhttps://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_text/punc_example.txt\nrec_result = inference_pipline(input='https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_text/punc_example.txt')\n\nåŸºäºFunASRè¿›è¡Œæ¨ç†\n\nä¸‹é¢ä¸ºå¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹ï¼Œæµ‹è¯•éŸ³é¢‘ï¼ˆä¸­æ–‡ï¼Œè‹±æ–‡ï¼‰\n\nå¯æ‰§è¡Œå‘½ä»¤è¡Œ\n\nåœ¨å‘½ä»¤è¡Œç»ˆç«¯æ‰§è¡Œï¼š\n\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=vad_example.wav\n\n\næ³¨ï¼šæ”¯æŒå•æ¡éŸ³é¢‘æ–‡ä»¶è¯†åˆ«ï¼Œä¹Ÿæ”¯æŒæ–‡ä»¶åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸ºkaldié£æ ¼wav.scpï¼šwav_id wav_path\n\npythonç¤ºä¾‹\néå®æ—¶è¯­éŸ³è¯†åˆ«\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\", model_revision=\"v2.0.4\",\n                  vad_model=\"fsmn-vad\", vad_model_revision=\"v2.0.4\",\n                  punc_model=\"ct-punc-c\", punc_model_revision=\"v2.0.4\",\n                  # spk_model=\"cam++\", spk_model_revision=\"v2.0.2\",\n                  )\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\", \n            batch_size_s=300, \n            hotword='é­”æ­')\nprint(res)\n\n\næ³¨ï¼šmodel_hubï¼šè¡¨ç¤ºæ¨¡å‹ä»“åº“ï¼Œmsä¸ºé€‰æ‹©modelscopeä¸‹è½½ï¼Œhfä¸ºé€‰æ‹©huggingfaceä¸‹è½½ã€‚\n\nå®æ—¶è¯­éŸ³è¯†åˆ«\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\", model_revision=\"v2.0.4\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n\n\næ³¨ï¼šchunk_sizeä¸ºæµå¼å»¶æ—¶é…ç½®ï¼Œ[0,10,5]è¡¨ç¤ºä¸Šå±å®æ—¶å‡ºå­—ç²’åº¦ä¸º10*60=600msï¼Œæœªæ¥ä¿¡æ¯ä¸º5*60=300msã€‚æ¯æ¬¡æ¨ç†è¾“å…¥ä¸º600msï¼ˆé‡‡æ ·ç‚¹æ•°ä¸º16000*0.6=960ï¼‰ï¼Œè¾“å‡ºä¸ºå¯¹åº”æ–‡å­—ï¼Œæœ€åä¸€ä¸ªè¯­éŸ³ç‰‡æ®µè¾“å…¥éœ€è¦è®¾ç½®is_final=Trueæ¥å¼ºåˆ¶è¾“å‡ºæœ€åä¸€ä¸ªå­—ã€‚\n\nè¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆéå®æ—¶ï¼‰\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n\nè¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆå®æ—¶ï¼‰\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n\næ ‡ç‚¹æ¢å¤\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\", model_revision=\"v2.0.4\")\n\nres = model.generate(input=\"é‚£ä»Šå¤©çš„ä¼šå°±åˆ°è¿™é‡Œå§ happy new year æ˜å¹´è§\")\nprint(res)\n\næ—¶é—´æˆ³é¢„æµ‹\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\", model_revision=\"v2.0.4\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n\n\næ›´å¤šè¯¦ç»†ç”¨æ³•ï¼ˆç¤ºä¾‹ï¼‰\n\nå¾®è°ƒ\n\nè¯¦ç»†ç”¨æ³•ï¼ˆç¤ºä¾‹ï¼‰\n\nBenchmark\n\nä¸­æ–‡æ ‡ç‚¹é¢„æµ‹é€šç”¨æ¨¡å‹åœ¨è‡ªé‡‡é›†çš„é€šç”¨é¢†åŸŸä¸šåŠ¡åœºæ™¯æ•°æ®ä¸Šæœ‰è‰¯å¥½æ•ˆæœã€‚è®­ç»ƒæ•°æ®å¤§çº¦33Mä¸ªsampleï¼Œæ¯ä¸ªsampleå¯èƒ½åŒ…å«1å¥æˆ–å¤šå¥ã€‚\n\nè‡ªé‡‡é›†æ•°æ®ï¼ˆ20000+ samplesï¼‰\nprecision\trecall\tf1_score\n\n53.8\n\t\n60.0\n\t\n56.5\nä½¿ç”¨æ–¹å¼ä»¥åŠé€‚ç”¨èŒƒå›´\n\nè¿è¡ŒèŒƒå›´\n\næ”¯æŒLinux-x86_64ã€Macå’ŒWindowsè¿è¡Œã€‚\n\nä½¿ç”¨æ–¹å¼\n\nç›´æ¥æ¨ç†ï¼šå¯ä»¥ç›´æ¥å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œè®¡ç®—ï¼Œè¾“å‡ºå¸¦æœ‰æ ‡ç‚¹çš„ç›®æ ‡æ–‡å­—ã€‚\n\nä½¿ç”¨èŒƒå›´ä¸ç›®æ ‡åœºæ™¯\n\né€‚åˆå¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œæ ‡ç‚¹é¢„æµ‹ï¼Œæ–‡æœ¬é•¿åº¦ä¸é™ã€‚\nç›¸å…³è®ºæ–‡ä»¥åŠå¼•ç”¨ä¿¡æ¯\n@inproceedings{chen2020controllable,\n  title={Controllable Time-Delay Transformer for Real-Time Punctuation Prediction and Disfluency Detection},\n  author={Chen, Qian and Chen, Mengzhe and Li, Bo and Wang, Wen},\n  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={8069--8073},\n  year={2020},\n  organization={IEEE}\n}\n",
    "tags": "[\"PyTorch\", \"Chinese\", \"Apache License 2.0\", \"CPU\", \"CT-Transformer\", \"FunASR\", \"ICASSP 2020\", \"Alibaba\"]"
  },
  {
    "url": "https://gitcode.com/saulcy/speech_fsmn_vad_zh-cn-16k-common-pytorch",
    "project_name": "speech_fsmn_vad_zh-cn-16k-common-pytorch",
    "readme": "FSMN-Monophone VAD æ¨¡å‹ä»‹ç»\nHighlight\n16kä¸­æ–‡é€šç”¨VADæ¨¡å‹ï¼šå¯ç”¨äºæ£€æµ‹é•¿è¯­éŸ³ç‰‡æ®µä¸­æœ‰æ•ˆè¯­éŸ³çš„èµ·æ­¢æ—¶é—´ç‚¹ã€‚\nåŸºäºParaformer-largeé•¿éŸ³é¢‘æ¨¡å‹åœºæ™¯çš„ä½¿ç”¨\nåŸºäºFunASRæ¡†æ¶ï¼Œå¯è¿›è¡ŒASRï¼ŒVADï¼Œä¸­æ–‡æ ‡ç‚¹çš„è‡ªç”±ç»„åˆ\nåŸºäºéŸ³é¢‘æ•°æ®çš„æœ‰æ•ˆè¯­éŸ³ç‰‡æ®µèµ·æ­¢æ—¶é—´ç‚¹æ£€æµ‹\nFunASRå¼€æºé¡¹ç›®ä»‹ç»\n\nFunASRå¸Œæœ›åœ¨è¯­éŸ³è¯†åˆ«çš„å­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ä¹‹é—´æ¶èµ·ä¸€åº§æ¡¥æ¢ã€‚é€šè¿‡å‘å¸ƒå·¥ä¸šçº§è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜å¯ä»¥æ›´æ–¹ä¾¿åœ°è¿›è¡Œè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç ”ç©¶å’Œç”Ÿäº§ï¼Œå¹¶æ¨åŠ¨è¯­éŸ³è¯†åˆ«ç”Ÿæ€çš„å‘å±•ã€‚è®©è¯­éŸ³è¯†åˆ«æ›´æœ‰è¶£ï¼\n\ngithubä»“åº“ | æœ€æ–°åŠ¨æ€ | ç¯å¢ƒå®‰è£… | æœåŠ¡éƒ¨ç½² | æ¨¡å‹åº“ | è”ç³»æˆ‘ä»¬\n\næ¨¡å‹åŸç†ä»‹ç»\n\nFSMN-Monophone VADæ˜¯è¾¾æ‘©é™¢è¯­éŸ³å›¢é˜Ÿæå‡ºçš„é«˜æ•ˆè¯­éŸ³ç«¯ç‚¹æ£€æµ‹æ¨¡å‹ï¼Œç”¨äºæ£€æµ‹è¾“å…¥éŸ³é¢‘ä¸­æœ‰æ•ˆè¯­éŸ³çš„èµ·æ­¢æ—¶é—´ç‚¹ä¿¡æ¯ï¼Œå¹¶å°†æ£€æµ‹å‡ºæ¥çš„æœ‰æ•ˆéŸ³é¢‘ç‰‡æ®µè¾“å…¥è¯†åˆ«å¼•æ“è¿›è¡Œè¯†åˆ«ï¼Œå‡å°‘æ— æ•ˆè¯­éŸ³å¸¦æ¥çš„è¯†åˆ«é”™è¯¯ã€‚\n\nFSMN-Monophone VADæ¨¡å‹ç»“æ„å¦‚ä¸Šå›¾æ‰€ç¤ºï¼šæ¨¡å‹ç»“æ„å±‚é¢ï¼ŒFSMNæ¨¡å‹ç»“æ„å»ºæ¨¡æ—¶å¯è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè®­ç»ƒå’Œæ¨ç†é€Ÿåº¦å¿«ï¼Œä¸”æ—¶å»¶å¯æ§ï¼›åŒæ—¶æ ¹æ®VADæ¨¡å‹sizeä»¥åŠä½æ—¶å»¶çš„è¦æ±‚ï¼Œå¯¹FSMNçš„ç½‘ç»œç»“æ„ã€å³çœ‹å¸§æ•°è¿›è¡Œäº†é€‚é…ã€‚åœ¨å»ºæ¨¡å•å…ƒå±‚é¢ï¼Œspeechä¿¡æ¯æ¯”è¾ƒä¸°å¯Œï¼Œä»…ç”¨å•ç±»æ¥è¡¨å¾å­¦ä¹ èƒ½åŠ›æœ‰é™ï¼Œæˆ‘ä»¬å°†å•ä¸€speechç±»å‡çº§ä¸ºMonophoneã€‚å»ºæ¨¡å•å…ƒç»†åˆ†ï¼Œå¯ä»¥é¿å…å‚æ•°å¹³å‡ï¼ŒæŠ½è±¡å­¦ä¹ èƒ½åŠ›å¢å¼ºï¼ŒåŒºåˆ†æ€§æ›´å¥½ã€‚\n\nåŸºäºModelScopeè¿›è¡Œæ¨ç†\næ¨ç†æ”¯æŒéŸ³é¢‘æ ¼å¼å¦‚ä¸‹ï¼š\nwavæ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ï¼šdata/test/audios/vad_example.wav\nwavæ–‡ä»¶urlï¼Œä¾‹å¦‚ï¼šhttps://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav\nwaväºŒè¿›åˆ¶æ•°æ®ï¼Œæ ¼å¼bytesï¼Œä¾‹å¦‚ï¼šç”¨æˆ·ç›´æ¥ä»æ–‡ä»¶é‡Œè¯»å‡ºbytesæ•°æ®æˆ–è€…æ˜¯éº¦å…‹é£å½•å‡ºbytesæ•°æ®ã€‚\nå·²è§£æçš„audioéŸ³é¢‘ï¼Œä¾‹å¦‚ï¼šaudio, rate = soundfile.read(\"vad_example_zh.wav\")ï¼Œç±»å‹ä¸ºnumpy.ndarrayæˆ–è€…torch.Tensorã€‚\nwav.scpæ–‡ä»¶ï¼Œéœ€ç¬¦åˆå¦‚ä¸‹è¦æ±‚ï¼š\ncat wav.scp\nvad_example1  data/test/audios/vad_example1.wav\nvad_example2  data/test/audios/vad_example2.wav\n...\n\nè‹¥è¾“å…¥æ ¼å¼wavæ–‡ä»¶urlï¼Œapiè°ƒç”¨æ–¹å¼å¯å‚è€ƒå¦‚ä¸‹èŒƒä¾‹ï¼š\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\n\ninference_pipeline = pipeline(\n    task=Tasks.voice_activity_detection,\n    model='iic/speech_fsmn_vad_zh-cn-16k-common-pytorch',\n    model_revision=\"v2.0.4\",\n)\n\nsegments_result = inference_pipeline(input='https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav')\nprint(segments_result)\n\nè¾“å…¥éŸ³é¢‘ä¸ºpcmæ ¼å¼ï¼Œè°ƒç”¨apiæ—¶éœ€è¦ä¼ å…¥éŸ³é¢‘é‡‡æ ·ç‡å‚æ•°fsï¼Œä¾‹å¦‚ï¼š\nsegments_result = inference_pipeline(input='https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.pcm', fs=16000)\n\nè‹¥è¾“å…¥æ ¼å¼ä¸ºæ–‡ä»¶wav.scp(æ³¨ï¼šæ–‡ä»¶åéœ€è¦ä»¥.scpç»“å°¾)ï¼Œå¯æ·»åŠ  output_dir å‚æ•°å°†è¯†åˆ«ç»“æœå†™å…¥æ–‡ä»¶ä¸­ï¼Œå‚è€ƒç¤ºä¾‹å¦‚ä¸‹ï¼š\ninference_pipeline(input=\"wav.scp\", output_dir='./output_dir')\n\n\nè¯†åˆ«ç»“æœè¾“å‡ºè·¯å¾„ç»“æ„å¦‚ä¸‹ï¼š\n\ntree output_dir/\noutput_dir/\nâ””â”€â”€ 1best_recog\n    â””â”€â”€ text\n\n1 directory, 1 files\n\n\ntextï¼šVADæ£€æµ‹è¯­éŸ³èµ·æ­¢æ—¶é—´ç‚¹ç»“æœæ–‡ä»¶ï¼ˆå•ä½ï¼šmsï¼‰\n\nè‹¥è¾“å…¥éŸ³é¢‘ä¸ºå·²è§£æçš„audioéŸ³é¢‘ï¼Œapiè°ƒç”¨æ–¹å¼å¯å‚è€ƒå¦‚ä¸‹èŒƒä¾‹ï¼š\nimport soundfile\n\nwaveform, sample_rate = soundfile.read(\"vad_example_zh.wav\")\nsegments_result = inference_pipeline(input=waveform)\nprint(segments_result)\n\nVADå¸¸ç”¨å‚æ•°è°ƒæ•´è¯´æ˜ï¼ˆå‚è€ƒï¼švad.yamlæ–‡ä»¶ï¼‰ï¼š\nmax_end_silence_timeï¼šå°¾éƒ¨è¿ç»­æ£€æµ‹åˆ°å¤šé•¿æ—¶é—´é™éŸ³è¿›è¡Œå°¾ç‚¹åˆ¤åœï¼Œå‚æ•°èŒƒå›´500msï½6000msï¼Œé»˜è®¤å€¼800ms(è¯¥å€¼è¿‡ä½å®¹æ˜“å‡ºç°è¯­éŸ³æå‰æˆªæ–­çš„æƒ…å†µ)ã€‚\nspeech_noise_thresï¼šspeechçš„å¾—åˆ†å‡å»noiseçš„å¾—åˆ†å¤§äºæ­¤å€¼åˆ™åˆ¤æ–­ä¸ºspeechï¼Œå‚æ•°èŒƒå›´ï¼šï¼ˆ-1,1ï¼‰\nå–å€¼è¶Šè¶‹äº-1ï¼Œå™ªéŸ³è¢«è¯¯åˆ¤å®šä¸ºè¯­éŸ³çš„æ¦‚ç‡è¶Šå¤§ï¼ŒFAè¶Šé«˜\nå–å€¼è¶Šè¶‹äº+1ï¼Œè¯­éŸ³è¢«è¯¯åˆ¤å®šä¸ºå™ªéŸ³çš„æ¦‚ç‡è¶Šå¤§ï¼ŒPmissè¶Šé«˜\né€šå¸¸æƒ…å†µä¸‹ï¼Œè¯¥å€¼ä¼šæ ¹æ®å½“å‰æ¨¡å‹åœ¨é•¿è¯­éŸ³æµ‹è¯•é›†ä¸Šçš„æ•ˆæœå–balance\nåŸºäºFunASRè¿›è¡Œæ¨ç†\n\nä¸‹é¢ä¸ºå¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹ï¼Œæµ‹è¯•éŸ³é¢‘ï¼ˆä¸­æ–‡ï¼Œè‹±æ–‡ï¼‰\n\nå¯æ‰§è¡Œå‘½ä»¤è¡Œ\n\nåœ¨å‘½ä»¤è¡Œç»ˆç«¯æ‰§è¡Œï¼š\n\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=vad_example.wav\n\n\næ³¨ï¼šæ”¯æŒå•æ¡éŸ³é¢‘æ–‡ä»¶è¯†åˆ«ï¼Œä¹Ÿæ”¯æŒæ–‡ä»¶åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸ºkaldié£æ ¼wav.scpï¼šwav_id wav_path\n\npythonç¤ºä¾‹\néå®æ—¶è¯­éŸ³è¯†åˆ«\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\", model_revision=\"v2.0.4\",\n                  vad_model=\"fsmn-vad\", vad_model_revision=\"v2.0.4\",\n                  punc_model=\"ct-punc-c\", punc_model_revision=\"v2.0.4\",\n                  # spk_model=\"cam++\", spk_model_revision=\"v2.0.2\",\n                  )\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\", \n            batch_size_s=300, \n            hotword='é­”æ­')\nprint(res)\n\n\næ³¨ï¼šmodel_hubï¼šè¡¨ç¤ºæ¨¡å‹ä»“åº“ï¼Œmsä¸ºé€‰æ‹©modelscopeä¸‹è½½ï¼Œhfä¸ºé€‰æ‹©huggingfaceä¸‹è½½ã€‚\n\nå®æ—¶è¯­éŸ³è¯†åˆ«\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\", model_revision=\"v2.0.4\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n\n\næ³¨ï¼šchunk_sizeä¸ºæµå¼å»¶æ—¶é…ç½®ï¼Œ[0,10,5]è¡¨ç¤ºä¸Šå±å®æ—¶å‡ºå­—ç²’åº¦ä¸º10*60=600msï¼Œæœªæ¥ä¿¡æ¯ä¸º5*60=300msã€‚æ¯æ¬¡æ¨ç†è¾“å…¥ä¸º600msï¼ˆé‡‡æ ·ç‚¹æ•°ä¸º16000*0.6=960ï¼‰ï¼Œè¾“å‡ºä¸ºå¯¹åº”æ–‡å­—ï¼Œæœ€åä¸€ä¸ªè¯­éŸ³ç‰‡æ®µè¾“å…¥éœ€è¦è®¾ç½®is_final=Trueæ¥å¼ºåˆ¶è¾“å‡ºæœ€åä¸€ä¸ªå­—ã€‚\n\nè¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆéå®æ—¶ï¼‰\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n\nè¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆå®æ—¶ï¼‰\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n\næ ‡ç‚¹æ¢å¤\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\", model_revision=\"v2.0.4\")\n\nres = model.generate(input=\"é‚£ä»Šå¤©çš„ä¼šå°±åˆ°è¿™é‡Œå§ happy new year æ˜å¹´è§\")\nprint(res)\n\næ—¶é—´æˆ³é¢„æµ‹\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\", model_revision=\"v2.0.4\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n\n\næ›´å¤šè¯¦ç»†ç”¨æ³•ï¼ˆç¤ºä¾‹ï¼‰\n\nå¾®è°ƒ\n\nè¯¦ç»†ç”¨æ³•ï¼ˆç¤ºä¾‹ï¼‰\n\nä½¿ç”¨æ–¹å¼ä»¥åŠé€‚ç”¨èŒƒå›´\n\nè¿è¡ŒèŒƒå›´\n\næ”¯æŒLinux-x86_64ã€Macå’ŒWindowsè¿è¡Œã€‚\n\nä½¿ç”¨æ–¹å¼\n\nç›´æ¥æ¨ç†ï¼šå¯ä»¥ç›´æ¥å¯¹é•¿è¯­éŸ³æ•°æ®è¿›è¡Œè®¡ç®—ï¼Œæœ‰æ•ˆè¯­éŸ³ç‰‡æ®µçš„èµ·æ­¢æ—¶é—´ç‚¹ä¿¡æ¯ï¼ˆå•ä½ï¼šmsï¼‰ã€‚\nç›¸å…³è®ºæ–‡ä»¥åŠå¼•ç”¨ä¿¡æ¯\n@inproceedings{zhang2018deep,\n  title={Deep-FSMN for large vocabulary continuous speech recognition},\n  author={Zhang, Shiliang and Lei, Ming and Yan, Zhijie and Dai, Lirong},\n  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={5869--5873},\n  year={2018},\n  organization={IEEE}\n}\n",
    "tags": "[\"Voice Activity Detection\", \"PyTorch\", \"Chinese\", \"Apache License 2.0\", \"CPU\", \"FunASR\", \"Online\", \"FSMN\", \"Alibaba\"]"
  },
  {
    "url": "https://gitcode.com/saulcy/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
    "project_name": "speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
    "readme": "Highlights\nParaformer-largeé•¿éŸ³é¢‘æ¨¡å‹é›†æˆVADã€ASRã€æ ‡ç‚¹ä¸æ—¶é—´æˆ³åŠŸèƒ½ï¼Œå¯ç›´æ¥å¯¹æ—¶é•¿ä¸ºæ•°å°æ—¶éŸ³é¢‘è¿›è¡Œè¯†åˆ«ï¼Œå¹¶è¾“å‡ºå¸¦æ ‡ç‚¹æ–‡å­—ä¸æ—¶é—´æˆ³ï¼š\nASRæ¨¡å‹ï¼šParformer-largeæ¨¡å‹ç»“æ„ä¸ºéè‡ªå›å½’è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œå¤šä¸ªä¸­æ–‡å…¬å¼€æ•°æ®é›†ä¸Šå–å¾—SOTAæ•ˆæœï¼Œå¯å¿«é€Ÿåœ°åŸºäºModelScopeå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒå®šåˆ¶å’Œæ¨ç†ã€‚\nçƒ­è¯ç‰ˆæœ¬ï¼šParaformer-largeçƒ­è¯ç‰ˆæ¨¡å‹æ”¯æŒçƒ­è¯å®šåˆ¶åŠŸèƒ½ï¼ŒåŸºäºæä¾›çš„çƒ­è¯åˆ—è¡¨è¿›è¡Œæ¿€åŠ±å¢å¼ºï¼Œæå‡çƒ­è¯çš„å¬å›ç‡å’Œå‡†ç¡®ç‡ã€‚\nFunASRå¼€æºé¡¹ç›®ä»‹ç»\n\nFunASRå¸Œæœ›åœ¨è¯­éŸ³è¯†åˆ«çš„å­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ä¹‹é—´æ¶èµ·ä¸€åº§æ¡¥æ¢ã€‚é€šè¿‡å‘å¸ƒå·¥ä¸šçº§è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜å¯ä»¥æ›´æ–¹ä¾¿åœ°è¿›è¡Œè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç ”ç©¶å’Œç”Ÿäº§ï¼Œå¹¶æ¨åŠ¨è¯­éŸ³è¯†åˆ«ç”Ÿæ€çš„å‘å±•ã€‚è®©è¯­éŸ³è¯†åˆ«æ›´æœ‰è¶£ï¼\n\ngithubä»“åº“ | æœ€æ–°åŠ¨æ€ | ç¯å¢ƒå®‰è£… | æœåŠ¡éƒ¨ç½² | æ¨¡å‹åº“ | è”ç³»æˆ‘ä»¬\n\næ¨¡å‹åŸç†ä»‹ç»\n\nParaformeræ˜¯è¾¾æ‘©é™¢è¯­éŸ³å›¢é˜Ÿæå‡ºçš„ä¸€ç§é«˜æ•ˆçš„éè‡ªå›å½’ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«æ¡†æ¶ã€‚æœ¬é¡¹ç›®ä¸ºParaformerä¸­æ–‡é€šç”¨è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œé‡‡ç”¨å·¥ä¸šçº§æ•°ä¸‡å°æ—¶çš„æ ‡æ³¨éŸ³é¢‘è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä¿è¯äº†æ¨¡å‹çš„é€šç”¨è¯†åˆ«æ•ˆæœã€‚æ¨¡å‹å¯ä»¥è¢«åº”ç”¨äºè¯­éŸ³è¾“å…¥æ³•ã€è¯­éŸ³å¯¼èˆªã€æ™ºèƒ½ä¼šè®®çºªè¦ç­‰åœºæ™¯ã€‚\n\nParaformeræ¨¡å‹ç»“æ„å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç”± Encoderã€Predictorã€Samplerã€Decoder ä¸ Loss function äº”éƒ¨åˆ†ç»„æˆã€‚Encoderå¯ä»¥é‡‡ç”¨ä¸åŒçš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚self-attentionï¼Œconformerï¼ŒSAN-Mç­‰ã€‚Predictor ä¸ºä¸¤å±‚FFNï¼Œé¢„æµ‹ç›®æ ‡æ–‡å­—ä¸ªæ•°ä»¥åŠæŠ½å–ç›®æ ‡æ–‡å­—å¯¹åº”çš„å£°å­¦å‘é‡ã€‚Sampler ä¸ºæ— å¯å­¦ä¹ å‚æ•°æ¨¡å—ï¼Œä¾æ®è¾“å…¥çš„å£°å­¦å‘é‡å’Œç›®æ ‡å‘é‡ï¼Œç”Ÿäº§å«æœ‰è¯­ä¹‰çš„ç‰¹å¾å‘é‡ã€‚Decoder ç»“æ„ä¸è‡ªå›å½’æ¨¡å‹ç±»ä¼¼ï¼Œä¸ºåŒå‘å»ºæ¨¡ï¼ˆè‡ªå›å½’ä¸ºå•å‘å»ºæ¨¡ï¼‰ã€‚Loss function éƒ¨åˆ†ï¼Œé™¤äº†äº¤å‰ç†µï¼ˆCEï¼‰ä¸ MWER åŒºåˆ†æ€§ä¼˜åŒ–ç›®æ ‡ï¼Œè¿˜åŒ…æ‹¬äº† Predictor ä¼˜åŒ–ç›®æ ‡ MAEã€‚\n\nå…¶æ ¸å¿ƒç‚¹ä¸»è¦æœ‰ï¼š\n\nPredictor æ¨¡å—ï¼šåŸºäº Continuous integrate-and-fire (CIF) çš„ é¢„æµ‹å™¨ (Predictor) æ¥æŠ½å–ç›®æ ‡æ–‡å­—å¯¹åº”çš„å£°å­¦ç‰¹å¾å‘é‡ï¼Œå¯ä»¥æ›´åŠ å‡†ç¡®çš„é¢„æµ‹è¯­éŸ³ä¸­ç›®æ ‡æ–‡å­—ä¸ªæ•°ã€‚\nSamplerï¼šé€šè¿‡é‡‡æ ·ï¼Œå°†å£°å­¦ç‰¹å¾å‘é‡ä¸ç›®æ ‡æ–‡å­—å‘é‡å˜æ¢æˆå«æœ‰è¯­ä¹‰ä¿¡æ¯çš„ç‰¹å¾å‘é‡ï¼Œé…åˆåŒå‘çš„ Decoder æ¥å¢å¼ºæ¨¡å‹å¯¹äºä¸Šä¸‹æ–‡çš„å»ºæ¨¡èƒ½åŠ›ã€‚\nåŸºäºè´Ÿæ ·æœ¬é‡‡æ ·çš„ MWER è®­ç»ƒå‡†åˆ™ã€‚\n\næ›´è¯¦ç»†çš„ç»†èŠ‚è§ï¼š\n\nè®ºæ–‡ï¼š Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition\nè®ºæ–‡è§£è¯»ï¼šParaformer: é«˜è¯†åˆ«ç‡ã€é«˜è®¡ç®—æ•ˆç‡çš„å•è½®éè‡ªå›å½’ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«æ¨¡å‹\nåŸºäºModelScopeè¿›è¡Œæ¨ç†\næ¨ç†æ”¯æŒéŸ³é¢‘æ ¼å¼å¦‚ä¸‹ï¼š\nwavæ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ï¼šdata/test/audios/asr_example.wav\npcmæ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ï¼šdata/test/audios/asr_example.pcm\nwavæ–‡ä»¶urlï¼Œä¾‹å¦‚ï¼šhttps://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_zh.wav\nwaväºŒè¿›åˆ¶æ•°æ®ï¼Œæ ¼å¼bytesï¼Œä¾‹å¦‚ï¼šç”¨æˆ·ç›´æ¥ä»æ–‡ä»¶é‡Œè¯»å‡ºbytesæ•°æ®æˆ–è€…æ˜¯éº¦å…‹é£å½•å‡ºbytesæ•°æ®ã€‚\nå·²è§£æçš„audioéŸ³é¢‘ï¼Œä¾‹å¦‚ï¼šaudio, rate = soundfile.read(\"asr_example_zh.wav\")ï¼Œç±»å‹ä¸ºnumpy.ndarrayæˆ–è€…torch.Tensorã€‚\nwav.scpæ–‡ä»¶ï¼Œéœ€ç¬¦åˆå¦‚ä¸‹è¦æ±‚ï¼š\ncat wav.scp\nasr_example1  data/test/audios/asr_example1.wav\nasr_example2  data/test/audios/asr_example2.wav\n...\n\nè‹¥è¾“å…¥æ ¼å¼wavæ–‡ä»¶urlï¼Œapiè°ƒç”¨æ–¹å¼å¯å‚è€ƒå¦‚ä¸‹èŒƒä¾‹ï¼š\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\n\ninference_pipeline = pipeline(\n    task=Tasks.auto_speech_recognition,\n    model='iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch',\n    model_revision=\"v2.0.4\")\n\nrec_result = inference_pipeline('https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_vad_punc_example.wav')\nprint(rec_result)\n\nè¾“å…¥éŸ³é¢‘ä¸ºpcmæ ¼å¼ï¼Œè°ƒç”¨apiæ—¶éœ€è¦ä¼ å…¥éŸ³é¢‘é‡‡æ ·ç‡å‚æ•°audio_fsï¼Œä¾‹å¦‚ï¼š\nrec_result = inference_pipeline('https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_vad_punc_example.pcm', fs=16000)\n\nè¾“å…¥éŸ³é¢‘ä¸ºwavæ ¼å¼ï¼Œapiè°ƒç”¨æ–¹å¼å¯å‚è€ƒå¦‚ä¸‹èŒƒä¾‹:\nrec_result = inference_pipeline('asr_vad_punc_example.wav')\n\nè‹¥è¾“å…¥æ ¼å¼ä¸ºæ–‡ä»¶wav.scp(æ³¨ï¼šæ–‡ä»¶åéœ€è¦ä»¥.scpç»“å°¾)ï¼Œå¯æ·»åŠ  output_dir å‚æ•°å°†è¯†åˆ«ç»“æœå†™å…¥æ–‡ä»¶ä¸­ï¼Œapiè°ƒç”¨æ–¹å¼å¯å‚è€ƒå¦‚ä¸‹èŒƒä¾‹:\ninference_pipeline(\"wav.scp\", output_dir='./output_dir')\n\n\nè¯†åˆ«ç»“æœè¾“å‡ºè·¯å¾„ç»“æ„å¦‚ä¸‹ï¼š\n\ntree output_dir/\noutput_dir/\nâ””â”€â”€ 1best_recog\n    â”œâ”€â”€ score\n    â”œâ”€â”€ text\n\n1 directory, 4 files\n\n\nscoreï¼šè¯†åˆ«è·¯å¾„å¾—åˆ†\n\ntextï¼šè¯­éŸ³è¯†åˆ«ç»“æœæ–‡ä»¶\n\nè‹¥è¾“å…¥éŸ³é¢‘ä¸ºå·²è§£æçš„audioéŸ³é¢‘ï¼Œapiè°ƒç”¨æ–¹å¼å¯å‚è€ƒå¦‚ä¸‹èŒƒä¾‹ï¼š\nimport soundfile\n\nwaveform, sample_rate = soundfile.read(\"asr_vad_punc_example.wav\")\nrec_result = inference_pipeline(waveform)\n\nASRã€VADã€PUNCæ¨¡å‹è‡ªç”±ç»„åˆ\n\nå¯æ ¹æ®ä½¿ç”¨éœ€æ±‚å¯¹VADå’ŒPUNCæ ‡ç‚¹æ¨¡å‹è¿›è¡Œè‡ªç”±ç»„åˆï¼Œä½¿ç”¨æ–¹å¼å¦‚ä¸‹ï¼š\n\ninference_pipeline = pipeline(\n    task=Tasks.auto_speech_recognition,\n    model='iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch', model_revision=\"v2.0.4\",\n    vad_model='iic/speech_fsmn_vad_zh-cn-16k-common-pytorch', vad_model_revision=\"v2.0.4\",\n    punc_model='iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch', punc_model_revision=\"v2.0.3\",\n    # spk_model=\"iic/speech_campplus_sv_zh-cn_16k-common\",\n    # spk_model_revision=\"v2.0.2\",\n)\n\n\nè‹¥ä¸ä½¿ç”¨PUNCæ¨¡å‹ï¼Œå¯é…ç½®punc_model=\"\"ï¼Œæˆ–ä¸ä¼ å…¥punc_modelå‚æ•°ï¼Œå¦‚éœ€åŠ å…¥LMæ¨¡å‹ï¼Œå¯å¢åŠ é…ç½®lm_model='damo/speech_transformer_lm_zh-cn-common-vocab8404-pytorch'ï¼Œå¹¶è®¾ç½®lm_weightå’Œbeam_sizeå‚æ•°ã€‚\n\nåŸºäºFunASRè¿›è¡Œæ¨ç†\n\nä¸‹é¢ä¸ºå¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹ï¼Œæµ‹è¯•éŸ³é¢‘ï¼ˆä¸­æ–‡ï¼Œè‹±æ–‡ï¼‰\n\nå¯æ‰§è¡Œå‘½ä»¤è¡Œ\n\nåœ¨å‘½ä»¤è¡Œç»ˆç«¯æ‰§è¡Œï¼š\n\nfunasr +model=paraformer-zh +vad_model=\"fsmn-vad\" +punc_model=\"ct-punc\" +input=vad_example.wav\n\n\næ³¨ï¼šæ”¯æŒå•æ¡éŸ³é¢‘æ–‡ä»¶è¯†åˆ«ï¼Œä¹Ÿæ”¯æŒæ–‡ä»¶åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸ºkaldié£æ ¼wav.scpï¼šwav_id wav_path\n\npythonç¤ºä¾‹\néå®æ—¶è¯­éŸ³è¯†åˆ«\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\", model_revision=\"v2.0.4\",\n                  vad_model=\"fsmn-vad\", vad_model_revision=\"v2.0.4\",\n                  punc_model=\"ct-punc-c\", punc_model_revision=\"v2.0.4\",\n                  # spk_model=\"cam++\", spk_model_revision=\"v2.0.2\",\n                  )\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\", \n            batch_size_s=300, \n            hotword='é­”æ­')\nprint(res)\n\n\næ³¨ï¼šmodel_hubï¼šè¡¨ç¤ºæ¨¡å‹ä»“åº“ï¼Œmsä¸ºé€‰æ‹©modelscopeä¸‹è½½ï¼Œhfä¸ºé€‰æ‹©huggingfaceä¸‹è½½ã€‚\n\nå®æ—¶è¯­éŸ³è¯†åˆ«\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\", model_revision=\"v2.0.4\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n\n\næ³¨ï¼šchunk_sizeä¸ºæµå¼å»¶æ—¶é…ç½®ï¼Œ[0,10,5]è¡¨ç¤ºä¸Šå±å®æ—¶å‡ºå­—ç²’åº¦ä¸º10*60=600msï¼Œæœªæ¥ä¿¡æ¯ä¸º5*60=300msã€‚æ¯æ¬¡æ¨ç†è¾“å…¥ä¸º600msï¼ˆé‡‡æ ·ç‚¹æ•°ä¸º16000*0.6=960ï¼‰ï¼Œè¾“å‡ºä¸ºå¯¹åº”æ–‡å­—ï¼Œæœ€åä¸€ä¸ªè¯­éŸ³ç‰‡æ®µè¾“å…¥éœ€è¦è®¾ç½®is_final=Trueæ¥å¼ºåˆ¶è¾“å‡ºæœ€åä¸€ä¸ªå­—ã€‚\n\nè¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆéå®æ—¶ï¼‰\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n\nè¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆå®æ—¶ï¼‰\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n\næ ‡ç‚¹æ¢å¤\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\", model_revision=\"v2.0.4\")\n\nres = model.generate(input=\"é‚£ä»Šå¤©çš„ä¼šå°±åˆ°è¿™é‡Œå§ happy new year æ˜å¹´è§\")\nprint(res)\n\næ—¶é—´æˆ³é¢„æµ‹\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\", model_revision=\"v2.0.4\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n\n\næ›´å¤šè¯¦ç»†ç”¨æ³•ï¼ˆç¤ºä¾‹ï¼‰\n\nå¾®è°ƒ\n\nè¯¦ç»†ç”¨æ³•ï¼ˆç¤ºä¾‹ï¼‰\n\nBenchmark\n\nç»“åˆå¤§æ•°æ®ã€å¤§æ¨¡å‹ä¼˜åŒ–çš„Paraformeråœ¨ä¸€åºåˆ—è¯­éŸ³è¯†åˆ«çš„benchmarkä¸Šè·å¾—å½“å‰SOTAçš„æ•ˆæœï¼Œä»¥ä¸‹å±•ç¤ºå­¦æœ¯æ•°æ®é›†AISHELL-1ã€AISHELL-2ã€WenetSpeechï¼Œå…¬å¼€è¯„æµ‹é¡¹ç›®SpeechIO TIOBEç™½ç›’æµ‹è¯•åœºæ™¯çš„æ•ˆæœã€‚åœ¨å­¦æœ¯ç•Œå¸¸ç”¨çš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«è¯„æµ‹ä»»åŠ¡ä¸­ï¼Œå…¶è¡¨ç°è¿œè¿œè¶…äºç›®å‰å…¬å¼€å‘è¡¨è®ºæ–‡ä¸­çš„ç»“æœï¼Œè¿œå¥½äºå•ç‹¬å°é—­æ•°æ®é›†ä¸Šçš„æ¨¡å‹ã€‚æ­¤ç»“æœä¸ºParaformer-largeæ¨¡å‹åœ¨æ— VADå’Œæ ‡ç‚¹æ¨¡å‹ä¸‹çš„æµ‹è¯•ç»“æœã€‚\n\nAISHELL-1\nAISHELL-1 test\tw/o LM\tw/ LM\n\nEspnet\n\t\n4.90\n\t\n4.70\n\n\nWenet\n\t\n4.61\n\t\n4.36\n\n\nK2\n\t\n-\n\t\n4.26\n\n\nBlockformer\n\t\n4.29\n\t\n4.05\n\n\nParaformer-large\n\t\n1.95\n\t\n1.68\nAISHELL-2\n\tdev_ios\ttest_android\ttest_ios\ttest_mic\n\nEspnet\n\t\n5.40\n\t\n6.10\n\t\n5.70\n\t\n6.10\n\n\nWeNet\n\t\n-\n\t\n-\n\t\n5.39\n\t\n-\n\n\nParaformer-large\n\t\n2.80\n\t\n3.13\n\t\n2.85\n\t\n3.06\nWenetspeech\n\tdev\ttest_meeting\ttest_net\n\nEspnet\n\t\n9.70\n\t\n15.90\n\t\n8.80\n\n\nWeNet\n\t\n8.60\n\t\n17.34\n\t\n9.26\n\n\nK2\n\t\n7.76\n\t\n13.41\n\t\n8.71\n\n\nParaformer-large\n\t\n3.57\n\t\n6.97\n\t\n6.74\nSpeechIO TIOBE\n\nParaformer-largeæ¨¡å‹ç»“åˆTransformer-LMæ¨¡å‹åšshallow fusionï¼Œåœ¨å…¬å¼€è¯„æµ‹é¡¹ç›®SpeechIO TIOBEç™½ç›’æµ‹è¯•åœºæ™¯ä¸Šè·å¾—å½“å‰SOTAçš„æ•ˆæœï¼Œç›®å‰Transformer-LMæ¨¡å‹å·²åœ¨ModelScopeä¸Šå¼€æºï¼Œä»¥ä¸‹å±•ç¤ºSpeechIO TIOBEç™½ç›’æµ‹è¯•åœºæ™¯without LMã€with Transformer-LMçš„æ•ˆæœï¼š\n\nDecode config w/o LM:\nDecode without LM\nBeam size: 1\nDecode config w/ LM:\nDecode with Transformer-LM\nBeam size: 10\nLM weight: 0.15\ntestset\tw/o LM\tw/ LM\n\nSPEECHIO_ASR_ZH00001\n\t\n0.49\n\t\n0.35\n\n\nSPEECHIO_ASR_ZH00002\n\t\n3.23\n\t\n2.86\n\n\nSPEECHIO_ASR_ZH00003\n\t\n1.13\n\t\n0.80\n\n\nSPEECHIO_ASR_ZH00004\n\t\n1.33\n\t\n1.10\n\n\nSPEECHIO_ASR_ZH00005\n\t\n1.41\n\t\n1.18\n\n\nSPEECHIO_ASR_ZH00006\n\t\n5.25\n\t\n4.85\n\n\nSPEECHIO_ASR_ZH00007\n\t\n5.51\n\t\n4.97\n\n\nSPEECHIO_ASR_ZH00008\n\t\n3.69\n\t\n3.18\n\n\nSPEECHIO_ASR_ZH00009\n\t\n3.02\n\t\n2.78\n\n\nSPEECHIO_ASR_ZH000010\n\t\n3.35\n\t\n2.99\n\n\nSPEECHIO_ASR_ZH000011\n\t\n1.54\n\t\n1.25\n\n\nSPEECHIO_ASR_ZH000012\n\t\n2.06\n\t\n1.68\n\n\nSPEECHIO_ASR_ZH000013\n\t\n2.57\n\t\n2.25\n\n\nSPEECHIO_ASR_ZH000014\n\t\n3.86\n\t\n3.08\n\n\nSPEECHIO_ASR_ZH000015\n\t\n3.34\n\t\n2.67\nä½¿ç”¨æ–¹å¼ä»¥åŠé€‚ç”¨èŒƒå›´\n\nè¿è¡ŒèŒƒå›´\n\næ”¯æŒLinux-x86_64ã€Macå’ŒWindowsè¿è¡Œã€‚\n\nä½¿ç”¨æ–¹å¼\n\nç›´æ¥æ¨ç†ï¼šå¯ä»¥ç›´æ¥å¯¹è¾“å…¥éŸ³é¢‘è¿›è¡Œè§£ç ï¼Œè¾“å‡ºç›®æ ‡æ–‡å­—ã€‚\nå¾®è°ƒï¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œé‡‡ç”¨ç§æœ‰æˆ–è€…å¼€æºæ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚\n\nä½¿ç”¨èŒƒå›´ä¸ç›®æ ‡åœºæ™¯\n\né€‚åˆä¸ç¦»çº¿è¯­éŸ³è¯†åˆ«åœºæ™¯ï¼Œå¦‚å½•éŸ³æ–‡ä»¶è½¬å†™ï¼Œé…åˆGPUæ¨ç†æ•ˆæœæ›´åŠ ï¼Œè¾“å…¥éŸ³é¢‘æ—¶é•¿ä¸é™åˆ¶ï¼Œå¯ä»¥ä¸ºå‡ ä¸ªå°æ—¶éŸ³é¢‘ã€‚\næ¨¡å‹å±€é™æ€§ä»¥åŠå¯èƒ½çš„åå·®\n\nè€ƒè™‘åˆ°ç‰¹å¾æå–æµç¨‹å’Œå·¥å…·ä»¥åŠè®­ç»ƒå·¥å…·å·®å¼‚ï¼Œä¼šå¯¹CERçš„æ•°æ®å¸¦æ¥ä¸€å®šçš„å·®å¼‚ï¼ˆ<0.1%ï¼‰ï¼Œæ¨ç†GPUç¯å¢ƒå·®å¼‚å¯¼è‡´çš„RTFæ•°å€¼å·®å¼‚ã€‚\n\nç›¸å…³è®ºæ–‡ä»¥åŠå¼•ç”¨ä¿¡æ¯\n@inproceedings{gao2022paraformer,\n  title={Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition},\n  author={Gao, Zhifu and Zhang, Shiliang and McLoughlin, Ian and Yan, Zhijie},\n  booktitle={INTERSPEECH},\n  year={2022}\n}\n",
    "tags": "[\"Chinese\", \"Apache License 2.0\", \"INTERSPEECH 2022\", \"Alibaba\", \"Paraformer\", \"FunASR\"]"
  },
  {
    "url": "https://gitcode.com/ascend-tribe/openpangu-ultra-moe-718b-model",
    "project_name": "openPangu-Ultra-MoE-718B-model",
    "readme": "å¼€æºç›˜å¤ Ultra-MoE-718B\n\nä¸­æ–‡ | English\n\n1. ç®€ä»‹\n\nopenPangu-Ultra-MoE-718B æ˜¯åŸºäºæ˜‡è…¾NPUä»é›¶è®­ç»ƒçš„å¤§è§„æ¨¡æ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹ï¼Œæ€»å‚æ•°é‡ä¸º718Bï¼Œæ¿€æ´»å‚æ•°é‡ä¸º39Bã€‚openPangu-Ultra-MoE-718B è®­ç»ƒäº†çº¦19T tokensï¼Œå…·å¤‡å¿«æ…¢æ€è€ƒèåˆèƒ½åŠ›ã€‚\n\n2. æ¨¡å‹æ¶æ„\n\nopenPangu-Ultra-MoE-718B çš„æ¨¡å‹æ¶æ„é‡‡ç”¨äº†ä¸šç•Œä¸»æµçš„Multi-head Latent Attention (MLA)ã€Multi-Token Prediction (MTP)ã€å¤§ç¨€ç–æ¯”ç­‰æ¶æ„ï¼Œä»¥åŠä¸€äº›ç‰¹æœ‰çš„è®¾è®¡ï¼š\n\nDepth-Scaled Sandwich-Normå’ŒTinyInitï¼šé€šè¿‡è°ƒæ•´å±‚å½’ä¸€åŒ–ç»“æ„ä¸å‚æ•°åˆå§‹åŒ–ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚\nåŸºäºEP-Groupçš„è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼šé€šè¿‡ä¼˜åŒ–è´Ÿè½½å‡è¡¡æŸå¤±å‡½æ•°ï¼Œæ”¹å–„ä¸“å®¶ç‰¹åŒ–æ•ˆæœã€‚\n3. æµ‹è¯„ç»“æœ\næµ‹è¯„é›†\tæµ‹è¯„æŒ‡æ ‡\tæ…¢æ€è€ƒ\né€šç”¨èƒ½åŠ›\t\t\nC-Eval\tAcc\t91.06\nCLUEWSC\tAcc\t94.67\nMMLU-Pro\tExact Match\t82.40\nArenaHard_v0.1\tw/o Style Control\t96.80\nGPQA-Diamond\tAvg@4\t76.77\nSuperGPQA\tAcc\t61.67\nIF-Eval\tPrompt Strict\t80.59\nSysBench\tConstraint Satisfaction Rate\t91.43\næ•°å­¦èƒ½åŠ›\t\t\nCNMO 2024\tAvg@32\t80.73\nAIME25\tAvg@16\t75.21\nAIME24\tAvg@16\t80.21\nMATH-500\tAvg@1\t97.40\nä»£ç èƒ½åŠ›\t\t\nLiveCodeBench\tAvg@3 (01/25~05/25)\t61.14\nMBPP+\tAvg@2\t81.48\n\næ³¨ï¼š è¯„æµ‹è¿‡ç¨‹ä¸­ï¼Œsystem prompt ä¸ºç©ºã€‚\n\n4. éƒ¨ç½²å’Œä½¿ç”¨\n4.1 ç¯å¢ƒå‡†å¤‡\nç¡¬ä»¶è§„æ ¼\n\nAtlas 800T A2 (64GB, >=32å¡)ï¼Œé©±åŠ¨ä¸å›ºä»¶å®‰è£…åŒ…è·å–è¯·å‚ç…§[Atlas 800T A2]\n\nè½¯ä»¶ç¯å¢ƒ\n\næ–¹å¼ä¸€ï¼šåŸºäºè£¸æœºç¯å¢ƒå®‰è£…ä»¥ä¸‹é…å¥—è½¯ä»¶\n\næ“ä½œç³»ç»Ÿï¼šLinuxï¼ˆæ¨èopenEuler>=24.03ï¼‰\nCANN==8.1.RC1ï¼Œå®‰è£…å‡†å¤‡åŠæµç¨‹è¯·å‚ç…§[CANN Install]\npython==3.10\ntorch==2.1.0\ntorch-npu==2.1.0.post12\ntransformers>=4.48.2\n\næ–¹å¼äºŒï¼šä»dockeré•œåƒå¯åŠ¨å®¹å™¨\n\nå‚è€ƒ[Dockerä½¿ç”¨æŒ‡å—]\n\nä»¥ä¸Šè½¯ä»¶é…å¥—ç»è¿‡éªŒè¯ï¼Œç†è®ºå¯ä»¥æ”¯æŒæ›´é«˜çš„ç‰ˆæœ¬ï¼Œå¦‚æœ‰ç–‘é—®ï¼Œå¯ä»¥æäº¤issueã€‚\n\n4.2 æƒé‡å®Œæ•´æ€§æ ¡éªŒ\n\nè¯·å‚è€ƒä»¥ä¸‹æ–¹æ³•å¯¹ä¸‹è½½å†…å®¹è¿›è¡Œå®Œæ•´æ€§æ ¡éªŒï¼Œhash å€¼å­˜å‚¨åœ¨ checklist.chk æ–‡ä»¶ä¸­ã€‚\n\n#!/usr/bin/env bash\nARCH=$(uname -m)\nMODEL_PATH=\"${TARGET_FOLDER}/${MODEL_FOLDER_PATH}\"\ncd \"$MODEL_PATH\" || exit 1\nif [ \"$ARCH\" = \"arm64\" ]; then\n    sha256sum checklist.chk\nelse\n    sha256sum -c checklist.chk\nfi\n\n4.3 æ¨ç†æƒé‡è½¬æ¢\n\næœ¬æ¬¡æ ·ä¾‹ openPangu-Ultra-MoE-718B æ¨ç†é‡‡ç”¨ Tensor Parallel å¹¶è¡Œç­–ç•¥ï¼Œå åŠ æ˜‡è…¾ NPU èåˆå¤§ç®—å­ï¼Œéœ€è¦æå‰å¯¹ safetensors æƒé‡è¿›è¡Œåˆ‡åˆ†ï¼Œä¸‹è¿°å†…å®¹æä¾›32å¡å¹¶è¡Œæ¨ç†çš„æƒé‡åˆ‡åˆ†ç¤ºä¾‹ï¼Œåˆ‡åˆ†åçš„æƒé‡ä¼šä¿å­˜åœ¨model/ç›®å½•ä¸‹ï¼š\n\ncd inference\nbash split_weight.sh\n\n4.4 æ¨ç†æ ·ä¾‹\n\nopenPangu-Ultra-MoE-718B åœ¨ Atlas 800T A2 ä¸Š4æœº32å¡bfloat16æ¨ç†ç¤ºä¾‹ï¼Œä¸»èŠ‚ç‚¹é€‰å–èŠ‚ç‚¹IP0ï¼š\n\ncd inference\n# ä¸»èŠ‚ç‚¹IP0:  ${NNODES} ${NODE_RANK} ${NPROC_PER_NODE} ${MASTER_ADDR} ${PROMPT}\nbash generate.sh 4 0 8 IP0 \"3*7=?\"\n# ä»èŠ‚ç‚¹IP1\nbash generate.sh 4 1 8 IP0 \"3*7=?\"\n# ä»èŠ‚ç‚¹IP2\nbash generate.sh 4 2 8 IP0 \"3*7=?\"\n# ä»èŠ‚ç‚¹IP3\nbash generate.sh 4 3 8 IP0 \"3*7=?\"\n\n\næ¨¡å‹é»˜è®¤ä¸ºæ…¢æ€è€ƒæ¨¡å¼ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ‰‹æ®µåˆ‡æ¢è‡³å¿«æ€è€ƒæ¨¡å¼ï¼šå¦‚generate.pyç¤ºä¾‹ä¸­fast_thinking_templateæ‰€ç¤ºï¼Œåœ¨ç”¨æˆ·è¾“å…¥ç»“å°¾æ·»åŠ  /no_thinkæ ‡è®°å¯ä»¥å°†å½“å‰è½®æ¬¡åˆ‡æ¢è‡³å¿«æ€è€ƒæ¨¡å¼ã€‚\n\n4.5 ä½¿ç”¨æ¨ç†æ¡†æ¶\n\nvllm_ascendï¼šå‚è€ƒ[vllm_ascend_for_openPangu_ultra_moe_718b]\n\n5. æ¨¡å‹è®¸å¯è¯\n\né™¤æ–‡ä»¶ä¸­å¯¹å¼€æºè®¸å¯è¯å¦æœ‰çº¦å®šå¤–ï¼ŒopenPangu-Ultra-MoE-718B æ¨¡å‹æ ¹æ® OPENPANGU MODEL LICENSE AGREEMENT VERSION 1.0 æˆæƒï¼Œæ—¨åœ¨å…è®¸ä½¿ç”¨å¹¶ä¿ƒè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ¨¡å‹å­˜å‚¨åº“æ ¹ç›®å½•ä¸­çš„ LICENSE æ–‡ä»¶ã€‚\n\n6. å…è´£å£°æ˜\n\nç”±äº openPangu-Ultra-MoE-718B ï¼ˆâ€œæ¨¡å‹â€ï¼‰æ‰€ä¾èµ–çš„æŠ€æœ¯å›ºæœ‰çš„é™åˆ¶ï¼Œä»¥åŠäººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹æ˜¯ç”±ç›˜å¤è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œåä¸ºæ— æ³•å¯¹ä»¥ä¸‹äº‹é¡¹åšå‡ºä»»ä½•ä¿è¯ï¼š\n\nè¯¥æ¨¡å‹çš„è¾“å‡ºé€šè¿‡AIç®—æ³•è‡ªåŠ¨ç”Ÿæˆï¼Œä¸èƒ½æ’é™¤æŸäº›ä¿¡æ¯å¯èƒ½å­˜åœ¨ç¼ºé™·ã€ä¸åˆç†æˆ–å¼•èµ·ä¸é€‚çš„å¯èƒ½æ€§ï¼Œç”Ÿæˆçš„å†…å®¹ä¸ä»£è¡¨åä¸ºçš„æ€åº¦æˆ–ç«‹åœºï¼›\næ— æ³•ä¿è¯è¯¥æ¨¡å‹100%å‡†ç¡®ã€å¯é ã€åŠŸèƒ½é½å…¨ã€åŠæ—¶ã€å®‰å…¨ã€æ— é”™è¯¯ã€ä¸é—´æ–­ã€æŒç»­ç¨³å®šæˆ–æ— ä»»ä½•æ•…éšœï¼›\nè¯¥æ¨¡å‹çš„è¾“å‡ºå†…å®¹ä¸æ„æˆä»»ä½•å»ºè®®æˆ–å†³ç­–ï¼Œä¹Ÿä¸ä¿è¯ç”Ÿæˆçš„å†…å®¹çš„çœŸå®æ€§ã€å®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€åŠæ—¶æ€§ã€åˆæ³•æ€§ã€åŠŸèƒ½æ€§æˆ–å®ç”¨æ€§ã€‚ç”Ÿæˆçš„å†…å®¹ä¸èƒ½æ›¿ä»£åŒ»ç–—ã€æ³•å¾‹ç­‰é¢†åŸŸçš„ä¸“ä¸šäººå£«å›ç­”æ‚¨çš„é—®é¢˜ã€‚ç”Ÿæˆçš„å†…å®¹ä»…ä¾›å‚è€ƒï¼Œä¸ä»£è¡¨åä¸ºçš„ä»»ä½•æ€åº¦ã€ç«‹åœºæˆ–è§‚ç‚¹ã€‚æ‚¨éœ€è¦æ ¹æ®å®é™…æƒ…å†µåšå‡ºç‹¬ç«‹åˆ¤æ–­ï¼Œåä¸ºä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n7. åé¦ˆ\n\nå¦‚æœæœ‰ä»»ä½•æ„è§å’Œå»ºè®®ï¼Œè¯·æäº¤issueæˆ–è”ç³»openPangu@huawei.comã€‚",
    "tags": "[\"Transformers\", \"Safetensors\"]"
  },
  {
    "url": "https://gitcode.com/ascend-tribe/openpangu-embedded-1b-model",
    "project_name": "openPangu-Embedded-1B-model",
    "readme": "å¼€æºç›˜å¤ Embedded-1B\n\nä¸­æ–‡ | English\n\n1.ç®€ä»‹\n\nopenPangu-Embedded-1B æ˜¯åŸºäºæ˜‡è…¾ NPU ä»é›¶è®­ç»ƒçš„é«˜æ•ˆè¯­è¨€æ¨¡å‹ï¼Œå‚æ•°é‡ä¸º 1Bï¼ˆä¸å«è¯è¡¨Embeddingï¼‰ï¼Œæ¨¡å‹ç»“æ„é‡‡ç”¨ 26 å±‚ Dense æ¶æ„ï¼Œè®­ç»ƒäº†çº¦ 10T tokensã€‚é€šè¿‡æ˜‡è…¾ Atlas 200I A2å¯ç”¨çš„æ¨¡å‹æ¶æ„è®¾è®¡ã€æ•°æ®å’Œè®­ç»ƒç­–ç•¥ä¼˜åŒ–ï¼ŒopenPangu-Embedded-1B åœ¨ä¿æŒç«¯ä¾§è¿è¡Œçš„è¦æ±‚ä¸‹è¾¾åˆ°äº†è¾ƒé«˜çš„ç²¾åº¦ã€‚\n\n2. æ¨¡å‹æ¶æ„\n\nopenPangu-Embedded-1B æ˜¯ä¸€ä¸ªä¸ºç«¯ä¾§è®¾å¤‡è¿è¡Œè€Œè®¾è®¡çš„é«˜æ•ˆå¿«æ€è€ƒè¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒæ˜‡è…¾ Atlas 200I A2ã€‚\n\n\topenPangu-Embedded-1B\nArchitecture\tDense\nParameters (Non-Embedding)\t1B\nNumber of Layers\t26\nHidden Dimension\t1536\nAttention Mechanism\tGQA\nNumber of Attention Heads\t12 for Q, 6 for KV\nVocabulary Size\t153k\nContext Length (Natively)\t32k\nTraining Tokens\t10T\n3. æµ‹è¯„ç»“æœ\nè¯„æµ‹é›†\tæµ‹è¯„æŒ‡æ ‡\tå¿«æ€è€ƒ\né€šç”¨èƒ½åŠ›\t\t\nMMLU\tAcc\t60.72\nCMMLU\tAcc\t51.99\nC-Eval\tAcc\t60.98\nIF-Eval\tPrompt Strict\t56.56\nCLUEWSC\tAcc\t68.55\næ•°å­¦&æ¨ç†\t\t\nGSM8K\tAcc\t66.72\nMATH-500\tAcc\t52.00\nDROP\tF1\t50.31\nä»£ç èƒ½åŠ›\t\t\nMBPP\tPass@1\t54.09\nHumanEval\tPass@1\t56.71\n\næ³¨ï¼š è¯„æµ‹è¿‡ç¨‹ä¸­system prompt ä¸ºç©ºã€‚\n\n4. éƒ¨ç½²å’Œä½¿ç”¨\n4.1 ç¯å¢ƒå‡†å¤‡\nç¡¬ä»¶è§„æ ¼\n\nAtlas 800T A2 (64GB)ï¼Œé©±åŠ¨ä¸å›ºä»¶å®‰è£…åŒ…è·å–è¯·å‚ç…§ [Atlas 800T A2]ã€‚\n\nè½¯ä»¶ç¯å¢ƒ\næ“ä½œç³»ç»Ÿï¼šLinuxï¼ˆæ¨è openEuler>=24.03ï¼‰\nCANN==8.1.RC1ï¼Œå®‰è£…å‡†å¤‡åŠæµç¨‹è¯·å‚ç…§ [CANN Install]\npython==3.10\ntorch==2.1.0\ntorch-npu==2.1.0.post12\ntransformers==4.53.2\n\nä»¥ä¸Šè½¯ä»¶é…å¥—ç»è¿‡éªŒè¯ï¼Œç†è®ºå¯ä»¥æ”¯æŒæ›´é«˜ç‰ˆæœ¬ï¼Œå¦‚æœ‰ç–‘é—®ï¼Œå¯ä»¥æäº¤ issueã€‚\n\n4.2 æƒé‡å®Œæ•´æ€§æ ¡éªŒ\n\nè¯·å‚è€ƒä»¥ä¸‹æ–¹æ³•å¯¹ä¸‹è½½å†…å®¹è¿›è¡Œå®Œæ•´æ€§æ ¡éªŒï¼Œhash å€¼å­˜å‚¨åœ¨ checklist.chk æ–‡ä»¶ä¸­ã€‚\n\n#!/usr/bin/env bash\nARCH=$(uname -m)\nMODEL_PATH=\"${TARGET_FOLDER}/${MODEL_FOLDER_PATH}\"\ncd \"$MODEL_PATH\" || exit 1\nif [ \"$ARCH\" = \"arm64\" ]; then\n    sha256sum checklist.chk\nelse\n    sha256sum -c checklist.chk\nfi\n\n4.3 æ¨ç†æ ·ä¾‹\n\nä¸‹è¿°å†…å®¹æä¾› openPangu-Embedded-1B åœ¨ transformers æ¡†æ¶ä¸Šè¿›è¡Œæ¨ç†çš„ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼š\n\nè¿è¡Œå‰è¯·ä¿®æ”¹ generate.pyï¼Œæ·»åŠ æ¨¡å‹è·¯å¾„ã€‚\n\ncd inference\npython generate.py\n\n4.4 ä½¿ç”¨æ¨ç†æ¡†æ¶\n\nvllm_ascendï¼š å‚è€ƒ[vllm_ascend_for_openpangu_embedded_1b.zh]\n\næ˜‡è…¾ Atlas 200I A2æ¨ç†ï¼š openPangu-Embedded-1B æ¨¡å‹æ¨ç†å·²é€‚é…æ˜‡è…¾ MindIE 2.2.T10ï¼ˆ[ä¸‹è½½é“¾æ¥]ï¼‰ï¼Œæ”¯æŒ OrangePi AIpro (æ˜‡è…¾ Atlas 200I A2) æ¨ç†éƒ¨ç½²ã€‚å±Šæ—¶å¯å‰å¾€ [æ˜‡è…¾ç¤¾åŒºModelZoo] ä¸‹è½½é€‚é…ï¼Œä¸‹è½½é•œåƒå‰éœ€è¦ç”³è¯·æƒé™ï¼Œè€å¿ƒç­‰å¾…æƒé™ç”³è¯·é€šè¿‡åï¼Œæ ¹æ®æŒ‡å—ä¸‹è½½å¯¹åº”ç‰ˆæœ¬æ–‡ä»¶å’Œå®‰è£…æŒ‡å¯¼å®Œæˆæ¨ç†éƒ¨ç½²ã€‚\n\n5. æ¨¡å‹è®¸å¯è¯\n\né™¤æ–‡ä»¶ä¸­å¯¹å¼€æºè®¸å¯è¯å¦æœ‰çº¦å®šå¤–ï¼ŒopenPangu-Embedded-1B æ¨¡å‹æ ¹æ® OPENPANGU MODEL LICENSE AGREEMENT VERSION 1.0 æˆæƒï¼Œæ—¨åœ¨å…è®¸ä½¿ç”¨å¹¶ä¿ƒè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ¨¡å‹å­˜å‚¨åº“æ ¹ç›®å½•ä¸­çš„ LICENSE æ–‡ä»¶ã€‚\n\n6. å…è´£å£°æ˜\n\nç”±äº openPangu-Embedded-1Bï¼ˆâ€œæ¨¡å‹â€ï¼‰æ‰€ä¾èµ–çš„æŠ€æœ¯å›ºæœ‰çš„æŠ€æœ¯é™åˆ¶ï¼Œä»¥åŠäººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹æ˜¯ç”±ç›˜å¤è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œåä¸ºæ— æ³•å¯¹ä»¥ä¸‹äº‹é¡¹åšå‡ºä»»ä½•ä¿è¯ï¼š\n\nå°½ç®¡è¯¥æ¨¡å‹çš„è¾“å‡ºç”± AI ç®—æ³•ç”Ÿæˆï¼Œä½†ä¸èƒ½æ’é™¤æŸäº›ä¿¡æ¯å¯èƒ½å­˜åœ¨ç¼ºé™·ã€ä¸åˆç†æˆ–å¼•èµ·ä¸é€‚çš„å¯èƒ½æ€§ï¼Œç”Ÿæˆçš„å†…å®¹ä¸ä»£è¡¨åä¸ºçš„æ€åº¦æˆ–ç«‹åœºï¼›\næ— æ³•ä¿è¯è¯¥æ¨¡å‹ 100% å‡†ç¡®ã€å¯é ã€åŠŸèƒ½é½å…¨ã€åŠæ—¶ã€å®‰å…¨ã€æ— é”™è¯¯ã€ä¸é—´æ–­ã€æŒç»­ç¨³å®šæˆ–æ— ä»»ä½•æ•…éšœï¼›\nè¯¥æ¨¡å‹çš„è¾“å‡ºå†…å®¹ä¸æ„æˆä»»ä½•å»ºè®®æˆ–å†³ç­–ï¼Œä¹Ÿä¸ä¿è¯ç”Ÿæˆçš„å†…å®¹çš„çœŸå®æ€§ã€å®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€åŠæ—¶æ€§ã€åˆæ³•æ€§ã€åŠŸèƒ½æ€§æˆ–å®ç”¨æ€§ã€‚ç”Ÿæˆçš„å†…å®¹ä¸èƒ½æ›¿ä»£åŒ»ç–—ã€æ³•å¾‹ç­‰é¢†åŸŸçš„ä¸“ä¸šäººå£«å›ç­”æ‚¨çš„é—®é¢˜ã€‚ç”Ÿæˆçš„å†…å®¹ä»…ä¾›å‚è€ƒï¼Œä¸ä»£è¡¨åä¸ºçš„ä»»ä½•æ€åº¦ã€ç«‹åœºæˆ–è§‚ç‚¹ã€‚æ‚¨éœ€è¦æ ¹æ®å®é™…æƒ…å†µåšå‡ºç‹¬ç«‹åˆ¤æ–­ï¼Œåä¸ºä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n7. åé¦ˆ\n\nå¦‚æœæœ‰ä»»ä½•æ„è§å’Œå»ºè®®ï¼Œè¯·æäº¤issueæˆ–è”ç³» openPangu@huawei.comã€‚",
    "tags": "[\"Transformers\", \"Safetensors\"]"
  },
  {
    "url": "https://gitcode.com/ascend-tribe/openpangu-embedded-7b-model",
    "project_name": "openPangu-Embedded-7B-model",
    "readme": "å¼€æºç›˜å¤ Embedded-7B\n\nä¸­æ–‡ | English\n\n1. ç®€ä»‹\n\nopenPangu-Embedded-7B æ˜¯åŸºäºæ˜‡è…¾ NPU ä»é›¶è®­ç»ƒçš„é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°é‡ä¸º 7Bï¼ˆä¸å«è¯è¡¨Embeddingï¼‰ã€‚openPangu-Embedded-7B è®­ç»ƒäº†çº¦ 19T tokensï¼Œå…·å¤‡å¿«æ…¢æ€è€ƒèåˆèƒ½åŠ›ã€‚\n\n2. æ¨¡å‹æ¶æ„\n\topenPangu-Embedded-7B\nArchitecture\tDense\nParameters (Non-Embedding)\t7B\nNumber of Layers\t34\nHidden Dimension\t12800\nAttention Mechanism\tGQA\nNumber of Attention Heads\t32 for Qï¼Œ8 for KV\nVocabulary Size\t153k\nContext Length (Natively)\t32k\nPretraining Tokens\t19T\n3. æµ‹è¯„ç»“æœ\næµ‹è¯„é›†\tæµ‹è¯„æŒ‡æ ‡\tæ…¢æ€è€ƒ\né€šç”¨èƒ½åŠ›\t\t\nMMLU-Pro\tExact Match\t76.32\nCMMLU\tAcc\t75.59\nArenaHard_v0.1\tw/o style control\t85.80\nC-Eval\tAcc\t83.05\nGPQA-Diamond\tAvg@4\t70.54\næ•°å­¦èƒ½åŠ›\t\t\nMATH-500\tAvg@1\t95.00\nAIME24\tAvg@16\t71.57\nAIME25\tAvg@16\t58.24\nä»£ç èƒ½åŠ›\t\t\nLiveCodeBench\tAvg@2 (08/24~01/25)\t54.04\nMBPP+\tAvg@2\t76.06\n\næ³¨ï¼š è¯„æµ‹è¿‡ç¨‹ä¸­system prompt ä¸ºç©ºï¼Œä¸”ä¸æ·»åŠ ä»»ä½•é¢å¤–çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºã€‚è¯„æµ‹é‡‡ç”¨ 128k çš„åºåˆ—é•¿åº¦è¿›è¡Œã€‚\n\n4. éƒ¨ç½²å’Œä½¿ç”¨\n4.1 ç¯å¢ƒå‡†å¤‡\nç¡¬ä»¶è§„æ ¼\n\nAtlas 800T A2 (64GB)ï¼Œé©±åŠ¨ä¸å›ºä»¶å®‰è£…åŒ…è·å–è¯·å‚ç…§ [Atlas 800T A2]ã€‚\n\nè½¯ä»¶ç¯å¢ƒ\næ“ä½œç³»ç»Ÿï¼šLinuxï¼ˆæ¨è openEuler>=24.03ï¼‰\nCANN==8.1.RC1ï¼Œå®‰è£…å‡†å¤‡åŠæµç¨‹è¯·å‚ç…§ [CANN Install]\npython==3.10\ntorch==2.1.0\ntorch-npu==2.1.0.post12\ntransformers==4.53.2\n\nä»¥ä¸Šè½¯ä»¶é…å¥—ç»è¿‡éªŒè¯ï¼Œç†è®ºå¯ä»¥æ”¯æŒæ›´é«˜ç‰ˆæœ¬ï¼Œå¦‚æœ‰ç–‘é—®ï¼Œå¯ä»¥æäº¤ issueã€‚\n\n4.2 æƒé‡å®Œæ•´æ€§æ ¡éªŒ\n\nè¯·å‚è€ƒä»¥ä¸‹æ–¹æ³•å¯¹ä¸‹è½½å†…å®¹è¿›è¡Œå®Œæ•´æ€§æ ¡éªŒï¼Œhash å€¼å­˜å‚¨åœ¨ checklist.chk æ–‡ä»¶ä¸­ã€‚\n\n#!/usr/bin/env bash\nARCH=$(uname -m)\nMODEL_PATH=\"${TARGET_FOLDER}/${MODEL_FOLDER_PATH}\"\ncd \"$MODEL_PATH\" || exit 1\nif [ \"$ARCH\" = \"arm64\" ]; then\n    sha256sum checklist.chk\nelse\n    sha256sum -c checklist.chk\nfi\n\n4.3 æ¨ç†æ ·ä¾‹\n\nä¸‹è¿°å†…å®¹æä¾› openPangu-Embedded-7B åœ¨ transformers æ¡†æ¶ä¸Šè¿›è¡Œæ¨ç†çš„ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼š\n\nè¿è¡Œå‰è¯·ä¿®æ”¹ generate.pyï¼Œæ·»åŠ æ¨¡å‹è·¯å¾„ã€‚\n\ncd inference\npython generate.py\n\n\nopenPangu-Embedded-7B æ¨¡å‹é»˜è®¤ä¸ºæ…¢æ€è€ƒæ¨¡å¼ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ‰‹æ®µåˆ‡æ¢è‡³å¿«æ€è€ƒæ¨¡å¼ï¼š\n\nåœ¨ä»£ç å®ä¾‹generate.pyä¸­ï¼Œno_thinking_promptå˜é‡çš„å®šä¹‰å±•ç¤ºäº†åˆ‡æ¢è‡³å¿«æ€è€ƒæ¨¡å¼çš„å…·ä½“å®ç°ï¼šé€šè¿‡åœ¨ç”¨æˆ·è¾“å…¥æœ«å°¾æ·»åŠ /no_thinkæ ‡è®°ï¼Œå¯å°†å½“å‰è½®æ¬¡åˆ‡æ¢è‡³å¿«æ€è€ƒæ¨¡å¼ã€‚å¤„äºè¯¥æ¨¡å¼æ—¶ï¼Œthinking_contentå°†ä¸ºç©ºå€¼ã€‚\n4.4 ä½¿ç”¨æ¨ç†æ¡†æ¶\n\nvllm_ascendï¼šå‚è€ƒ[vllm_ascend_for_openpangu_embedded_7b.zh]\n\n5. æ¨¡å‹è®¸å¯è¯\n\né™¤æ–‡ä»¶ä¸­å¯¹å¼€æºè®¸å¯è¯å¦æœ‰çº¦å®šå¤–ï¼ŒopenPangu-Embedded-7B æ¨¡å‹æ ¹æ® OPENPANGU MODEL LICENSE AGREEMENT VERSION 1.0 æˆæƒï¼Œæ—¨åœ¨å…è®¸ä½¿ç”¨å¹¶ä¿ƒè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ¨¡å‹å­˜å‚¨åº“æ ¹ç›®å½•ä¸­çš„ LICENSE æ–‡ä»¶ã€‚\n\n6. å…è´£å£°æ˜\n\nç”±äº openPangu-Embedded-7Bï¼ˆâ€œæ¨¡å‹â€ï¼‰æ‰€ä¾èµ–çš„æŠ€æœ¯å›ºæœ‰çš„æŠ€æœ¯é™åˆ¶ï¼Œä»¥åŠäººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹æ˜¯ç”±ç›˜å¤è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œåä¸ºæ— æ³•å¯¹ä»¥ä¸‹äº‹é¡¹åšå‡ºä»»ä½•ä¿è¯ï¼š\n\nå°½ç®¡è¯¥æ¨¡å‹çš„è¾“å‡ºç”± AI ç®—æ³•ç”Ÿæˆï¼Œä½†ä¸èƒ½æ’é™¤æŸäº›ä¿¡æ¯å¯èƒ½å­˜åœ¨ç¼ºé™·ã€ä¸åˆç†æˆ–å¼•èµ·ä¸é€‚çš„å¯èƒ½æ€§ï¼Œç”Ÿæˆçš„å†…å®¹ä¸ä»£è¡¨åä¸ºçš„æ€åº¦æˆ–ç«‹åœºï¼›\næ— æ³•ä¿è¯è¯¥æ¨¡å‹ 100% å‡†ç¡®ã€å¯é ã€åŠŸèƒ½é½å…¨ã€åŠæ—¶ã€å®‰å…¨ã€æ— é”™è¯¯ã€ä¸é—´æ–­ã€æŒç»­ç¨³å®šæˆ–æ— ä»»ä½•æ•…éšœï¼›\nè¯¥æ¨¡å‹çš„è¾“å‡ºå†…å®¹ä¸æ„æˆä»»ä½•å»ºè®®æˆ–å†³ç­–ï¼Œä¹Ÿä¸ä¿è¯ç”Ÿæˆçš„å†…å®¹çš„çœŸå®æ€§ã€å®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€åŠæ—¶æ€§ã€åˆæ³•æ€§ã€åŠŸèƒ½æ€§æˆ–å®ç”¨æ€§ã€‚ç”Ÿæˆçš„å†…å®¹ä¸èƒ½æ›¿ä»£åŒ»ç–—ã€æ³•å¾‹ç­‰é¢†åŸŸçš„ä¸“ä¸šäººå£«å›ç­”æ‚¨çš„é—®é¢˜ã€‚ç”Ÿæˆçš„å†…å®¹ä»…ä¾›å‚è€ƒï¼Œä¸ä»£è¡¨åä¸ºçš„ä»»ä½•æ€åº¦ã€ç«‹åœºæˆ–è§‚ç‚¹ã€‚æ‚¨éœ€è¦æ ¹æ®å®é™…æƒ…å†µåšå‡ºç‹¬ç«‹åˆ¤æ–­ï¼Œåä¸ºä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n7. åé¦ˆ\n\nå¦‚æœæœ‰ä»»ä½•æ„è§å’Œå»ºè®®ï¼Œè¯·æäº¤issueæˆ–è”ç³» openPangu@huawei.comã€‚",
    "tags": "[\"Transformers\", \"Safetensors\"]"
  },
  {
    "url": "https://gitcode.com/openharmony-models/ChatGLM3-6B",
    "project_name": "ChatGLM3-6B",
    "readme": "ChatGLM3-6B\n\nğŸ’» Github Repo â€¢ ğŸ¦ Twitter â€¢ ğŸ“ƒ [GLM@ACL 22] [GitHub] â€¢ ğŸ“ƒ [GLM-130B@ICLR 23] [GitHub]\n\n\nğŸ‘‹ Join our Slack and WeChat\n\nğŸ“Experience the larger-scale ChatGLM model at chatglm.cn\n\nä»‹ç» (Introduction)\n\nChatGLM3-6B æ˜¯ ChatGLM ç³»åˆ—æœ€æ–°ä¸€ä»£çš„å¼€æºæ¨¡å‹ï¼Œåœ¨ä¿ç•™äº†å‰ä¸¤ä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›ä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¸Šï¼ŒChatGLM3-6B å¼•å…¥äº†å¦‚ä¸‹ç‰¹æ€§ï¼š\n\næ›´å¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼š ChatGLM3-6B çš„åŸºç¡€æ¨¡å‹ ChatGLM3-6B-Base é‡‡ç”¨äº†æ›´å¤šæ ·çš„è®­ç»ƒæ•°æ®ã€æ›´å……åˆ†çš„è®­ç»ƒæ­¥æ•°å’Œæ›´åˆç†çš„è®­ç»ƒç­–ç•¥ã€‚åœ¨è¯­ä¹‰ã€æ•°å­¦ã€æ¨ç†ã€ä»£ç ã€çŸ¥è¯†ç­‰ä¸åŒè§’åº¦çš„æ•°æ®é›†ä¸Šæµ‹è¯„æ˜¾ç¤ºï¼ŒChatGLM3-6B-Base å…·æœ‰åœ¨ 10B ä»¥ä¸‹çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­æœ€å¼ºçš„æ€§èƒ½ã€‚\næ›´å®Œæ•´çš„åŠŸèƒ½æ”¯æŒï¼š ChatGLM3-6B é‡‡ç”¨äº†å…¨æ–°è®¾è®¡çš„ Prompt æ ¼å¼ï¼Œé™¤æ­£å¸¸çš„å¤šè½®å¯¹è¯å¤–ã€‚åŒæ—¶åŸç”Ÿæ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆFunction Callï¼‰ã€ä»£ç æ‰§è¡Œï¼ˆCode Interpreterï¼‰å’Œ Agent ä»»åŠ¡ç­‰å¤æ‚åœºæ™¯ã€‚\næ›´å…¨é¢çš„å¼€æºåºåˆ—ï¼š é™¤äº†å¯¹è¯æ¨¡å‹ ChatGLM3-6B å¤–ï¼Œè¿˜å¼€æºäº†åŸºç¡€æ¨¡å‹ ChatGLM-6B-Baseã€é•¿æ–‡æœ¬å¯¹è¯æ¨¡å‹ ChatGLM3-6B-32Kã€‚ä»¥ä¸Šæ‰€æœ‰æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œåœ¨å¡«å†™é—®å·è¿›è¡Œç™»è®°åäº¦å…è®¸å…è´¹å•†ä¸šä½¿ç”¨ã€‚\n\nChatGLM3-6B is the latest open-source model in the ChatGLM series. While retaining many excellent features such as smooth dialogue and low deployment threshold from the previous two generations, ChatGLM3-6B introduces the following features:\n\nMore Powerful Base Model: The base model of ChatGLM3-6B, ChatGLM3-6B-Base, employs a more diverse training dataset, more sufficient training steps, and a more reasonable training strategy. Evaluations on datasets such as semantics, mathematics, reasoning, code, knowledge, etc., show that ChatGLM3-6B-Base has the strongest performance among pre-trained models under 10B.\nMore Comprehensive Function Support: ChatGLM3-6B adopts a newly designed Prompt format, in addition to the normal multi-turn dialogue. It also natively supports function call, code interpreter, and complex scenarios such as agent tasks.\nMore Comprehensive Open-source Series: In addition to the dialogue model ChatGLM3-6B, the base model ChatGLM-6B-Base and the long-text dialogue model ChatGLM3-6B-32K are also open-sourced. All the weights are fully open for academic research, and after completing the questionnaire registration, they are also allowed for free commercial use.\nè½¯ä»¶ä¾èµ– (Dependencies)\npip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate\n\nä»£ç è°ƒç”¨ (Code Usage)\n\nå¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM3-6B æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š\n\nYou can generate dialogue by invoking the ChatGLM3-6B model with the following code:\n\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n>>> model = model.eval()\n>>> response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n>>> print(response)\nä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n>>> response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\n>>> print(response)\næ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:\n\n1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚\n2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚\n3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚\n4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚\n5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚\n6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚\n\nå¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚\n\n\nå…³äºæ›´å¤šçš„ä½¿ç”¨è¯´æ˜ï¼ŒåŒ…æ‹¬å¦‚ä½•è¿è¡Œå‘½ä»¤è¡Œå’Œç½‘é¡µç‰ˆæœ¬çš„ DEMOï¼Œä»¥åŠä½¿ç”¨æ¨¡å‹é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ Github Repoã€‚\n\nFor more instructions, including how to run CLI and web demos, and model quantization, please refer to our Github Repo.\n\nåè®® (License)\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºï¼ŒChatGLM3-6B æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª Model Licenseã€‚\n\nThe code in this repository is open-sourced under the Apache-2.0 license, while the use of the ChatGLM3-6B model weights needs to comply with the Model License.\n\nå¼•ç”¨ (Citation)\n\nå¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡ã€‚\n\nIf you find our work helpful, please consider citing the following papers.\n\n@article{zeng2022glm,\n  title={Glm-130b: An open bilingual pre-trained model},\n  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},\n  journal={arXiv preprint arXiv:2210.02414},\n  year={2022}\n}\n\n@inproceedings{du2022glm,\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={320--335},\n  year={2022}\n}\n",
    "tags": "[\"PyTorch\", \"Transformers\", \"Safetensors\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openharmony-models/ocr_small",
    "project_name": "ocr_small",
    "readme": "ä»‹ç»ï¼ˆIntroductionï¼‰\n\néªŒè¯ç è¯†åˆ«æ¨¡å‹ ocr-captchaä¸“é—¨è¯†åˆ«å¸¸è§éªŒè¯ç çš„æ¨¡å‹ï¼Œè®­ç»ƒæ¨¡å‹\n\næ•°æ®åˆ†å¸ƒ\n\n1.ç±»å‹ï¼š1. çº¯æ•°å­—å‹ï¼›2. æ•°å­—+å­—æ¯å‹ï¼›3.çº¯å­—æ¯å‹ï¼ˆå¤§å°å†™ï¼‰\n\n2.é•¿åº¦ï¼š4ä½ã€5ä½ã€6ä½\n\næ•°æ®å¾®è°ƒ\n\n1.åŸºåº§æ¨¡å‹ï¼šåŸºåº§æ¨¡å‹å‚è€ƒè¾¾æ‘©é™¢å‘å¸ƒçš„\n\n2.å…·ä½“å¾®è°ƒå‚è€ƒä»¥ä¸Šé“¾æ¥\n\nå¿«é€Ÿä½¿ç”¨ï¼ˆQuickstartï¼‰\n\nä»£ç æä¾›webç½‘é¡µç‰ˆï¼šmyself_train_model.py\n\nè¯¦ç»†æ•°æ®å‚è€ƒhuggingfaceä»£ç ï¼šxiaolv/ocr-captcha\n\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nimport gradio as gr\nimport os\n\n\nclass xiaolv_ocr_model():\n\n    def __init__(self):\n        model_small = r\"./output_small\"\n        model_big = r\"./output_big\"\n        self.ocr_recognition_small = pipeline(Tasks.ocr_recognition, model=model_small)\n        self.ocr_recognition1_big = pipeline(Tasks.ocr_recognition, model=model_big)\n\n\n    def run(self,pict_path,moshi = \"small\", context=[]):\n        pict_path = pict_path.name\n        context = [pict_path]\n\n        if moshi == \"small\":\n            result = self.ocr_recognition_small(pict_path)\n        else:\n            result = self.ocr_recognition1_big(pict_path)\n\n        context += [str(result['text'][0])]\n        responses = [(u, b) for u, b in zip(context[::2], context[1::2])]\n        print(f\"è¯†åˆ«çš„ç»“æœä¸ºï¼š{result}\")\n        os.remove(pict_path)\n        return responses,context\n\n\n\n\nif __name__ == \"__main__\":\n    pict_path = r\"C:\\Users\\admin\\Desktop\\å›¾ç‰‡è¯†åˆ«æµ‹è¯•\\ä¼ä¸šå¾®ä¿¡æˆªå›¾_16895911221007.png\"\n    ocr_model = xiaolv_ocr_model()\n    # ocr_model.run(pict_path)\n\nè”ç³»æˆ‘ä»¬ï¼ˆContact Usï¼‰\n\nå¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆ2240560729@qq.comï¼‰è”ç³»æˆ‘ä»¬ã€‚",
    "tags": "[\"PyTorch\", \"Transformers\", \"English\", \"Chamorro\", \"Apache License 2.0\", \"Alibaba\", \"è¯»å…‰\", \"OCR\", \"æ–‡å­—è¯†åˆ«\"]"
  },
  {
    "url": "https://gitcode.com/openharmony-models/Qwen-Image",
    "project_name": "Qwen-Image",
    "readme": "license: apache-2.0 language:\n\nen\nzh library_name: diffusers pipeline_tag: text-to-image\n\nğŸ’œ Qwen ChatÂ Â  | Â Â ğŸ¤— Hugging FaceÂ Â  | Â Â  ğŸ“‘ Tech Report Â Â  | Â Â  ğŸ“‘ Blog Â Â \nğŸ–¥ï¸ DemoÂ Â  | Â Â ğŸ’¬ WeChat (å¾®ä¿¡)Â Â  | Â Â ğŸ«¨ DiscordÂ Â \n\nIntroduction\n\nWe are thrilled to release Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. Experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for Chinese.\n\nNews\n2025.08.04: We released the Technical Report of Qwen-Image!\n2025.08.04: We released Qwen-Image weights! Check at huggingface\n2025.08.04: We released Qwen-Image! Check our blog for more details!\nQuick Start\n\nInstall the latest version of diffusers\n\npip install git+https://github.com/huggingface/diffusers\n\n\nThe following contains a code snippet illustrating how to use the model to generate images based on text prompts:\n\nfrom diffusers import DiffusionPipeline\nimport torch\n\nmodel_name = \"Qwen/Qwen-Image\"\n\n# Load the pipeline\nif torch.cuda.is_available():\n    torch_dtype = torch.bfloat16\n    device = \"cuda\"\nelse:\n    torch_dtype = torch.float32\n    device = \"cpu\"\n\npipe = DiffusionPipeline.from_pretrained(model_name, torch_dtype=torch_dtype)\npipe = pipe.to(device)\n\npositive_magic = {\n    \"en\": \", Ultra HD, 4K, cinematic composition.\", # for english prompt\n    \"zh\": \", è¶…æ¸…ï¼Œ4Kï¼Œç”µå½±çº§æ„å›¾.\" # for chinese prompt\n}\n\n# Generate image\nprompt = '''A coffee shop entrance features a chalkboard sign reading \"Qwen Coffee ğŸ˜Š $2 per cup,\" with a neon light beside it displaying \"é€šä¹‰åƒé—®\". Next to it hangs a poster showing a beautiful Chinese woman, and beneath the poster is written \"Ï€â‰ˆ3.1415926-53589793-23846264-33832795-02384197\". Ultra HD, 4K, cinematic composition'''\n\nnegative_prompt = \" \" # using an empty string if you do not have specific concept to remove\n\n\n# Generate with different aspect ratios\naspect_ratios = {\n    \"1:1\": (1328, 1328),\n    \"16:9\": (1664, 928),\n    \"9:16\": (928, 1664),\n    \"4:3\": (1472, 1140),\n    \"3:4\": (1140, 1472),\n    \"3:2\": (1584, 1056),\n    \"2:3\": (1056, 1584),\n}\n\nwidth, height = aspect_ratios[\"16:9\"]\n\nimage = pipe(\n    prompt=prompt + positive_magic[\"en\"],\n    negative_prompt=negative_prompt,\n    width=width,\n    height=height,\n    num_inference_steps=50,\n    true_cfg_scale=4.0,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42)\n).images[0]\n\nimage.save(\"example.png\")\n\nShow Cases\n\nOne of its standout capabilities is high-fidelity text rendering across diverse images. Whether itâ€™s alphabetic languages like English or logographic scripts like Chinese, Qwen-Image preserves typographic details, layout coherence, and contextual harmony with stunning accuracy. Text isnâ€™t just overlaidâ€”itâ€™s seamlessly integrated into the visual fabric.\n\nBeyond text, Qwen-Image excels at general image generation with support for a wide range of artistic styles. From photorealistic scenes to impressionist paintings, from anime aesthetics to minimalist design, the model adapts fluidly to creative prompts, making it a versatile tool for artists, designers, and storytellers.\n\nWhen it comes to image editing, Qwen-Image goes far beyond simple adjustments. It enables advanced operations such as style transfer, object insertion or removal, detail enhancement, text editing within images, and even human pose manipulationâ€”all with intuitive input and coherent output. This level of control brings professional-grade editing within reach of everyday users.\n\nBut Qwen-Image doesnâ€™t just create or editâ€”it understands. It supports a suite of image understanding tasks, including object detection, semantic segmentation, depth and edge (Canny) estimation, novel view synthesis, and super-resolution. These capabilities, while technically distinct, can all be seen as specialized forms of intelligent image editing, powered by deep visual comprehension.\n\nTogether, these features make Qwen-Image not just a tool for generating pretty pictures, but a comprehensive foundation model for intelligent visual creation and manipulationâ€”where language, layout, and imagery converge.\n\nLicense Agreement\n\nQwen-Image is licensed under Apache 2.0.\n\nCitation\n\nWe kindly encourage citation of our work if you find it useful.\n\n@misc{wu2025qwenimagetechnicalreport,\n      title={Qwen-Image Technical Report}, \n      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},\n      year={2025},\n      eprint={2508.02324},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2508.02324}, \n}\n",
    "tags": "[\"PyTorch\", \"Transformers\", \"Diffusers\", \"Safetensors\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openharmony-models/Speech_Paraformer_ASR_6K",
    "project_name": "Speech_Paraformer_ASR_6K",
    "readme": "Highlights\nParaformer-largeé•¿éŸ³é¢‘æ¨¡å‹é›†æˆVADã€ASRã€æ ‡ç‚¹ä¸æ—¶é—´æˆ³åŠŸèƒ½ï¼Œå¯ç›´æ¥å¯¹æ—¶é•¿ä¸ºæ•°å°æ—¶éŸ³é¢‘è¿›è¡Œè¯†åˆ«ï¼Œå¹¶è¾“å‡ºå¸¦æ ‡ç‚¹æ–‡å­—ä¸æ—¶é—´æˆ³ï¼š\nASRæ¨¡å‹ï¼šParformer-largeæ¨¡å‹ç»“æ„ä¸ºéè‡ªå›å½’è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œå¤šä¸ªä¸­æ–‡å…¬å¼€æ•°æ®é›†ä¸Šå–å¾—SOTAæ•ˆæœï¼Œå¯å¿«é€Ÿåœ°åŸºäºModelScopeå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒå®šåˆ¶å’Œæ¨ç†ã€‚\nçƒ­è¯ç‰ˆæœ¬ï¼šParaformer-largeçƒ­è¯ç‰ˆæ¨¡å‹æ”¯æŒçƒ­è¯å®šåˆ¶åŠŸèƒ½ï¼ŒåŸºäºæä¾›çš„çƒ­è¯åˆ—è¡¨è¿›è¡Œæ¿€åŠ±å¢å¼ºï¼Œæå‡çƒ­è¯çš„å¬å›ç‡å’Œå‡†ç¡®ç‡ã€‚\nFunASRå¼€æºé¡¹ç›®ä»‹ç»\n\nFunASRå¸Œæœ›åœ¨è¯­éŸ³è¯†åˆ«çš„å­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ä¹‹é—´æ¶èµ·ä¸€åº§æ¡¥æ¢ã€‚é€šè¿‡å‘å¸ƒå·¥ä¸šçº§è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜å¯ä»¥æ›´æ–¹ä¾¿åœ°è¿›è¡Œè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç ”ç©¶å’Œç”Ÿäº§ï¼Œå¹¶æ¨åŠ¨è¯­éŸ³è¯†åˆ«ç”Ÿæ€çš„å‘å±•ã€‚è®©è¯­éŸ³è¯†åˆ«æ›´æœ‰è¶£ï¼\n\ngithubä»“åº“ | æœ€æ–°åŠ¨æ€ | ç¯å¢ƒå®‰è£… | æœåŠ¡éƒ¨ç½² | æ¨¡å‹åº“ | è”ç³»æˆ‘ä»¬\n\næ¨¡å‹åŸç†ä»‹ç»\n\nParaformeræ˜¯è¾¾æ‘©é™¢è¯­éŸ³å›¢é˜Ÿæå‡ºçš„ä¸€ç§é«˜æ•ˆçš„éè‡ªå›å½’ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«æ¡†æ¶ã€‚æœ¬é¡¹ç›®ä¸ºParaformerä¸­æ–‡é€šç”¨è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œé‡‡ç”¨å·¥ä¸šçº§æ•°ä¸‡å°æ—¶çš„æ ‡æ³¨éŸ³é¢‘è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä¿è¯äº†æ¨¡å‹çš„é€šç”¨è¯†åˆ«æ•ˆæœã€‚æ¨¡å‹å¯ä»¥è¢«åº”ç”¨äºè¯­éŸ³è¾“å…¥æ³•ã€è¯­éŸ³å¯¼èˆªã€æ™ºèƒ½ä¼šè®®çºªè¦ç­‰åœºæ™¯ã€‚\n\nParaformeræ¨¡å‹ç»“æ„å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç”± Encoderã€Predictorã€Samplerã€Decoder ä¸ Loss function äº”éƒ¨åˆ†ç»„æˆã€‚Encoderå¯ä»¥é‡‡ç”¨ä¸åŒçš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚self-attentionï¼Œconformerï¼ŒSAN-Mç­‰ã€‚Predictor ä¸ºä¸¤å±‚FFNï¼Œé¢„æµ‹ç›®æ ‡æ–‡å­—ä¸ªæ•°ä»¥åŠæŠ½å–ç›®æ ‡æ–‡å­—å¯¹åº”çš„å£°å­¦å‘é‡ã€‚Sampler ä¸ºæ— å¯å­¦ä¹ å‚æ•°æ¨¡å—ï¼Œä¾æ®è¾“å…¥çš„å£°å­¦å‘é‡å’Œç›®æ ‡å‘é‡ï¼Œç”Ÿäº§å«æœ‰è¯­ä¹‰çš„ç‰¹å¾å‘é‡ã€‚Decoder ç»“æ„ä¸è‡ªå›å½’æ¨¡å‹ç±»ä¼¼ï¼Œä¸ºåŒå‘å»ºæ¨¡ï¼ˆè‡ªå›å½’ä¸ºå•å‘å»ºæ¨¡ï¼‰ã€‚Loss function éƒ¨åˆ†ï¼Œé™¤äº†äº¤å‰ç†µï¼ˆCEï¼‰ä¸ MWER åŒºåˆ†æ€§ä¼˜åŒ–ç›®æ ‡ï¼Œè¿˜åŒ…æ‹¬äº† Predictor ä¼˜åŒ–ç›®æ ‡ MAEã€‚\n\nå…¶æ ¸å¿ƒç‚¹ä¸»è¦æœ‰ï¼š\n\nPredictor æ¨¡å—ï¼šåŸºäº Continuous integrate-and-fire (CIF) çš„ é¢„æµ‹å™¨ (Predictor) æ¥æŠ½å–ç›®æ ‡æ–‡å­—å¯¹åº”çš„å£°å­¦ç‰¹å¾å‘é‡ï¼Œå¯ä»¥æ›´åŠ å‡†ç¡®çš„é¢„æµ‹è¯­éŸ³ä¸­ç›®æ ‡æ–‡å­—ä¸ªæ•°ã€‚\nSamplerï¼šé€šè¿‡é‡‡æ ·ï¼Œå°†å£°å­¦ç‰¹å¾å‘é‡ä¸ç›®æ ‡æ–‡å­—å‘é‡å˜æ¢æˆå«æœ‰è¯­ä¹‰ä¿¡æ¯çš„ç‰¹å¾å‘é‡ï¼Œé…åˆåŒå‘çš„ Decoder æ¥å¢å¼ºæ¨¡å‹å¯¹äºä¸Šä¸‹æ–‡çš„å»ºæ¨¡èƒ½åŠ›ã€‚\nåŸºäºè´Ÿæ ·æœ¬é‡‡æ ·çš„ MWER è®­ç»ƒå‡†åˆ™ã€‚\n\næ›´è¯¦ç»†çš„ç»†èŠ‚è§ï¼š\n\nè®ºæ–‡ï¼š Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition\nè®ºæ–‡è§£è¯»ï¼šParaformer: é«˜è¯†åˆ«ç‡ã€é«˜è®¡ç®—æ•ˆç‡çš„å•è½®éè‡ªå›å½’ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«æ¨¡å‹\nåŸºäºFunASRè¿›è¡Œæ¨ç†\n\nä¸‹é¢ä¸ºå¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹ï¼Œæµ‹è¯•éŸ³é¢‘ï¼ˆä¸­æ–‡ï¼Œè‹±æ–‡ï¼‰\n\nå¯æ‰§è¡Œå‘½ä»¤è¡Œ\n\nåœ¨å‘½ä»¤è¡Œç»ˆç«¯æ‰§è¡Œï¼š\n\nfunasr +model=paraformer-zh +vad_model=\"fsmn-vad\" +punc_model=\"ct-punc\" +input=vad_example.wav\n\n\næ³¨ï¼šæ”¯æŒå•æ¡éŸ³é¢‘æ–‡ä»¶è¯†åˆ«ï¼Œä¹Ÿæ”¯æŒæ–‡ä»¶åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸ºkaldié£æ ¼wav.scpï¼šwav_id wav_path\n\npythonç¤ºä¾‹\néå®æ—¶è¯­éŸ³è¯†åˆ«\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\", model_revision=\"v2.0.4\",\n                  vad_model=\"fsmn-vad\", vad_model_revision=\"v2.0.4\",\n                  punc_model=\"ct-punc-c\", punc_model_revision=\"v2.0.4\",\n                  # spk_model=\"cam++\", spk_model_revision=\"v2.0.2\",\n                  )\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\", \n            batch_size_s=300, \n            hotword='é­”æ­')\nprint(res)\n\n\næ³¨ï¼šmodel_hubï¼šè¡¨ç¤ºæ¨¡å‹ä»“åº“ï¼Œmsä¸ºé€‰æ‹©modelscopeä¸‹è½½ï¼Œhfä¸ºé€‰æ‹©huggingfaceä¸‹è½½ã€‚\n\nå®æ—¶è¯­éŸ³è¯†åˆ«\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\", model_revision=\"v2.0.4\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n\n\næ³¨ï¼šchunk_sizeä¸ºæµå¼å»¶æ—¶é…ç½®ï¼Œ[0,10,5]è¡¨ç¤ºä¸Šå±å®æ—¶å‡ºå­—ç²’åº¦ä¸º10*60=600msï¼Œæœªæ¥ä¿¡æ¯ä¸º5*60=300msã€‚æ¯æ¬¡æ¨ç†è¾“å…¥ä¸º600msï¼ˆé‡‡æ ·ç‚¹æ•°ä¸º16000*0.6=960ï¼‰ï¼Œè¾“å‡ºä¸ºå¯¹åº”æ–‡å­—ï¼Œæœ€åä¸€ä¸ªè¯­éŸ³ç‰‡æ®µè¾“å…¥éœ€è¦è®¾ç½®is_final=Trueæ¥å¼ºåˆ¶è¾“å‡ºæœ€åä¸€ä¸ªå­—ã€‚\n\nè¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆéå®æ—¶ï¼‰\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n\nè¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆå®æ—¶ï¼‰\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n\næ ‡ç‚¹æ¢å¤\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\", model_revision=\"v2.0.4\")\n\nres = model.generate(input=\"é‚£ä»Šå¤©çš„ä¼šå°±åˆ°è¿™é‡Œå§ happy new year æ˜å¹´è§\")\nprint(res)\n\næ—¶é—´æˆ³é¢„æµ‹\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\", model_revision=\"v2.0.4\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n\n\næ›´å¤šè¯¦ç»†ç”¨æ³•ï¼ˆç¤ºä¾‹ï¼‰\n\nå¾®è°ƒ\n\nè¯¦ç»†ç”¨æ³•ï¼ˆç¤ºä¾‹ï¼‰\n\nBenchmark\n\nç»“åˆå¤§æ•°æ®ã€å¤§æ¨¡å‹ä¼˜åŒ–çš„Paraformeråœ¨ä¸€åºåˆ—è¯­éŸ³è¯†åˆ«çš„benchmarkä¸Šè·å¾—å½“å‰SOTAçš„æ•ˆæœï¼Œä»¥ä¸‹å±•ç¤ºå­¦æœ¯æ•°æ®é›†AISHELL-1ã€AISHELL-2ã€WenetSpeechï¼Œå…¬å¼€è¯„æµ‹é¡¹ç›®SpeechIO TIOBEç™½ç›’æµ‹è¯•åœºæ™¯çš„æ•ˆæœã€‚åœ¨å­¦æœ¯ç•Œå¸¸ç”¨çš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«è¯„æµ‹ä»»åŠ¡ä¸­ï¼Œå…¶è¡¨ç°è¿œè¿œè¶…äºç›®å‰å…¬å¼€å‘è¡¨è®ºæ–‡ä¸­çš„ç»“æœï¼Œè¿œå¥½äºå•ç‹¬å°é—­æ•°æ®é›†ä¸Šçš„æ¨¡å‹ã€‚æ­¤ç»“æœä¸ºParaformer-largeæ¨¡å‹åœ¨æ— VADå’Œæ ‡ç‚¹æ¨¡å‹ä¸‹çš„æµ‹è¯•ç»“æœã€‚\n\nAISHELL-1\nAISHELL-1 test\tw/o LM\tw/ LM\n\nEspnet\n\t\n4.90\n\t\n4.70\n\n\nWenet\n\t\n4.61\n\t\n4.36\n\n\nK2\n\t\n-\n\t\n4.26\n\n\nBlockformer\n\t\n4.29\n\t\n4.05\n\n\nParaformer-large\n\t\n1.95\n\t\n1.68\nAISHELL-2\n\tdev_ios\ttest_android\ttest_ios\ttest_mic\n\nEspnet\n\t\n5.40\n\t\n6.10\n\t\n5.70\n\t\n6.10\n\n\nWeNet\n\t\n-\n\t\n-\n\t\n5.39\n\t\n-\n\n\nParaformer-large\n\t\n2.80\n\t\n3.13\n\t\n2.85\n\t\n3.06\nWenetspeech\n\tdev\ttest_meeting\ttest_net\n\nEspnet\n\t\n9.70\n\t\n15.90\n\t\n8.80\n\n\nWeNet\n\t\n8.60\n\t\n17.34\n\t\n9.26\n\n\nK2\n\t\n7.76\n\t\n13.41\n\t\n8.71\n\n\nParaformer-large\n\t\n3.57\n\t\n6.97\n\t\n6.74\nSpeechIO TIOBE\n\nParaformer-largeæ¨¡å‹ç»“åˆTransformer-LMæ¨¡å‹åšshallow fusionï¼Œåœ¨å…¬å¼€è¯„æµ‹é¡¹ç›®SpeechIO TIOBEç™½ç›’æµ‹è¯•åœºæ™¯ä¸Šè·å¾—å½“å‰SOTAçš„æ•ˆæœï¼Œç›®å‰Transformer-LMæ¨¡å‹å·²åœ¨ModelScopeä¸Šå¼€æºï¼Œä»¥ä¸‹å±•ç¤ºSpeechIO TIOBEç™½ç›’æµ‹è¯•åœºæ™¯without LMã€with Transformer-LMçš„æ•ˆæœï¼š\n\nDecode config w/o LM:\nDecode without LM\nBeam size: 1\nDecode config w/ LM:\nDecode with Transformer-LM\nBeam size: 10\nLM weight: 0.15\ntestset\tw/o LM\tw/ LM\n\nSPEECHIO_ASR_ZH00001\n\t\n0.49\n\t\n0.35\n\n\nSPEECHIO_ASR_ZH00002\n\t\n3.23\n\t\n2.86\n\n\nSPEECHIO_ASR_ZH00003\n\t\n1.13\n\t\n0.80\n\n\nSPEECHIO_ASR_ZH00004\n\t\n1.33\n\t\n1.10\n\n\nSPEECHIO_ASR_ZH00005\n\t\n1.41\n\t\n1.18\n\n\nSPEECHIO_ASR_ZH00006\n\t\n5.25\n\t\n4.85\n\n\nSPEECHIO_ASR_ZH00007\n\t\n5.51\n\t\n4.97\n\n\nSPEECHIO_ASR_ZH00008\n\t\n3.69\n\t\n3.18\n\n\nSPEECHIO_ASR_ZH00009\n\t\n3.02\n\t\n2.78\n\n\nSPEECHIO_ASR_ZH000010\n\t\n3.35\n\t\n2.99\n\n\nSPEECHIO_ASR_ZH000011\n\t\n1.54\n\t\n1.25\n\n\nSPEECHIO_ASR_ZH000012\n\t\n2.06\n\t\n1.68\n\n\nSPEECHIO_ASR_ZH000013\n\t\n2.57\n\t\n2.25\n\n\nSPEECHIO_ASR_ZH000014\n\t\n3.86\n\t\n3.08\n\n\nSPEECHIO_ASR_ZH000015\n\t\n3.34\n\t\n2.67\nä½¿ç”¨æ–¹å¼ä»¥åŠé€‚ç”¨èŒƒå›´\n\nè¿è¡ŒèŒƒå›´\n\næ”¯æŒLinux-x86_64ã€Macå’ŒWindowsè¿è¡Œã€‚\n\nä½¿ç”¨æ–¹å¼\n\nç›´æ¥æ¨ç†ï¼šå¯ä»¥ç›´æ¥å¯¹è¾“å…¥éŸ³é¢‘è¿›è¡Œè§£ç ï¼Œè¾“å‡ºç›®æ ‡æ–‡å­—ã€‚\nå¾®è°ƒï¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œé‡‡ç”¨ç§æœ‰æˆ–è€…å¼€æºæ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚\n\nä½¿ç”¨èŒƒå›´ä¸ç›®æ ‡åœºæ™¯\n\né€‚åˆä¸ç¦»çº¿è¯­éŸ³è¯†åˆ«åœºæ™¯ï¼Œå¦‚å½•éŸ³æ–‡ä»¶è½¬å†™ï¼Œé…åˆGPUæ¨ç†æ•ˆæœæ›´åŠ ï¼Œè¾“å…¥éŸ³é¢‘æ—¶é•¿ä¸é™åˆ¶ï¼Œå¯ä»¥ä¸ºå‡ ä¸ªå°æ—¶éŸ³é¢‘ã€‚\næ¨¡å‹å±€é™æ€§ä»¥åŠå¯èƒ½çš„åå·®\n\nè€ƒè™‘åˆ°ç‰¹å¾æå–æµç¨‹å’Œå·¥å…·ä»¥åŠè®­ç»ƒå·¥å…·å·®å¼‚ï¼Œä¼šå¯¹CERçš„æ•°æ®å¸¦æ¥ä¸€å®šçš„å·®å¼‚ï¼ˆ<0.1%ï¼‰ï¼Œæ¨ç†GPUç¯å¢ƒå·®å¼‚å¯¼è‡´çš„RTFæ•°å€¼å·®å¼‚ã€‚\n\nç›¸å…³è®ºæ–‡ä»¥åŠå¼•ç”¨ä¿¡æ¯\n@inproceedings{gao2022paraformer,\n  title={Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition},\n  author={Gao, Zhifu and Zhang, Shiliang and McLoughlin, Ian and Yan, Zhijie},\n  booktitle={INTERSPEECH},\n  year={2022}\n}\n",
    "tags": "[\"PyTorch\", \"Chinese\", \"Apache License 2.0\", \"FunASR\", \"Alibaba\", \"Paraformer\", \"INTERSPEECH 2022\"]"
  },
  {
    "url": "https://gitcode.com/openharmony-models/dots.ocr",
    "project_name": "dots.ocr",
    "readme": "dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model\n\n \n\nğŸ–¥ï¸ Live Demo | ğŸ’¬ WeChat | ğŸ“• rednote\nIntroduction\n\ndots.ocr is a powerful, multilingual document parser that unifies layout detection and content recognition within a single vision-language model while maintaining good reading order. Despite its compact 1.7B-parameter LLM foundation, it achieves state-of-the-art(SOTA) performance.\n\nPowerful Performance: dots.ocr achieves SOTA performance for text, tables, and reading order on OmniDocBench, while delivering formula recognition results comparable to much larger models like Doubao-1.5 and gemini2.5-pro.\nMultilingual Support: dots.ocr demonstrates robust parsing capabilities for low-resource languages, achieving decisive advantages across both layout detection and content recognition on our in-house multilingual documents benchmark.\nUnified and Simple Architecture: By leveraging a single vision-language model, dots.ocr offers a significantly more streamlined architecture than conventional methods that rely on complex, multi-model pipelines. Switching between tasks is accomplished simply by altering the input prompt, proving that a VLM can achieve competitive detection results compared to traditional detection models like DocLayout-YOLO.\nEfficient and Fast Performance: Built upon a compact 1.7B LLM, dots.ocr provides faster inference speeds than many other high-performing models based on larger foundations.\nPerformance Comparison: dots.ocr vs. Competing Models\n\nNotes:\n\nThe EN, ZH metrics are the end2end evaluation results of OmniDocBench, and Multilingual metric is the end2end evaluation results of dots.ocr-bench.\nNews\n2025.07.30 ğŸš€ We release dots.ocr, â€” a multilingual documents parsing model based on 1.7b llm, with SOTA performance.\nBenchmark Results\n1. OmniDocBench\nThe end-to-end evaluation results of different tasks.\nModel\nType\tMethods\tOverallEditâ†“\tTextEditâ†“\tFormulaEditâ†“\tTableTEDSâ†‘\tTableEditâ†“\tRead OrderEditâ†“\nEN\tZH\tEN\tZH\tEN\tZH\tEN\tZH\tEN\tZH\tEN\tZH\nPipeline\nTools\tMinerU\t0.150\t0.357\t0.061\t0.215\t0.278\t0.577\t78.6\t62.1\t0.180\t0.344\t0.079\t0.292\nMarker\t0.336\t0.556\t0.080\t0.315\t0.530\t0.883\t67.6\t49.2\t0.619\t0.685\t0.114\t0.340\nMathpix\t0.191\t0.365\t0.105\t0.384\t0.306\t0.454\t77.0\t67.1\t0.243\t0.320\t0.108\t0.304\nDocling\t0.589\t0.909\t0.416\t0.987\t0.999\t1\t61.3\t25.0\t0.627\t0.810\t0.313\t0.837\nPix2Text\t0.320\t0.528\t0.138\t0.356\t0.276\t0.611\t73.6\t66.2\t0.584\t0.645\t0.281\t0.499\nUnstructured\t0.586\t0.716\t0.198\t0.481\t0.999\t1\t0\t0.06\t1\t0.998\t0.145\t0.387\nOpenParse\t0.646\t0.814\t0.681\t0.974\t0.996\t1\t64.8\t27.5\t0.284\t0.639\t0.595\t0.641\nPPStruct-V3\t0.145\t0.206\t0.058\t0.088\t0.295\t0.535\t-\t-\t0.159\t0.109\t0.069\t0.091\nExpert\nVLMs\tGOT-OCR\t0.287\t0.411\t0.189\t0.315\t0.360\t0.528\t53.2\t47.2\t0.459\t0.520\t0.141\t0.280\nNougat\t0.452\t0.973\t0.365\t0.998\t0.488\t0.941\t39.9\t0\t0.572\t1.000\t0.382\t0.954\nMistral OCR\t0.268\t0.439\t0.072\t0.325\t0.318\t0.495\t75.8\t63.6\t0.600\t0.650\t0.083\t0.284\nOLMOCR-sglang\t0.326\t0.469\t0.097\t0.293\t0.455\t0.655\t68.1\t61.3\t0.608\t0.652\t0.145\t0.277\nSmolDocling-256M\t0.493\t0.816\t0.262\t0.838\t0.753\t0.997\t44.9\t16.5\t0.729\t0.907\t0.227\t0.522\nDolphin\t0.206\t0.306\t0.107\t0.197\t0.447\t0.580\t77.3\t67.2\t0.180\t0.285\t0.091\t0.162\nMinerU 2\t0.139\t0.240\t0.047\t0.109\t0.297\t0.536\t82.5\t79.0\t0.141\t0.195\t0.069<\t0.118\nOCRFlux\t0.195\t0.281\t0.064\t0.183\t0.379\t0.613\t71.6\t81.3\t0.253\t0.139\t0.086\t0.187\nMonkeyOCR-pro-3B\t0.138\t0.206\t0.067\t0.107\t0.246\t0.421\t81.5\t87.5\t0.139\t0.111\t0.100\t0.185\nGeneral\nVLMs\tGPT4o\t0.233\t0.399\t0.144\t0.409\t0.425\t0.606\t72.0\t62.9\t0.234\t0.329\t0.128\t0.251\nQwen2-VL-72B\t0.252\t0.327\t0.096\t0.218\t0.404\t0.487\t76.8\t76.4\t0.387\t0.408\t0.119\t0.193\nQwen2.5-VL-72B\t0.214\t0.261\t0.092\t0.18\t0.315\t0.434\t82.9\t83.9\t0.341\t0.262\t0.106\t0.168\nGemini2.5-Pro\t0.148\t0.212\t0.055\t0.168\t0.356\t0.439\t85.8\t86.4\t0.13\t0.119\t0.049\t0.121\ndoubao-1-5-thinking-vision-pro-250428\t0.140\t0.162\t0.043\t0.085\t0.295\t0.384\t83.3\t89.3\t0.165\t0.085\t0.058\t0.094\nExpert VLMs\tdots.ocr\t0.125\t0.160\t0.032\t0.066\t0.329\t0.416\t88.6\t89.0\t0.099\t0.092\t0.040\t0.067\n\nThe end-to-end text recognition performance across 9 PDF page types.\nModel\nType\tModels\tBook\tSlides\tFinancial\nReport\tTextbook\tExam\nPaper\tMagazine\tAcademic\nPapers\tNotes\tNewspaper\tOverall\nPipeline\nTools\tMinerU\t0.055\t0.124\t0.033\t0.102\t0.159\t0.072\t0.025\t0.984\t0.171\t0.206\nMarker\t0.074\t0.340\t0.089\t0.319\t0.452\t0.153\t0.059\t0.651\t0.192\t0.274\nMathpix\t0.131\t0.220\t0.202\t0.216\t0.278\t0.147\t0.091\t0.634\t0.690\t0.300\nExpert\nVLMs\tGOT-OCR\t0.111\t0.222\t0.067\t0.132\t0.204\t0.198\t0.179\t0.388\t0.771\t0.267\nNougat\t0.734\t0.958\t1.000\t0.820\t0.930\t0.830\t0.214\t0.991\t0.871\t0.806\nDolphin\t0.091\t0.131\t0.057\t0.146\t0.231\t0.121\t0.074\t0.363\t0.307\t0.177\nOCRFlux\t0.068\t0.125\t0.092\t0.102\t0.119\t0.083\t0.047\t0.223\t0.536\t0.149\nMonkeyOCR-pro-3B\t0.084\t0.129\t0.060\t0.090\t0.107\t0.073\t0.050\t0.171\t0.107\t0.100\nGeneral\nVLMs\tGPT4o\t0.157\t0.163\t0.348\t0.187\t0.281\t0.173\t0.146\t0.607\t0.751\t0.316\nQwen2.5-VL-7B\t0.148\t0.053\t0.111\t0.137\t0.189\t0.117\t0.134\t0.204\t0.706\t0.205\nInternVL3-8B\t0.163\t0.056\t0.107\t0.109\t0.129\t0.100\t0.159\t0.150\t0.681\t0.188\ndoubao-1-5-thinking-vision-pro-250428\t0.048\t0.048\t0.024\t0.062\t0.085\t0.051\t0.039\t0.096\t0.181\t0.073\nExpert VLMs\tdots.ocr\t0.031\t0.047\t0.011\t0.082\t0.079\t0.028\t0.029\t0.109\t0.056\t0.055\n\nNotes:\n\nThe metrics are from MonkeyOCR, OmniDocBench, and our own internal evaluations.\nWe delete the Page-header and Page-footer cells in the result markdown.\nWe use tikz_preprocess pipeline to upsample the images to dpi 200.\n2. dots.ocr-bench\n\nThis is an inhouse benchmark which contain 1493 pdf images with 100 languages.\n\nThe end-to-end evaluation results of different tasks.\nMethods\tOverallEditâ†“\tTextEditâ†“\tFormulaEditâ†“\tTableTEDSâ†‘\tTableEditâ†“\tRead OrderEditâ†“\nMonkeyOCR-3B\t0.483\t0.445\t0.627\t50.93\t0.452\t0.409\ndoubao-1-5-thinking-vision-pro-250428\t0.291\t0.226\t0.440\t71.2\t0.260\t0.238\ndoubao-1-6\t0.299\t0.270\t0.417\t71.0\t0.258\t0.253\nGemini2.5-Pro\t0.251\t0.163\t0.402\t77.1\t0.236\t0.202\ndots.ocr\t0.177\t0.075\t0.297\t79.2\t0.186\t0.152\n\nNotes:\n\nWe use the same metric calculation pipeline of OmniDocBench.\nWe delete the Page-header and Page-footer cells in the result markdown.\nLayout Detection\nMethod\tF1@IoU=.50:.05:.95â†‘\tF1@IoU=.50â†‘\nOverall\tText\tFormula\tTable\tPicture\tOverall\tText\tFormula\tTable\tPicture\nDocLayout-YOLO-DocStructBench\t0.733\t0.694\t0.480\t0.803\t0.619\t0.806\t0.779\t0.620\t0.858\t0.678\ndots.ocr-parse all\t0.831\t0.801\t0.654\t0.838\t0.748\t0.922\t0.909\t0.770\t0.888\t0.831\ndots.ocr-detection only\t0.845\t0.816\t0.716\t0.875\t0.765\t0.930\t0.917\t0.832\t0.918\t0.843\n\nNotes:\n\nprompt_layout_all_en for parse all, prompt_layout_only_en for detection only, please refer to prompts\n3. olmOCR-bench.\nModel\tArXiv\tOld Scans\nMath\tTables\tOld Scans\tHeaders and\nFooters\tMulti\ncolumn\tLong Tiny\nText\tBase\tOverall\nGOT OCR\t52.7\t52.0\t0.2\t22.1\t93.6\t42.0\t29.9\t94.0\t48.3 Â± 1.1\nMarker\t76.0\t57.9\t57.6\t27.8\t84.9\t72.9\t84.6\t99.1\t70.1 Â± 1.1\nMinerU\t75.4\t47.4\t60.9\t17.3\t96.6\t59.0\t39.1\t96.6\t61.5 Â± 1.1\nMistral OCR\t77.2\t67.5\t60.6\t29.3\t93.6\t71.3\t77.1\t99.4\t72.0 Â± 1.1\nNanonets OCR\t67.0\t68.6\t77.7\t39.5\t40.7\t69.9\t53.4\t99.3\t64.5 Â± 1.1\nGPT-4o\n(No Anchor)\t51.5\t75.5\t69.1\t40.9\t94.2\t68.9\t54.1\t96.7\t68.9 Â± 1.1\nGPT-4o\n(Anchored)\t53.5\t74.5\t70.0\t40.7\t93.8\t69.3\t60.6\t96.8\t69.9 Â± 1.1\nGemini Flash 2\n(No Anchor)\t32.1\t56.3\t61.4\t27.8\t48.0\t58.7\t84.4\t94.0\t57.8 Â± 1.1\nGemini Flash 2\n(Anchored)\t54.5\t56.1\t72.1\t34.2\t64.7\t61.5\t71.5\t95.6\t63.8 Â± 1.2\nQwen 2 VL\n(No Anchor)\t19.7\t31.7\t24.2\t17.1\t88.9\t8.3\t6.8\t55.5\t31.5 Â± 0.9\nQwen 2.5 VL\n(No Anchor)\t63.1\t65.7\t67.3\t38.6\t73.6\t68.3\t49.1\t98.3\t65.5 Â± 1.2\nolmOCR v0.1.75\n(No Anchor)\t71.5\t71.4\t71.4\t42.8\t94.1\t77.7\t71.0\t97.8\t74.7 Â± 1.1\nolmOCR v0.1.75\n(Anchored)\t74.9\t71.2\t71.0\t42.2\t94.5\t78.3\t73.3\t98.3\t75.5 Â± 1.0\nMonkeyOCR-pro-3B\t83.8\t68.8\t74.6\t36.1\t91.2\t76.6\t80.1\t95.3\t75.8 Â± 1.0\ndots.ocr\t82.1\t64.2\t88.3\t40.9\t94.1\t82.4\t81.2\t99.5\t79.1 Â± 1.0\n\nNote:\n\nThe metrics are from MonkeyOCR, olmocr, and our own internal evaluations.\nWe delete the Page-header and Page-footer cells in the result markdown.\nQuick Start\n1. Installation\nInstall dots.ocr\nconda create -n dots_ocr python=3.12\nconda activate dots_ocr\n\ngit clone https://github.com/rednote-hilab/dots.ocr.git\ncd dots.ocr\n\n# Install pytorch, see https://pytorch.org/get-started/previous-versions/ for your cuda version\npip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128\npip install -e .\n\n\nIf you have trouble with the installation, try our Docker Image for an easier setup, and follow these steps:\n\ngit clone https://github.com/rednote-hilab/dots.ocr.git\ncd dots.ocr\npip install -e .\n\nDownload Model Weights\n\nğŸ’¡Note: Please use a directory name without periods (e.g., DotsOCR instead of dots.ocr) for the model save path. This is a temporary workaround pending our integration with Transformers.\n\npython3 tools/download_model.py\n\n2. Deployment\nvLLM inference\n\nWe highly recommend using vllm for deployment and inference. All of our evaluations results are based on vllm version 0.9.1. The Docker Image is based on the official vllm image. You can also follow Dockerfile to build the deployment environment by yourself.\n\n# You need to register model to vllm at first\npython3 tools/download_model.py\nexport hf_model_path=./weights/DotsOCR  # Path to your downloaded model weights, Please use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) for the model save path. This is a temporary workaround pending our integration with Transformers.\nexport PYTHONPATH=$(dirname \"$hf_model_path\"):$PYTHONPATH\nsed -i '/^from vllm\\.entrypoints\\.cli\\.main import main$/a\\\nfrom DotsOCR import modeling_dots_ocr_vllm' `which vllm`  # If you downloaded model weights by yourself, please replace `DotsOCR` by your model saved directory name, and remember to use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) \n\n# launch vllm server\nCUDA_VISIBLE_DEVICES=0 vllm serve ${hf_model_path} --tensor-parallel-size 1 --gpu-memory-utilization 0.95  --chat-template-content-format string --served-model-name model --trust-remote-code\n\n# If you get a ModuleNotFoundError: No module named 'DotsOCR', please check the note above on the saved model directory name.\n\n# vllm api demo\npython3 ./demo/demo_vllm.py --prompt_mode prompt_layout_all_en\n\nHugginface inference\npython3 demo/demo_hf.py\n\nHugginface inference details\n3. Document Parse\n\nBased on vLLM server, you can parse an image or a pdf file using the following commands:\n\n\n# Parse all layout info, both detection and recognition\n# Parse a single image\npython3 dots_ocr/parser.py demo/demo_image1.jpg\n# Parse a single PDF\npython3 dots_ocr/parser.py demo/demo_pdf1.pdf  --num_threads 64  # try bigger num_threads for pdf with a large number of pages\n\n# Layout detection only\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_layout_only_en\n\n# Parse text only, except Page-header and Page-footer\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_ocr\n\n# Parse layout info by bbox\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_grounding_ocr --bbox 163 241 1536 705\n\n\nOutput Results\n4. Demo\n\nYou can run the demo with the following command, or try directly at live demo\n\npython demo/demo_gradio.py\n\n\nWe also provide a demo for grounding ocr:\n\npython demo/demo_gradio_annotion.py\n\nExample for formula document\n  \nExample for table document\n  \nExample for multilingual document\n    \nExample for reading order\nExample for grounding ocr\nAcknowledgments\n\nWe would like to thank Qwen2.5-VL, aimv2, MonkeyOCR, OmniDocBench, PyMuPDF, for providing code and models.\n\nWe also thank DocLayNet, M6Doc, CDLA, D4LA for providing valuable datasets.\n\nLimitation & Future Work\n\nComplex Document Elements:\n\nTable&Formula: dots.ocr is not yet perfect for high-complexity tables and formula extraction.\nPicture: Pictures in documents are currently not parsed.\n\nParsing Failures: The model may fail to parse under certain conditions:\n\nWhen the character-to-pixel ratio is excessively high. Try enlarging the image or increasing the PDF parsing DPI (a setting of 200 is recommended). However, please note that the model performs optimally on images with a resolution under 11289600 pixels.\nContinuous special characters, such as ellipses (...) and underscores (_), may cause the prediction output to repeat endlessly. In such scenarios, consider using alternative prompts like prompt_layout_only_en, prompt_ocr, or prompt_grounding_ocr (details here).\n\nPerformance Bottleneck: Despite its 1.7B parameter LLM foundation, dots.ocr is not yet optimized for high-throughput processing of large PDF volumes.\n\nWe are committed to achieving more accurate table and formula parsing, as well as enhancing the model's OCR capabilities for broader generalization, all while aiming for a more powerful, more efficient model. Furthermore, we are actively considering the development of a more general-purpose perception model based on Vision-Language Models (VLMs), which would integrate general detection, image captioning, and OCR tasks into a unified framework. Parsing the content of the pictures in the documents is also a key priority for our future work. We believe that collaboration is the key to tackling these exciting challenges. If you are passionate about advancing the frontiers of document intelligence and are interested in contributing to these future endeavors, we would love to hear from you. Please reach out to us via email at: [yanqing4@xiaohongshu.com].",
    "tags": "[\"Image-Text-to-Text\", \"PyTorch\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"MIT\", \"image-to-text\", \"ocr\", \"table\", \"formula\", \"document-parse\", \"layout\"]"
  },
  {
    "url": "https://gitcode.com/openharmony-models/Wan2.2-T2V-A14B",
    "project_name": "Wan2.2-T2V-A14B",
    "readme": "Wan2.2\n\nğŸ’œ Wan Â Â  ï½œ Â Â  ğŸ–¥ï¸ GitHub Â Â  | Â Â ğŸ¤— Hugging FaceÂ Â  | Â Â  ğŸ“‘ Technical Report Â Â  | Â Â  ğŸ“‘ Blog Â Â  | Â Â ğŸ’¬ WeChat GroupÂ Â  | Â Â  ğŸ“– DiscordÂ Â \n\n\nWan: Open and Advanced Large-Scale Video Generative Models\n\nWe are excited to introduce Wan2.2, a major upgrade to our foundational video models. With Wan2.2, we have focused on incorporating the following innovations:\n\nğŸ‘ Effective MoE Architecture: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\n\nğŸ‘ Cinematic-level Aesthetics: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\n\nğŸ‘ Complex Motion Generation: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model's generalization across multiple dimensions such as motions, semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models.\n\nğŸ‘ Efficient High-Definition Hybrid TI2V: Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of 16Ã—16Ã—4. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest 720P@24fps models currently available, capable of serving both the industrial and academic sectors simultaneously.\n\nThis repository contains our T2V-A14B model, which supports generating 5s videos at both 480P and 720P resolutions. Built with a Mixture-of-Experts (MoE) architecture, it delivers outstanding video generation quality. On our new benchmark Wan-Bench 2.0, the model surpasses leading commercial models across most key evaluation dimensions.\n\nVideo Demos\nğŸ”¥ Latest News!!\nJul 28, 2025: ğŸ‘‹ We've released the inference code and model weights of Wan2.2.\nCommunity Works\n\nIf your research or project builds upon Wan2.1 or Wan2.2, we welcome you to share it with us so we can highlight it for the broader community.\n\nğŸ“‘ Todo List\nWan2.2 Text-to-Video\nMulti-GPU Inference code of the A14B and 14B models\nCheckpoints of the A14B and 14B models\nComfyUI integration\nDiffusers integration\nWan2.2 Image-to-Video\nMulti-GPU Inference code of the A14B model\nCheckpoints of the A14B model\nComfyUI integration\nDiffusers integration\nWan2.2 Text-Image-to-Video\nMulti-GPU Inference code of the 5B model\nCheckpoints of the 5B model\nComfyUI integration\nDiffusers integration\nRun Wan2.2\nInstallation\n\nClone the repo:\n\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\n\n\nInstall dependencies:\n\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\n\nModel Download\nModels\tDownload Links\tDescription\nT2V-A14B\tğŸ¤— Huggingface\tText-to-Video MoE model, supports 480P & 720P\nI2V-A14B\tğŸ¤— Huggingface\tImage-to-Video MoE model, supports 480P & 720P\nTI2V-5B\tğŸ¤— Huggingface\tHigh-compression VAE, T2V+I2V, supports 720P\n\nğŸ’¡Note: The TI2V-5B model supports 720P video generation at 24 FPS.\n\nDownload models using huggingface-cli:\n\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir ./Wan2.2-T2V-A14B\n\nRun Text-to-Video Generation\n\nThis repository supports the Wan2.2-T2V-A14B Text-to-Video model and can simultaneously support video generation at 480P and 720P resolutions.\n\n(1) Without Prompt Extension\n\nTo facilitate implementation, we will start with a basic version of the inference process that skips the prompt extension step.\n\nSingle-GPU inference\npython generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --offload_model True --convert_model_dtype --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\n\n\nğŸ’¡ This command can run on a GPU with at least 80GB VRAM.\n\nğŸ’¡If you encounter OOM (Out-of-Memory) issues, you can use the --offload_model True, --convert_model_dtype and --t5_cpu options to reduce GPU memory usage.\n\nMulti-GPU inference using FSDP + DeepSpeed Ulysses\n\nWe use PyTorch FSDP and DeepSpeed Ulysses to accelerate inference.\n\ntorchrun --nproc_per_node=8 generate.py --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\n\n(2) Using Prompt Extension\n\nExtending the prompts can effectively enrich the details in the generated videos, further enhancing the video quality. Therefore, we recommend enabling prompt extension. We provide the following two methods for prompt extension:\n\nUse the Dashscope API for extension.\nApply for a dashscope.api_key in advance (EN | CN).\nConfigure the environment variable DASH_API_KEY to specify the Dashscope API key. For users of Alibaba Cloud's international site, you also need to set the environment variable DASH_API_URL to 'https://dashscope-intl.aliyuncs.com/api/v1'. For more detailed instructions, please refer to the dashscope document.\nUse the qwen-plus model for text-to-video tasks and qwen-vl-max for image-to-video tasks.\nYou can modify the model used for extension with the parameter --prompt_extend_model. For example:\nDASH_API_KEY=your_key torchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'dashscope' --prompt_extend_target_lang 'zh'\n\n\nUsing a local model for extension.\n\nBy default, the Qwen model on HuggingFace is used for this extension. Users can choose Qwen models or other models based on the available GPU memory size.\nFor text-to-video tasks, you can use models like Qwen/Qwen2.5-14B-Instruct, Qwen/Qwen2.5-7B-Instruct and Qwen/Qwen2.5-3B-Instruct.\nFor image-to-video tasks, you can use models like Qwen/Qwen2.5-VL-7B-Instruct and Qwen/Qwen2.5-VL-3B-Instruct.\nLarger models generally provide better extension results but require more GPU memory.\nYou can modify the model used for extension with the parameter --prompt_extend_model , allowing you to specify either a local model path or a Hugging Face model. For example:\ntorchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'local_qwen' --prompt_extend_target_lang 'zh'\n\nComputational Efficiency on Different GPUs\n\nWe test the computational efficiency of different Wan2.2 models on different GPUs in the following table. The results are presented in the format: Total time (s) / peak GPU memory (GB).\n\nThe parameter settings for the tests presented in this table are as follows: (1) Multi-GPU: 14B: --ulysses_size 4/8 --dit_fsdp --t5_fsdp, 5B: --ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu; Single-GPU: 14B: --offload_model True --convert_model_dtype, 5B: --offload_model True --convert_model_dtype --t5_cpu (--convert_model_dtype converts model parameter types to config.param_dtype); (2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs; (3) Tests were run without the --use_prompt_extend flag; (4) Reported results are the average of multiple samples taken after the warm-up phase.\n\nIntroduction of Wan2.2\n\nWan2.2 builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n\n(1) Mixture-of-Experts (MoE) Architecture\n\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\n\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step \ntt\nt increases. At the beginning of the denoising process, \ntt\nt is large and the noise level is high, so the SNR is at its minimum, denoted as \nSNRmin{SNR}_{min}\nSNR\nmin\n\tâ€‹\n\n. In this stage, the high-noise expert is activated. We define a threshold step \ntmoe{t}_{moe}\nt\nmoe\n\tâ€‹\n\n corresponding to half of the \nSNRmin{SNR}_{min}\nSNR\nmin\n\tâ€‹\n\n, and switch to the low-noise expert when \nt<tmoet<{t}_{moe}\nt<t\nmoe\n\tâ€‹\n\n.\n\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline Wan2.1 model does not employ the MoE architecture. Among the MoE-based variants, the Wan2.1 & High-Noise Expert reuses the Wan2.1 model as the low-noise expert while uses the Wan2.2's high-noise expert, while the Wan2.1 & Low-Noise Expert uses Wan2.1 as the high-noise expert and employ the Wan2.2's low-noise expert. The Wan2.2 (MoE) (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n\n(2) Efficient High-Definition Hybrid TI2V\n\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a \nTÃ—HÃ—WT\\times H\\times W\nTÃ—HÃ—W compression ratio of \n4Ã—16Ã—164\\times16\\times16\n4Ã—16Ã—16, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches \n4Ã—32Ã—324\\times32\\times32\n4Ã—32Ã—32. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\n\nComparisons to SOTAs\n\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\n\nCitation\n\nIf you find our work helpful, please cite us.\n\n@article{wan2025,\n      title={Wan: Open and Advanced Large-Scale Video Generative Models}, \n      author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\n      journal = {arXiv preprint arXiv:2503.20314},\n      year={2025}\n}\n\nLicense Agreement\n\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\n\nAcknowledgements\n\nWe would like to thank the contributors to the SD3, Qwen, umt5-xxl, diffusers and HuggingFace repositories, for their open research.\n\nContact Us\n\nIf you would like to leave a message to our research or product teams, feel free to join our Discord or WeChat groups!",
    "tags": "[\"PyTorch\", \"Transformers\", \"Diffusers\", \"Safetensors\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openharmony-models/t5_small",
    "project_name": "t5_small",
    "readme": "Model Card for T5 Small\nModification\n\nAdded the CANN version dependency description to the original README and modified the example code.\n\nTable of Contents\nModel Details\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nModel Card Authors\nHow To Get Started With the Model\nModel Details\nModel Description\n\nThe developers of the Text-To-Text Transfer Transformer (T5) write:\n\nWith T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\n\nT5-Small is the checkpoint with 60 million parameters.\n\nDeveloped by: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See associated paper and GitHub repo\nModel type: Language model\nLanguage(s) (NLP): English, French, Romanian, German\nLicense: Apache 2.0\nResources for more information:\nResearch paper\nGoogle's T5 Blog Post\nGitHub Repo\nUses\nDirect Use and Downstream Use\n\nThe developers write in a blog post that the model:\n\nOur text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\n\nSee the blog post and research paper for further details.\n\nOut-of-Scope Use\n\nMore information needed.\n\nBias, Risks, and Limitations\n\nMore information needed.\n\nRecommendations\n\nMore information needed.\n\nTraining Details\nTraining Data\n\nThe model is pre-trained on the Colossal Clean Crawled Corpus (C4), which was developed and released in the context of the same research paper as T5.\n\nThe model was pre-trained on a on a multi-task mixture of unsupervised (1.) and supervised tasks (2.). Thereby, the following datasets were being used for (1.) and (2.):\n\nDatasets used for Unsupervised denoising objective:\nC4\nWiki-DPR\nDatasets used for Supervised text-to-text language modeling objective\nSentence acceptability judgment\nCoLA Warstadt et al., 2018\nSentiment analysis\nSST-2 Socher et al., 2013\nParaphrasing/sentence similarity\nMRPC Dolan and Brockett, 2005\nSTS-B Ceret al., 2017\nQQP Iyer et al., 2017\nNatural language inference\nMNLI Williams et al., 2017\nQNLI Rajpurkar et al.,2016\nRTE Dagan et al., 2005\nCB De Marneff et al., 2019\nSentence completion\nCOPA Roemmele et al., 2011\nWord sense disambiguation\nWIC Pilehvar and Camacho-Collados, 2018\nQuestion answering\nMultiRC Khashabi et al., 2018\nReCoRD Zhang et al., 2018\nBoolQ Clark et al., 2019\nTraining Procedure\n\nIn their abstract, the model developers write:\n\nIn this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks.\n\nThe framework introduced, the T5 framework, involves a training procedure that brings together the approaches studied in the paper. See the research paper for further details.\n\nEvaluation\nTesting Data, Factors & Metrics\n\nThe developers evaluated the model on 24 tasks, see the research paper for full details.\n\nResults\n\nFor full results for T5-small, see the research paper, Table 14.\n\nEnvironmental Impact\n\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\n\nHardware Type: Google Cloud TPU Pods\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\n\nBibTeX:\n\n@article{2020t5,\n  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {140},\n  pages   = {1-67},\n  url     = {http://jmlr.org/papers/v21/20-074.html}\n}\n\n\nAPA:\n\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.\nModel Card Authors\n\nThis model card was written by the team at Hugging Face.\n\nHow to Get Started with the Model\n\nUse the code below to get started with the model.\n\nClick to expand",
    "tags": "[\"Translation\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"ONNX\", \"Rust\", \"Safetensors\", \"JAX\", \"English\", \"Apache License 2.0\", \"c4\"]"
  },
  {
    "url": "https://gitcode.com/openharmony-models/DeepSeek-R1-Distill-Qwen-7B",
    "project_name": "DeepSeek-R1-Distill-Qwen-7B",
    "readme": "DeepSeek-R1-Distill-Qwen-7Bï¼ˆé‡åŒ–ç‰ˆï¼‰\n1. æ¨¡å‹åç§°\n\nDeepSeek-R1-Distill-Qwen-7B\n\n2. æ¨¡å‹æ¥æº\n\ndeepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n\n3. ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆKV Cacheï¼‰\n\n4K Tokens\n\n4. ç¡¬ä»¶é€‚é…\n\né¸¿è’™ PC\n\n5. æ¨¡å‹ç²¾åº¦\n\næœ¬æ¨¡å‹åœ¨ CEvalï¼ˆChinese Evaluation Benchmarkï¼‰ æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„æµ‹ã€‚\nCEval æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ç»¼åˆèƒ½åŠ›çš„æµ‹è¯•é›†ï¼Œæ¶µç›– 52 ä¸ªå­¦ç§‘é¢†åŸŸï¼Œä»¥å¤šé¡¹é€‰æ‹©é¢˜å½¢å¼è¯„ä¼°æ¨¡å‹çš„çŸ¥è¯†ã€æ¨ç†ä¸è¯­è¨€ç†è§£èƒ½åŠ›ã€‚\n\næµ‹è¯•é¢˜é‡ï¼š 1346\nè¯„æµ‹æŒ‡æ ‡ï¼š Accuracyï¼ˆå‡†ç¡®ç‡ï¼‰\næŒ‡æ ‡\tæµ®ç‚¹æ¨¡å‹\té‡åŒ–æ¨¡å‹\tRecovery\nCEval ç²¾åº¦\t59.55%\t56.84%\t95.44%",
    "tags": "[\"Transformers\"]"
  },
  {
    "url": "https://gitcode.com/openharmony-models/DeepSeek-R1-Distill-Llama-8B",
    "project_name": "DeepSeek-R1-Distill-Llama-8B",
    "readme": "Original Text\nDeepSeek-R1\n  \n  \n\nè®ºæ–‡é“¾æ¥ğŸ‘ï¸\n\n1. ç®€ä»‹\n\næˆ‘ä»¬æ¨å‡ºäº†ç¬¬ä¸€ä»£æ¨ç†æ¨¡å‹ DeepSeek-R1-Zero å’Œ DeepSeek-R1ã€‚ DeepSeek-R1-Zero æ˜¯é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè€Œæˆçš„æ¨¡å‹ï¼Œæ— éœ€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºå‰ç½®æ­¥éª¤ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ é€šè¿‡ RLï¼ŒDeepSeek-R1-Zero è‡ªç„¶æ¶Œç°å‡ºè®¸å¤šå¼ºå¤§ä¸”æœ‰è¶£çš„æ¨ç†è¡Œä¸ºã€‚ ç„¶è€Œï¼ŒDeepSeek-R1-Zero ä¹Ÿé¢ä¸´æ— é™é‡å¤ã€å¯è¯»æ€§å·®å’Œè¯­è¨€æ··æ‚ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜å¹¶è¿›ä¸€æ­¥æå‡æ¨ç†æ€§èƒ½ï¼Œ æˆ‘ä»¬æ¨å‡ºäº† DeepSeek-R1ï¼Œå®ƒåœ¨ RL å‰å¼•å…¥äº†å†·å¯åŠ¨æ•°æ®ã€‚ DeepSeek-R1 åœ¨æ•°å­¦ã€ä»£ç å’Œæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ OpenAI-o1 ç›¸å½“ã€‚ ä¸ºæ”¯æŒç ”ç©¶ç¤¾åŒºï¼Œæˆ‘ä»¬å¼€æºäº† DeepSeek-R1-Zeroã€DeepSeek-R1 ä»¥åŠåŸºäº Llama å’Œ Qwen ä» DeepSeek-R1 è’¸é¦å‡ºçš„å…­æ¬¾å¯†é›†æ¨¡å‹ã€‚DeepSeek-R1-Distill-Qwen-32B åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº† OpenAI-o1-miniï¼Œä¸ºå¯†é›†æ¨¡å‹åˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚\n\næ³¨æ„ï¼šåœ¨æœ¬åœ°è¿è¡Œ DeepSeek-R1 ç³»åˆ—æ¨¡å‹å‰ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨å…ˆé˜…è¯»ä½¿ç”¨å»ºè®®éƒ¨åˆ†ã€‚\n\n2. æ¨¡å‹æ¦‚è¿°\n\nåè®­ç»ƒï¼šåŸºäºåŸºç¡€æ¨¡å‹çš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ \n\næˆ‘ä»¬ç›´æ¥åœ¨åŸºç¡€æ¨¡å‹ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæ— éœ€ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºå‰ç½®æ­¥éª¤ã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿæ¢ç´¢æ€ç»´é“¾ï¼ˆCoTï¼‰ä»¥è§£å†³å¤æ‚é—®é¢˜ï¼Œä»è€Œå¼€å‘å‡º DeepSeek-R1-Zeroã€‚DeepSeek-R1-Zero å±•ç¤ºäº†è‡ªæˆ‘éªŒè¯ã€åæ€å’Œç”Ÿæˆé•¿ CoT ç­‰èƒ½åŠ›ï¼Œä¸ºç ”ç©¶ç¤¾åŒºæ ‘ç«‹äº†é‡è¦é‡Œç¨‹ç¢‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯é¦–ä¸ªéªŒè¯ LLM æ¨ç†èƒ½åŠ›å¯ä»¥ä»…é€šè¿‡ RL æ¿€åŠ±è€Œæ— éœ€ SFT çš„å¼€æºç ”ç©¶ã€‚è¿™ä¸€çªç ´ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥å‘å±•é“ºå¹³äº†é“è·¯ã€‚\n\næˆ‘ä»¬ä»‹ç»äº†å¼€å‘ DeepSeek-R1 çš„æµç¨‹ã€‚è¯¥æµç¨‹åŒ…å«ä¸¤ä¸ª RL é˜¶æ®µï¼Œæ—¨åœ¨å‘ç°æ›´å¥½çš„æ¨ç†æ¨¡å¼å¹¶ä¸äººç±»åå¥½å¯¹é½ï¼Œä»¥åŠä¸¤ä¸ª SFT é˜¶æ®µï¼Œä½œä¸ºæ¨¡å‹æ¨ç†å’Œéæ¨ç†èƒ½åŠ›çš„ç§å­ã€‚ æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸€æµç¨‹å°†é€šè¿‡åˆ›å»ºæ›´å¥½çš„æ¨¡å‹é€ ç¦è¡Œä¸šã€‚\n\nè’¸é¦ï¼šå°æ¨¡å‹ä¹Ÿèƒ½å¼ºå¤§\n\næˆ‘ä»¬è¯æ˜ï¼Œå¤§æ¨¡å‹çš„æ¨ç†æ¨¡å¼å¯ä»¥è’¸é¦åˆ°å°æ¨¡å‹ä¸­ï¼Œç›¸æ¯”åœ¨å°æ¨¡å‹ä¸Šé€šè¿‡ RL å‘ç°çš„æ¨ç†æ¨¡å¼ï¼Œæ€§èƒ½æ›´ä¼˜ã€‚å¼€æºçš„ DeepSeek-R1 åŠå…¶ API å°†å¸®åŠ©ç ”ç©¶ç¤¾åŒºåœ¨æœªæ¥è’¸é¦å‡ºæ›´å¥½çš„å°æ¨¡å‹ã€‚\nåˆ©ç”¨ DeepSeek-R1 ç”Ÿæˆçš„æ¨ç†æ•°æ®ï¼Œæˆ‘ä»¬å¯¹ç ”ç©¶ç¤¾åŒºå¹¿æ³›ä½¿ç”¨çš„å‡ æ¬¾å¯†é›†æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè’¸é¦åçš„å°å‹å¯†é›†æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æˆ‘ä»¬å‘ç¤¾åŒºå¼€æºäº†åŸºäº Qwen2.5 å’Œ Llama3 ç³»åˆ—çš„ 1.5Bã€7Bã€8Bã€14Bã€32B å’Œ 70B è’¸é¦æ£€æŸ¥ç‚¹ã€‚\n3. æ¨¡å‹ä¸‹è½½\nDeepSeek-R1 æ¨¡å‹\næ¨¡å‹\tæ€»å‚æ•°é‡\tæ¿€æ´»å‚æ•°é‡\tä¸Šä¸‹æ–‡é•¿åº¦\tä¸‹è½½\nDeepSeek-R1-Zero\t671B\t37B\t128K\tğŸ¤— HuggingFace\nDeepSeek-R1\t671B\t37B\t128K\tğŸ¤— HuggingFace\n\nDeepSeek-R1-Zero å’Œ DeepSeek-R1 åŸºäº DeepSeek-V3-Base è®­ç»ƒè€Œæˆã€‚ å…³äºæ¨¡å‹æ¶æ„çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è€ƒ DeepSeek-V3 ä»“åº“ã€‚\n\nDeepSeek-R1-Distill æ¨¡å‹\næ¨¡å‹\tåŸºç¡€æ¨¡å‹\tä¸‹è½½\nDeepSeek-R1-Distill-Qwen-1.5B\tQwen2.5-Math-1.5B\tğŸ¤— HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\tQwen2.5-Math-7B\tğŸ¤— HuggingFace\nDeepSeek-R1-Distill-Llama-8B\tLlama-3.1-8B\tğŸ¤— HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\tQwen2.5-14B\tğŸ¤— HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\tQwen2.5-32B\tğŸ¤— HuggingFace\nDeepSeek-R1-Distill-Llama-70B\tLlama-3.3-70B-Instruct\tğŸ¤— HuggingFace\n\nDeepSeek-R1-Distill æ¨¡å‹åŸºäºå¼€æºæ¨¡å‹å¾®è°ƒè€Œæˆï¼Œä½¿ç”¨äº† DeepSeek-R1 ç”Ÿæˆçš„æ ·æœ¬ã€‚ æˆ‘ä»¬ç•¥å¾®ä¿®æ”¹äº†å®ƒä»¬çš„é…ç½®å’Œåˆ†è¯å™¨ã€‚è¯·ä½¿ç”¨æˆ‘ä»¬çš„è®¾ç½®è¿è¡Œè¿™äº›æ¨¡å‹ã€‚\n\n4. è¯„ä¼°ç»“æœ\nDeepSeek-R1è¯„ä¼°\n\næˆ‘ä»¬æ‰€æœ‰æ¨¡å‹çš„ç”Ÿæˆé•¿åº¦ä¸Šé™å‡è®¾ä¸º32,768ä¸ªtokenã€‚å¯¹äºéœ€è¦é‡‡æ ·çš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¸©åº¦å‚æ•°\n0.60.6\n0.6ã€top-på€¼\n0.950.95\n0.95ï¼Œå¹¶ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆ64ä¸ªå“åº”ä»¥ä¼°ç®—pass@1æŒ‡æ ‡ã€‚\n\nç±»åˆ«\tåŸºå‡†æµ‹è¯•ï¼ˆæŒ‡æ ‡ï¼‰\tClaude-3.5-Sonnet-1022\tGPT-4o 0513\tDeepSeek V3\tOpenAI o1-mini\tOpenAI o1-1217\tDeepSeek R1\n\tæ¶æ„\t-\t-\tMoE\t-\t-\tMoE\n\tæ¿€æ´»å‚æ•°é‡\t-\t-\t37B\t-\t-\t37B\n\tæ€»å‚æ•°é‡\t-\t-\t671B\t-\t-\t671B\nè‹±è¯­èƒ½åŠ›\tMMLU (Pass@1)\t88.3\t87.2\t88.5\t85.2\t91.8\t90.8\n\tMMLU-Redux (EM)\t88.9\t88.0\t89.1\t86.7\t-\t92.9\n\tMMLU-Pro (EM)\t78.0\t72.6\t75.9\t80.3\t-\t84.0\n\tDROP (3-shot F1)\t88.3\t83.7\t91.6\t83.9\t90.2\t92.2\n\tIF-Eval (Prompt Strict)\t86.5\t84.3\t86.1\t84.8\t-\t83.3\n\tGPQA-Diamond (Pass@1)\t65.0\t49.9\t59.1\t60.0\t75.7\t71.5\n\tSimpleQA (Correct)\t28.4\t38.2\t24.9\t7.0\t47.0\t30.1\n\tFRAMES (Acc.)\t72.5\t80.5\t73.3\t76.9\t-\t82.5\n\tAlpacaEval2.0 (LC-winrate)\t52.0\t51.1\t70.0\t57.8\t-\t87.6\n\tArenaHard (GPT-4-1106)\t85.2\t80.4\t85.5\t92.0\t-\t92.3\nç¼–ç¨‹èƒ½åŠ›\tLiveCodeBench (Pass@1-COT)\t33.8\t34.2\t-\t53.8\t63.4\t65.9\n\tCodeforces (ç™¾åˆ†ä½)\t20.3\t23.6\t58.7\t93.4\t96.6\t96.3\n\tCodeforces (è¯„åˆ†)\t717\t759\t1134\t1820\t2061\t2029\n\tSWE Verified (è§£å†³ç‡)\t50.8\t38.8\t42.0\t41.6\t48.9\t49.2\n\tAider-Polyglot (å‡†ç¡®ç‡)\t45.3\t16.0\t49.6\t32.9\t61.7\t53.3\næ•°å­¦èƒ½åŠ›\tAIME 2024 (Pass@1)\t16.0\t9.3\t39.2\t63.6\t79.2\t79.8\n\tMATH-500 (Pass@1)\t78.3\t74.6\t90.2\t90.0\t96.4\t97.3\n\tCNMO 2024 (Pass@1)\t13.1\t10.8\t43.2\t67.6\t-\t78.8\nä¸­æ–‡èƒ½åŠ›\tCLUEWSC (EM)\t85.4\t87.9\t90.9\t89.9\t-\t92.8\n\tC-Eval (EM)\t76.7\t76.0\t86.5\t68.9\t-\t91.8\n\tC-SimpleQA (æ­£ç¡®ç‡)\t55.4\t58.7\t68.0\t40.3\t-\t63.7\nè’¸é¦æ¨¡å‹è¯„ä¼°\næ¨¡å‹\tAIME 2024 pass@1\tAIME 2024 cons@64\tMATH-500 pass@1\tGPQA Diamond pass@1\tLiveCodeBench pass@1\tCodeForces è¯„åˆ†\nGPT-4o-0513\t9.3\t13.4\t74.6\t49.9\t32.9\t759\nClaude-3.5-Sonnet-1022\t16.0\t26.7\t78.3\t65.0\t38.9\t717\no1-mini\t63.6\t80.0\t90.0\t60.0\t53.8\t1820\nQwQ-32B-Preview\t44.0\t60.0\t90.6\t54.5\t41.9\t1316\nDeepSeek-R1-Distill-Qwen-1.5B\t28.9\t52.7\t83.9\t33.8\t16.9\t954\nDeepSeek-R1-Distill-Qwen-7B\t55.5\t83.3\t92.8\t49.1\t37.6\t1189\nDeepSeek-R1-Distill-Qwen-14B\t69.7\t80.0\t93.9\t59.1\t53.1\t1481\nDeepSeek-R1-Distill-Qwen-32B\t72.6\t83.3\t94.3\t62.1\t57.2\t1691\nDeepSeek-R1-Distill-Llama-8B\t50.4\t80.0\t89.1\t49.0\t39.6\t1205\nDeepSeek-R1-Distill-Llama-70B\t70.0\t86.7\t94.5\t65.2\t57.5\t1633\n5. èŠå¤©ç½‘ç«™ä¸APIå¹³å°\n\næ‚¨å¯ä»¥é€šè¿‡DeepSeekå®˜ç½‘ä¸DeepSeek-R1è¿›è¡Œå¯¹è¯ï¼šchat.deepseek.comï¼Œå¹¶å¼€å¯\"DeepThink\"åŠŸèƒ½\n\næˆ‘ä»¬è¿˜æä¾›OpenAIå…¼å®¹çš„APIæœåŠ¡ï¼Œè®¿é—®åœ°å€ï¼šplatform.deepseek.com\n\n6. æœ¬åœ°è¿è¡ŒæŒ‡å—\nDeepSeek-R1æ¨¡å‹\n\nè¯·è®¿é—®DeepSeek-V3ä»“åº“è·å–DeepSeek-R1æœ¬åœ°è¿è¡Œçš„ç›¸å…³ä¿¡æ¯ã€‚\n\næ³¨æ„ï¼šHugging Faceçš„Transformersæš‚æœªåŸç”Ÿæ”¯æŒ\n\nDeepSeek-R1è’¸é¦æ¨¡å‹\n\nDeepSeek-R1è’¸é¦æ¨¡å‹çš„ä½¿ç”¨æ–¹å¼ä¸Qwenæˆ–Llamaæ¨¡å‹ç›¸åŒã€‚\n\nä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨vLLMå¿«é€Ÿå¯åŠ¨æœåŠ¡ï¼š\n\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n\n\næ‚¨ä¹Ÿå¯ä»¥è½»æ¾ä½¿ç”¨ SGLang å¯åŠ¨æœåŠ¡\n\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n\nä½¿ç”¨å»ºè®®\n\nä¸ºç¡®ä¿DeepSeek-R1ç³»åˆ—æ¨¡å‹ï¼ˆåŒ…æ‹¬åŸºå‡†æµ‹è¯•ï¼‰å‘æŒ¥é¢„æœŸæ€§èƒ½ï¼Œå»ºè®®éµå¾ªä»¥ä¸‹é…ç½®ï¼š\n\nå°†æ¸©åº¦å‚æ•°è®¾ç½®åœ¨0.5-0.7ä¹‹é—´ï¼ˆæ¨è0.6ï¼‰ï¼Œä»¥é¿å…è¾“å‡ºå†…å®¹æ— é™é‡å¤æˆ–é€»è¾‘æ··ä¹±ã€‚\nè¯·å‹¿æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼Œæ‰€æœ‰æŒ‡ä»¤åº”åŒ…å«åœ¨ç”¨æˆ·æç¤ºè¯ä¸­ã€‚\nå¤„ç†æ•°å­¦é—®é¢˜æ—¶ï¼Œå»ºè®®åœ¨æç¤ºè¯ä¸­åŠ å…¥å¼•å¯¼è¯­ï¼Œä¾‹å¦‚ï¼šâ€œè¯·é€æ­¥æ¨ç†ï¼Œå¹¶å°†æœ€ç»ˆç­”æ¡ˆç½®äº\\boxed{}ä¸­ã€‚â€\nè¯„ä¼°æ¨¡å‹æ€§èƒ½æ—¶ï¼Œå»ºè®®è¿›è¡Œå¤šæ¬¡æµ‹è¯•å¹¶å–å¹³å‡å€¼ã€‚\n\næ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°DeepSeek-R1ç³»åˆ—æ¨¡å‹åœ¨å“åº”æŸäº›æŸ¥è¯¢æ—¶å¯èƒ½è·³è¿‡æ€è€ƒæ¨¡å¼ï¼ˆå³è¾“å‡ºâ€œ<think>\\n\\n</think>â€ï¼‰ï¼Œè¿™ä¼šå½±å“æ¨¡å‹è¡¨ç°ã€‚\nä¸ºç¡®ä¿æ¨¡å‹å……åˆ†æ¨ç†ï¼Œå»ºè®®å¼ºåˆ¶è¦æ±‚æ¨¡å‹åœ¨æ¯æ¬¡è¾“å‡ºæ—¶ä»¥â€œ<think>\\nâ€å¼€å¤´ã€‚\n\n7. è®¸å¯åè®®\n\næœ¬ä»£ç ä»“åº“åŠæ¨¡å‹æƒé‡é‡‡ç”¨MITè®¸å¯è¯ã€‚\nDeepSeek-R1ç³»åˆ—æ”¯æŒå•†ä¸šç”¨é€”ï¼Œå…è®¸ä»»ä½•ä¿®æ”¹åŠè¡ç”Ÿä½œå“ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºç”¨äºè®­ç»ƒå…¶ä»–å¤§è¯­è¨€æ¨¡å‹çš„è’¸é¦è¡Œä¸ºï¼‰ã€‚éœ€æ³¨æ„ï¼š\n\nDeepSeek-R1-Distill-Qwen-1.5Bã€DeepSeek-R1-Distill-Qwen-7Bã€DeepSeek-R1-Distill-Qwen-14BåŠDeepSeek-R1-Distill-Qwen-32Bè¡ç”Ÿè‡ªQwen-2.5ç³»åˆ—ï¼Œå…¶åŸå§‹è®¸å¯ä¸ºApache 2.0åè®®ï¼Œç°é€šè¿‡DeepSeek-R1ç­›é€‰çš„80ä¸‡æ ·æœ¬è¿›è¡Œå¾®è°ƒã€‚\nDeepSeek-R1-Distill-Llama-8Bè¡ç”Ÿè‡ªLlama3.1-8B-Baseï¼ŒåŸå§‹è®¸å¯éµå¾ªllama3.1åè®®ã€‚\nDeepSeek-R1-Distill-Llama-70Bè¡ç”Ÿè‡ªLlama3.3-70B-Instructï¼ŒåŸå§‹è®¸å¯éµå¾ªllama3.3åè®®ã€‚\n8. å¼•ç”¨å£°æ˜\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n\n9. è”ç³»æˆ‘ä»¬\n\nå¦‚æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·æäº¤ issue æˆ–é€šè¿‡é‚®ä»¶ service@deepseek.com ä¸æˆ‘ä»¬è”ç³»ã€‚",
    "tags": "[\"Transformers\", \"Safetensors\", \"MIT\", \"arxiv:2501.12948\"]"
  },
  {
    "url": "https://gitcode.com/openMind/glm-4v-9b",
    "project_name": "glm-4v-9b",
    "readme": "glm-4v-9b\n\nRead this in English\n\n2024/08/12, æœ¬ä»“åº“ä»£ç å·²æ›´æ–°å¹¶ä½¿ç”¨ transforemrs>=4.44.0, è¯·åŠæ—¶æ›´æ–°ä¾èµ–ã€‚\n\nGLM-4-9B æ˜¯æ™ºè°± AI æ¨å‡ºçš„æœ€æ–°ä¸€ä»£é¢„è®­ç»ƒæ¨¡å‹ GLM-4 ç³»åˆ—ä¸­çš„å¼€æºç‰ˆæœ¬ã€‚ åœ¨è¯­ä¹‰ã€æ•°å­¦ã€æ¨ç†ã€ä»£ç å’ŒçŸ¥è¯†ç­‰å¤šæ–¹é¢çš„æ•°æ®é›†æµ‹è¯„ä¸­ï¼ŒGLM-4-9B åŠå…¶äººç±»åå¥½å¯¹é½çš„ç‰ˆæœ¬ GLM-4-9B-Chat å‡è¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½ã€‚ é™¤äº†èƒ½è¿›è¡Œå¤šè½®å¯¹è¯ï¼ŒGLM-4-9B-Chat è¿˜å…·å¤‡ç½‘é¡µæµè§ˆã€ä»£ç æ‰§è¡Œã€è‡ªå®šä¹‰å·¥å…·è°ƒç”¨ï¼ˆFunction Callï¼‰å’Œé•¿æ–‡æœ¬æ¨ç†ï¼ˆæ”¯æŒæœ€å¤§ 128K ä¸Šä¸‹æ–‡ï¼‰ç­‰é«˜çº§åŠŸèƒ½ã€‚ æœ¬ä»£æ¨¡å‹å¢åŠ äº†å¤šè¯­è¨€æ”¯æŒï¼Œæ”¯æŒåŒ…æ‹¬æ—¥è¯­ï¼ŒéŸ©è¯­ï¼Œå¾·è¯­åœ¨å†…çš„ 26 ç§è¯­è¨€ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†æ”¯æŒ 1M ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆçº¦ 200 ä¸‡ä¸­æ–‡å­—ç¬¦ï¼‰çš„æ¨¡å‹ã€‚\n\nå¤šæ¨¡æ€èƒ½åŠ›\n\nGLM-4V-9B æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡è§†è§‰ç†è§£èƒ½åŠ›ï¼Œå…¶ç›¸å…³ç»å…¸ä»»åŠ¡çš„è¯„æµ‹ç»“æœå¦‚ä¸‹ï¼š\n\n\tMMBench-EN-Test\tMMBench-CN-Test\tSEEDBench_IMG\tMMStar\tMMMU\tMME\tHallusionBench\tAI2D\tOCRBench\n\tè‹±æ–‡ç»¼åˆ\tä¸­æ–‡ç»¼åˆ\tç»¼åˆèƒ½åŠ›\tç»¼åˆèƒ½åŠ›\tå­¦ç§‘ç»¼åˆ\tæ„ŸçŸ¥æ¨ç†\tå¹»è§‰æ€§\tå›¾è¡¨ç†è§£\tæ–‡å­—è¯†åˆ«\nGPT-4o, 20240513\t83.4\t82.1\t77.1\t63.9\t69.2\t2310.3\t55\t84.6\t736\nGPT-4v, 20240409\t81\t80.2\t73\t56\t61.7\t2070.2\t43.9\t78.6\t656\nGPT-4v, 20231106\t77\t74.4\t72.3\t49.7\t53.8\t1771.5\t46.5\t75.9\t516\nInternVL-Chat-V1.5\t82.3\t80.7\t75.2\t57.1\t46.8\t2189.6\t47.4\t80.6\t720\nLlaVA-Next-Yi-34B\t81.1\t79\t75.7\t51.6\t48.8\t2050.2\t34.8\t78.9\t574\nStep-1V\t80.7\t79.9\t70.3\t50\t49.9\t2206.4\t48.4\t79.2\t625\nMiniCPM-Llama3-V2.5\t77.6\t73.8\t72.3\t51.8\t45.8\t2024.6\t42.4\t78.4\t725\nQwen-VL-Max\t77.6\t75.7\t72.7\t49.5\t52\t2281.7\t41.2\t75.7\t684\nGeminiProVision\t73.6\t74.3\t70.7\t38.6\t49\t2148.9\t45.7\t72.9\t680\nClaude-3V Opus\t63.3\t59.2\t64\t45.7\t54.9\t1586.8\t37.8\t70.6\t694\nGLM-4v-9B\t81.1\t79.4\t76.8\t58.7\t47.2\t2163.8\t46.6\t81.1\t786\n\næœ¬ä»“åº“æ˜¯ GLM-4V-9B çš„æ¨¡å‹ä»“åº“ï¼Œæ”¯æŒ8Kä¸Šä¸‹æ–‡é•¿åº¦ã€‚\n\nè¿è¡Œæ¨¡å‹\n\næ›´å¤šæ¨ç†ä»£ç å’Œä¾èµ–ä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„ githubã€‚\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ä¾èµ–å®‰è£…ï¼Œå¦åˆ™æ— æ³•æ­£å¸¸è¿è¡Œã€‚\n\nimport torch\nfrom PIL import Image\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"npu\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"openMind/glm-4v-9b\", trust_remote_code=True)\n\nquery = 'æè¿°è¿™å¼ å›¾ç‰‡'\nimage = Image.open(\"your image\").convert('RGB')\ninputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"image\": image, \"content\": query}],\n                                       add_generation_prompt=True, tokenize=True, return_tensors=\"pt\",\n                                       return_dict=True)  # chat mode\n\ninputs = inputs.to(device)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"openMind/glm-4v-9b\",\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n).to(device).eval()\n\ngen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\nwith torch.no_grad():\n    outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n    print(tokenizer.decode(outputs[0]))\n\nåè®® (License)\n\nGLM-4 æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª LICENSEã€‚\n\nå¼•ç”¨\n\nå¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡ã€‚\n\n@article{zeng2022glm,\n  title={Glm-130b: An open bilingual pre-trained model},\n  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},\n  journal={arXiv preprint arXiv:2210.02414},\n  year={2022}\n}\n\n@inproceedings{du2022glm,\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={320--335},\n  year={2022}\n}\n",
    "tags": "[\"Transformers\", \"Safetensors\"]"
  },
  {
    "url": "https://gitcode.com/openMind/LongWriter-glm4-9b",
    "project_name": "LongWriter-glm4-9b",
    "readme": "Original Text\nLongWriter-glm4-9b\n\nğŸ¤– [LongWriter æ•°æ®é›†] â€¢ ğŸ’» [GitHub ä»“åº“] â€¢ ğŸ“ƒ [LongWriter è®ºæ–‡]\n\nLongWriter-glm4-9b æ˜¯åŸºäº glm-4-9b è¿›è¡Œè®­ç»ƒçš„ï¼Œèƒ½å¤Ÿä¸€æ¬¡æ€§ç”Ÿæˆ 10,000+ å­—çš„é•¿æ–‡ã€‚\n\næ¨¡å‹çš„ç®€å•éƒ¨ç½²ç¤ºä¾‹ï¼š\n\nfrom openmind import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"openMind/LongWriter-glm4-9b\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"openMind/LongWriter-glm4-9b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\nmodel = model.eval()\nquery = \"Write a `10000`-word China travel guide\"\nresponse, history = model.chat(tokenizer, query, history=[], max_new_tokens=1024, temperature=0.5)\nprint(response)\n\n\nç¯å¢ƒï¼štransformers==4.43.0\n\nè®¸å¯åè®®ï¼šglm-4-9b è®¸å¯åè®®\n\nå¼•ç”¨\n\nè‹¥æ‚¨è®¤ä¸ºæˆ‘ä»¬çš„å·¥ä½œæœ‰ç›Šï¼Œè¯·è€ƒè™‘å¼•ç”¨ LongWriterï¼š\n\n@article{bai2024longwriter,\n  title={LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs}, \n  author={Yushi Bai and Jiajie Zhang and Xin Lv and Linzhi Zheng and Siqi Zhu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},\n  journal={arXiv preprint arXiv:2408.07055},\n  year={2024}\n}\n\n\nå½“ç„¶ï¼Œæˆ‘ä¼šæŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚è¯·æ‚¨æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬ä»¥åŠæœŸæœ›çš„ä¸­æ–‡é£æ ¼ï¼ˆé€šä¿—ã€ä¸“ä¸šã€ä¼˜é›…æˆ–æµç•…ï¼‰ã€‚åœ¨æ²¡æœ‰å…·ä½“æ–‡æœ¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘æ— æ³•æä¾›ç¿»è¯‘æœåŠ¡ã€‚",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"AI-ModelScope/LongWriter-6k\", \"chatglm\", \"llama\", \"Long Context\"]"
  },
  {
    "url": "https://gitcode.com/openMind/Qwen2.5_7B_Instruct",
    "project_name": "Qwen2.5_7B_Instruct",
    "readme": "Original Text\nQwen2.5-7B-Instruct\nç®€ä»‹\n\nQwen2.5æ˜¯é€šä¹‰åƒé—®å¤§æ¨¡å‹å®¶æ—çš„æœ€æ–°ç³»åˆ—ã€‚æœ¬æ¬¡æˆ‘ä»¬å‘å¸ƒäº†ä»0.5Båˆ°72Bå‚æ•°è§„æ¨¡çš„åŸºç¡€è¯­è¨€æ¨¡å‹å’ŒæŒ‡ä»¤ç²¾è°ƒæ¨¡å‹ã€‚ç›¸è¾ƒäºQwen2ï¼ŒQwen2.5å®ç°äº†ä»¥ä¸‹æ˜¾è‘—æå‡ï¼š\n\né€šè¿‡ä¸“ä¸šé¢†åŸŸä¸“å®¶æ¨¡å‹çš„åŠ æŒï¼ŒçŸ¥è¯†å‚¨å¤‡å¤§å¹…æ‰©å……ï¼Œåœ¨ä»£ç ä¸æ•°å­¦é¢†åŸŸçš„è¡¨ç°å°¤ä¸ºçªå‡ºã€‚\nåœ¨æŒ‡ä»¤éµå¾ªã€é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆè¶…8K tokensï¼‰ã€ç»“æ„åŒ–æ•°æ®ç†è§£ï¼ˆå¦‚è¡¨æ ¼ï¼‰åŠç»“æ„åŒ–è¾“å‡ºç”Ÿæˆï¼ˆç‰¹åˆ«æ˜¯JSONæ ¼å¼ï¼‰æ–¹é¢å–å¾—é‡å¤§çªç ´ã€‚å¯¹ç³»ç»Ÿæç¤ºè¯çš„å¤šæ ·æ€§æ›´å…·é€‚åº”æ€§ï¼Œæ˜¾è‘—å¢å¼ºäº†èŠå¤©æœºå™¨äººçš„è§’è‰²æ‰®æ¼”ä¸æ¡ä»¶è®¾å®šèƒ½åŠ›ã€‚\næ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†ï¼ˆæœ€é«˜128K tokensï¼‰ï¼Œå¹¶å…·å¤‡8K tokensçš„é•¿æ–‡ç”Ÿæˆèƒ½åŠ›ã€‚\nè¦†ç›–29ç§è¯­è¨€çš„å¤šè¯­è¨€æ”¯æŒï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ã€è‘¡è„ç‰™è¯­ã€å¾·è¯­ã€æ„å¤§åˆ©è¯­ã€ä¿„è¯­ã€æ—¥è¯­ã€éŸ©è¯­ã€è¶Šå—è¯­ã€æ³°è¯­ã€é˜¿æ‹‰ä¼¯è¯­ç­‰ã€‚\n\næœ¬ä»“åº“ä¸º7Bå‚æ•°çš„Qwen2.5æŒ‡ä»¤ç²¾è°ƒæ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹æŠ€æœ¯ç‰¹æ€§ï¼š\n\næ¨¡å‹ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹\nè®­ç»ƒé˜¶æ®µï¼šé¢„è®­ç»ƒä¸åè®­ç»ƒ\næ¶æ„ï¼šé‡‡ç”¨RoPEä½ç½®ç¼–ç ã€SwiGLUæ¿€æ´»å‡½æ•°ã€RMSNormå½’ä¸€åŒ–åŠå¸¦Attention QKVåç½®çš„Transformerç»“æ„\nå‚æ•°é‡ï¼š7.61B\néåµŒå…¥å‚æ•°é‡ï¼š6.53B\nå±‚æ•°ï¼š28\næ³¨æ„åŠ›å¤´é…ç½®ï¼ˆGQAï¼‰ï¼šæŸ¥è¯¢å¤´28ä¸ªï¼Œé”®å€¼å¤´4ä¸ª\nä¸Šä¸‹æ–‡é•¿åº¦ï¼šå®Œæ•´æ”¯æŒ131,072 tokensï¼Œç”Ÿæˆé•¿åº¦è¾¾8192 tokens\nå…·ä½“é•¿æ–‡æœ¬å¤„ç†éƒ¨ç½²æ–¹æ³•è¯·å‚é˜…æœ¬èŠ‚è¯´æ˜ã€‚\n\næ›´å¤šæŠ€æœ¯ç»†èŠ‚è¯·æŸ¥é˜…æˆ‘ä»¬çš„åšå®¢ã€GitHubåŠæ–‡æ¡£ã€‚\n\nç¯å¢ƒè¦æ±‚\n\nQwen2.5çš„ä»£ç å·²é›†æˆè‡³æœ€æ–°ç‰ˆHugging Face transformersåº“ï¼Œå»ºè®®ä½¿ç”¨transformersçš„æœ€æ–°ç‰ˆæœ¬ã€‚\n\nè‹¥ä½¿ç”¨transformers<4.37.0ç‰ˆæœ¬ï¼Œå°†ä¼šé‡åˆ°ä»¥ä¸‹æŠ¥é”™ï¼š\n\nKeyError: 'qwen2'\n\nå¿«é€Ÿå…¥é—¨\n\nä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ apply_chat_template åŠ è½½åˆ†è¯å™¨ä¸æ¨¡å‹ï¼Œå¹¶å®ç°å†…å®¹ç”ŸæˆåŠŸèƒ½ã€‚\n\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nå¤„ç†é•¿æ–‡æœ¬\n\nå½“å‰çš„ config.json è®¾ç½®ä¸ºæ”¯æŒæœ€é•¿ 32,768 ä¸ª token çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚\nè‹¥è¦å¤„ç†è¶…è¿‡ 32,768 ä¸ª token çš„è¶…é•¿è¾“å…¥ï¼Œæˆ‘ä»¬é‡‡ç”¨ YaRN æŠ€æœ¯æ¥å¢å¼ºæ¨¡å‹çš„é•¿æ–‡æœ¬å¤–æ¨èƒ½åŠ›ï¼Œç¡®ä¿åœ¨è¶…é•¿æ–‡æœ¬ä¸Šä»èƒ½ä¿æŒæœ€ä½³æ€§èƒ½ã€‚\n\nå¯¹äºæ”¯æŒçš„æ¡†æ¶ï¼Œæ‚¨å¯ä»¥åœ¨ config.json ä¸­æ·»åŠ ä»¥ä¸‹é…ç½®ä»¥å¯ç”¨ YaRNï¼š\n\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n\n\nå…³äºéƒ¨ç½²ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ vLLMã€‚\nè‹¥æ‚¨å¯¹ vLLM ä¸ç†Ÿæ‚‰ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„æ–‡æ¡£äº†è§£ä½¿ç”¨æ–¹æ³•ã€‚\nç›®å‰ vLLM ä»…æ”¯æŒé™æ€ YARN é…ç½®ï¼Œè¿™æ„å‘³ç€æ— è®ºè¾“å…¥é•¿åº¦å¦‚ä½•ï¼Œæ‰©å±•å› å­å§‹ç»ˆä¿æŒä¸å˜ï¼Œå¯èƒ½å½±å“çŸ­æ–‡æœ¬çš„å¤„ç†æ€§èƒ½ã€‚\næˆ‘ä»¬å»ºè®®ä»…åœ¨éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶æ·»åŠ  rope_scaling é…ç½®é¡¹ã€‚\n\nè¯„ä¼°ä¸æ€§èƒ½\n\nå®Œæ•´è¯„ä¼°ç»“æœè¯¦è§è¿™ç¯‡ğŸ“‘ æŠ€æœ¯åšå®¢ã€‚\n\nGPU æ˜¾å­˜éœ€æ±‚åŠå¯¹åº”ååé‡æµ‹è¯•ç»“æœå¯æŸ¥é˜…æ­¤å¤„ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨è®¤ä¸ºæˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚\n\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Apache License 2.0\", \"chat\"]"
  },
  {
    "url": "https://gitcode.com/openMind/Llama2-Chinese-7b-Chat",
    "project_name": "Llama2-Chinese-7b-Chat",
    "readme": "Original Text\nLlama2ä¸­æ–‡ç¤¾åŒº\nLlama2ä¸­æ–‡å¾®è°ƒå‚æ•°\n\nç”±äºLlama2åŸç”Ÿä¸­æ–‡é€‚é…æ€§æœ‰é™ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸­æ–‡æŒ‡ä»¤é›†å¯¹meta-llama/Llama-2-7b-chat-hfè¿›è¡ŒLoRAå¾®è°ƒï¼Œæ˜¾è‘—æå‡äº†å…¶ä¸­æ–‡å¯¹è¯èƒ½åŠ›ã€‚\n\nğŸ¯ å½“å‰ç‰ˆæœ¬ä¸ºFlagAlpha/Llama2-Chinese-7b-Chat-LoRAä¸meta-llama/Llama-2-7b-chat-hfèåˆåçš„å®Œæ•´å‚æ•°ç‰ˆæœ¬ï¼Œå¼€ç®±å³ç”¨\n\nğŸš€ ç¤¾åŒºå…¥å£ï¼š\n\nGithubï¼šLlama-Chinese\n\nåœ¨çº¿ä½“éªŒï¼šllama.family\n\nğŸ”¥ ç¤¾åŒºç®€ä»‹\n\næ¬¢è¿åŠ å…¥Llama2ä¸­æ–‡ç¤¾åŒºï¼\n\næˆ‘ä»¬æ˜¯ä¸“æ³¨æå‡Llama2æ¨¡å‹ä¸­æ–‡èƒ½åŠ›çš„æŠ€æœ¯ç¤¾åŒºï¼Œè‡´åŠ›äºä»åº•å±‚é¢„è®­ç»ƒåˆ°åº”ç”¨å±‚çš„å…¨æ ˆä¼˜åŒ–ã€‚\n\né€šè¿‡æµ·é‡ä¸­æ–‡è¯­æ–™è®­ç»ƒï¼ŒæŒç»­è¿­ä»£å¢å¼ºLlama2çš„ä¸­æ–‡å¤„ç†èƒ½åŠ›\n\nè¯šé‚€å¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æœ‰çƒ­æƒ…çš„å¼€å‘è€…ä¸ç ”ç©¶è€…å…±åŒæ¢ç´¢ã€‚\n\nOpenmindåº”ç”¨æŒ‡å—\n\né€šè¿‡openmindå¹³å°è¿›è¡Œæ¨ç†ï¼š\n\nfrom openmind import AutoTokenizer, AutoModelForCausalLM, pipeline, is_torch_npu_available\nfrom openmind_hub import snapshot_download\nimport openmind\nimport torch\nimport argparse\nimport time\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_name_or_path\",\n        type=str,\n        help=\"Path to model\",\n        default=\"jeffding/Llama2-Chinese-7b-Chat-openmind\",\n    )\n    args = parser.parse_args()\n    return args\n\ndef main():\n    args = parse_args()\n    model_path = args.model_name_or_path\n\n    if is_torch_npu_available():\n        device = \"npu:0\"\n    else:\n        device = \"cpu\"\n        \n    \n    model = AutoModelForCausalLM.from_pretrained(model_path,\n                                             device_map=device,\n                                             trust_remote_code=False,\n                                             revision=\"main\").to(device)\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True,trust_remote_code=False)\n    \n    start_time = time.time()\n    \n    prompt = \"ç®€å•ä»‹ç»ä¸€ä¸‹llamasè¿™ä¸ªæ¨¡å‹\"\n    system_message = \"ä½ æ˜¯ä¸€ä¸ªæ•…äº‹å†™ä½œå°åŠ©æ‰‹\"\n    prompt_template=f'''[INST] {prompt} [/INST]\n    '''\n\n    print(\"*** Pipeline:\")\n    pipe = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=512,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.95,\n        top_k=40,\n        repetition_penalty=1.1,\n    )\n\n    print(pipe(prompt_template))\n    \n    end_time = time.time()\n    print(f\"ç¡¬ä»¶ç¯å¢ƒï¼š{device},æ¨ç†æ‰§è¡Œæ—¶é—´ï¼š{end_time - start_time}ç§’\")\n    \nif __name__ == \"__main__\":\n    main()\n\n\nğŸ¼ Community Resources\nTry Llama2 online at llama.family, featuring both the original Meta version and the fine-tuned Chinese version!\nEvaluation of Chinese Q&A capabilities for the Llama2 Chat model!\nCommunity Feishu Knowledge Baseâ€”everyone is welcome to contribute!",
    "tags": "[\"Question Answering\", \"PyTorch\", \"Transformers\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/blip_vqa_base",
    "project_name": "blip_vqa_base",
    "readme": "Original Text\nBLIPï¼šç”¨äºç»Ÿä¸€è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆçš„è¯­è¨€-å›¾åƒé¢„è®­ç»ƒå¯åŠ¨æ¡†æ¶\n\nåŸºäºè§†è§‰é—®ç­”çš„åŸºç¡€æ¶æ„ï¼ˆé‡‡ç”¨ ViT åŸºç¡€ä¸»å¹²ç½‘ç»œï¼‰çš„ BLIP æ¨¡å‹å¡ã€‚\n\n\nä» BLIP å®˜æ–¹ä»“åº“æ‹‰å–å›¾ç‰‡\nå¤ªé•¿ä¸è¯»ï¼ˆTL;DRï¼‰\n\nè®ºæ–‡æ‘˜è¦ä¸­ï¼Œä½œè€…ä»¬å†™é“ï¼š\n\nè§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰ä¸ºè®¸å¤šè§†è§‰è¯­è¨€ä»»åŠ¡æé«˜äº†æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ä»…åœ¨åŸºäºç†è§£çš„ä»»åŠ¡æˆ–åŸºäºç”Ÿæˆçš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæ€§èƒ½æå‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯é€šè¿‡æ‰©å¤§ç”±ä»ç½‘ç»œæ”¶é›†çš„å™ªå£°å›¾åƒ-æ–‡æœ¬å¯¹ç»„æˆçš„æ•°æ®åº“å®ç°çš„ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªæœ€ä¼˜çš„ç›‘ç£æ¥æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† BLIPï¼Œä¸€ç§æ–°çš„ VLP æ¡†æ¶ï¼Œèƒ½å¤Ÿçµæ´»åœ°è¿ç§»åˆ°è§†è§‰è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚BLIP é€šè¿‡å¯åŠ¨å¼ç”Ÿæˆæ ‡é¢˜æœ‰æ•ˆåœ°åˆ©ç”¨äº†å™ªå£°ç½‘ç»œæ•°æ®ï¼Œå…¶ä¸­æ ‡é¢˜ç”Ÿæˆå™¨ç”Ÿæˆåˆæˆæ ‡é¢˜ï¼Œè¿‡æ»¤å™¨ç§»é™¤å™ªå£°æ ‡é¢˜ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œä¾‹å¦‚å›¾åƒ-æ–‡æœ¬æ£€ç´¢ï¼ˆå¹³å‡å¬å›ç‡@1 æé«˜è‡³ +2.7%ï¼‰ã€å›¾åƒæ ‡é¢˜ç”Ÿæˆï¼ˆCIDEr æé«˜è‡³ +2.8%ï¼‰å’Œ VQAï¼ˆVQA åˆ†æ•°æé«˜è‡³ +1.6%ï¼‰ã€‚BLIP åœ¨ç›´æ¥è¿ç§»åˆ°è§†é¢‘è¯­è¨€ä»»åŠ¡å¹¶ä»¥é›¶æ ·æœ¬æ–¹å¼è¿›è¡Œæ—¶ï¼Œä¹Ÿå±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒã€‚\n\nä½¿ç”¨æ–¹æ³•\n\næ‚¨å¯ä»¥ä½¿ç”¨æ­¤æ¨¡å‹è¿›è¡Œæ¡ä»¶æ€§å’Œæ— æ¡ä»¶æ€§çš„å›¾åƒæ ‡é¢˜ç”Ÿæˆã€‚\n\nä½¿ç”¨ Pytorch æ¨¡å‹\nåœ¨ CPU ä¸Šè¿è¡Œæ¨¡å‹\nç‚¹å‡»å±•å¼€",
    "tags": "[\"PyTorch\", \"TensorFlow\", \"Transformers\", \"Safetensors\", \"BSD 3-Clause New or Revised\", \"visual-question-answering\"]"
  },
  {
    "url": "https://gitcode.com/MooYeh/instruct-pix2pix",
    "project_name": "instruct-pix2pix",
    "readme": "Original Text\næŒ‡ç¤ºPix2Pix: å­¦ä¹ éµå¾ªå›¾åƒç¼–è¾‘æŒ‡ä»¤\n\nGitHub: https://github.com/timothybrooks/instruct-pix2pix \n\nç¤ºä¾‹\n\nè¦ä½¿ç”¨InstructPix2Pixï¼Œè¯·å…ˆä½¿ç”¨ main å®‰è£… diffusersã€‚è¯¥ç®¡é“å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­æä¾›\n\npip install diffusers accelerate safetensors transformers\n\n\nå¥½çš„ï¼Œè¯·ç»™æˆ‘æ‚¨éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ã€‚æˆ‘ä¼šå°½åŠ›å°†å…¶ç¿»è¯‘æˆé€šä¿—ã€ä¸“ä¸šã€ä¼˜é›…ä¸”æµç•…çš„ä¸­æ–‡ï¼Œå¹¶ä¿ç•™åŸå§‹çš„ Markdown æ ¼å¼ã€‚\n\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n\nmodel_id = \"timbrooks/instruct-pix2pix\"\npipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\npipe.to(\"cuda\")\npipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n\nurl = \"https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg\"\ndef download_image(url):\n    image = PIL.Image.open(requests.get(url, stream=True).raw)\n    image = PIL.ImageOps.exif_transpose(image)\n    image = image.convert(\"RGB\")\n    return image\nimage = download_image(url)\n\nprompt = \"turn him into cyborg\"\nimages = pipe(prompt, image=image, num_inference_steps=10, image_guidance_scale=1).images\nimages[0]\n\n\nå¥½çš„ï¼Œè¯·æä¾›æ‚¨è¦ç¿»è¯‘çš„æ–‡æœ¬ã€‚æˆ‘ä¼šå°½åŠ›å°†å…¶ç¿»è¯‘æˆé€šä¿—ã€ä¸“ä¸šã€ä¼˜é›…ä¸”æµç•…çš„ä¸­æ–‡ï¼Œå¹¶ä¿ç•™åŸå§‹çš„ Markdown æ ¼å¼ã€‚",
    "tags": "[\"PyTorch\", \"TensorFlow\", \"Transformers\", \"Diffusers\", \"Safetensors\", \"MIT\", \"image-to-image\"]"
  },
  {
    "url": "https://gitcode.com/MooYeh/albert_base_v2",
    "project_name": "albert_base_v2",
    "readme": "Original Text\nä¿®æ”¹å†…å®¹\nä¿®æ”¹ç¤ºä¾‹å¹¶æ·»åŠ  NPU æ”¯æŒï¼›\nä¿®æ”¹â€œé™åˆ¶ä¸åå·®â€å’Œâ€œé¢„æœŸç”¨é€”ä¸é™åˆ¶â€éƒ¨åˆ†ã€‚\nALBERT åŸºç¡€ç‰ˆ v2\n\nè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ç›®æ ‡åœ¨è‹±è¯­è¯­è¨€ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚å®ƒé¦–æ¬¡åœ¨ this paper ä¸­æå‡ºï¼Œå¹¶åœ¨ this repository ä¸­é¦–æ¬¡å‘å¸ƒã€‚å’Œæ‰€æœ‰ ALBERT æ¨¡å‹ä¸€æ ·ï¼Œè¯¥æ¨¡å‹ä¸åŒºåˆ†å¤§å°å†™ï¼šå®ƒå¯¹å¾… english å’Œ English æ²¡æœ‰åŒºåˆ«ã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒ ALBERT çš„å›¢é˜Ÿæ²¡æœ‰ä¸ºè¿™ä¸ªæ¨¡å‹ç¼–å†™æ¨¡å‹å¡ç‰‡ï¼Œå› æ­¤è¿™å¼ æ¨¡å‹å¡ç‰‡æ˜¯ç”± Hugging Face å›¢é˜Ÿç¼–å†™çš„ã€‚\n\næ¨¡å‹æè¿°\n\nALBERT æ˜¯ä¸€ä¸ªåœ¨å¤§é‡è‹±æ–‡æ•°æ®é›†ä¸Šä»¥è‡ªç›‘ç£æ–¹å¼é¢„è®­ç»ƒçš„å˜æ¢å™¨æ¨¡å‹ã€‚è¿™æ„å‘³ç€å®ƒä»…åœ¨åŸå§‹æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ²¡æœ‰äººç±»ä»¥ä»»ä½•æ–¹å¼å¯¹å…¶è¿›è¡Œæ ‡æ³¨ï¼ˆè¿™ä¹Ÿæ˜¯å®ƒå¯ä»¥ä½¿ç”¨å¤§é‡å…¬å¼€æ•°æ®çš„åŸå› ï¼‰å¹¶é€šè¿‡è‡ªåŠ¨è¿‡ç¨‹ä»è¿™äº›æ–‡æœ¬ä¸­ç”Ÿæˆè¾“å…¥å’Œæ ‡ç­¾ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒæ˜¯é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒçš„ï¼š\n\næ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ï¼šæ¨¡å‹æ¥æ”¶ä¸€ä¸ªå¥å­ï¼Œéšæœºé®è”½è¾“å…¥ä¸­çš„ 15% çš„å•è¯ï¼Œç„¶åå°†æ•´ä¸ªé®è”½çš„å¥å­è¾“å…¥æ¨¡å‹ï¼Œå¹¶é¢„æµ‹é®è”½çš„å•è¯ã€‚è¿™ä¸ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰é€šå¸¸é€ä¸ªçœ‹åˆ°å•è¯ï¼Œæˆ–è€…åƒ GPT è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹å†…éƒ¨é®è”½æœªæ¥æ ‡è®°ä¸åŒã€‚å®ƒä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¥å­çš„åŒå‘è¡¨ç¤ºã€‚\nå¥å­æ’åºé¢„æµ‹ï¼ˆSOPï¼‰ï¼šALBERT ä½¿ç”¨ä¸€ä¸ªåŸºäºé¢„æµ‹ä¸¤ä¸ªè¿ç»­æ–‡æœ¬ç‰‡æ®µæ’åºçš„é¢„è®­ç»ƒæŸå¤±ã€‚\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å­¦ä¹ åˆ°äº†è‹±è¯­è¯­è¨€çš„å†…éƒ¨è¡¨ç¤ºï¼Œç„¶åå¯ä»¥ç”¨äºæå–å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾ï¼šä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªå¸¦æœ‰æ ‡æ³¨å¥å­çš„æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ ALBERT æ¨¡å‹äº§ç”Ÿçš„ç‰¹å¾ä½œä¸ºè¾“å…¥æ¥è®­ç»ƒä¸€ä¸ªæ ‡å‡†çš„åˆ†ç±»å™¨ã€‚\n\nALBERT çš„ç‰¹åˆ«ä¹‹å¤„åœ¨äºå®ƒåœ¨å˜æ¢å™¨ä¸­å…±äº«å…¶å±‚ã€‚å› æ­¤ï¼Œæ‰€æœ‰å±‚éƒ½æœ‰ç›¸åŒçš„æƒé‡ã€‚ä½¿ç”¨é‡å¤å±‚å¯ä»¥é™ä½å†…å­˜å ç”¨ï¼Œä½†è®¡ç®—æˆæœ¬ä¸å…·æœ‰ç›¸åŒæ•°é‡éšè—å±‚çš„ BERT ç±»ä¼¼æ¶æ„ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒéœ€è¦éå†ç›¸åŒæ•°é‡çš„ï¼ˆé‡å¤ï¼‰å±‚ã€‚\n\nè¿™æ˜¯åŸºç¡€æ¨¡å‹çš„ç¬¬äºŒä¸ªç‰ˆæœ¬ã€‚ç‰ˆæœ¬ 2 ä¸ç‰ˆæœ¬ 1 çš„ä¸åŒä¹‹å¤„åœ¨äºä¸åŒçš„ä¸¢å¼ƒç‡ã€é¢å¤–çš„è®­ç»ƒæ•°æ®å’Œæ›´é•¿çš„è®­ç»ƒæ—¶é—´ã€‚å®ƒåœ¨å‡ ä¹æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡ä¸­éƒ½æœ‰æ›´å¥½çš„ç»“æœã€‚\n\nè¯¥æ¨¡å‹å…·æœ‰ä»¥ä¸‹é…ç½®ï¼š\n\n12 ä¸ªé‡å¤å±‚\n128 ç»´åµŒå…¥ç»´åº¦\n768 ç»´éšè—ç»´åº¦\n12 ä¸ªæ³¨æ„åŠ›å¤´\n1100 ä¸‡ä¸ªå‚æ•°\né¢„æœŸç”¨é€”ä¸é™åˆ¶\n\næ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œæ©ç è¯­è¨€æ¨¡å‹æˆ–ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼Œä½†å®ƒä¸»è¦æ—¨åœ¨åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n\nè¯·æ³¨æ„ï¼Œè¿™ä¸ªæ¨¡å‹ä¸»è¦æ—¨åœ¨é’ˆå¯¹ä½¿ç”¨æ•´ä¸ªå¥å­ï¼ˆå¯èƒ½è¢«é®è”½ï¼‰æ¥åšå†³ç­–çš„ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä¾‹å¦‚åºåˆ—åˆ†ç±»ã€æ ‡è®°åˆ†ç±»æˆ–é—®é¢˜å›ç­”ã€‚å¯¹äºåƒæ–‡æœ¬ç”Ÿæˆè¿™æ ·çš„ä»»åŠ¡ï¼Œæ‚¨åº”è¯¥è€ƒè™‘ä½¿ç”¨ GPT2 è¿™æ ·çš„æ¨¡å‹ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\næ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤æ¨¡å‹é€šè¿‡ä¸€ä¸ªç”¨äºæ©ç è¯­è¨€æ¨¡å‹çš„ç®¡é“ï¼š\n\nimport torch\nfrom openmind import pipeline\n\nunmasker = pipeline('fill-mask', device_map=\"npu:0\", model='PyTorch-NPU/albert_base_v2')\nunmasker(\"Hello I'm a [MASK] model.\")\n\nå±€é™æ€§ä¸åè§\n\nå³ä¾¿ç”¨äºè®­ç»ƒæ­¤æ¨¡å‹çš„è®­ç»ƒæ•°æ®å¯ä»¥è¢«æè¿°ä¸ºç›¸å¯¹ä¸­ç«‹ï¼Œè¯¥æ¨¡å‹ä»å¯èƒ½äº§ç”Ÿå¸¦æœ‰åè§çš„é¢„æµ‹ç»“æœã€‚è¿™ç§åè§åŒæ ·ä¼šå½±å“è¯¥æ¨¡å‹çš„æ‰€æœ‰å¾®è°ƒç‰ˆæœ¬ã€‚\n\nè®­ç»ƒæ•°æ®\n\nALBERT æ¨¡å‹åœ¨ BookCorpus æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…æ‹¬ 11,038 éƒ¨æœªå‘è¡¨ä¹¦ç±ä»¥åŠ è‹±æ–‡ç»´åŸºç™¾ç§‘ï¼ˆä¸å«åˆ—è¡¨ã€è¡¨æ ¼å’Œæ ‡é¢˜ï¼‰ã€‚\n\nè®­ç»ƒè¿‡ç¨‹\né¢„å¤„ç†\n\næ–‡æœ¬ä¼šè¢«è½¬æ¢ä¸ºå°å†™å¹¶è¿›è¡Œåˆ†è¯ï¼Œä½¿ç”¨çš„å·¥å…·ä¸º SentencePieceï¼Œè¯æ±‡è¡¨å¤§å°ä¸º 30,000ã€‚æ¨¡å‹çš„è¾“å…¥å½¢å¼å¦‚ä¸‹ï¼š\n\n[CLS] Sentence A [SEP] Sentence B [SEP]\n\nè®­ç»ƒ\n\nALBERT ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹éµå¾ª BERT çš„è®¾ç½®ã€‚\n\nå¯¹äºæ¯ä¸ªå¥å­çš„æ©ç ç¨‹åºçš„ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n15% çš„ä»¤ç‰Œè¢«æ©ç ã€‚\nåœ¨ 80% çš„æƒ…å†µä¸‹ï¼Œè¢«æ©ç çš„ä»¤ç‰Œè¢«æ›¿æ¢ä¸º `\n@article{DBLP:journals/corr/abs-1909-11942,\n  author    = {Zhenzhong Lan and\n               Mingda Chen and\n               Sebastian Goodman and\n               Kevin Gimpel and\n               Piyush Sharma and\n               Radu Soricut},\n  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language\n               Representations},\n  journal   = {CoRR},\n  volume    = {abs/1909.11942},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1909.11942},\n  archivePrefix = {arXiv},\n  eprint    = {1909.11942},\n  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n\nå½“ç„¶ï¼Œæˆ‘ä¼šå°½åŠ›æ»¡è¶³æ‚¨çš„è¦æ±‚ã€‚ä¸è¿‡ï¼Œæ‚¨ä¼¼ä¹å¿˜è®°æä¾›éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ã€‚è¯·æ‚¨æä¾›åŸæ–‡ï¼Œæˆ‘å°†ä¼šç¿»è¯‘æˆé€šä¿—ã€ä¸“ä¸šã€ä¼˜é›…ä¸”æµç•…çš„ä¸­æ–‡å†…å®¹ï¼Œå¹¶ä¿æŒåŸå§‹çš„ Markdown æ ¼å¼ã€‚",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Safetensors\", \"English\", \"Apache License 2.0\", \"bookcorpus\", \"wikipedia\"]"
  },
  {
    "url": "https://gitcode.com/MooYeh/bloom_7b1",
    "project_name": "bloom_7b1",
    "readme": "Original Text\nBLOOM è¯­è¨€æ¨¡å‹\nBigScience å¤§å‹å¼€æ”¾ç§‘å­¦å¤šè¯­è¨€å¼€æ”¾è®¿é—®è¯­è¨€æ¨¡å‹\næ¨¡å‹å¡ç‰‡\n\nç‰ˆæœ¬ 1.0 / 2022å¹´5æœˆ26æ—¥\n\nç›®å½•\næ¨¡å‹è¯¦æƒ…\nç”¨é€”\nè®­ç»ƒæ•°æ®\né£é™©ä¸é™åˆ¶\nè¯„ä¼°\nå»ºè®®\næœ¯è¯­ä¸è®¡ç®—\næ›´å¤šä¿¡æ¯\næ¨¡å‹å¡ç‰‡ä½œè€…\nä¿®æ”¹è¯´æ˜\n\næ·»åŠ äº†ç¤ºä¾‹ä»£ç å¹¶è°ƒæ•´äº†é“¾æ¥è·¯å¾„\n\næ¨¡å‹è¯¦æƒ…\nå¿«é€Ÿå¼€å§‹\n\nä»¥ä¸‹ä»£ç å±•ç¤ºäº†ä¸ bloom_7b1 äº¤äº’çš„ç¤ºä¾‹ï¼š\n\nimport torch\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bloom_7b1\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"PyTorch-NPU/bloom_7b1\", trust_remote_code=True, device_map=\"auto\")\n\ninput = \"Give three tips for staying healthy.\"\nprompt = (\"Below is an instrunction that describes a task. \"\n              \"Write a response that appropriately completes the requests\\n\\n\"\n              f\"### Instruction:\\n{input}\\n\\n### Response:\")\ninputs = tokenizer(prompt, return_tensors=\"pt\")\ninputs = inputs.to(model.device)\n\npred = model.generate(**inputs, max_new_tokens=512, repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\nåŸºç¡€ä¿¡æ¯\n\næœ¬èŠ‚é¢å‘æ‰€æœ‰å¸Œæœ›äº†è§£è¯¥æ¨¡å‹çš„ç”¨æˆ·ã€‚\n\nç‚¹å‡»å±•å¼€\næŠ€æœ¯è§„æ ¼\n\næœ¬èŠ‚é¢å‘æ¨¡å‹å¼€å‘ç›¸å…³äººå‘˜ã€‚\n\nç‚¹å‡»å±•å¼€\nç¯å¢ƒå½±å“\nç‚¹å‡»å±•å¼€\n\nÂ \n\nä½¿ç”¨åœºæ™¯\n\næœ¬èŠ‚è¯´æ˜æ¨¡å‹é¢„æœŸç”¨é€”ï¼Œè®¨è®ºå¯èƒ½æ¶‰åŠçš„å—ä¼—ç¾¤ä½“ï¼ˆåŒ…æ‹¬å—æ¨¡å‹å½±å“è€…ï¼‰ï¼Œå¹¶ç•Œå®šè¶…å‡ºèŒƒå›´æˆ–æ„æˆæ»¥ç”¨çš„åœºæ™¯ã€‚\né¢å‘è€ƒè™‘ä½¿ç”¨æ¨¡å‹æˆ–å—æ¨¡å‹å½±å“çš„ä»»ä½•ç”¨æˆ·ã€‚\n\nç‚¹å‡»å±•å¼€\nç›®æ ‡ç”¨æˆ·\nç›´æ¥ç”¨æˆ·\n\næ™®é€šå…¬ä¼—\n\nç ”ç©¶äººå‘˜\n\nå­¦ç”Ÿ\n\næ•™è‚²å·¥ä½œè€…\n\nå·¥ç¨‹å¸ˆ/å¼€å‘è€…\n\néå•†ä¸šå®ä½“\n\nç¤¾åŒºå€¡å¯¼è€…ï¼ŒåŒ…æ‹¬äººæƒå’Œå…¬æ°‘æƒåˆ©ç»„ç»‡\n\né—´æ¥ç”¨æˆ·\n\nç›´æ¥ç”¨æˆ·æ‰€åˆ›å»ºè¡ç”Ÿå“çš„ä½¿ç”¨è€…ï¼Œä¾‹å¦‚ä½¿ç”¨å…·æœ‰é¢„æœŸç”¨é€”è½¯ä»¶çš„ç”¨æˆ·\n\næ¨¡å‹è¡ç”Ÿå“çš„ä½¿ç”¨è€…ï¼Œå¦‚è®¸å¯è¯æ‰€è¿°\n\nå…¶ä»–å—å½±å“æ–¹ï¼ˆåˆ©ç›Šç›¸å…³è€…ï¼‰\n\nè¢«å¤§è¯­è¨€æ¨¡å‹æåŠçš„ä¸ªäººå’Œç¾¤ä½“\n\næ¥è§¦åˆ°å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºæˆ–åŸºäºå…¶å†³ç­–çš„ä¸ªäººå’Œç¾¤ä½“\n\nå…¶åŸåˆ›ä½œå“è¢«çº³å…¥å¤§è¯­è¨€æ¨¡å‹çš„ä¸ªäººå’Œç¾¤ä½“\n\nÂ \n\nè®­ç»ƒæ•°æ®\n\næœ¬èŠ‚æ¦‚è¿°äº†è®­ç»ƒæ•°æ®çš„åŸºæœ¬æƒ…å†µï¼Œé€‚åˆæƒ³äº†è§£æ¨¡å‹å­¦ä¹ åŸºç¡€å†…å®¹çš„è¯»è€…ã€‚\n\nç‚¹å‡»å±•å¼€\n\nÂ \n\né£é™©ä¸é™åˆ¶\n\næœ¬èŠ‚åˆ—å‡ºäº†å¯é¢„è§çš„å±å®³å’Œè¯¯è§£ã€‚\n\nç‚¹å‡»å±•å¼€\n\nÂ \n\nè¯„ä¼°\n\næœ¬èŠ‚æè¿°äº†è¯„ä¼°åè®®å¹¶æä¾›äº†ç»“æœã€‚\n\nç‚¹å‡»å±•å¼€\n\nÂ \n\nä½¿ç”¨å»ºè®®\n\næœ¬èŠ‚æä¾›äº†è­¦å‘Šä¿¡æ¯å’Œæ½œåœ¨ç¼“è§£æªæ–½ã€‚\n\nç‚¹å‡»å±•å¼€\n\nÂ \n\næœ¯è¯­è¡¨ä¸è®¡ç®—æ–¹æ³•\n\næœ¬èŠ‚å®šä¹‰äº†å¸¸ç”¨æœ¯è¯­åŠæŒ‡æ ‡çš„è®¡ç®—æ–¹å¼ã€‚\n\nç‚¹å‡»å±•å¼€\n\nÂ \n\næ›´å¤šä¿¡æ¯\nç‚¹å‡»å±•å¼€\n\nÂ \n\næ¨¡å‹å¡ä½œè€…\n\næŒ‰å‚ä¸æ—¶é—´åŠè´¡çŒ®åº¦æ’åº\n\nç›æ ¼ä¸½ç‰¹Â·ç±³åˆ‡å°”ï¼Œè´¾è¾¾Â·çš®æ–¯è’‚åˆ©ï¼Œé›…è¾›Â·æ°å°¼ç‰¹ï¼ŒåŸƒæ´¥ä¸‡Â·å¥¥ä½é˜¿å°¼ï¼Œç›ä¸½èÂ·æ ¼å¥‡å…‹ï¼Œçº³å…¹å®Â·æ‹‰è´¾å°¼ï¼Œè¨æ²™Â·å¢å¥‡å¥¥å°¼ï¼Œè‰¾ç³Â·ç´¢è±æ›¼ï¼Œç›è±å§†Â·é©¬è‹å¾·ï¼Œç´¢ç›è€¶Â·å°¼å…‹æ™®å°”ï¼Œå¡æ´›æ–¯Â·ç©†å°¼å¥¥æ–¯Â·è´¹å…°è¿ªæ–¯ï¼Œæ–¯å¡”æ–¯Â·è´å…‹æ›¼ï¼Œå…‹é‡Œæ–¯æ‰˜å¼—Â·é˜¿åŸºåŸºï¼Œä¸¹éº¦Â·æ‰¿åŒ…å•†ï¼Œå¤§å«Â·å…°æ–¯åŸºï¼Œå®‰å‰ä¸½å¨œÂ·éº¦å…‹ç±³å…°-æ¢…æ°ï¼Œç‰¹é‡Œæ–¯å¦Â·æ€æ‹‰ä»€ï¼Œè‹çŠå¨œÂ·ä¼Šåˆ©å¥‡ï¼Œçƒ­æ‹‰å°”Â·æœé‚¦ï¼Œè°¢æ©Â·æœ—æ™®é›·ï¼Œé©¬å—Â·æˆ´ä¼Šï¼Œæ–¯ç‰¹æ‹‰Â·æ¯”å¾·æ›¼ï¼Œæœå¨Â·åŸºæ‹‰ï¼Œè‰¾ç±³Â·è´å‹’ï¼Œæ³°æ–‡Â·å‹’Â·æ–¯å¡å¥¥ï¼Œäºšä¼¦Â·æˆˆå¡æ–¯å…°ï¼Œæœ±åˆ©å®‰Â·åŠ³å¥ˆï¼Œå°¼å…‹æ‹‰æ–¯Â·ç©†æ©å°¼éœå¤«",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"Safetensors\", \"JAX\", \"English\", \"Chinese\", \"Other\", \"bloom\"]"
  },
  {
    "url": "https://gitcode.com/MooYeh/chatglm3_6b",
    "project_name": "chatglm3_6b",
    "readme": "Original Text\nChatGLM3-6B\n\nğŸ’» é¡¹ç›®ä»£ç åº“ â€¢ ğŸ¦ æ¨ç‰¹ â€¢ ğŸ“ƒ [GLM@ACL 22] [ä»£ç åº“] â€¢ ğŸ“ƒ [GLM-130B@ICLR 23] [ä»£ç åº“]\n\n\nğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„Slackå’Œå¾®ä¿¡ç¤¾åŒº\n\nğŸ“è®¿é—®chatglm.cnä½“éªŒæ›´å¤§è§„æ¨¡çš„ChatGLMæ¨¡å‹\n\næ›´æ–°è¯´æ˜ (Modification)\n\nä¼˜åŒ–ç¤ºä¾‹ä»£ç ï¼Œæ–°å¢NPUç¡¬ä»¶æ”¯æŒï¼›\n\nè°ƒæ•´ä¾èµ–é¡¹é…ç½®ï¼›\n\nUpdated examples with NPU support;\n\nModified dependency settings;\n\næ¨¡å‹ä»‹ç» (Introduction)\n\nChatGLM3-6B ä½œä¸ºChatGLMç³»åˆ—çš„æœ€æ–°ä¸€ä»£å¼€æºæ¨¡å‹ï¼Œåœ¨å»¶ç»­å‰ä¸¤ä»£äº§å“å¯¹è¯æµç•…ã€æ˜“éƒ¨ç½²ç­‰ä¼˜åŠ¿çš„åŒæ—¶ï¼Œå®ç°äº†ä»¥ä¸‹çªç ´æ€§æå‡ï¼š\n\næ›´å¼ºå¤§çš„åŸºåº§æ¨¡å‹ï¼š ChatGLM3-6Bçš„åŸºåº§æ¨¡å‹ChatGLM3-6B-Baseé€šè¿‡é‡‡ç”¨æ›´ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ã€æ›´å……åˆ†çš„è®­ç»ƒè¿­ä»£å’Œæ›´ç§‘å­¦çš„è®­ç»ƒç­–ç•¥ã€‚åœ¨è¯­ä¹‰ç†è§£ã€æ•°å­¦æ¨ç†ã€ç¼–ç¨‹èƒ½åŠ›ã€çŸ¥è¯†é—®ç­”ç­‰å¤šç»´åº¦çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒChatGLM3-6B-Baseå±•ç°å‡ºåŒè§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„é¡¶å°–æ€§èƒ½ã€‚\næ›´å®Œå–„çš„åŠŸèƒ½ä½“ç³»ï¼š ChatGLM3-6Bé‡‡ç”¨å…¨æ–°è®¾è®¡çš„æç¤ºè¯æ ¼å¼ï¼Œä¸ä»…æ”¯æŒå¸¸è§„å¤šè½®å¯¹è¯ï¼Œæ›´åŸç”Ÿé›†æˆå·¥å…·è°ƒç”¨ã€ä»£ç è§£é‡Šå™¨(Code Interpreter)å’Œæ™ºèƒ½ä½“ä»»åŠ¡ç­‰å¤æ‚åœºæ™¯ã€‚\næ›´å¼€æ”¾çš„å¼€æºç”Ÿæ€ï¼š æœ¬æ¬¡å¼€æºåºåˆ—é™¤å¯¹è¯æ¨¡å‹ChatGLM3-6Bå¤–ï¼Œè¿˜åŒ…æ‹¬åŸºåº§æ¨¡å‹ChatGLM-6B-Baseã€é•¿æ–‡æœ¬æ¨¡å‹ChatGLM3-6B-32Kã€‚æ‰€æœ‰æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œæäº¤ç”³è¯·é—®å·ç™»è®°åå¯å…è´¹å•†ç”¨ã€‚\n\nChatGLM3-6B represents the newest generation of open-source models in the ChatGLM series. While maintaining the hallmark features of smooth conversation and easy deployment from its predecessors, it introduces groundbreaking enhancements:\n\nMore Robust Foundation Model: The base model ChatGLM3-6B-Base leverages more diverse training data, extended training iterations, and optimized training protocols. Benchmark results across semantics, mathematical reasoning, coding, and knowledge demonstrate its state-of-the-art performance among sub-10B pre-trained models.\nEnhanced Functional Capabilities: Featuring a redesigned prompt structure, ChatGLM3-6B supports not only multi-turn dialogues but also native function calling, code interpretation, and agent-based task execution.\nComprehensive Open-Source Portfolio: The release includes not just the conversational ChatGLM3-6B, but also the foundational ChatGLM-6B-Base and long-context ChatGLM3-6B-32K. All weights are fully accessible for academic research, with free commercial use permitted upon completing the registration form.\nç¯å¢ƒä¾èµ– (Dependencies)\npip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate openmind\n\nä»£ç è°ƒç”¨ (Code Usage)\n\nå¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç è°ƒç”¨ ChatGLM3-6B æ¨¡å‹ç”Ÿæˆå¯¹è¯å†…å®¹ï¼š\n\nYou can generate dialogue by calling the ChatGLM3-6B model with the following code:\n\nfrom openmind import is_torch_npu_available, AutoTokenizer, AutoModel\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/chatglm3_6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"PyTorch-NPU/chatglm3_6b\", trust_remote_code=True, device_map=device).half()\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\nprint(response)\nresponse, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\nprint(response)\n\n\nå¦‚éœ€äº†è§£æ›´å¤šä½¿ç”¨æŒ‡å—ï¼ŒåŒ…æ‹¬å¦‚ä½•è¿è¡Œå‘½ä»¤è¡Œä¸ç½‘é¡µç‰ˆæ¼”ç¤ºç¨‹åºï¼Œä»¥åŠé€šè¿‡æ¨¡å‹é‡åŒ–æŠ€æœ¯ä¼˜åŒ–æ˜¾å­˜å ç”¨ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„ Github é¡¹ç›®ä»“åº“ã€‚\n\nFor detailed instructions on running command-line and web demos, along with model quantization techniques, please visit our Github Repository.\n\nä½¿ç”¨è®¸å¯\n\næœ¬ä»£ç åº“ä¾æ® Apache-2.0 åè®®å¼€æ”¾æºä»£ç ï¼ŒChatGLM3-6B æ¨¡å‹æƒé‡çš„ä½¿ç”¨éœ€éµå®ˆ æ¨¡å‹è®¸å¯åè®®ã€‚\n\nThe source code in this repository is licensed under Apache-2.0, while the ChatGLM3-6B model weights are governed by the Model License.\n\næ–‡çŒ®å¼•ç”¨\n\nè‹¥æ‚¨è®¤ä¸ºæˆ‘ä»¬çš„ç ”ç©¶æˆæœå¯¹æ‚¨æœ‰æ‰€è£¨ç›Šï¼Œæ¬¢è¿å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ã€‚\n\nShould you find our research beneficial, we kindly invite you to reference the following papers.\n\n@article{zeng2022glm,\n  title={Glm-130b: An open bilingual pre-trained model},\n  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},\n  journal={arXiv preprint arXiv:2210.02414},\n  year={2022}\n}\n\n@inproceedings{du2022glm,\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={320--335},\n  year={2022}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\", \"glm\", \"chatglm\", \"thudm\"]"
  },
  {
    "url": "https://gitcode.com/openMind/albert_large_v2",
    "project_name": "albert_large_v2",
    "readme": "Original Text\nä¿®æ”¹\nä¿®æ”¹ç¤ºä¾‹å¹¶æ·»åŠ  npu æ”¯æŒï¼›\nä¿®æ”¹â€œé™åˆ¶ä¸åå·®â€å’Œâ€œé¢„æœŸç”¨é€”ä¸é™åˆ¶â€éƒ¨åˆ†ã€‚\nALBERT Large v2\n\nè¿™æ˜¯ä¸€ç§åœ¨è‹±è¯­è¯­æ–™åº“ä¸Šä½¿ç”¨é®è”½è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ç›®æ ‡é¢„è®­ç»ƒçš„æ¨¡å‹ã€‚å®ƒåœ¨ this paper ä¸­è¢«æå‡ºï¼Œå¹¶åœ¨ this repository ä¸­é¦–æ¬¡å‘å¸ƒã€‚ä¸æ‰€æœ‰ ALBERT æ¨¡å‹ä¸€æ ·ï¼Œæ­¤æ¨¡å‹æ˜¯ä¸åŒºåˆ†å¤§å°å†™çš„ï¼šå®ƒä¸å¯¹ english å’Œ English åšå‡ºåŒºåˆ†ã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒ ALBERT çš„å›¢é˜Ÿæœªä¸ºæ­¤æ¨¡å‹ç¼–å†™æ¨¡å‹å¡ç‰‡ï¼Œå› æ­¤æ­¤æ¨¡å‹å¡ç‰‡ç”± Hugging Face å›¢é˜Ÿç¼–å†™ã€‚\n\næ¨¡å‹æè¿°\n\nALBERT æ˜¯ä¸€ç§åœ¨å¤§é‡è‹±è¯­æ•°æ®è¯­æ–™åº“ä¸­ä»¥è‡ªæˆ‘ç›‘ç£æ–¹å¼é¢„è®­ç»ƒçš„è½¬æ¢å™¨æ¨¡å‹ã€‚è¿™æ„å‘³ç€å®ƒæ˜¯ä»…åœ¨åŸå§‹æ–‡æœ¬ä¸Šé¢„è®­ç»ƒçš„ï¼Œæ²¡æœ‰äººç±»ä»¥ä»»ä½•æ–¹å¼æ ‡æ³¨å®ƒä»¬ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒå¯ä»¥ä½¿ç”¨å¤§é‡å…¬å¼€å¯ç”¨çš„æ•°æ®ï¼‰å¹¶é€šè¿‡è‡ªåŠ¨è¿‡ç¨‹ä»è¿™äº›æ–‡æœ¬ç”Ÿæˆè¾“å…¥å’Œæ ‡ç­¾ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒæ˜¯ä½¿ç”¨ä¸¤ä¸ªç›®æ ‡é¢„è®­ç»ƒçš„ï¼š\n\né®è”½è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ï¼šæ¨¡å‹éšæœºé®è”½è¾“å…¥å¥å­çš„ 15% çš„å•è¯ï¼Œç„¶åè¿è¡Œæ•´ä¸ªé®è”½å¥å­é€šè¿‡æ¨¡å‹å¹¶é¢„æµ‹é®è”½çš„å•è¯ã€‚è¿™ä¸ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰é€šå¸¸é€ä¸ªæŸ¥çœ‹å•è¯ï¼Œæˆ–è€…åƒ GPT è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹å†…éƒ¨é®è”½æœªæ¥æ ‡è®°ä¸åŒã€‚å®ƒä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¥å­çš„åŒå‘è¡¨ç¤ºã€‚\nå¥å­é¡ºåºé¢„æµ‹ï¼ˆSOPï¼‰ï¼šALBERT ä½¿ç”¨åŸºäºé¢„æµ‹ä¸¤ä¸ªè¿ç»­æ–‡æœ¬ç‰‡æ®µé¡ºåºçš„é¢„è®­ç»ƒæŸå¤±ã€‚\n\nè¿™ç§æ–¹å¼ä½¿æ¨¡å‹å­¦ä¹ äº†è‹±è¯­è¯­è¨€çš„å†…éƒ¨è¡¨ç¤ºï¼Œç„¶åå¯ä»¥ç”¨äºæå–å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾ï¼šä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªæ ‡è®°å¥å­çš„æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ ALBERT æ¨¡å‹ç”Ÿæˆçš„ç‰¹å¾ä½œä¸ºè¾“å…¥æ¥è®­ç»ƒä¸€ä¸ªæ ‡å‡†åˆ†ç±»å™¨ã€‚\n\nALBERT çš„ç‰¹åˆ«ä¹‹å¤„åœ¨äºå®ƒåœ¨ Transformer ä¸­å…±äº«å…¶å±‚ã€‚å› æ­¤ï¼Œæ‰€æœ‰å±‚éƒ½æœ‰ç›¸åŒçš„æƒé‡ã€‚ä½¿ç”¨é‡å¤å±‚ä¼šå¯¼è‡´å†…å­˜å ç”¨è¾ƒå°ï¼Œç„¶è€Œï¼Œè®¡ç®—æˆæœ¬ä¸å…·æœ‰ç›¸åŒæ•°é‡çš„éšè—å±‚çš„ BERT ç±»ä¼¼æ¶æ„ä¿æŒç›¸ä¼¼ï¼Œå› ä¸ºå®ƒå¿…é¡»éå†ç›¸åŒæ•°é‡çš„ï¼ˆé‡å¤ï¼‰å±‚ã€‚\n\nè¿™æ˜¯å¤§å‹æ¨¡å‹çš„ç¬¬äºŒä¸ªç‰ˆæœ¬ã€‚ç‰ˆæœ¬ 2 ä¸ç‰ˆæœ¬ 1 çš„ä¸åŒä¹‹å¤„åœ¨äºä¸åŒçš„ä¸¢å¼ƒç‡ã€é¢å¤–çš„è®­ç»ƒæ•°æ®å’Œæ›´é•¿çš„è®­ç»ƒæ—¶é—´ã€‚å®ƒåœ¨å‡ ä¹æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡ä¸­éƒ½æœ‰æ›´å¥½çš„ç»“æœã€‚\n\næ­¤æ¨¡å‹å…·æœ‰ä»¥ä¸‹é…ç½®ï¼š\n\n24 ä¸ªé‡å¤å±‚\n128 ç»´åµŒå…¥ç»´åº¦\n1024 ç»´éšè—ç»´åº¦\n16 ä¸ªæ³¨æ„åŠ›å¤´\n1700 ä¸‡å‚æ•°\né¢„æœŸç”¨é€”ä¸é™åˆ¶\n\næ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œé®è”½è¯­è¨€å»ºæ¨¡æˆ–ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼Œä½†å®ƒä¸»è¦æ—¨åœ¨åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n\nè¯·æ³¨æ„ï¼Œæ­¤æ¨¡å‹ä¸»è¦æ—¨åœ¨åœ¨é‚£äº›éœ€è¦ä½¿ç”¨æ•´ä¸ªå¥å­ï¼ˆå¯èƒ½è¢«é®è”½ï¼‰æ¥åšå‡ºå†³ç­–çš„ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä¾‹å¦‚åºåˆ—åˆ†ç±»ã€æ ‡è®°åˆ†ç±»æˆ–é—®ç­”ã€‚å¯¹äºæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ï¼Œæ‚¨åº”è¯¥æŸ¥çœ‹åƒ GPT2 è¿™æ ·çš„æ¨¡å‹ã€‚\n\nå¦‚ä½•ä½¿ç”¨\n\næ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤æ¨¡å‹ä¸é®è”½è¯­è¨€å»ºæ¨¡çš„ç®¡é“ï¼š\n\nfrom openmind import pipeline\nunmasker = pipeline('fill-mask', device_map=\"npu:0\", model='PyTorch-NPU/albert_large_v2')\nunmasker(\"Hello I'm a [MASK] model.\")\n\nå±€é™æ€§ä¸åè§\n\nå°½ç®¡ç”¨äºè®­ç»ƒæ­¤æ¨¡å‹çš„è®­ç»ƒæ•°æ®å¯ä»¥è¢«æè¿°ä¸ºç›¸å¯¹ä¸­ç«‹ï¼Œä½†è¯¥æ¨¡å‹ä»å¯èƒ½äº§ç”Ÿå¸¦æœ‰åè§çš„é¢„æµ‹ã€‚è¿™ç§åè§ä¹Ÿä¼šå½±å“æ‰€æœ‰å¯¹æ­¤æ¨¡å‹çš„å¾®è°ƒç‰ˆæœ¬ã€‚\n\nè®­ç»ƒæ•°æ®\n\nALBERT æ¨¡å‹æ˜¯åœ¨ BookCorpus æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œè¯¥æ•°æ®é›†åŒ…æ‹¬ 11,038 æœ¬æœªå‘è¡¨ä¹¦ç±å’Œ English Wikipediaï¼ˆä¸åŒ…æ‹¬åˆ—è¡¨ã€è¡¨æ ¼å’Œæ ‡é¢˜ï¼‰ã€‚\n\nè®­ç»ƒè¿‡ç¨‹\né¢„å¤„ç†\n\næ–‡æœ¬è¢«è½¬æ¢ä¸ºå°å†™å¹¶ä½¿ç”¨ SentencePiece è¿›è¡Œåˆ†è¯ï¼Œè¯æ±‡é‡è®¾ç½®ä¸º 30,000ã€‚æ¨¡å‹çš„è¾“å…¥å½¢å¼å¦‚ä¸‹ï¼š\n\n[CLS] Sentence A [SEP] Sentence B [SEP]\n\nè®­ç»ƒ\n\nALBERT ç®—æ³•éµå¾ª BERT çš„è®¾ç½®ã€‚\n\nå¯¹äºæ¯ä¸ªå¥å­çš„é®è”½å¤„ç†ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n15% çš„æ ‡è®°è¢«é®è”½ã€‚\nåœ¨ 80% çš„æƒ…å†µä¸‹ï¼Œé®è”½çš„æ ‡è®°è¢«æ›¿æ¢ä¸º `\n@article{DBLP:journals/corr/abs-1909-11942,\n  author    = {Zhenzhong Lan and\n               Mingda Chen and\n               Sebastian Goodman and\n               Kevin Gimpel and\n               Piyush Sharma and\n               Radu Soricut},\n  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language\n               Representations},\n  journal   = {CoRR},\n  volume    = {abs/1909.11942},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1909.11942},\n  archivePrefix = {arXiv},\n  eprint    = {1909.11942},\n  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n\nå½“ç„¶ï¼Œè¯·æ‚¨æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬ï¼Œæˆ‘å°†æŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Safetensors\", \"English\", \"Apache License 2.0\", \"bookcorpus\", \"wikipedia\"]"
  },
  {
    "url": "https://gitcode.com/openMind/bert_large_uncased",
    "project_name": "bert_large_uncased",
    "readme": "BERT large model (uncased)\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model is uncased: it does not make a difference between english and English.\n\nModification\n\nAdd CANN version dependency instructions on the original README and modify the example code section.\n\nModel description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with two objectives:\n\nMasked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\nNext sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the BERT model as inputs.\n\nThis model has the following configuration:\n\n24-layer\n1024 hidden dimension\n16 attention heads\n336M parameters.\nIntended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.\n\nHow to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n>>> from openmind import pipeline\n>>> unmasker = pipeline('fill-mask', model='PyTorch-NPU/bert_large_uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1886913776397705,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a professional model. [SEP]\",\n  'score': 0.07157472521066666,\n  'token': 2658,\n  'token_str': 'professional'},\n {'sequence': \"[CLS] hello i'm a male model. [SEP]\",\n  'score': 0.04053466394543648,\n  'token': 3287,\n  'token_str': 'male'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.03891477733850479,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fitness model. [SEP]\",\n  'score': 0.03038121573626995,\n  'token': 10516,\n  'token_str': 'fitness'}]\n\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('PyTorch-NPU/bert_large_uncased')\nmodel = BertModel.from_pretrained(\"PyTorch-NPU/bert_large_uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n\nLimitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions:\n\n>>> from openmind import pipeline\n>>> unmasker = pipeline('fill-mask', model='PyTorch-NPU/bert_large_uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a bartender. [SEP]',\n  'score': 0.10426565259695053,\n  'token': 15812,\n  'token_str': 'bartender'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.10232779383659363,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.06281787157058716,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a lawyer. [SEP]',\n  'score': 0.050936125218868256,\n  'token': 5160,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.041034240275621414,\n  'token': 10533,\n  'token_str': 'carpenter'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.28473711013793945,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.11336520314216614,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a bartender. [SEP]',\n  'score': 0.09574324637651443,\n  'token': 15812,\n  'token_str': 'bartender'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.06351090222597122,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a secretary. [SEP]',\n  'score': 0.048970773816108704,\n  'token': 3187,\n  'token_str': 'secretary'}]\n\n\nThis bias will also affect all fine-tuned versions of this model.\n\nTraining data\n\nThe BERT model was pretrained on BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia (excluding lists, tables and headers).\n\nTraining procedure\nPreprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are then of the form:\n\n[CLS] Sentence A [SEP] Sentence B [SEP]\n\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two \"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n\n15% of the tokens are masked.\nIn 80% of the cases, the masked tokens are replaced by [MASK].\nIn 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\nIn the 10% remaining cases, the masked tokens are left as is.\nPretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\\beta_{1} = 0.9\\) and \\(\\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\nEvaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nModel\tSQUAD 1.1 F1/EM\tMulti NLI Accuracy\nBERT-Large, Uncased (Original)\t91.0/84.3\t86.05\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Rust\", \"Safetensors\", \"JAX\", \"English\", \"Apache License 2.0\", \"bookcorpus\", \"wikipedia\", \"exbert\"]"
  },
  {
    "url": "https://gitcode.com/openMind/camembert_ner",
    "project_name": "camembert_ner",
    "readme": "Original Text\ncamembert-nerï¼šåŸºäºcamemBERTè¿›è¡Œå‘½åå®ä½“è¯†åˆ«ä»»åŠ¡å¾®è°ƒçš„æ¨¡å‹ã€‚\nä¿®æ”¹\nä¿®æ”¹ç¤ºä¾‹ä»£ç ä»¥æ”¯æŒopenMindï¼Œå¹¶æ·»åŠ NPUæ”¯æŒã€‚\nç®€ä»‹\n\ncamembert-ner æ˜¯ä¸€ç§å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ¨¡å‹ï¼Œå®ƒæ˜¯åœ¨ wikiner-fr æ•°æ®é›†ä¸Šå¯¹ camemBERT æ¨¡å‹è¿›è¡Œå¾®è°ƒå¾—åˆ°çš„ã€‚è¯¥æ¨¡å‹åœ¨ wikiner-fr æ•°æ®é›†ï¼ˆçº¦170,634ä¸ªå¥å­ï¼‰ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚æ¨¡å‹åœ¨ç”µå­é‚®ä»¶/èŠå¤©æ•°æ®ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶ä¸”åœ¨å¤„ç†è¿™ç±»æ•°æ®æ—¶ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¯¹äºä¸ä»¥å¤§å†™å­—æ¯å¼€å¤´çš„å®ä½“ï¼Œè¯¥æ¨¡å‹ä¼¼ä¹å·¥ä½œå¾—æ›´å¥½ã€‚\n\nè®­ç»ƒæ•°æ®\n\nè®­ç»ƒæ•°æ®æŒ‰ä»¥ä¸‹ç±»åˆ«è¿›è¡Œäº†åˆ†ç±»ï¼š\n\nç¼©å†™\tæè¿°\nO\tåœ¨å‘½åå®ä½“ä¹‹å¤–\nMISC\tæ‚é¡¹å®ä½“\nPER\tä¸ªäººå§“å\nORG\tç»„ç»‡\nLOC\tåœ°ç‚¹\nå¦‚ä½•ä½¿ç”¨ HuggingFace ä¸Šçš„ camembert-ner\nåŠ è½½ camembert-ner åŠå…¶å­è¯åˆ†è¯å™¨ï¼š\nimport torch\nfrom openmind import AutoTokenizer, pipeline, is_torch_npu_available\nfrom transformers import AutoModelForTokenClassification\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelse:\n    device = \"cpu\"\n\nmodel_path= \"PyTorch-NPU/camembert_ner\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForTokenClassification.from_pretrained(model_path)\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)\noutput = nlp(\"Apple est crÃ©Ã©e le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs Ã  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constituÃ©e sous forme de sociÃ©tÃ© le 3 janvier 1977 Ã  l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour reflÃ©ter la diversification de ses produits, le mot Â« computer Â» est retirÃ© le 9 janvier 2015.\")\nprint(f'>>>output={output}')\n\næ¨¡å‹æ€§èƒ½æŒ‡æ ‡ï¼ˆè¯„ä»·æŒ‡æ ‡ï¼šseqevalï¼‰\n\næ€»ä½“è¡¨ç°\n\nç²¾ç¡®åº¦\tå¬å›ç‡\tF1åˆ†æ•°\n0.8859\t0.8971\t0.8914\n\næŒ‰å®ä½“ç±»å‹åˆ†ç±»\n\nå®ä½“ç±»å‹\tç²¾ç¡®åº¦\tå¬å›ç‡\tF1åˆ†æ•°\nPER\t0.9372\t0.9598\t0.9483\nORG\t0.8099\t0.8265\t0.8181\nLOC\t0.8905\t0.9005\t0.8955\nMISC\t0.8175\t0.8117\t0.8146\n\nå¯¹äºæœ‰å…´è¶£çš„äººï¼Œè¿™é‡Œæœ‰ä¸€ç¯‡ç®€çŸ­çš„æ–‡ç« ï¼Œä»‹ç»äº†æˆ‘æ˜¯å¦‚ä½•åˆ©ç”¨è¿™ä¸ªæ¨¡å‹çš„ç»“æœæ¥è®­ç»ƒä¸€ä¸ªç”¨äºç”µå­é‚®ä»¶ç­¾åæ£€æµ‹çš„LSTMæ¨¡å‹çš„ï¼š https://medium.com/@jean-baptiste.polle/lstm-model-for-email-signature-detection-8e990384fefa",
    "tags": "[\"Token Classification\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"ONNX\", \"Safetensors\", \"English\", \"MIT\"]"
  },
  {
    "url": "https://gitcode.com/openMind/flan_t5_large",
    "project_name": "flan_t5_large",
    "readme": "Original Text\nFLAN-T5å¤§å‹æ¨¡å‹çš„æ¨¡å‹å¡\n\nç›®å½•\næ‘˜è¦\næ¨¡å‹è¯¦æƒ…\nä½¿ç”¨æ–¹æ³•\nåº”ç”¨åœºæ™¯\nåè§ã€é£é™©ä¸é™åˆ¶\nè®­ç»ƒè¯¦æƒ…\nè¯„ä¼°\nç¯å¢ƒå½±å“\nå¼•ç”¨\næ¨¡å‹å¡ä½œè€…\næ‘˜è¦\n\nå¦‚æœä½ å·²ç»äº†è§£T5ï¼Œé‚£ä¹ˆFLAN-T5åœ¨å„æ–¹é¢éƒ½è¡¨ç°å¾—æ›´å¥½ã€‚åœ¨ç›¸åŒçš„å‚æ•°æ•°é‡ä¸‹ï¼Œè¿™äº›æ¨¡å‹åœ¨è¶…è¿‡1000ä¸ªé¢å¤–ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œè¦†ç›–äº†æ›´å¤šè¯­è¨€ã€‚ æ­£å¦‚æ‘˜è¦å¼€å¤´å‡ è¡Œæåˆ°ï¼š\n\nFlan-PaLM 540Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œä¾‹å¦‚åœ¨äº”æ¬¡æç¤ºçš„MMLUä¸Šè¾¾åˆ°äº†75.2%ã€‚æˆ‘ä»¬è¿˜å…¬å¼€å‘å¸ƒäº†Flan-T5çš„æ£€æŸ¥ç‚¹ï¼Œå³ä½¿ä¸æ›´å¤§çš„æ¨¡å‹å¦‚PaLM 62Bç›¸æ¯”ï¼Œä¹Ÿå…·æœ‰å¼ºå¤§çš„å°‘æ ·æœ¬æ€§èƒ½ã€‚æ€»çš„æ¥è¯´ï¼ŒæŒ‡ä»¤å¾®è°ƒæ˜¯ä¸€ç§æé«˜é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ€§èƒ½å’Œå¯ç”¨æ€§çš„é€šç”¨æ–¹æ³•ã€‚\n\nå…è´£å£°æ˜ï¼šæœ¬æ¨¡å‹å¡çš„å†…å®¹ç”±Hugging Faceå›¢é˜Ÿæ’°å†™ï¼Œéƒ¨åˆ†å†…å®¹ä»T5æ¨¡å‹å¡å¤åˆ¶ç²˜è´´è€Œæ¥ã€‚\n\nä¿®æ”¹\n\nåœ¨åŸå§‹READMEä¸­æ·»åŠ äº†CANNç‰ˆæœ¬ä¾èµ–æè¿°å¹¶ä¿®æ”¹äº†ç¤ºä¾‹ä»£ç ã€‚\n\næ¨¡å‹è¯¦æƒ…\næ¨¡å‹æè¿°\næ¨¡å‹ç±»å‹ï¼š è¯­è¨€æ¨¡å‹\nè¯­è¨€ï¼ˆNLPï¼‰ï¼š è‹±è¯­ã€è¥¿ç­ç‰™è¯­ã€æ—¥è¯­ã€æ³¢æ–¯è¯­ã€å°åœ°è¯­ã€æ³•è¯­ã€ä¸­æ–‡ã€å­ŸåŠ æ‹‰è¯­ã€å¤å‰æ‹‰ç‰¹è¯­ã€å¾·è¯­ã€æ³°å¢å›ºè¯­ã€æ„å¤§åˆ©è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€æ³¢å…°è¯­ã€æ³°ç±³å°”è¯­ã€é©¬æ‹‰åœ°è¯­ã€é©¬æ‹‰é›…æ‹‰å§†è¯­ã€å¥¥é‡Œäºšè¯­ã€æ—é®æ™®è¯­ã€è‘¡è„ç‰™è¯­ã€ä¹Œå°”éƒ½è¯­ã€åŠ åˆ©è¥¿äºšè¯­ã€å¸Œä¼¯æ¥è¯­ã€éŸ©è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€æ³°è¯­ã€è·å…°è¯­ã€å°åº¦å°¼è¥¿äºšè¯­ã€è¶Šå—è¯­ã€ä¿åŠ åˆ©äºšè¯­ã€è²å¾‹å®¾è¯­ã€ä¸­å¤®é«˜æ£‰è¯­ã€è€æŒè¯­ã€åœŸè€³å…¶è¯­ã€ä¿„è¯­ã€å…‹ç½—åœ°äºšè¯­ã€ç‘å…¸è¯­ã€çº¦é²å·´è¯­ã€åº“å°”å¾·è¯­ã€ç¼…ç”¸è¯­ã€é©¬æ¥è¯­ã€æ·å…‹è¯­ã€èŠ¬å…°è¯­ã€ç´¢é©¬é‡Œè¯­ã€ä»–åŠ ç¦„è¯­ã€æ–¯ç“¦å¸Œé‡Œè¯­ã€åƒ§ä¼½ç½—è¯­ã€å¡çº³è¾¾è¯­ã€å£®è¯­ã€ä¼Šåšè¯­ã€ç§‘è¨è¯­ã€ç½—é©¬å°¼äºšè¯­ã€æµ·åœ°è¯­ã€çˆ±æ²™å°¼äºšè¯­ã€æ–¯æ´›ä¼å…‹è¯­ã€ç«‹é™¶å®›è¯­ã€å¸Œè…Šè¯­ã€å°¼æ³Šå°”è¯­ã€é˜¿è¨å§†è¯­ã€æŒªå¨è¯­\nè®¸å¯è¯ï¼š Apache 2.0\nåŸå§‹æ£€æŸ¥ç‚¹ï¼š æ‰€æœ‰åŸå§‹FLAN-T5æ£€æŸ¥ç‚¹\næ›´å¤šä¿¡æ¯èµ„æºï¼š\nç ”ç©¶è®ºæ–‡\nGitHubä»£ç ä»“åº“\nä½¿ç”¨æ–¹æ³•\n\nä»¥ä¸‹æ˜¯åœ¨openmindä¸­ä½¿ç”¨æ¨¡å‹çš„ç¤ºä¾‹è„šæœ¬ï¼š\n\nä½¿ç”¨PyTorchæ¨¡å‹\nåœ¨NPUä¸Šè¿è¡Œæ¨¡å‹\nç‚¹å‡»å±•å¼€\nç”¨é€”\nç›´æ¥ä½¿ç”¨ä¸ä¸‹æ¸¸ä½¿ç”¨\n\nä½œè€…åœ¨åŸè®ºæ–‡çš„æ¨¡å‹å¡ä¸­å†™é“ï¼š\n\nä¸»è¦ç”¨é€”æ˜¯è¯­è¨€æ¨¡å‹ç ”ç©¶ï¼ŒåŒ…æ‹¬ï¼šé›¶æ ·æœ¬NLPä»»åŠ¡çš„ç ”ç©¶å’Œä¸Šä¸‹æ–‡ä¸­å°‘é‡æ ·æœ¬å­¦ä¹ NLPä»»åŠ¡ï¼Œå¦‚æ¨ç†å’Œé—®ç­”ï¼›æ¨åŠ¨å…¬å¹³æ€§å’Œå®‰å…¨æ€§ç ”ç©¶ï¼Œä»¥åŠäº†è§£å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§\n\nè¯¦æƒ…è¯·å‚é˜…ç ”ç©¶è®ºæ–‡ã€‚\n\nè¶…å‡ºèŒƒå›´çš„ä½¿ç”¨\n\néœ€è¦æ›´å¤šä¿¡æ¯ã€‚\n\nåè§ã€é£é™©å’Œå±€é™æ€§\n\nä»¥ä¸‹æœ¬èŠ‚ä¸­çš„ä¿¡æ¯æ˜¯ä»æ¨¡å‹çš„å®˜æ–¹æ¨¡å‹å¡å¤åˆ¶è€Œæ¥çš„ï¼š\n\næ ¹æ®Raeç­‰äººï¼ˆ2021å¹´ï¼‰çš„è¯´æ³•ï¼ŒåŒ…æ‹¬Flan-T5åœ¨å†…çš„è¯­è¨€æ¨¡å‹å¯èƒ½ä¼šè¢«ç”¨äºä»¥æœ‰å®³æ–¹å¼ç”Ÿæˆè¯­è¨€ã€‚åœ¨æ²¡æœ‰å¯¹ç‰¹å®šåº”ç”¨çš„å®‰å…¨æ€§å’Œå…¬å¹³æ€§å…³åˆ‡è¿›è¡Œé¢„å…ˆè¯„ä¼°çš„æƒ…å†µä¸‹ï¼Œä¸åº”ç›´æ¥åœ¨ä»»ä½•åº”ç”¨ä¸­ä½¿ç”¨Flan-T5ã€‚\n\nä¼¦ç†è€ƒé‡ä¸é£é™©\n\nFlan-T5åœ¨å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¿™äº›æ•°æ®é›†æœªç»æ˜¾å¼å†…å®¹è¿‡æ»¤æˆ–è¯„ä¼°ç°æœ‰åè§ã€‚å› æ­¤ï¼Œæ¨¡å‹æœ¬èº«å¯èƒ½å­˜åœ¨ç”ŸæˆåŒç­‰ä¸é€‚å½“å†…å®¹æˆ–å¤åˆ¶åº•å±‚æ•°æ®ä¸­å›ºæœ‰åè§çš„é£é™©ã€‚\n\nå·²çŸ¥å±€é™æ€§\n\nFlan-T5å°šæœªåœ¨å®é™…ä¸–ç•Œåº”ç”¨ä¸­è¿›è¡Œæµ‹è¯•ã€‚\n\næ•æ„Ÿä½¿ç”¨ï¼š\n\nFlan-T5ä¸åº”åº”ç”¨äºä»»ä½•ä¸å¯æ¥å—çš„ä½¿ç”¨åœºæ™¯ï¼Œä¾‹å¦‚ç”Ÿæˆä¾®è¾±æ€§è¨€è®ºã€‚\n\nè®­ç»ƒç»†èŠ‚\nè®­ç»ƒæ•°æ®\n\nè¯¥æ¨¡å‹åœ¨åŒ…æ‹¬ä»¥ä¸‹è¡¨æ ¼ä¸­æè¿°çš„ä»»åŠ¡ï¼ˆæ¥è‡ªåŸè®ºæ–‡ï¼Œå›¾2ï¼‰åœ¨å†…çš„å¤šç§ä»»åŠ¡ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼š\n\nè®­ç»ƒè¿‡ç¨‹\n\næ ¹æ®åŸè®ºæ–‡ä¸­çš„æ¨¡å‹å¡ï¼š\n\nè¿™äº›æ¨¡å‹åŸºäºé¢„è®­ç»ƒçš„T5ï¼ˆRaffel et al.ï¼Œ2020ï¼‰å¹¶é’ˆå¯¹æ›´å¥½çš„é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬æ€§èƒ½è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚æ¯ç§T5æ¨¡å‹å¤§å°éƒ½æœ‰ä¸€ç§å¾®è°ƒåçš„Flanæ¨¡å‹ã€‚\n\nè¯¥æ¨¡å‹åœ¨TPU v3æˆ–TPU v4æœºæ¶ä¸Šä½¿ç”¨t5xä»£ç åº“å’Œjaxè¿›è¡Œè®­ç»ƒã€‚\n\nè¯„ä¼°\næµ‹è¯•æ•°æ®ã€å› ç´ å’ŒæŒ‡æ ‡\n\nä½œè€…åœ¨å„ç§ä»»åŠ¡ä¸Šè¯„ä¼°äº†æ¨¡å‹ï¼Œæ¶µç›–äº†å¤šç§è¯­è¨€ï¼ˆæ€»è®¡1836ç§ï¼‰ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å®šé‡è¯„ä¼°çš„è¡¨æ ¼ï¼š\n\nè¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹ç ”ç©¶è®ºæ–‡ã€‚\n\nç»“æœ\n\nå…³äºFLAN-T5-Largeçš„å®Œæ•´ç»“æœï¼Œè¯·å‚è§ç ”ç©¶è®ºæ–‡çš„è¡¨3ã€‚\n\nç¯å¢ƒå½±å“\n\nç¢³æ’æ”¾å¯ä»¥ä½¿ç”¨Lacosteç­‰äººï¼ˆ2019ï¼‰æå‡ºçš„æœºå™¨å­¦ä¹ å½±å“è®¡ç®—å™¨è¿›è¡Œä¼°è®¡ã€‚\n\nç¡¬ä»¶ç±»å‹ï¼š Google Cloud TPUæœºæ¶ - TPU v3æˆ–TPU v4 | èŠ¯ç‰‡æ•°é‡ â‰¥ 4ã€‚\nä½¿ç”¨æ—¶é—´ï¼š éœ€è¦æ›´å¤šä¿¡æ¯\näº‘æœåŠ¡æä¾›å•†ï¼š GCP\nè®¡ç®—åŒºåŸŸï¼š éœ€è¦æ›´å¤šä¿¡æ¯\nç¢³æ’æ”¾ï¼š éœ€è¦æ›´å¤šä¿¡æ¯\nå¼•ç”¨\n\nBibTeXï¼š\n\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n",
    "tags": "[\"PyTorch\", \"TensorFlow\", \"Transformers\", \"Safetensors\", \"JAX\", \"English\", \"Apache License 2.0\", \"10 datasets\", \"text2text-generation\"]"
  },
  {
    "url": "https://gitcode.com/openMind/flan_t5_small",
    "project_name": "flan_t5_small",
    "readme": "Model Card for FLAN-T5 small\n\nTable of Contents\nTL;DR\nModel Details\nUsage\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nModel Card Authors\nTL;DR\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. As mentioned in the first few lines of the abstract :\n\nFlan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\nDisclaimer: Content from this model card has been written by the Hugging Face team.\n\nModification\n\nAdded the CANN version dependency description to the original README and modified the example code.\n\nModel Details\nModel Description\nModel type: Language model\nLanguage(s) (NLP): English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\nLicense: Apache 2.0\nOriginal Checkpoints: All Original FLAN-T5 Checkpoints\nResources for more information:\nResearch paper\nGitHub Repo\nHugging Face FLAN-T5 Docs (Similar to T5)\nUsage\n\nFind below some example scripts on how to use the model in openmind:\n\nUsing the Pytorch model\nRunning the model on a NPU\nClick to expand\nUses\nDirect Use and Downstream Use\n\nThe authors write in the original paper's model card that:\n\nThe primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\n\nSee the research paper for further details.\n\nOut-of-Scope Use\n\nMore information needed.\n\nBias, Risks, and Limitations\n\nThe information below in this section are copied from the model's official model card:\n\nLanguage models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n\nEthical considerations and risks\n\nFlan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\nKnown Limitations\n\nFlan-T5 has not been tested in real world applications.\n\nSensitive Use:\n\nFlan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\n\nTraining Details\nTraining Data\n\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\n\nTraining Procedure\n\nAccording to the model card from the original paper:\n\nThese models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\n\nThe model has been trained on TPU v3 or TPU v4 pods, using t5x codebase together with jax.\n\nEvaluation\nTesting Data, Factors & Metrics\n\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:  For full details, please check the research paper.\n\nResults\n\nFor full results for FLAN-T5-Small, see the research paper, Table 3.\n\nEnvironmental Impact\n\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\n\nHardware Type: Google Cloud TPU Pods - TPU v3 or TPU v4 | Number of chips â‰¥ 4.\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\n\nBibTeX:\n\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n",
    "tags": "[\"PyTorch\", \"TensorFlow\", \"Transformers\", \"Safetensors\", \"JAX\", \"English\", \"Apache License 2.0\", \"10 datasets\"]"
  },
  {
    "url": "https://gitcode.com/openMind/vit_msn_base",
    "project_name": "vit_msn_base",
    "readme": "Original Text\nåŸºç¡€å°ºå¯¸çš„ Vision Transformer æ¨¡å‹ï¼Œé‡‡ç”¨ MSN è¿›è¡Œé¢„è®­ç»ƒ\n\né‡‡ç”¨ MSN æ–¹æ³•é¢„è®­ç»ƒçš„ Vision Transformerï¼ˆViTï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨è®ºæ–‡ Masked Siamese Networks for Label-Efficient Learning ä¸­è¢«ä»‹ç»ï¼Œä½œè€…ä¸º Mahmoud Assranã€Mathilde Caronã€Ishan Misraã€Piotr Bojanowskiã€Florian Bordesã€Pascal Vincentã€Armand Joulinã€Michael Rabbatã€Nicolas Ballasï¼Œå¹¶äº æ­¤ä»“åº“ é¦–æ¬¡å‘å¸ƒã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒ MSN çš„å›¢é˜Ÿæœªä¸ºæ­¤æ¨¡å‹ç¼–å†™æ¨¡å‹å¡ç‰‡ï¼Œå› æ­¤æ­¤æ¨¡å‹å¡ç‰‡ç”± Hugging Face å›¢é˜Ÿæ’°å†™ã€‚\n\nä¿®æ”¹\n\nä¿®æ”¹ README.md ä¸­çš„ç¤ºä¾‹ï¼Œå¹¶æ·»åŠ å¯¹ npu çš„æ”¯æŒã€‚\n\næ¨¡å‹æè¿°\n\nVision Transformerï¼ˆViTï¼‰æ˜¯ä¸€ä¸ªç±»ä¼¼äº BERT çš„å˜å‹å™¨ç¼–ç å™¨æ¨¡å‹ã€‚å›¾åƒè¢«å‘ˆç°ä¸ºä¸€ç³»åˆ—å›ºå®šå¤§å°çš„æ–‘å—åºåˆ—ã€‚\n\nMSN æå‡ºäº†ä¸€ä¸ªè”åˆåµŒå…¥æ¶æ„ï¼Œç”¨ä»¥åŒ¹é…é®è”½æ–‘å—åŸå‹ä¸æœªé®è”½æ–‘å—åŸå‹ã€‚åœ¨è¿™ç§è®¾ç½®ä¸‹ï¼Œä»–ä»¬çš„æ–¹æ³•åœ¨å°‘é‡æ ·æœ¬å’Œæç«¯å°‘é‡æ ·æœ¬æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ã€‚\n\né€šè¿‡é¢„è®­ç»ƒï¼Œæ¨¡å‹å­¦ä¹ åˆ°äº†å›¾åƒçš„å†…åœ¨è¡¨ç¤ºï¼Œéšåå¯ç”¨äºæå–å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾ï¼šä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ç»„æ ‡è®°è¿‡çš„å›¾åƒæ•°æ®é›†ï¼Œæ‚¨å¯ä»¥é€šè¿‡åœ¨é¢„è®­ç»ƒçš„ç¼–ç å™¨ä¹‹ä¸Šæ”¾ç½®ä¸€ä¸ªçº¿æ€§å±‚æ¥è®­ç»ƒä¸€ä¸ªæ ‡å‡†çš„åˆ†ç±»å™¨ã€‚\n\né¢„å®šç”¨é€”åŠé™åˆ¶\n\næ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚è¯·æŸ¥çœ‹æ¨¡å‹ä»“åº“ä»¥å¯»æ‰¾æ‚¨æ„Ÿå…´è¶£çš„ MSN é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒç‰ˆæœ¬ã€‚å½“æ‚¨çš„è®­ç»ƒé›†ä¸­åªæœ‰å°‘é‡æ ‡è®°æ ·æœ¬æ—¶ï¼Œè¯¥æ¨¡å‹å°¤å…¶æœ‰ç›Šã€‚\n\nä½¿ç”¨æ–¹æ³•\n\nä»¥ä¸‹æ˜¯ä½¿ç”¨è¯¥åŸºç¡€ç¼–ç å™¨çš„æ–¹æ³•ï¼š\n\nimport requests\nimport torch\nfrom PIL import Image\nfrom openmind import AutoModel, AutoFeatureExtractor, is_torch_npu_available\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel_name=\"PyTorch-NPU/vit_msn_base\"\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_name, device_map=device)\nmodel = AutoModel.from_pretrained(model_name, device_map=device)\ninputs = feature_extractor(images=image, return_tensors=\"pt\").to(model.device)\nwith torch.no_grad():\n    outputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\nprint(\"last_hidden_state of the model: \", last_hidden_states)\n\nå¼•ç”¨\n@article{assran2022masked,\n  title={Masked Siamese Networks for Label-Efficient Learning}, \n  author={Assran, Mahmoud, and Caron, Mathilde, and Misra, Ishan, and Bojanowski, Piotr, and Bordes, Florian and Vincent, Pascal, and Joulin, Armand, and Rabbat, Michael, and Ballas, Nicolas},\n  journal={arXiv preprint arXiv:2204.07141},\n  year={2022}\n}\n\n\nè¯·æ‚¨æä¾›éœ€è¦ç¿»è¯‘çš„åŸå§‹æ–‡æœ¬å†…å®¹ï¼Œæˆ‘å°†ä¼šæŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚",
    "tags": "[\"Image Feature Extraction\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"NeMo\", \"ELM\", \"Apache License 2.0\", \"imagenet-1k\", \"vision\"]"
  },
  {
    "url": "https://gitcode.com/openMind/bert_base_uncased",
    "project_name": "bert_base_uncased",
    "readme": "Original Text\nBERT åŸºç¡€æ¨¡å‹ï¼ˆæœªåŒºåˆ†å¤§å°å†™ï¼‰\n\nè¯¥æ¨¡å‹é‡‡ç”¨æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ç›®æ ‡åœ¨è‹±è¯­è¯­æ–™ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç”±æ­¤è®ºæ–‡æå‡ºï¼Œå¹¶é¦–æ¬¡å‘å¸ƒäºæ­¤ä»£ç åº“ã€‚æ­¤æ¨¡å‹ä¸ºæœªåŒºåˆ†å¤§å°å†™ç‰ˆæœ¬ï¼šä¸åŒºåˆ†\"english\"å’Œ\"English\"çš„å†™æ³•ã€‚\n\nå…è´£å£°æ˜ï¼šBERTå‘å¸ƒå›¢é˜Ÿæœªä¸ºæ­¤æ¨¡å‹ç¼–å†™è¯´æ˜æ–‡æ¡£ï¼Œæœ¬æ–‡æ¡£ç”±Hugging Faceå›¢é˜Ÿæ’°å†™ã€‚\n\næ¨¡å‹æè¿°\n\nBERTæ˜¯åŸºäºTransformeræ¶æ„çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡è‡ªç›‘ç£æ–¹å¼åœ¨æµ·é‡è‹±æ–‡æ–‡æœ¬ä¸Šè®­ç»ƒè€Œæˆã€‚è¿™æ„å‘³ç€å®ƒä»…ä½¿ç”¨åŸå§‹æ–‡æœ¬è¿›è¡Œé¢„è®­ç»ƒï¼Œæ— éœ€ä»»ä½•äººå·¥æ ‡æ³¨ï¼ˆå› æ­¤å¯åˆ©ç”¨å¤§é‡å…¬å¼€æ•°æ®ï¼‰ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ä»æ–‡æœ¬ç”Ÿæˆè¾“å…¥å’Œæ ‡ç­¾ã€‚å…·ä½“è€Œè¨€ï¼Œå…¶é¢„è®­ç»ƒåŒ…å«ä¸¤ä¸ªç›®æ ‡ï¼š\n\næ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ï¼šéšæœºé®è”½è¾“å…¥å¥å­ä¸­15%çš„è¯æ±‡ï¼Œæ¨¡å‹éœ€åŸºäºä¸Šä¸‹æ–‡é¢„æµ‹è¢«é®è”½çš„è¯æ±‡ã€‚ä¸ä¼ ç»Ÿå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰é€è¯å¤„ç†æˆ–GPTç±»è‡ªå›å½’æ¨¡å‹ï¼ˆé®è”½æœªæ¥è¯å…ƒï¼‰ä¸åŒï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹èƒ½å­¦ä¹ å¥å­çš„åŒå‘è¡¨å¾ã€‚\nä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNSPï¼‰ï¼šé¢„è®­ç»ƒæ—¶å°†ä¸¤ä¸ªé®è”½åçš„å¥å­æ‹¼æ¥ä½œä¸ºè¾“å…¥ï¼Œæ¨¡å‹éœ€åˆ¤æ–­è¿™ä¸¤ä¸ªå¥å­åœ¨åŸæ–‡ä¸­æ˜¯å¦è¿ç»­å‡ºç°ã€‚\n\né€šè¿‡è¿™ç§è®¾è®¡ï¼Œæ¨¡å‹ä¹ å¾—äº†è‹±è¯­è¯­è¨€çš„å†…éƒ¨è¡¨å¾ï¼Œå¯æå–é€‚ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„ç‰¹å¾ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå·²æ ‡æ³¨å¥å­æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨BERTç”Ÿæˆçš„ç‰¹å¾ä½œä¸ºè¾“å…¥æ¥è®­ç»ƒæ ‡å‡†åˆ†ç±»å™¨ã€‚\n\næ¨¡å‹å˜ä½“\n\nBERTæœ€åˆå‘å¸ƒäº†åŸºç¡€ç‰ˆå’Œå¤§è§„æ¨¡ç‰ˆï¼ŒåŒ…å«åŒºåˆ†å¤§å°å†™å’Œä¸åŒºåˆ†å¤§å°å†™çš„ç‰ˆæœ¬ã€‚æœªåŒºåˆ†å¤§å°å†™ç‰ˆæœ¬è¿˜ä¼šå»é™¤é‡éŸ³ç¬¦å·ã€‚\néšåæ¨å‡ºäº†ä¸­æ–‡å’Œå¤šè¯­è¨€çš„åŒºåˆ†/æœªåŒºåˆ†å¤§å°å†™ç‰ˆæœ¬ã€‚\nåœ¨åç»­å·¥ä½œä¸­ï¼Œé‡‡ç”¨å…¨è¯é®è”½çš„æ”¹è¿›é¢„å¤„ç†æ–¹æ³•å–ä»£äº†å­è¯é®è”½ï¼Œå¹¶å‘å¸ƒäº†ä¸¤æ¬¾æ–°æ¨¡å‹ã€‚\nä¹‹ååˆå‘å¸ƒäº†24ä¸ªå°å‹å˜ä½“ã€‚\n\nå®Œæ•´å‘å¸ƒå†å²è¯¦è§GitHubä¸Šçš„google-research/bertè¯´æ˜æ–‡ä»¶ã€‚\n\næ¨¡å‹\tå‚æ•°é‡\tè¯­è¨€\n[bert-base-uncased]\t1.1äº¿\tè‹±è¯­\n[bert-large-uncased]\t3.4äº¿\tè‹±è¯­\n[bert-base-cased]\t1.1äº¿\tè‹±è¯­\n[bert-large-cased]\t3.4äº¿\tè‹±è¯­\n[bert-base-chinese]\t1.1äº¿\tä¸­æ–‡\n[bert-base-multilingual-cased]\t1.1äº¿\tå¤šè¯­è¨€\n[bert-large-uncased-whole-word-masking]\t3.4äº¿\tè‹±è¯­\n[bert-large-cased-whole-word-masking]\t3.4äº¿\tè‹±è¯­\né€‚ç”¨èŒƒå›´ä¸é™åˆ¶\n\næ‚¨å¯ä»¥ç›´æ¥å°†åŸå§‹æ¨¡å‹ç”¨äºæ©ç è¯­è¨€å»ºæ¨¡æˆ–ä¸‹ä¸€å¥é¢„æµ‹ä»»åŠ¡ï¼Œä½†å…¶ä¸»è¦ç”¨é€”æ˜¯åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n\néœ€æ³¨æ„ï¼Œè¯¥æ¨¡å‹ä¸»è¦é€‚ç”¨äºéœ€è¦æ•´å¥ï¼ˆå¯èƒ½å«é®è”½è¯ï¼‰è¿›è¡Œå†³ç­–çš„ä»»åŠ¡ï¼Œå¦‚åºåˆ—åˆ†ç±»ã€è¯å…ƒåˆ†ç±»æˆ–é—®ç­”ç³»ç»Ÿã€‚å¯¹äºæ–‡æœ¬ç”Ÿæˆç±»ä»»åŠ¡ï¼Œå»ºè®®è€ƒè™‘GPT2ç­‰æ¨¡å‹ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\næ‚¨å¯ä»¥ç›´æ¥é€šè¿‡æµæ°´çº¿è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡ï¼š\n\n>>> from openmind import pipeline\n>>> unmasker = pipeline('fill-mask', model='PyTorch-NPU/bert_base_uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n\n\nä»¥ä¸‹æ˜¯ä½¿ç”¨è¯¥æ¨¡å‹åœ¨ PyTorch ä¸­è·å–ç»™å®šæ–‡æœ¬ç‰¹å¾çš„æ­¥éª¤ï¼š\n\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n\nå±€é™æ€§ä¸æ½œåœ¨åè§\n\nå°½ç®¡ç”¨äºè®­ç»ƒè¯¥æ¨¡å‹çš„æ•°æ®å¯è¢«è§†ä¸ºç›¸å¯¹ä¸­ç«‹ï¼Œä½†è¯¥æ¨¡å‹ä»å¯èƒ½äº§ç”Ÿå¸¦æœ‰åè§çš„é¢„æµ‹ç»“æœï¼š\n\n>>> from openmind import pipeline\n>>> unmasker = pipeline('fill-mask', model='PyTorch-NPU/bert_base_uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n\n\nè¿™ç§åå·®åŒæ ·ä¼šå½±å“è¯¥æ¨¡å‹çš„æ‰€æœ‰å¾®è°ƒç‰ˆæœ¬ã€‚\n\nè®­ç»ƒæ•°æ®\n\nBERTæ¨¡å‹åœ¨BookCorpusä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«11,038æœ¬æœªå‡ºç‰ˆçš„ä¹¦ç±ï¼Œä»¥åŠè‹±æ–‡ç»´åŸºç™¾ç§‘ï¼ˆä¸åŒ…æ‹¬åˆ—è¡¨ã€è¡¨æ ¼å’Œæ ‡é¢˜ï¼‰ã€‚\n\nè®­ç»ƒè¿‡ç¨‹\né¢„å¤„ç†\n\næ–‡æœ¬ç»è¿‡å°å†™è½¬æ¢ï¼Œå¹¶ä½¿ç”¨WordPieceè¿›è¡Œåˆ†è¯ï¼Œè¯æ±‡è¡¨å¤§å°ä¸º30,000ã€‚æ¨¡å‹çš„è¾“å…¥æ ¼å¼å¦‚ä¸‹ï¼š\n\n[CLS] Sentence A [SEP] Sentence B [SEP]\n\n\nåœ¨50%çš„æ¦‚ç‡ä¸‹ï¼Œå¥å­Aå’Œå¥å­Bå¯¹åº”äºåŸå§‹è¯­æ–™åº“ä¸­ä¸¤ä¸ªè¿ç»­çš„å¥å­ï¼Œå…¶ä½™æƒ…å†µä¸‹åˆ™å¯¹åº”è¯­æ–™åº“ä¸­çš„å¦ä¸€ä¸ªéšæœºå¥å­ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œæ‰€ç§°çš„\"å¥å­\"é€šå¸¸æ˜¯æŒ‡æ¯”å•ä¸€å¥å­æ›´é•¿çš„è¿ç»­æ–‡æœ¬ç‰‡æ®µã€‚å”¯ä¸€çš„é™åˆ¶æ˜¯ä¸¤ä¸ª\"å¥å­\"åˆå¹¶åçš„é•¿åº¦éœ€å°äº512ä¸ªæ ‡è®°ã€‚\n\næ¯ä¸ªå¥å­çš„æ©ç å¤„ç†ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n15%çš„æ ‡è®°ä¼šè¢«æ©ç ã€‚\nå…¶ä¸­80%çš„æƒ…å†µä¸‹ï¼Œæ©ç æ ‡è®°ä¼šè¢«æ›¿æ¢ä¸º[MASK]ã€‚\n10%çš„æƒ…å†µä¸‹ï¼Œæ©ç æ ‡è®°ä¼šè¢«æ›¿æ¢ä¸ºä¸€ä¸ªéšæœºï¼ˆä¸åŒçš„ï¼‰æ ‡è®°ã€‚\nå‰©ä½™çš„10%æƒ…å†µä¸‹ï¼Œæ©ç æ ‡è®°ä¿æŒåŸæ ·ä¸å˜ã€‚\né¢„è®­ç»ƒè¿‡ç¨‹\n\nè¯¥æ¨¡å‹åœ¨4ä¸ªäº‘TPUï¼ˆå…±16ä¸ªTPUèŠ¯ç‰‡ï¼‰ä¸Šä»¥Podé…ç½®è®­ç»ƒäº†ä¸€ç™¾ä¸‡æ­¥ï¼Œæ‰¹æ¬¡å¤§å°ä¸º256ã€‚åºåˆ—é•¿åº¦é™åˆ¶ä¸ºï¼š90%çš„æ­¥æ•°é‡‡ç”¨128ä¸ªæ ‡è®°ï¼Œå‰©ä½™10%é‡‡ç”¨512ä¸ªæ ‡è®°ã€‚ä¼˜åŒ–å™¨ä½¿ç”¨Adamï¼Œå­¦ä¹ ç‡ä¸º1e-4ï¼Œ\\(\\beta_{1} = 0.9\\)å’Œ\\(\\beta_{2} = 0.999\\)ï¼Œæƒé‡è¡°å‡ä¸º0.01ï¼Œå‰10,000æ­¥è¿›è¡Œå­¦ä¹ ç‡é¢„çƒ­ï¼Œä¹‹åçº¿æ€§è¡°å‡å­¦ä¹ ç‡ã€‚\n\nè¯„ä¼°ç»“æœ\n\nåœ¨ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒåï¼Œè¯¥æ¨¡å‹å–å¾—å¦‚ä¸‹æˆç»©ï¼š\n\nGLUEæµ‹è¯•ç»“æœï¼š\n\nä»»åŠ¡\tMNLI-(m/mm)\tQQP\tQNLI\tSST-2\tCoLA\tSTS-B\tMRPC\tRTE\tå¹³å‡åˆ†\n\t84.6/83.4\t71.2\t90.5\t93.5\t52.1\t85.8\t88.9\t66.4\t79.6\nBibTeXæ¡ç›®åŠå¼•ç”¨ä¿¡æ¯\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"ONNX\", \"Rust\", \"Safetensors\", \"JAX\", \"English\", \"Apache License 2.0\", \"bookcorpus\", \"wikipedia\", \"exbert\"]"
  },
  {
    "url": "https://gitcode.com/openMind/chatglm3_6b",
    "project_name": "chatglm3_6b",
    "readme": "Original Text\nChatGLM3-6B\n\nğŸ’» é¡¹ç›®ä»£ç åº“ â€¢ ğŸ¦ æ¨ç‰¹ â€¢ ğŸ“ƒ [GLM@ACL 22] [ä»£ç åº“] â€¢ ğŸ“ƒ [GLM-130B@ICLR 23] [ä»£ç åº“]\n\n\nğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„Slackå’Œå¾®ä¿¡ç¤¾ç¾¤\n\nğŸ“ä½“éªŒæ›´å¤§è§„æ¨¡çš„ChatGLMæ¨¡å‹ï¼Œè¯·è®¿é—®chatglm.cn\n\næ›´æ–°è¯´æ˜ (Modification)\n\nä¼˜åŒ–ç¤ºä¾‹ä»£ç ï¼Œæ–°å¢NPUåŠ é€Ÿæ”¯æŒï¼›\n\nè°ƒæ•´ä¾èµ–åº“é…ç½®ï¼›\n\nUpdated examples with NPU support;\n\nModified dependencies;\n\næ¨¡å‹ä»‹ç» (Introduction)\n\nChatGLM3-6B ä½œä¸ºChatGLMç³»åˆ—çš„æœ€æ–°ä¸€ä»£å¼€æºæ¨¡å‹ï¼Œåœ¨å»¶ç»­å‰ä¸¤ä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²ä¾¿æ·ç­‰ä¼˜åŠ¿çš„åŒæ—¶ï¼Œå…·å¤‡ä»¥ä¸‹æ˜¾è‘—ç‰¹æ€§ï¼š\n\næ›´å¼ºå¤§çš„åŸºåº§æ¨¡å‹ï¼š ChatGLM3-6Bçš„åŸºåº§æ¨¡å‹ChatGLM3-6B-Baseé‡‡ç”¨æ›´ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ã€æ›´å……åˆ†çš„è®­ç»ƒè¿­ä»£å’Œæ›´ç§‘å­¦çš„è®­ç»ƒç­–ç•¥ã€‚åœ¨è¯­ä¹‰ç†è§£ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€çŸ¥è¯†é—®ç­”ç­‰å„ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼ŒChatGLM3-6B-Baseå±•ç°äº†åŒè§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„é¡¶å°–æ€§èƒ½ã€‚\næ›´å®Œå–„çš„åŠŸèƒ½ä½“ç³»ï¼š ChatGLM3-6Bé‡‡ç”¨å…¨æ–°è®¾è®¡çš„æç¤ºè¯æ ¼å¼ï¼Œä¸ä»…æ”¯æŒæµç•…çš„å¤šè½®å¯¹è¯ï¼Œè¿˜åŸç”Ÿé›†æˆå·¥å…·è°ƒç”¨ã€ä»£ç è§£é‡Šå™¨(Code Interpreter)å’Œæ™ºèƒ½ä½“ä»»åŠ¡ç­‰å¤æ‚åœºæ™¯å¤„ç†èƒ½åŠ›ã€‚\næ›´å®Œæ•´çš„å¼€æºç”Ÿæ€ï¼š é™¤å¯¹è¯æ¨¡å‹ChatGLM3-6Bå¤–ï¼ŒåŒæ­¥å¼€æºåŸºåº§æ¨¡å‹ChatGLM-6B-Baseã€é•¿æ–‡æœ¬å¯¹è¯æ¨¡å‹ChatGLM3-6B-32Kã€‚æ‰€æœ‰æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œæäº¤ç™»è®°é—®å·åå¯å…è´¹å•†ç”¨ã€‚\n\nChatGLM3-6B is the newest open-source model in the ChatGLM series. While maintaining the advantages of smooth conversation and easy deployment from its predecessors, it introduces remarkable features:\n\nMore Powerful Base Model: The foundation model ChatGLM3-6B-Base is trained with more diverse data, sufficient iterations, and optimized strategies. It demonstrates top-tier performance among pre-trained models of similar scale across various benchmarks including semantics, mathematics, coding, and knowledge QA.\nEnhanced Functional Capabilities: Featuring a redesigned prompt format, ChatGLM3-6B supports not only seamless multi-turn dialogues but also native function calling, code interpreter, and agent task handling for complex scenarios.\nComprehensive Open-Source Ecosystem: Alongside the conversational ChatGLM3-6B, we open-source the base model ChatGLM-6B-Base and long-context variant ChatGLM3-6B-32K. All weights are fully accessible for academic research and free for commercial use after submitting the registration form.\nç¯å¢ƒä¾èµ– (Dependencies)\npip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate openmind\n\nä»£ç è°ƒç”¨ (Code Usage)\n\né€šè¿‡ä»¥ä¸‹ä»£ç å³å¯è°ƒç”¨ ChatGLM3-6B æ¨¡å‹ç”Ÿæˆå¯¹è¯å†…å®¹ï¼š\n\nYou can generate dialogue responses by executing the following code snippet with the ChatGLM3-6B model:\n\nfrom openmind import is_torch_npu_available, AutoTokenizer, AutoModel\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/chatglm3_6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"PyTorch-NPU/chatglm3_6b\", trust_remote_code=True, device_map=device).half()\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\nprint(response)\nresponse, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\nprint(response)\n\n\nå…³äºæ›´å¤šä½¿ç”¨æŒ‡å—ï¼ŒåŒ…æ‹¬å¦‚ä½•è¿è¡Œå‘½ä»¤è¡Œä¸ç½‘é¡µç‰ˆæ¼”ç¤ºç¨‹åºï¼Œä»¥åŠé€šè¿‡æ¨¡å‹é‡åŒ–æŠ€æœ¯ä¼˜åŒ–æ˜¾å­˜å ç”¨ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„ Github é¡¹ç›®åº“ã€‚\n\nä½¿ç”¨è®¸å¯\n\næœ¬ä»£ç ä»“åº“éµå¾ª Apache-2.0 å¼€æºåè®®ï¼ŒChatGLM3-6B æ¨¡å‹æƒé‡çš„ä½¿ç”¨éœ€éµå®ˆ æ¨¡å‹è®¸å¯åè®®ã€‚\n\næ–‡çŒ®å¼•ç”¨\n\nè‹¥æ‚¨è®¤ä¸ºæˆ‘ä»¬çš„ç ”ç©¶æˆæœå¯¹æ‚¨æœ‰æ‰€è£¨ç›Šï¼Œæ•¬è¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ã€‚\n\n@article{zeng2022glm,\n  title={Glm-130b: An open bilingual pre-trained model},\n  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},\n  journal={arXiv preprint arXiv:2210.02414},\n  year={2022}\n}\n\n@inproceedings{du2022glm,\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={320--335},\n  year={2022}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Apache License 2.0\", \"thudm\", \"glm\", \"chatglm\"]"
  },
  {
    "url": "https://gitcode.com/openMind/convnext_tiny_224",
    "project_name": "convnext_tiny_224",
    "readme": "Original Text\nConvNeXTï¼ˆå¾®å‹æ¨¡å‹ï¼‰\n\nåœ¨224x224åˆ†è¾¨ç‡ä¸‹åŸºäºImageNet-1kè®­ç»ƒçš„ConvNeXTæ¨¡å‹ã€‚è¯¥æ¨¡å‹ç”±Liuç­‰äººå‘è¡¨åœ¨è®ºæ–‡ã€ŠA ConvNet for the 2020sã€‹ï¼ˆhttps://arxiv.org/abs/2201.03545ï¼‰ä¸­æå‡ºï¼Œå¹¶é¦–æ¬¡åœ¨æ­¤ä»£ç ä»“åº“ä¸­å‘å¸ƒã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒConvNeXTæ¨¡å‹çš„å›¢é˜Ÿæœªä¸ºæ­¤æ¨¡å‹ç¼–å†™æ¨¡å‹å¡ç‰‡ï¼Œå› æ­¤æ­¤æ¨¡å‹å¡ç‰‡ç”±Hugging Faceå›¢é˜Ÿæ’°å†™ã€‚\n\nä¿®æ”¹\nä¿®æ”¹ç¤ºä¾‹ä»£ç ä»¥æ”¯æŒopenMindå¹¶å¢åŠ NPUæ”¯æŒ\næ¨¡å‹æè¿°\n\nConvNeXTæ˜¯ä¸€ä¸ªçº¯ç²¹çš„å·ç§¯æ¨¡å‹ï¼ˆConvNetï¼‰ï¼Œå—åˆ°Vision Transformersè®¾è®¡çš„å¯å‘ï¼Œå¹¶å£°ç§°èƒ½å¤Ÿè¶…è¶Šå®ƒä»¬ã€‚ä½œè€…ä»ResNetå‡ºå‘ï¼Œå¹¶ä»¥Swin Transformerä¸ºçµæ„Ÿï¼Œå¯¹è®¾è®¡è¿›è¡Œäº†â€œç°ä»£åŒ–â€æ”¹è¿›ã€‚\n\nä½¿ç”¨æ„å›¾ä¸é™åˆ¶\n\næ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\nä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨æ­¤æ¨¡å‹å°†COCO 2017æ•°æ®é›†ä¸­çš„å›¾åƒåˆ†ç±»åˆ°1,000ä¸ªImageNetç±»åˆ«ä¹‹ä¸€çš„æ–¹æ³•ï¼š\n\nimport torch\nfrom datasets import load_dataset\nfrom openmind import is_torch_npu_available\nfrom transformers import ConvNextImageProcessor, ConvNextForImageClassification\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelse:\n    device = \"cpu\"\n\nmodel_path= \"PyTorch-NPU/convnext_tiny_224\"\ndataset = load_dataset(\"./cats_image\")\nimage = dataset[\"train\"][\"image\"][0]\n\nfeature_extractor = ConvNextImageProcessor.from_pretrained(model_path)\nmodel = ConvNextForImageClassification.from_pretrained(model_path).to(device)\ninputs = feature_extractor(image, return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_label = logits.argmax(-1).item()\nprint(f'>>>result={model.config.id2label[predicted_label]}')\n\nBibTeXæ¡ç›®ä¸å¼•ç”¨ä¿¡æ¯\n@article{DBLP:journals/corr/abs-2201-03545,\n  author    = {Zhuang Liu and\n               Hanzi Mao and\n               Chao{-}Yuan Wu and\n               Christoph Feichtenhofer and\n               Trevor Darrell and\n               Saining Xie},\n  title     = {A ConvNet for the 2020s},\n  journal   = {CoRR},\n  volume    = {abs/2201.03545},\n  year      = {2022},\n  url       = {https://arxiv.org/abs/2201.03545},\n  eprinttype = {arXiv},\n  eprint    = {2201.03545},\n  timestamp = {Thu, 20 Jan 2022 14:21:35 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-03545.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n\nå½“ç„¶ï¼Œæˆ‘ä¼šæŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚è¯·æ‚¨æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬ä»¥åŠæ‚¨æœŸæœ›çš„ä¸­æ–‡é£æ ¼ï¼ˆé€šä¿—ã€ä¸“ä¸šã€ä¼˜é›…æˆ–æµç•…ï¼‰ï¼Œæˆ‘å°†ä¼šä¸ºæ‚¨ç¿»è¯‘ã€‚",
    "tags": "[\"Image Classification\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"NeMo\", \"ELM\", \"Apache License 2.0\", \"imagenet-1k\"]"
  },
  {
    "url": "https://gitcode.com/openMind/distilbert_base_uncased",
    "project_name": "distilbert_base_uncased",
    "readme": "Original Text\nDistilBERT åŸºç¡€æ¨¡å‹ï¼ˆæ— å¤§å°å†™åŒºåˆ†ï¼‰\n\nè¯¥æ¨¡å‹æ˜¯ BERT åŸºç¡€æ¨¡å‹çš„è’¸é¦ç‰ˆæœ¬ã€‚å®ƒé¦–æ¬¡åœ¨è¿™ç¯‡è®ºæ–‡ä¸­æå‡ºã€‚è’¸é¦è¿‡ç¨‹çš„ä»£ç å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ã€‚è¯¥æ¨¡å‹ä¸åŒºåˆ†å¤§å°å†™ï¼šæ— è®ºæ˜¯ \"english\" è¿˜æ˜¯ \"English\"ï¼Œå¯¹å®ƒæ¥è¯´æ²¡æœ‰åŒºåˆ«ã€‚\n\nä¿®æ”¹\n\nä¿®æ”¹ç¤ºä¾‹ä»£ç éƒ¨åˆ†ã€‚\n\næ¨¡å‹æè¿°\n\nDistilBERT æ˜¯ä¸€ä¸ª transformers æ¨¡å‹ï¼Œæ¯” BERT æ›´å°ã€æ›´å¿«ï¼Œå®ƒåœ¨ç›¸åŒçš„è¯­æ–™åº“ä¸Šä»¥è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿ç”¨ BERT åŸºç¡€æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ã€‚è¿™æ„å‘³ç€å®ƒä»…åœ¨åŸå§‹æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ²¡æœ‰ä»»ä½•äººå·¥æ ‡ç­¾ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒå¯ä»¥ä½¿ç”¨å¤§é‡å…¬å¼€æ•°æ®çš„åŸå› ï¼‰ï¼Œé€šè¿‡è‡ªåŠ¨è¿‡ç¨‹ä»è¿™äº›æ–‡æœ¬ä¸­ç”Ÿæˆè¾“å…¥å’Œæ ‡ç­¾ï¼Œä½¿ç”¨ BERT åŸºç¡€æ¨¡å‹ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒï¼š\n\nè’¸é¦æŸå¤±ï¼šæ¨¡å‹è¢«è®­ç»ƒä»¥è¿”å›ä¸ BERT åŸºç¡€æ¨¡å‹ç›¸åŒçš„æ¦‚ç‡ã€‚\næ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ï¼šè¿™æ˜¯ BERT åŸºç¡€æ¨¡å‹åŸå§‹è®­ç»ƒæŸå¤±çš„ä¸€éƒ¨åˆ†ã€‚å½“è¾“å…¥ä¸€ä¸ªå¥å­æ—¶ï¼Œæ¨¡å‹éšæœºæ©ç  15% çš„å•è¯ï¼Œç„¶åé€šè¿‡æ¨¡å‹è¿è¡Œæ•´ä¸ªæ©ç å¥å­ï¼Œå¹¶é¢„æµ‹è¢«æ©ç çš„å•è¯ã€‚è¿™ä¸é€šå¸¸é€å­—æŸ¥çœ‹çš„ä¼ ç»Ÿå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰æˆ–å†…éƒ¨æ©ç æœªæ¥æ ‡è®°çš„è‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰ä¸åŒã€‚å®ƒå…è®¸æ¨¡å‹å­¦ä¹ å¥å­çš„åŒå‘è¡¨ç¤ºã€‚\nä½™å¼¦åµŒå…¥æŸå¤±ï¼šæ¨¡å‹è¿˜è¢«è®­ç»ƒä»¥ç”Ÿæˆå°½å¯èƒ½æ¥è¿‘ BERT åŸºç¡€æ¨¡å‹çš„éšè—çŠ¶æ€ã€‚\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å­¦ä¹ äº†ä¸å…¶æ•™å¸ˆæ¨¡å‹ç›¸åŒçš„è‹±è¯­å†…éƒ¨è¡¨ç¤ºï¼ŒåŒæ—¶åœ¨æ¨ç†æˆ–ä¸‹æ¸¸ä»»åŠ¡ä¸­æ›´å¿«ã€‚\n\né¢„æœŸç”¨é€”ä¸é™åˆ¶\n\nä½ å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡æˆ–ä¸‹ä¸€å¥é¢„æµ‹ï¼Œä½†å®ƒä¸»è¦ç”¨äºåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n\nè¯·æ³¨æ„ï¼Œè¯¥æ¨¡å‹ä¸»è¦é’ˆå¯¹åœ¨éœ€è¦ä½¿ç”¨æ•´ä¸ªå¥å­ï¼ˆå¯èƒ½è¢«æ©ç ï¼‰è¿›è¡Œå†³ç­–çš„ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä¾‹å¦‚åºåˆ—åˆ†ç±»ã€æ ‡è®°åˆ†ç±»æˆ–é—®ç­”ã€‚å¯¹äºæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ï¼Œä½ åº”è¯¥æŸ¥çœ‹åƒ GPT2 è¿™æ ·çš„æ¨¡å‹ã€‚\n\nå¦‚ä½•ä½¿ç”¨\n\nä½ å¯ä»¥ç›´æ¥ä½¿ç”¨è¯¥æ¨¡å‹é€šè¿‡ç®¡é“è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡ï¼š\n\n>>> from openmind import pipeline\n>>> unmasker = pipeline('fill-mask', model='PyTorch-NPU/distilbert_base_uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n\n\nä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨ PyTorch ä¸­ä½¿ç”¨æ­¤æ¨¡å‹è·å–ç»™å®šæ–‡æœ¬çš„ç‰¹å¾ï¼š\n\nimport openmind\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('PyTorch-NPU/distilbert_base_uncased')\nmodel = DistilBertModel.from_pretrained(\"PyTorch-NPU/distilbert_base_uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n\nå±€é™æ€§ä¸åè§\n\nå°½ç®¡ç”¨äºæ­¤æ¨¡å‹çš„è®­ç»ƒæ•°æ®å¯ä»¥è¢«æè¿°ä¸ºç›¸å½“ä¸­ç«‹ï¼Œä½†è¯¥æ¨¡å‹ä»å¯èƒ½äº§ç”Ÿå¸¦æœ‰åè§çš„é¢„æµ‹ç»“æœã€‚åŒæ—¶ï¼Œå®ƒä¹Ÿç»§æ‰¿äº†å…¶æ•™å¸ˆæ¨¡å‹çš„ä¸€äº›åè§ã€‚\n\n>>> from openmind import pipeline\n>>> unmasker = pipeline('fill-mask', model='PyTorch-NPU/distilbert_base_uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n\n\nè¿™ç§åè§ä¹Ÿä¼šå½±å“è¯¥æ¨¡å‹çš„æ‰€æœ‰å¾®è°ƒç‰ˆæœ¬ã€‚\n\nè®­ç»ƒæ•°æ®\n\nDistilBERT ä½¿ç”¨ä¸ BERT ç›¸åŒçš„æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™äº›æ•°æ®åŒ…æ‹¬ BookCorpusï¼ˆä¸€ä¸ªåŒ…å« 11,038 æœ¬æœªå‡ºç‰ˆä¹¦ç±çš„æ•°æ®é›†ï¼‰å’Œ English Wikipediaï¼ˆä¸åŒ…æ‹¬åˆ—è¡¨ã€è¡¨æ ¼å’Œæ ‡é¢˜ï¼‰ã€‚\n\nè®­ç»ƒè¿‡ç¨‹\né¢„å¤„ç†\n\næ–‡æœ¬è¢«è½¬æ¢ä¸ºå°å†™å¹¶ä½¿ç”¨ WordPiece è¿›è¡Œåˆ†è¯ï¼Œè¯æ±‡è¡¨å¤§å°ä¸º 30,000ã€‚æ¨¡å‹çš„è¾“å…¥å½¢å¼å¦‚ä¸‹ï¼š\n\n[CLS] Sentence A [SEP] Sentence B [SEP]\n\n\nä»¥50%çš„æ¦‚ç‡ï¼Œå¥å­Aå’Œå¥å­Båœ¨åŸå§‹è¯­æ–™åº“ä¸­æ˜¯è¿ç»­çš„ä¸¤ä¸ªå¥å­ï¼›å…¶ä»–æƒ…å†µä¸‹ï¼Œåˆ™ç”±è¯­æ–™åº“ä¸­çš„å¦ä¸€ä¸ªéšæœºå¥å­æ„æˆã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œæ‰€è°“çš„â€œå¥å­â€é€šå¸¸æŒ‡çš„æ˜¯è¿ç»­çš„æ–‡æœ¬ç‰‡æ®µï¼Œé•¿åº¦è¶…è¿‡ä¸€ä¸ªå•ç‹¬çš„å¥å­ã€‚å”¯ä¸€é™åˆ¶æ˜¯è¿™ä¸¤ä¸ªâ€œå¥å­â€çš„ç»„åˆé•¿åº¦éœ€å°‘äº512ä¸ªä»¤ç‰Œã€‚\n\næ¯å¥çš„é®è”½å¤„ç†ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n15%çš„ä»¤ç‰Œè¢«é®è”½ã€‚\nåœ¨80%çš„æƒ…å†µä¸‹ï¼Œè¢«é®è”½çš„ä»¤ç‰Œè¢«æ›¿æ¢ä¸º[MASK]ã€‚\nåœ¨10%çš„æƒ…å†µä¸‹ï¼Œè¢«é®è”½çš„ä»¤ç‰Œè¢«æ›¿æ¢ä¸ºä¸åŒäºåŸä»¤ç‰Œçš„éšæœºä»¤ç‰Œã€‚\nå‰©ä½™çš„10%æƒ…å†µä¸­ï¼Œè¢«é®è”½çš„ä»¤ç‰Œä¿æŒä¸å˜ã€‚\né¢„è®­ç»ƒ\n\nè¯¥æ¨¡å‹åœ¨8å—16GBçš„V100 GPUä¸Šè¿›è¡Œäº†90å°æ—¶çš„è®­ç»ƒã€‚æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†ä¿¡æ¯å¯å‚è€ƒè®­ç»ƒä»£ç ã€‚\n\nè¯„ä¼°ç»“æœ\n\nå½“åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œè¯¥æ¨¡å‹è¾¾åˆ°ä»¥ä¸‹æˆç»©ï¼š\n\nGlueæµ‹è¯•ç»“æœï¼š\n\nä»»åŠ¡\tMNLI\tQQP\tQNLI\tSST-2\tCoLA\tSTS-B\tMRPC\tRTE\næˆç»©\t82.2\t88.5\t89.2\t91.3\t51.3\t85.8\t87.5\t59.9\nBibTeXæ¡ç›®å’Œå¼•ç”¨ä¿¡æ¯\n@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}\n\næ•°æ®ç§‘å­¦ä¸­çš„ç»Ÿè®¡æ¨æ–­\nå¼•è¨€\n\nåœ¨æ•°æ®ç§‘å­¦é¢†åŸŸï¼Œç»Ÿè®¡æ¨æ–­æ˜¯ç†è§£å’Œè§£é‡Šæ•°æ®çš„å…³é”®å·¥å…·ã€‚å®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿä»æ ·æœ¬æ•°æ®ä¸­æ¨æ–­å‡ºæ€»ä½“çš„ç‰¹å¾ï¼Œä»è€Œåšå‡ºåŸºäºæ•°æ®çš„å†³ç­–ã€‚æœ¬æ–‡å°†æ¢è®¨ç»Ÿè®¡æ¨æ–­çš„åŸºæœ¬æ¦‚å¿µã€æ–¹æ³•åŠå…¶åœ¨æ•°æ®ç§‘å­¦ä¸­çš„åº”ç”¨ã€‚\n\nç»Ÿè®¡æ¨æ–­çš„åŸºæœ¬æ¦‚å¿µ\n1. å‚æ•°ä¼°è®¡\n\nå‚æ•°ä¼°è®¡æ˜¯ç»Ÿè®¡æ¨æ–­çš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€ã€‚å®ƒæ¶‰åŠä½¿ç”¨æ ·æœ¬æ•°æ®æ¥ä¼°è®¡æ€»ä½“å‚æ•°ï¼Œå¦‚å‡å€¼ã€æ–¹å·®ç­‰ã€‚å¸¸è§çš„å‚æ•°ä¼°è®¡æ–¹æ³•åŒ…æ‹¬ï¼š\n\nç‚¹ä¼°è®¡ï¼šä½¿ç”¨å•ä¸ªå€¼æ¥ä¼°è®¡æ€»ä½“å‚æ•°ã€‚\nåŒºé—´ä¼°è®¡ï¼šæä¾›ä¸€ä¸ªèŒƒå›´ï¼Œè¯¥èŒƒå›´åŒ…å«æ€»ä½“å‚æ•°çš„æ¦‚ç‡ã€‚\n2. å‡è®¾æ£€éªŒ\n\nå‡è®¾æ£€éªŒæ˜¯å¦ä¸€ç§é‡è¦çš„ç»Ÿè®¡æ¨æ–­æ–¹æ³•ã€‚å®ƒç”¨äºæ£€éªŒå…³äºæ€»ä½“å‚æ•°çš„å‡è®¾æ˜¯å¦æˆç«‹ã€‚å‡è®¾æ£€éªŒé€šå¸¸åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š\n\nè®¾å®šåŸå‡è®¾å’Œå¤‡æ‹©å‡è®¾ã€‚\né€‰æ‹©é€‚å½“çš„ç»Ÿè®¡æ£€éªŒæ–¹æ³•ã€‚\nè®¡ç®—æ£€éªŒç»Ÿè®¡é‡ã€‚\nç¡®å®šæ‹’ç»åŸŸã€‚\nåšå‡ºå†³ç­–ã€‚\nç»Ÿè®¡æ¨æ–­çš„æ–¹æ³•\n1. è´å¶æ–¯æ¨æ–­\n\nè´å¶æ–¯æ¨æ–­æ˜¯ä¸€ç§åŸºäºè´å¶æ–¯å®šç†çš„ç»Ÿè®¡æ¨æ–­æ–¹æ³•ã€‚å®ƒé€šè¿‡ç»“åˆå…ˆéªŒçŸ¥è¯†å’Œæ ·æœ¬æ•°æ®æ¥æ›´æ–°å¯¹æ€»ä½“å‚æ•°çš„ä¿¡å¿µã€‚è´å¶æ–¯æ¨æ–­åœ¨å¤„ç†å°æ ·æœ¬æ•°æ®å’Œå¤æ‚æ¨¡å‹æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚\n\n2. é¢‘ç‡æ´¾æ¨æ–­\n\né¢‘ç‡æ´¾æ¨æ–­æ˜¯ä¼ ç»Ÿçš„ç»Ÿè®¡æ¨æ–­æ–¹æ³•ï¼ŒåŸºäºå¤§æ•°å®šå¾‹å’Œä¸­å¿ƒæé™å®šç†ã€‚å®ƒé€šè¿‡å¤§é‡é‡å¤å®éªŒæ¥ä¼°è®¡æ€»ä½“å‚æ•°ï¼Œå¹¶ä½¿ç”¨ç½®ä¿¡åŒºé—´æ¥è¡¨è¾¾ä¼°è®¡çš„ä¸ç¡®å®šæ€§ã€‚\n\nç»Ÿè®¡æ¨æ–­åœ¨æ•°æ®ç§‘å­¦ä¸­çš„åº”ç”¨\n1. æ¨¡å‹é€‰æ‹©\n\nåœ¨æ•°æ®ç§‘å­¦ä¸­ï¼Œç»Ÿè®¡æ¨æ–­ç”¨äºé€‰æ‹©æœ€ä¼˜æ¨¡å‹ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ‹Ÿåˆä¼˜åº¦å’Œé¢„æµ‹èƒ½åŠ›ï¼Œç»Ÿè®¡æ¨æ–­å¸®åŠ©æˆ‘ä»¬é€‰æ‹©æœ€èƒ½è§£é‡Šæ•°æ®çš„æ¨¡å‹ã€‚\n\n2. å¼‚å¸¸æ£€æµ‹\n\nç»Ÿè®¡æ¨æ–­åœ¨å¼‚å¸¸æ£€æµ‹ä¸­ä¹Ÿæœ‰å¹¿æ³›åº”ç”¨ã€‚é€šè¿‡è®¾å®šé€‚å½“çš„ç»Ÿè®¡æ£€éªŒï¼Œæˆ‘ä»¬å¯ä»¥è¯†åˆ«æ•°æ®ä¸­çš„å¼‚å¸¸ç‚¹æˆ–ç¦»ç¾¤å€¼ï¼Œä»è€Œæé«˜æ•°æ®åˆ†æçš„å‡†ç¡®æ€§ã€‚\n\n3. å› æœæ¨æ–­\n\nå› æœæ¨æ–­æ˜¯ç»Ÿè®¡æ¨æ–­çš„é«˜çº§åº”ç”¨ï¼Œç”¨äºç¡®å®šå˜é‡ä¹‹é—´çš„å› æœå…³ç³»ã€‚åœ¨æ•°æ®ç§‘å­¦ä¸­ï¼Œå› æœæ¨æ–­å¸®åŠ©æˆ‘ä»¬ç†è§£æ•°æ®èƒŒåçš„æœºåˆ¶ï¼Œä»è€Œåšå‡ºæ›´æœ‰æ•ˆçš„å†³ç­–ã€‚\n\nç»“è®º\n\nç»Ÿè®¡æ¨æ–­æ˜¯æ•°æ®ç§‘å­¦ä¸­ä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚é€šè¿‡æŒæ¡å‚æ•°ä¼°è®¡ã€å‡è®¾æ£€éªŒç­‰åŸºæœ¬æ–¹æ³•ï¼Œä»¥åŠè´å¶æ–¯æ¨æ–­å’Œé¢‘ç‡æ´¾æ¨æ–­ç­‰é«˜çº§æŠ€æœ¯ï¼Œæ•°æ®ç§‘å­¦å®¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œè§£é‡Šæ•°æ®ï¼Œä»è€Œåšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Rust\", \"Safetensors\", \"JAX\", \"English\", \"Apache License 2.0\", \"exbert\"]"
  },
  {
    "url": "https://gitcode.com/openMind/flan_t5_base",
    "project_name": "flan_t5_base",
    "readme": "Model Card for FLAN-T5 base\nTable of Contents\nTL;DR\nModel Details\nUsage\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nModel Card Authors\nTL;DR\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. As mentioned in the first few lines of the abstract :\n\nFlan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\nDisclaimer: Content from this model card has been written by the openMind team, and parts of it were copy pasted from the T5 model card.\n\nModification\n\nAdded the CANN version dependency description to the original README and modified the example code.\n\nModel Details\nModel Description\nModel type: Language model\nLanguage(s) (NLP): English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\nLicense: Apache 2.0\nResources for more information:\nResearch paper\nGitHub Repo\nUsage\n\nFind below some example scripts on how to use the model in openmind:\n\nUsing the Pytorch model\nRunning the model on a NPU\nClick to expand\nUses\nDirect Use and Downstream Use\n\nThe authors write in the original paper's model card that:\n\nThe primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\n\nSee the research paper for further details.\n\nOut-of-Scope Use\n\nMore information needed.\n\nBias, Risks, and Limitations\n\nThe information below in this section are copied from the model's official model card:\n\nLanguage models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n\nEthical considerations and risks\n\nFlan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\nKnown Limitations\n\nFlan-T5 has not been tested in real world applications.\n\nSensitive Use:\n\nFlan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\n\nTraining Details\nTraining Data\n\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\n\nTraining Procedure\n\nAccording to the model card from the original paper:\n\nThese models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\n\nThe model has been trained on TPU v3 or TPU v4 pods, using t5x codebase together with jax.\n\nEvaluation\nTesting Data, Factors & Metrics\n\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:  For full details, please check the research paper.\n\nResults\n\nFor full results for FLAN-T5-Base, see the research paper, Table 3.\n\nEnvironmental Impact\n\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\n\nHardware Type: Google Cloud TPU Pods - TPU v3 or TPU v4 | Number of chips â‰¥ 4.\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\n\nBibTeX:\n\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n\nModel Recycling\n\nEvaluation on 36 datasets using google/flan-t5-base as a base model yields average score of 77.98 in comparison to 68.82 by google/t5-v1_1-base.\n\nThe model is ranked 1st among all tested models for the google/t5-v1_1-base architecture as of 06/02/2023 Results:\n\n20_newsgroup\tag_news\tamazon_reviews_multi\tanli\tboolq\tcb\tcola\tcopa\tdbpedia\tesnli\tfinancial_phrasebank\timdb\tisear\tmnli\tmrpc\tmultirc\tpoem_sentiment\tqnli\tqqp\trotten_tomatoes\trte\tsst2\tsst_5bins\tstsb\ttrec_coarse\ttrec_fine\ttweet_ev_emoji\ttweet_ev_emotion\ttweet_ev_hate\ttweet_ev_irony\ttweet_ev_offensive\ttweet_ev_sentiment\twic\twnli\twsc\tyahoo_answers\n86.2188\t89.6667\t67.12\t51.9688\t82.3242\t78.5714\t80.1534\t75\t77.6667\t90.9507\t85.4\t93.324\t72.425\t87.2457\t89.4608\t62.3762\t82.6923\t92.7878\t89.7724\t89.0244\t84.8375\t94.3807\t57.2851\t89.4759\t97.2\t92.8\t46.848\t80.2252\t54.9832\t76.6582\t84.3023\t70.6366\t70.0627\t56.338\t53.8462\t73.4\n\nFor more information, see: Model Recycling",
    "tags": "[\"PyTorch\", \"TensorFlow\", \"Transformers\", \"Safetensors\", \"JAX\", \"English\", \"Apache License 2.0\", \"10 datasets\", \"text2text-generation\"]"
  },
  {
    "url": "https://gitcode.com/openMind/qwen1.5_7b",
    "project_name": "qwen1.5_7b",
    "readme": "Original Text\nQwen1.5-7B\nç®€ä»‹\n\nQwen1.5 æ˜¯ Qwen2 çš„æµ‹è¯•ç‰ˆæœ¬ï¼Œä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„ä»…è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼Œé¢„è®­ç»ƒäºå¤§é‡æ•°æ®ä¹‹ä¸Šã€‚ä¸ä¹‹å‰å‘å¸ƒçš„ Qwen ç›¸æ¯”ï¼Œæ”¹è¿›åŒ…æ‹¬ï¼š\n\n6 ç§æ¨¡å‹å°ºå¯¸ï¼ŒåŒ…æ‹¬ 0.5Bã€1.8Bã€4Bã€7Bã€14B å’Œ 72Bï¼›\nèŠå¤©æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼›\nåŸºç¡€æ¨¡å‹å’ŒèŠå¤©æ¨¡å‹å‡æ”¯æŒå¤šè¯­è¨€ï¼›\næ‰€æœ‰å°ºå¯¸æ¨¡å‹å‡ç¨³å®šæ”¯æŒ 32K ä¸Šä¸‹æ–‡é•¿åº¦ï¼›\næ— éœ€ trust_remote_codeã€‚\n\næ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„ åšå®¢æ–‡ç«  å’Œ GitHub ä»“åº“ã€‚\n\næ¨¡å‹è¯¦æƒ…\n\nQwen1.5 æ˜¯ä¸€ä¸ªåŒ…å«ä¸åŒå°ºå¯¸è§£ç å™¨è¯­è¨€æ¨¡å‹çš„ç³»åˆ—ã€‚å¯¹äºæ¯ç§å°ºå¯¸ï¼Œæˆ‘ä»¬å‘å¸ƒäº†åŸºç¡€è¯­è¨€æ¨¡å‹å’Œå¯¹å…¶çš„èŠå¤©æ¨¡å‹ã€‚å®ƒåŸºäº Transformer æ¶æ„ï¼Œé‡‡ç”¨ SwiGLU æ¿€æ´»å‡½æ•°ã€æ³¨æ„åŠ› QKV åç½®ã€ç»„æŸ¥è¯¢æ³¨æ„åŠ›ã€æ»‘åŠ¨çª—å£æ³¨æ„åŠ›å’Œå…¨æ³¨æ„åŠ›æ··åˆç­‰æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ”¹è¿›äº†é€‚åº”å¤šç§è‡ªç„¶è¯­è¨€å’Œä»£ç çš„æ ‡è®°å™¨ã€‚å¯¹äºæµ‹è¯•ç‰ˆæœ¬ï¼Œæš‚æ—¶æœªåŒ…å« GQA å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸å…¨æ³¨æ„åŠ›çš„æ··åˆã€‚\n\nè¦æ±‚\n\nQwen1.5 çš„ä»£ç å·²é›†æˆè‡³æœ€æ–°çš„ Hugging Face transformersï¼Œå»ºè®®å®‰è£… transformers>=4.37.0ï¼Œå¦åˆ™å¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š\n\nKeyError: 'qwen2'.\n\nä½¿ç”¨è¯´æ˜\n\næˆ‘ä»¬ä¸å»ºè®®ç›´æ¥ä½¿ç”¨åŸºç¡€è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚ç›¸åï¼Œæ‚¨å¯ä»¥åœ¨æ­¤æ¨¡å‹ä¸Šåº”ç”¨åç»­è®­ç»ƒï¼Œä¾‹å¦‚ SFTã€RLHFã€ç»§ç»­é¢„è®­ç»ƒç­‰ã€‚\n\nimport torch\nfrom openmind import pipeline\nfrom openmind_hub import snapshot_download\n\n\nmodel_path = snapshot_download(\"PyTorch-NPU/qwen1.5_7b\", revision=\"main\", resume_download=True,\n                                ignore_patterns=[\"*.h5\", \"*.ot\", \"*.msgpack\"])\n\ngenerator = pipeline('text-generation', model=model_path, device_map=\"auto\")\n\noutput = generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1)\nprint(f\">>>output={output}\", flush=True)\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚\n\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n\nä½¿ç”¨æŒ‡å—\næ¦‚è¿°\n\næœ¬æŒ‡å—æ—¨åœ¨å¸®åŠ©ç”¨æˆ·å……åˆ†åˆ©ç”¨ MyApp çš„åŠŸèƒ½ã€‚æ— è®ºæ‚¨æ˜¯åˆæ¬¡ä½¿ç”¨è¿˜æ˜¯å¸Œæœ›æ·±å…¥äº†è§£ï¼Œæœ¬æŒ‡å—éƒ½å°†ä¸ºæ‚¨æä¾›å¿…è¦çš„æŒ‡å¯¼ã€‚\n\nå®‰è£…ä¸é…ç½®\nç³»ç»Ÿè¦æ±‚\næ“ä½œç³»ç»Ÿ: Windows 10 åŠä»¥ä¸Šç‰ˆæœ¬\nå†…å­˜: è‡³å°‘ 4GB RAM\nå­˜å‚¨ç©ºé—´: è‡³å°‘ 10GB å¯ç”¨ç©ºé—´\nå®‰è£…æ­¥éª¤\nä¸‹è½½ MyApp å®‰è£…åŒ…ã€‚\nåŒå‡»å®‰è£…åŒ…ï¼ŒæŒ‰ç…§æç¤ºå®Œæˆå®‰è£…ã€‚\nå¯åŠ¨ MyAppï¼Œè¿›å…¥é…ç½®å‘å¯¼ã€‚\næ ¹æ®æ‚¨çš„éœ€æ±‚é…ç½®åŸºæœ¬è®¾ç½®ã€‚\nåŠŸèƒ½ä»‹ç»\næ ¸å¿ƒåŠŸèƒ½\næ•°æ®åˆ†æ: æä¾›å¼ºå¤§çš„æ•°æ®å¤„ç†å’Œåˆ†æå·¥å…·ã€‚\nç”¨æˆ·ç®¡ç†: æ”¯æŒå¤šç”¨æˆ·æƒé™è®¾ç½®å’Œç”¨æˆ·è¡Œä¸ºç›‘æ§ã€‚\næŠ¥å‘Šç”Ÿæˆ: è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„æ•°æ®æŠ¥å‘Šã€‚\né«˜çº§åŠŸèƒ½\nè‡ªå®šä¹‰æ’ä»¶: å…è®¸ç”¨æˆ·å¼€å‘å’Œé›†æˆè‡ªå®šä¹‰æ’ä»¶ã€‚\nAPI æ¥å£: æä¾› RESTful API æ¥å£ï¼Œä¾¿äºç³»ç»Ÿé›†æˆã€‚\nå¸¸è§é—®é¢˜\nå¦‚ä½•æ›´æ–° MyAppï¼Ÿ\næ‰“å¼€ MyAppã€‚\nå¯¼èˆªè‡³â€œå¸®åŠ©â€èœå•ã€‚\né€‰æ‹©â€œæ£€æŸ¥æ›´æ–°â€ã€‚\næŒ‰ç…§æç¤ºå®Œæˆæ›´æ–°ã€‚\nå¿˜è®°å¯†ç æ€ä¹ˆåŠï¼Ÿ\nç‚¹å‡»ç™»å½•é¡µé¢çš„â€œå¿˜è®°å¯†ç â€é“¾æ¥ã€‚\nè¾“å…¥æ‚¨çš„æ³¨å†Œé‚®ç®±ã€‚\næŒ‰ç…§é‚®ä»¶ä¸­çš„æŒ‡ç¤ºé‡ç½®å¯†ç ã€‚\nè”ç³»æˆ‘ä»¬\n\nå¦‚æœæ‚¨åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»æˆ‘ä»¬ï¼š\n\né‚®ç®±: support@myapp.com\nç”µè¯: +86 123-4567-8901\n\næ„Ÿè°¢æ‚¨é€‰æ‹© MyAppï¼",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Other\"]"
  },
  {
    "url": "https://gitcode.com/openMind/stable-diffusion-xl-base-1_0",
    "project_name": "stable-diffusion-xl-base-1_0",
    "readme": "Original Text\nSD-XL 1.0åŸºç¡€æ¨¡å‹å¡\n\nä¿®æ”¹è¯´æ˜\nä¼˜åŒ–ç¤ºä¾‹ä»¥é€‚é…openMindå¹³å°å¹¶å¢åŠ npuæ”¯æŒ\næ¨¡å‹æ¶æ„\n\nSDXLé‡‡ç”¨ä¸“å®¶é›†æˆçš„æ½œåœ¨æ‰©æ•£æµç¨‹ï¼š ç¬¬ä¸€é˜¶æ®µç”±åŸºç¡€æ¨¡å‹ç”Ÿæˆå«å™ªæ½œåœ¨ç‰¹å¾ï¼Œ éšåé€šè¿‡ä¸“ç²¾æœ€ç»ˆå»å™ªæ­¥éª¤çš„ä¼˜åŒ–æ¨¡å‹ï¼ˆè¯¦è§ï¼šhttps://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/ï¼‰è¿›è¡Œç»†åŒ–å¤„ç†ã€‚ éœ€æ³¨æ„åŸºç¡€æ¨¡å‹å¯ç‹¬ç«‹è¿è¡Œã€‚\n\næ›¿ä»£æ–¹æ¡ˆå¯é‡‡ç”¨åŒé˜¶æ®µæµç¨‹ï¼š é¦–å…ˆç”±åŸºç¡€æ¨¡å‹ç”Ÿæˆç›®æ ‡å°ºå¯¸çš„æ½œåœ¨ç‰¹å¾ï¼Œ éšåè¿ç”¨ä¸“ç²¾é«˜åˆ†è¾¨ç‡çš„æ¨¡å‹ï¼Œå¯¹é¦–é˜¶æ®µç”Ÿæˆçš„æ½œåœ¨ç‰¹å¾å®æ–½SDEditæŠ€æœ¯ï¼ˆhttps://arxiv.org/abs/2108.01073ï¼Œäº¦ç§°\"å›¾ç”Ÿå›¾\"ï¼‰ï¼Œ ä¿æŒæç¤ºè¯ä¸å˜ã€‚æ­¤æ–¹æ¡ˆå› éœ€æ›´å¤šå‡½æ•°è¯„ä¼°è€Œç•¥æ…¢äºå‰è€…ã€‚\n\næºä»£ç è¯¦è§https://github.com/Stability-AI/generative-modelsã€‚\n\næ¨¡å‹æè¿°\nç ”å‘æœºæ„: Stability AI\næ¨¡å‹ç±»å‹: åŸºäºæ‰©æ•£çš„æ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹\nè®¸å¯åè®®: CreativeML Open RAIL++-Mè®¸å¯è¯\næ¨¡å‹åŠŸèƒ½: æœ¬æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå¹¶ä¿®æ”¹å›¾åƒï¼Œæ˜¯åŸºäºåŒå›ºå®šé¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ï¼ˆOpenCLIP-ViT/Gä¸CLIP-ViT/Lï¼‰çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚\nå»¶ä¼¸é˜…è¯»: è¯·è®¿é—®æˆ‘ä»¬çš„GitHubä»“åº“åŠSDXL arXivæŠ€æœ¯æŠ¥å‘Šã€‚\næ¨¡å‹èµ„æº\n\nç ”ç©¶ç”¨é€”æ¨èä½¿ç”¨generative-modelsGithubä»“åº“(https://github.com/Stability-AI/generative-models)ï¼Œè¯¥åº“å®ç°äº†ä¸»æµæ‰©æ•£æ¡†æ¶ï¼ˆè®­ç»ƒä¸æ¨ç†ï¼‰ï¼Œå¹¶å°†æŒç»­æ–°å¢è’¸é¦ç­‰ç‰¹æ€§ã€‚ Clipdropæä¾›å…è´¹SDXLæ¨ç†æœåŠ¡ã€‚\n\nä»£ç åº“: https://github.com/Stability-AI/generative-models\næ¼”ç¤ºå¹³å°: https://clipdrop.co/stable-diffusion\næ€§èƒ½è¯„ä¼°\n\n ä¸Šå›¾å±•ç¤ºäº†ç”¨æˆ·å¯¹SDXLï¼ˆå«/ä¸å«ä¼˜åŒ–æ¨¡å—ï¼‰ä¸SDXL 0.9åŠStable Diffusion 1.5/2.1çš„åå¥½è¯„ä¼°ã€‚ SDXLåŸºç¡€æ¨¡å‹è¡¨ç°æ˜¾è‘—ä¼˜äºå‰ä»£ç‰ˆæœ¬ï¼Œé…åˆä¼˜åŒ–æ¨¡å—åè¾¾åˆ°æœ€ä½³ç»¼åˆæ€§èƒ½ã€‚\n\nğŸ§¨ Diffusers\n\nè¯·ç¡®ä¿å‡çº§diffusersè‡³0.19.0åŠä»¥ä¸Šç‰ˆæœ¬ï¼š\n\npip install diffusers --upgrade\n\n\næ­¤å¤–ï¼Œè¯·ç¡®ä¿å®‰è£… transformersã€safetensorsã€accelerate ä»¥åŠä¸å¯è§æ°´å°ç»„ä»¶ï¼š\n\npip install invisible_watermark transformers accelerate safetensors\n\n\nè‹¥ä»…éœ€ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼Œå¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"PyTorch-NPU/stable-diffusion-xl-base-1_0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\npipe.to(\"npu:0\")\n\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\n\nprompt = \"An astronaut riding a green horse\"\n\nimages = pipe(prompt=prompt).images[0]\n\n\nè¦ä½¿ç”¨æ•´ä¸ªåŸºç¡€æ¨¡å‹ + ç²¾ç‚¼å™¨ç®¡é“ä½œä¸ºä¸“å®¶é›†æˆï¼Œæ‚¨å¯ä»¥è¿è¡Œï¼š\n\nfrom diffusers import DiffusionPipeline\nimport torch\n\n# load both base & refiner\nbase = DiffusionPipeline.from_pretrained(\n    \"PyTorch-NPU/stable-diffusion-xl-base-1_0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\nbase.to(\"npu:0\")\nrefiner = DiffusionPipeline.from_pretrained(\n    \"PyTorch-NPU/stable-diffusion-xl-base-1_0\",\n    text_encoder_2=base.text_encoder_2,\n    vae=base.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\",\n)\nrefiner.to(\"npu:0\")\n\n# Define how many steps and what % of steps to be run on each experts (80/20) here\nn_steps = 40\nhigh_noise_frac = 0.8\n\nprompt = \"A majestic lion jumping from a big stone at night\"\n\n# run both experts\nimage = base(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_end=high_noise_frac,\n    output_type=\"latent\",\n).images\nimage = refiner(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_start=high_noise_frac,\n    image=image,\n).images[0]\n\n\nå½“ä½¿ç”¨ torch >= 2.0 æ—¶ï¼Œé€šè¿‡ torch.compile å¯å°†æ¨ç†é€Ÿåº¦æå‡ 20-30%ã€‚åªéœ€åœ¨è¿è¡Œæµç¨‹å‰ç”¨ torch.compile ç®€å•å°è£… unet å³å¯ï¼š\n\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\n\nå¦‚æœå—é™äº NPU æ˜¾å­˜å®¹é‡ï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨ pipe.enable_model_cpu_offload æ›¿ä»£ .to(\"npu:0\") æ¥å¯ç”¨ CPU å¸è½½ åŠŸèƒ½ï¼š\n\n- pipe.to(\"npu:0\")\n+ pipe.enable_model_cpu_offload()\n\n\nå¦‚éœ€äº†è§£æ›´å¤šå…³äºå¦‚ä½•åœ¨ diffusers ä¸­ä½¿ç”¨ Stable Diffusion XL çš„ä¿¡æ¯ï¼Œè¯·å‚é˜… Stable Diffusion XL æ–‡æ¡£ã€‚\n\nOptimum\n\nOptimum æä¾›äº†ä¸€ä¸ªå…¼å®¹ OpenVINO å’Œ ONNX Runtime çš„ Stable Diffusion ç®¡é“ã€‚\n\nOpenVINO\n\nè‹¥è¦å®‰è£… Optimum åŠå…¶ OpenVINO æ‰€éœ€çš„ä¾èµ–é¡¹ï¼š\n\npip install optimum[openvino]\n\n\nè¦åŠ è½½ OpenVINO æ¨¡å‹å¹¶ä½¿ç”¨ OpenVINO Runtime è¿›è¡Œæ¨ç†ï¼Œæ‚¨éœ€è¦å°† StableDiffusionXLPipeline æ›¿æ¢ä¸º Optimum çš„ OVStableDiffusionXLPipelineã€‚è‹¥éœ€åŠ è½½ PyTorch æ¨¡å‹å¹¶å®æ—¶è½¬æ¢ä¸º OpenVINO æ ¼å¼ï¼Œå¯è®¾ç½® export=Trueã€‚\n\n- from diffusers import StableDiffusionXLPipeline\n+ from optimum.intel import OVStableDiffusionXLPipeline\n\nmodel_id = \"PyTorch-NPU/stable-diffusion-xl-base-1_0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]\n\n\næ‚¨å¯ä»¥åœ¨Optimumçš„æ–‡æ¡£ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ï¼ˆå¦‚é™æ€é‡å¡‘å’Œæ¨¡å‹ç¼–è¯‘ï¼‰ã€‚\n\nONNX\n\nè¦å®‰è£…æ”¯æŒONNX Runtimeæ¨ç†æ‰€éœ€çš„Optimumä¾èµ–é¡¹ï¼š\n\npip install optimum[onnxruntime]\n\n\nè¦åŠ è½½ ONNX æ¨¡å‹å¹¶ä½¿ç”¨ ONNX Runtime è¿›è¡Œæ¨ç†ï¼Œæ‚¨éœ€è¦å°† StableDiffusionXLPipeline æ›¿æ¢ä¸º Optimum çš„ ORTStableDiffusionXLPipelineã€‚è‹¥å¸Œæœ›åŠ è½½ PyTorch æ¨¡å‹å¹¶å®æ—¶è½¬æ¢ä¸º ONNX æ ¼å¼ï¼Œå¯è®¾ç½®å‚æ•° export=Trueã€‚\n\n- from diffusers import StableDiffusionXLPipeline\n+ from optimum.onnxruntime import ORTStableDiffusionXLPipeline\n\nmodel_id = \"PyTorch-NPU/stable-diffusion-xl-base-1_0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = ORTStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]\n\n\næ›´å¤šç¤ºä¾‹å¯åœ¨ Optimum æ–‡æ¡£ ä¸­æ‰¾åˆ°ã€‚\n\nç”¨é€”\nç›´æ¥ä½¿ç”¨\n\næœ¬æ¨¡å‹ä»…ä¾›ç ”ç©¶ç”¨é€”ã€‚å¯èƒ½çš„ç ”ç©¶é¢†åŸŸå’Œä»»åŠ¡åŒ…æ‹¬ï¼š\n\nè‰ºæœ¯ä½œå“ç”Ÿæˆï¼Œç”¨äºè®¾è®¡åŠå…¶ä»–è‰ºæœ¯åˆ›ä½œè¿‡ç¨‹ã€‚\næ•™è‚²æˆ–åˆ›æ„å·¥å…·ä¸­çš„åº”ç”¨ã€‚\nç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶ã€‚\nå®‰å…¨éƒ¨ç½²å¯èƒ½ç”Ÿæˆæœ‰å®³å†…å®¹çš„æ¨¡å‹ã€‚\næ¢ç´¢å’Œç†è§£ç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§ä¸åè§ã€‚\n\nä»¥ä¸‹ä¸ºä¸é€‚ç”¨åœºæ™¯ã€‚\n\néé€‚ç”¨ç”¨é€”\n\nè¯¥æ¨¡å‹æœªç»è®­ç»ƒä»¥ç”Ÿæˆäººç‰©æˆ–äº‹ä»¶çš„çœŸå®å‡†ç¡®è¡¨ç°ï¼Œå› æ­¤å°†å…¶ç”¨äºæ­¤ç±»å†…å®¹ç”Ÿæˆè¶…å‡ºæ¨¡å‹èƒ½åŠ›èŒƒå›´ã€‚\n\nå±€é™æ€§ä¸åè§\nå±€é™æ€§\næ¨¡å‹æ— æ³•å®ç°å®Œç¾çš„ç…§ç‰‡çº§çœŸå®æ„Ÿ\næ¨¡å‹æ— æ³•ç”Ÿæˆæ¸…æ™°å¯è¾¨çš„æ–‡å­—\nåœ¨å¤„ç†æ¶‰åŠç»„åˆæ€§çš„å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°æ¬ ä½³ï¼Œä¾‹å¦‚ç”Ÿæˆ\"è“è‰²çƒä½“ä¸Šçš„çº¢è‰²ç«‹æ–¹ä½“\"å¯¹åº”å›¾åƒ\né¢éƒ¨åŠäººç‰©ç”Ÿæˆå¯èƒ½ä¸å¤Ÿå‡†ç¡®\næ¨¡å‹çš„è‡ªåŠ¨ç¼–ç éƒ¨åˆ†å­˜åœ¨ä¿¡æ¯æŸè€—\nåè§\n\nå°½ç®¡å›¾åƒç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½å¼ºåŒ–æˆ–åŠ å‰§ç¤¾ä¼šåè§ã€‚",
    "tags": "[\"PyTorch\", \"Transformers\", \"Diffusers\", \"ONNX\", \"OpenVINO\", \"Safetensors\", \"Open Rail++-M License\", \"text-to-image\", \"stable-diffusion\"]"
  },
  {
    "url": "https://gitcode.com/openMind/gpt2",
    "project_name": "gpt2",
    "readme": "Original Text\nGPT-2\n\nåŸºäºå› æœè¯­è¨€æ¨¡å‹ï¼ˆCLMï¼‰ç›®æ ‡ï¼Œåœ¨è‹±è¯­è¯­è¨€ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨ this paper ä¸­æå‡ºï¼Œå¹¶åœ¨ è¿™ä¸ªé¡µé¢ ä¸Šé¦–æ¬¡å‘å¸ƒã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒ GPT-2 çš„å›¢é˜Ÿè¿˜ä¸ºå…¶æ¨¡å‹ç¼–å†™äº† model cardã€‚æœ¬æ¨¡å‹å¡çš„å†…å®¹ç”± Hugging Face å›¢é˜Ÿæ’°å†™ï¼Œä»¥è¡¥å……ä»–ä»¬æä¾›çš„ä¿¡æ¯ï¼Œå¹¶ç»™å‡ºåè§çš„å…·ä½“ç¤ºä¾‹ã€‚\n\nä¿®æ”¹\n\nåœ¨åŸå§‹ README ä¸­æ·»åŠ äº† CANN ç‰ˆæœ¬ä¾èµ–æè¿°ï¼Œå¹¶ä¿®æ”¹äº†ç¤ºä¾‹ä»£ç ã€‚\n\næ¨¡å‹æè¿°\n\nGPT-2 æ˜¯ä¸€ä¸ªåœ¨å¤§é‡è‹±è¯­æ•°æ®é›†ä¸Šä»¥è‡ªç›‘ç£æ–¹å¼é¢„è®­ç»ƒçš„è½¬æ¢å™¨æ¨¡å‹ã€‚è¿™æ„å‘³ç€å®ƒä»…é€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„è¾“å…¥å’Œæ ‡ç­¾åœ¨åŸå§‹æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ˆè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå®ƒèƒ½ä½¿ç”¨å¤§é‡å…¬å¼€å¯ç”¨çš„æ•°æ®ï¼‰ï¼Œè€Œæ²¡æœ‰ç»è¿‡ä»»ä½•å½¢å¼çš„æ ‡æ³¨ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒæ˜¯é€šè¿‡é¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯æ¥è¿›è¡Œè®­ç»ƒçš„ã€‚\n\næ›´å…·ä½“åœ°ï¼Œè¾“å…¥æ˜¯ä¸€å®šé•¿åº¦çš„è¿ç»­æ–‡æœ¬åºåˆ—ï¼Œç›®æ ‡åºåˆ—ä¸è¾“å…¥åºåˆ—ç›¸åŒï¼Œä½†å‘å³ç§»åŠ¨äº†ä¸€ä¸ªæ ‡è®°ï¼ˆå•è¯æˆ–å•è¯çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚æ¨¡å‹å†…éƒ¨ä½¿ç”¨é®è”½æœºåˆ¶ä»¥ç¡®ä¿å¯¹æ ‡è®° i çš„é¢„æµ‹ä»…ä½¿ç”¨ä» 1 åˆ° i çš„è¾“å…¥ï¼Œè€Œä¸åŒ…æ‹¬æœªæ¥çš„æ ‡è®°ã€‚\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å­¦ä¹ åˆ°äº†è‹±è¯­è¯­è¨€çš„å†…éƒ¨è¡¨ç¤ºï¼Œç„¶åå¯ç”¨äºæå–å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾ã€‚ç„¶è€Œï¼Œæ¨¡å‹æœ€æ“…é•¿çš„æ˜¯å…¶é¢„è®­ç»ƒçš„ç›®çš„ï¼Œå³åŸºäºæç¤ºç”Ÿæˆæ–‡æœ¬ã€‚\n\nè¿™æ˜¯ GPT-2 çš„ æœ€å° ç‰ˆæœ¬ï¼Œæ‹¥æœ‰ 124M å‚æ•°ã€‚\n\nä½¿ç”¨ç›®çš„ä¸é™åˆ¶\n\næ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œæˆ–è€…å°†å…¶å¾®è°ƒåˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\næ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤æ¨¡å‹ä»¥åŠæ–‡æœ¬ç”Ÿæˆæµç¨‹ã€‚ç”±äºç”Ÿæˆä¾èµ–äºä¸€å®šçš„éšæœºæ€§ï¼Œæˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªç§å­ä»¥å®ç°å¯é‡å¤æ€§ï¼š\n\n>>> from openmind import pipeline\n>>> from transformers import set_seed\n>>> generator = pipeline('text-generation', model='PyTorch-NPU/gpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n\nå±€é™æ€§ä¸åè§\n\nç”¨äºæœ¬æ¨¡å‹çš„è®­ç»ƒæ•°æ®å°šæœªä½œä¸ºå¯ä¾›æµè§ˆçš„æ•°æ®é›†å‘å¸ƒã€‚æˆ‘ä»¬çŸ¥é“å®ƒåŒ…å«å¤§é‡æœªç»ç­›é€‰çš„äº’è”ç½‘å†…å®¹ï¼Œè¿™äº›å†…å®¹è¿œéä¸­ç«‹ã€‚æ­£å¦‚ OpenAI å›¢é˜Ÿåœ¨å…¶ æ¨¡å‹å¡ç‰‡ ä¸­æ‰€æŒ‡å‡ºçš„ï¼š\n\nç”±äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¦‚ GPT-2 æ— æ³•åŒºåˆ†äº‹å®ä¸è™šæ„ï¼Œæˆ‘ä»¬ä¸æ”¯æŒç”Ÿæˆæ–‡æœ¬éœ€ä¸ºçœŸå®çš„ç”¨ä¾‹ã€‚\n\næ­¤å¤–ï¼Œåƒ GPT-2 è¿™æ ·çš„è¯­è¨€æ¨¡å‹åæ˜ äº†å®ƒä»¬æ‰€è®­ç»ƒç³»ç»Ÿå›ºæœ‰çš„åè§ï¼Œå› æ­¤æˆ‘ä»¬ä¸å»ºè®®å°†å…¶éƒ¨ç½²åˆ°ä¸äººç±»äº¤äº’çš„ç³»ç»Ÿä¸­ï¼Œé™¤ééƒ¨ç½²è€…é¦–å…ˆé’ˆå¯¹é¢„æœŸç”¨ä¾‹è¿›è¡Œäº†ç›¸å…³åè§çš„è°ƒæŸ¥ç ”ç©¶ã€‚æˆ‘ä»¬åœ¨ 774M å’Œ 1.5B ç‰ˆæœ¬ä¹‹é—´æœªå‘ç°æ€§åˆ«ã€ç§æ—å’Œå®—æ•™åè§æ¢æµ‹çš„ç»Ÿè®¡å­¦æ˜¾è‘—å·®å¼‚ï¼Œè¿™è¡¨æ˜æ‰€æœ‰ç‰ˆæœ¬çš„ GPT-2 åœ¨å¤„ç†å¯¹äººç±»å±æ€§åè§æ•æ„Ÿçš„ç”¨ä¾‹æ—¶ï¼Œéƒ½åº”é‡‡å–ç±»ä¼¼çš„è°¨æ…æ€åº¦ã€‚\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªå…³äºæ¨¡å‹å¯èƒ½äº§ç”Ÿå¸¦æœ‰åè§çš„é¢„æµ‹çš„ç¤ºä¾‹ï¼š\n\n>>> from openmind import pipeline\n>>> from transformers import set_seed\n>>> generator = pipeline('text-generation', model='PyTorch-NPU/gpt2')\n>>> set_seed(42)\n>>> generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The White man worked as a mannequin for'},\n {'generated_text': 'The White man worked as a maniser of the'},\n {'generated_text': 'The White man worked as a bus conductor by day'},\n {'generated_text': 'The White man worked as a plumber at the'},\n {'generated_text': 'The White man worked as a journalist. He had'}]\n\n>>> set_seed(42)\n>>> generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The Black man worked as a man at a restaurant'},\n {'generated_text': 'The Black man worked as a car salesman in a'},\n {'generated_text': 'The Black man worked as a police sergeant at the'},\n {'generated_text': 'The Black man worked as a man-eating monster'},\n {'generated_text': 'The Black man worked as a slave, and was'}]\n\n\nè¿™ç§åè§è¿˜å°†å½±å“è¯¥æ¨¡å‹çš„æ‰€æœ‰å¾®è°ƒç‰ˆæœ¬ã€‚\n\nè®­ç»ƒæ•°æ®\n\nOpenAI å›¢é˜Ÿå¸Œæœ›å»ºç«‹åœ¨å°½å¯èƒ½å¤§çš„è¯­æ–™åº“ä¸Šè®­ç»ƒæ­¤æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬æŠ“å–äº† Reddit ä¸Šæ”¶åˆ°è‡³å°‘ 3 ç‚¹å£°èª‰çš„æ‰€æœ‰å‡ºç«™é“¾æ¥çš„ç½‘é¡µã€‚è¯·æ³¨æ„ï¼Œæ­¤æ•°æ®é›†ä¸­åˆ é™¤äº†æ‰€æœ‰ Wikipedia é¡µé¢ï¼Œå› æ­¤æ¨¡å‹æ²¡æœ‰åœ¨ä»»ä½•éƒ¨åˆ†çš„ Wikipedia ä¸Šè¿›è¡Œè®­ç»ƒã€‚ç”Ÿæˆçš„æ•°æ®é›†ï¼ˆç§°ä¸º WebTextï¼‰åŒ…å« 40GB çš„æ–‡æœ¬ï¼Œä½†å°šæœªå…¬å¼€å‘å¸ƒã€‚æ‚¨å¯ä»¥åœ¨æ­¤å¤„æ‰¾åˆ° WebText ä¸­æ’åå‰ 1,000 çš„åŸŸååˆ—è¡¨ã€‚\n\nè®­ç»ƒè¿‡ç¨‹\né¢„å¤„ç†\n\næ–‡æœ¬ä½¿ç”¨å­—èŠ‚çº§åˆ«çš„å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰ï¼ˆé’ˆå¯¹ unicode å­—ç¬¦ï¼‰å’Œä¸€ä¸ªè¯æ±‡é‡å¤§å°ä¸º 50,257 è¿›è¡Œåˆ†è¯ã€‚è¾“å…¥æ˜¯è¿ç»­ 1024 ä¸ªæ ‡è®°çš„åºåˆ—ã€‚\n\nè¾ƒå¤§çš„æ¨¡å‹åœ¨ 256 ä¸ªäº‘ TPU v3 æ ¸å¿ƒä¸Šè¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒæŒç»­æ—¶é—´å¹¶æœªå…¬å¼€ï¼Œè®­ç»ƒçš„å…·ä½“ç»†èŠ‚ä¹ŸæœªæŠ«éœ²ã€‚\n\nè¯„ä¼°ç»“æœ\n\nè¯¥æ¨¡å‹åœ¨æ²¡æœ‰è¿›è¡Œä»»ä½•å¾®è°ƒï¼ˆé›¶æ ·æœ¬ï¼‰çš„æƒ…å†µä¸‹å®ç°äº†ä»¥ä¸‹ç»“æœï¼š\n\næ•°æ®é›†\tLAMBADA\tLAMBADA\tCBT-CN\tCBT-NE\tWikiText2\tPTB\tenwiki8\ttext8\tWikiText103\t1BW\n(æŒ‡æ ‡)\t(PPL)\t(ACC)\t(ACC)\t(ACC)\t(PPL)\t(PPL)\t(BPB)\t(BPC)\t(PPL)\t(PPL)\n\t35.13\t45.99\t87.65\t83.4\t29.41\t65.85\t1.16\t1.17\t37.50\t75.20\nBibTeX æ¡ç›®å’Œå¼•ç”¨ä¿¡æ¯\n@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}\n\n\nå½“ç„¶ï¼Œæˆ‘å¯ä»¥å¸®åŠ©æ‚¨ç¿»è¯‘æ–‡æœ¬ã€‚ä¸è¿‡ï¼Œæ‚¨ä¼¼ä¹å¿˜è®°æä¾›è¦ç¿»è¯‘çš„å…·ä½“æ–‡æœ¬å†…å®¹ã€‚è¯·æä¾›æ‚¨å¸Œæœ›ç¿»è¯‘çš„æ–‡æœ¬ï¼Œç„¶åæˆ‘å°†æŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚",
    "tags": "[\"Text Generation\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"LiteRT\", \"Rust\", \"Safetensors\", \"JAX\", \"English\", \"MIT\", \"exbert\"]"
  },
  {
    "url": "https://gitcode.com/openMind/internlm2_chat_7b",
    "project_name": "internlm2_chat_7b",
    "readme": "InternLM\nÂ \nInternLM HOT\nÂ \n\nğŸ’»Github Repo â€¢ ğŸ¤”Reporting Issues\n\nIntroduction\n\nInternLM2 has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:\n\n200K Context window: Nearly perfect at finding needles in the haystack with 200K-long context, with leading performance on long-context tasks like LongBench and L-Eval. Try it with LMDeploy for 200K-context inference.\n\nOutstanding comprehensive performance: Significantly better than the last generation in all dimensions, especially in reasoning, math, code, chat experience, instruction following, and creative writing, with leading performance among open-source models in similar sizes. In some evaluations, InternLM2-Chat-20B may match or even surpass ChatGPT (GPT-3.5).\n\nCode interpreter & Data analysis: With code interpreter, InternLM2-Chat-20B obtains compatible performance with GPT-4 on GSM8K and MATH. InternLM2-Chat also provides data analysis capability.\n\nStronger tool use: Based on better tool utilization-related capabilities in instruction following, tool selection and reflection, InternLM2 can support more kinds of agents and multi-step tool calling for complex tasks. See examples.\n\nInternLM2-Chat-7B\nPerformance Evaluation\n\nWe conducted a comprehensive evaluation of InternLM using the open-source evaluation tool OpenCompass. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the OpenCompass leaderboard for more evaluation results.\n\nDataset\\Models\tInternLM2-7B\tInternLM2-Chat-7B\tInternLM2-20B\tInternLM2-Chat-20B\tChatGPT\tGPT-4\nMMLU\t65.8\t63.7\t67.7\t66.5\t69.1\t83.0\nAGIEval\t49.9\t47.2\t53.0\t50.3\t39.9\t55.1\nBBH\t65.0\t61.2\t72.1\t68.3\t70.1\t86.7\nGSM8K\t70.8\t70.7\t76.1\t79.6\t78.2\t91.4\nMATH\t20.2\t23.0\t25.5\t31.9\t28.0\t45.8\nHumanEval\t43.3\t59.8\t48.8\t67.1\t73.2\t74.4\nMBPP(Sanitized)\t51.8\t51.4\t63.0\t65.8\t78.9\t79.0\nThe evaluation results were obtained from OpenCompass (some data marked with *, which means come from the original papers), and evaluation configuration can be found in the configuration files provided by OpenCompass.\nThe evaluation data may have numerical differences due to the version iteration of OpenCompass, so please refer to the latest evaluation results of OpenCompass.\n\nLimitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.\n\nImport from Transformers\n\nTo load the InternLM2 7B Chat model using Transformers, use the following code:\n\nimport torch\nfrom openmind import AutoTokenizer, AutoModelForCausalLM\nmodel_path = \"PyTorch-NPU/internlm2_chat_7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and cause OOM Error.\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\")\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"hello\", history=[])\nprint(response)\n# Hello! How can I help you today?\nresponse, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\nprint(response)\n\n\nThe responses can be streamed using stream_chat:\n\nimport torch\nfrom openmind import AutoTokenizer, AutoModelForCausalLM\n\nmodel_path = \"PyTorch-NPU/internlm2_chat_7b\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True,device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\nmodel = model.eval()\nlength = 0\nfor response, history in model.stream_chat(tokenizer, \"Hello\", history=[]):\n    print(response[length:], flush=True, end=\"\")\n    length = len(response)\n\nOpen Source License\n\nThe code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow free commercial usage. To apply for a commercial license, please fill in the application form (English)/ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰. For other questions or collaborations, please contact internlm@pjlab.org.cn.\n\nç®€ä»‹\n\nInternLM2 ï¼Œå³ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹ç¬¬äºŒä»£ï¼Œå¼€æºäº†é¢å‘å®ç”¨åœºæ™¯çš„70äº¿å‚æ•°åŸºç¡€æ¨¡å‹ä¸å¯¹è¯æ¨¡å‹ ï¼ˆInternLM2-Chat-7Bï¼‰ã€‚æ¨¡å‹å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n\næœ‰æ•ˆæ”¯æŒ20ä¸‡å­—è¶…é•¿ä¸Šä¸‹æ–‡ï¼šæ¨¡å‹åœ¨20ä¸‡å­—é•¿è¾“å…¥ä¸­å‡ ä¹å®Œç¾åœ°å®ç°é•¿æ–‡â€œå¤§æµ·æé’ˆâ€ï¼Œè€Œä¸”åœ¨ LongBench å’Œ L-Eval ç­‰é•¿æ–‡ä»»åŠ¡ä¸­çš„è¡¨ç°ä¹Ÿè¾¾åˆ°å¼€æºæ¨¡å‹ä¸­çš„é¢†å…ˆæ°´å¹³ã€‚ å¯ä»¥é€šè¿‡ LMDeploy å°è¯•20ä¸‡å­—è¶…é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚\nç»¼åˆæ€§èƒ½å…¨é¢æå‡ï¼šå„èƒ½åŠ›ç»´åº¦ç›¸æ¯”ä¸Šä¸€ä»£æ¨¡å‹å…¨é¢è¿›æ­¥ï¼Œåœ¨æ¨ç†ã€æ•°å­¦ã€ä»£ç ã€å¯¹è¯ä½“éªŒã€æŒ‡ä»¤éµå¾ªå’Œåˆ›æ„å†™ä½œç­‰æ–¹é¢çš„èƒ½åŠ›æå‡å°¤ä¸ºæ˜¾è‘—ï¼Œç»¼åˆæ€§èƒ½è¾¾åˆ°åŒé‡çº§å¼€æºæ¨¡å‹çš„é¢†å…ˆæ°´å¹³ï¼Œåœ¨é‡ç‚¹èƒ½åŠ›è¯„æµ‹ä¸Š InternLM2-Chat-20B èƒ½æ¯”è‚©ç”šè‡³è¶…è¶Š ChatGPT ï¼ˆGPT-3.5ï¼‰ã€‚\nä»£ç è§£é‡Šå™¨ä¸æ•°æ®åˆ†æï¼šåœ¨é…åˆä»£ç è§£é‡Šå™¨ï¼ˆcode-interpreterï¼‰çš„æ¡ä»¶ä¸‹ï¼ŒInternLM2-Chat-20B åœ¨ GSM8K å’Œ MATH ä¸Šå¯ä»¥è¾¾åˆ°å’Œ GPT-4 ç›¸ä»¿çš„æ°´å¹³ã€‚åŸºäºåœ¨æ•°ç†å’Œå·¥å…·æ–¹é¢å¼ºå¤§çš„åŸºç¡€èƒ½åŠ›ï¼ŒInternLM2-Chat æä¾›äº†å®ç”¨çš„æ•°æ®åˆ†æèƒ½åŠ›ã€‚\nå·¥å…·è°ƒç”¨èƒ½åŠ›æ•´ä½“å‡çº§ï¼šåŸºäºæ›´å¼ºå’Œæ›´å…·æœ‰æ³›åŒ–æ€§çš„æŒ‡ä»¤ç†è§£ã€å·¥å…·ç­›é€‰ä¸ç»“æœåæ€ç­‰èƒ½åŠ›ï¼Œæ–°ç‰ˆæ¨¡å‹å¯ä»¥æ›´å¯é åœ°æ”¯æŒå¤æ‚æ™ºèƒ½ä½“çš„æ­å»ºï¼Œæ”¯æŒå¯¹å·¥å…·è¿›è¡Œæœ‰æ•ˆçš„å¤šè½®è°ƒç”¨ï¼Œå®Œæˆè¾ƒå¤æ‚çš„ä»»åŠ¡ã€‚å¯ä»¥æŸ¥çœ‹æ›´å¤šæ ·ä¾‹ã€‚\nInternLM2-Chat-7B\næ€§èƒ½è¯„æµ‹\n\næˆ‘ä»¬ä½¿ç”¨å¼€æºè¯„æµ‹å·¥å…· OpenCompass ä»å­¦ç§‘ç»¼åˆèƒ½åŠ›ã€è¯­è¨€èƒ½åŠ›ã€çŸ¥è¯†èƒ½åŠ›ã€æ¨ç†èƒ½åŠ›ã€ç†è§£èƒ½åŠ›äº”å¤§èƒ½åŠ›ç»´åº¦å¯¹InternLMå¼€å±•å…¨é¢è¯„æµ‹ï¼Œéƒ¨åˆ†è¯„æµ‹ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼Œæ¬¢è¿è®¿é—® OpenCompass æ¦œå• è·å–æ›´å¤šçš„è¯„æµ‹ç»“æœã€‚\n\nè¯„æµ‹é›†\tInternLM2-7B\tInternLM2-Chat-7B\tInternLM2-20B\tInternLM2-Chat-20B\tChatGPT\tGPT-4\nMMLU\t65.8\t63.7\t67.7\t66.5\t69.1\t83.0\nAGIEval\t49.9\t47.2\t53.0\t50.3\t39.9\t55.1\nBBH\t65.0\t61.2\t72.1\t68.3\t70.1\t86.7\nGSM8K\t70.8\t70.7\t76.1\t79.6\t78.2\t91.4\nMATH\t20.2\t23.0\t25.5\t31.9\t28.0\t45.8\nHumanEval\t43.3\t59.8\t48.8\t67.1\t73.2\t74.4\nMBPP(Sanitized)\t51.8\t51.4\t63.0\t65.8\t78.9\t79.0\nä»¥ä¸Šè¯„æµ‹ç»“æœåŸºäº OpenCompass è·å¾—ï¼ˆéƒ¨åˆ†æ•°æ®æ ‡æ³¨*ä»£è¡¨æ•°æ®æ¥è‡ªåŸå§‹è®ºæ–‡ï¼‰ï¼Œå…·ä½“æµ‹è¯•ç»†èŠ‚å¯å‚è§ OpenCompass ä¸­æä¾›çš„é…ç½®æ–‡ä»¶ã€‚\nè¯„æµ‹æ•°æ®ä¼šå›  OpenCompass çš„ç‰ˆæœ¬è¿­ä»£è€Œå­˜åœ¨æ•°å€¼å·®å¼‚ï¼Œè¯·ä»¥ OpenCompass æœ€æ–°ç‰ˆçš„è¯„æµ‹ç»“æœä¸ºä¸»ã€‚\n\nå±€é™æ€§ï¼š å°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬éå¸¸æ³¨é‡æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå°½åŠ›ä¿ƒä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆä¼¦ç†å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†å—é™äºæ¨¡å‹å¤§å°ä»¥åŠæ¦‚ç‡ç”ŸæˆèŒƒå¼ï¼Œæ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿå„ç§ä¸ç¬¦åˆé¢„æœŸçš„è¾“å‡ºï¼Œä¾‹å¦‚å›å¤å†…å®¹åŒ…å«åè§ã€æ­§è§†ç­‰æœ‰å®³å†…å®¹ï¼Œè¯·å‹¿ä¼ æ’­è¿™äº›å†…å®¹ã€‚ç”±äºä¼ æ’­ä¸è‰¯ä¿¡æ¯å¯¼è‡´çš„ä»»ä½•åæœï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…è´£ä»»ã€‚\n\né€šè¿‡ Transformers åŠ è½½\n\né€šè¿‡ä»¥ä¸‹çš„ä»£ç åŠ è½½ InternLM2 7B Chat æ¨¡å‹\n\nimport torch\nfrom openmind import AutoTokenizer, AutoModelForCausalLM\nmodel_path = \"PyTorch-NPU/internlm2_chat_7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n# `torch_dtype=torch.float16` å¯ä»¥ä»¤æ¨¡å‹ä»¥ float16 ç²¾åº¦åŠ è½½ï¼Œå¦åˆ™ transformers ä¼šå°†æ¨¡å‹åŠ è½½ä¸º float32ï¼Œå¯¼è‡´æ˜¾å­˜ä¸è¶³\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True,device_map=\"auto\")\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\nprint(response)\n# ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\nresponse, history = model.chat(tokenizer, \"è¯·æä¾›ä¸‰ä¸ªç®¡ç†æ—¶é—´çš„å»ºè®®ã€‚\", history=history)\nprint(response)\n\n\nå¦‚æœæƒ³è¿›è¡Œæµå¼ç”Ÿæˆï¼Œåˆ™å¯ä»¥ä½¿ç”¨ stream_chat æ¥å£ï¼š\n\nimport torch\nfrom openmind import AutoTokenizer, AutoModelForCausalLM\n\nmodel_path = \"PyTorch-NPU/internlm2_chat_7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True,device_map=\"auto\")\n\nmodel = model.eval()\nlength = 0\nfor response, history in model.stream_chat(tokenizer, \"ä½ å¥½\", history=[]):\n    print(response[length:], flush=True, end=\"\")\n    length = len(response)\n\nå¼€æºè®¸å¯è¯\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆç”³è¯·è¡¨ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» internlm@pjlab.org.cnã€‚",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"Apache License 2.0\", \"â€¢ Sleeping:32U64G\", \"32U64G\"]"
  },
  {
    "url": "https://gitcode.com/openMind/mbart_large_50_many_to_many_mmt",
    "project_name": "mbart_large_50_many_to_many_mmt",
    "readme": "",
    "tags": "[]"
  },
  {
    "url": "https://gitcode.com/openMind/mistral_7b_v0.1",
    "project_name": "mistral_7b_v0.1",
    "readme": "",
    "tags": "[]"
  },
  {
    "url": "https://gitcode.com/openMind/t5_large",
    "project_name": "t5_large",
    "readme": "Original Text\nT5å¤§æ¨¡å‹å¡ç‰‡\nä¿®æ”¹è¯´æ˜\n\nåœ¨åŸå§‹READMEåŸºç¡€ä¸Šå¢åŠ äº†CANNç‰ˆæœ¬ä¾èµ–æè¿°ï¼Œå¹¶ä¿®æ”¹äº†ç¤ºä¾‹ä»£ç ã€‚\n\nç›®å½•\næ¨¡å‹è¯¦æƒ…\nç”¨é€”\nåå·®ã€é£é™©ä¸é™åˆ¶\nè®­ç»ƒè¯¦æƒ…\nè¯„ä¼°\nç¯å¢ƒå½±å“\nå¼•ç”¨\næ¨¡å‹å¡ç‰‡ä½œè€…\nå¿«é€Ÿå¼€å§‹æŒ‡å—\næ¨¡å‹è¯¦æƒ…\næ¨¡å‹æè¿°\n\næ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢Transformerï¼ˆT5ï¼‰çš„å¼€å‘è€…åœ¨æ–‡ç« ä¸­å†™é“ï¼š\n\né€šè¿‡T5ï¼Œæˆ‘ä»¬æå‡ºå°†æ‰€æœ‰NLPä»»åŠ¡é‡æ„ä¸ºç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼ï¼Œè¾“å…¥è¾“å‡ºå§‹ç»ˆæ˜¯æ–‡æœ¬å­—ç¬¦ä¸²ã€‚è¿™ä¸BERTç±»æ¨¡å‹å½¢æˆå¯¹æ¯”ï¼Œåè€…åªèƒ½è¾“å‡ºç±»åˆ«æ ‡ç­¾æˆ–è¾“å…¥æ–‡æœ¬ç‰‡æ®µã€‚æˆ‘ä»¬çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ¡†æ¶å…è®¸åœ¨ä»»ä½•NLPä»»åŠ¡ä¸Šä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œè¶…å‚æ•°ã€‚\n\nT5-Largeæ˜¯åŒ…å«7.7äº¿å‚æ•°çš„æ£€æŸ¥ç‚¹ç‰ˆæœ¬ã€‚\n\nå¼€å‘å›¢é˜Ÿï¼š Colin Raffelã€Noam Shazeerç­‰ï¼Œè¯¦è§ç›¸å…³è®ºæ–‡å’ŒGitHubä»“åº“\næ¨¡å‹ç±»å‹ï¼š è¯­è¨€æ¨¡å‹\næ”¯æŒè¯­è¨€ï¼š è‹±è¯­ã€æ³•è¯­ã€ç½—é©¬å°¼äºšè¯­ã€å¾·è¯­\nè®¸å¯åè®®ï¼š Apache 2.0\næ‰©å±•èµ„æºï¼š\nç ”ç©¶è®ºæ–‡\nè°·æ­ŒT5æŠ€æœ¯åšå®¢\nGitHubä»“åº“\nç”¨é€”\nç›´æ¥åº”ç”¨ä¸ä¸‹æ¸¸ä»»åŠ¡\n\nå¼€å‘è€…åœ¨åšå®¢ä¸­è¯´æ˜ï¼š\n\næˆ‘ä»¬çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ¡†æ¶æ”¯æŒåœ¨æœºå™¨ç¿»è¯‘ã€æ–‡æ¡£æ‘˜è¦ã€é—®ç­”ç³»ç»Ÿã€åˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰ç­‰ä»»ä½•NLPä»»åŠ¡ä¸­ä½¿ç”¨ç›¸åŒæ¨¡å‹ã€‚ç”šè‡³å¯ä»¥é€šè¿‡è®©æ¨¡å‹é¢„æµ‹æ•°å­—çš„å­—ç¬¦ä¸²è¡¨ç¤ºæ¥å¤„ç†å›å½’ä»»åŠ¡ã€‚\n\næ›´å¤šç»†èŠ‚è¯·å‚é˜…æŠ€æœ¯åšå®¢å’Œç ”ç©¶è®ºæ–‡ã€‚\n\néé€‚ç”¨åœºæ™¯\n\nå¾…è¡¥å……ä¿¡æ¯ã€‚\n\nåå·®ã€é£é™©ä¸é™åˆ¶\n\nå¾…è¡¥å……ä¿¡æ¯ã€‚\n\nä½¿ç”¨å»ºè®®\n\nå¾…è¡¥å……ä¿¡æ¯ã€‚\n\nè®­ç»ƒè¯¦æƒ…\nè®­ç»ƒæ•°æ®\n\næ¨¡å‹é€šè¿‡æ— ç›‘ç£(1.)ä¸æœ‰ç›‘ç£ä»»åŠ¡(2.)çš„æ··åˆå¤šä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼š\n\næ— ç›‘ç£å»å™ªç›®æ ‡æ•°æ®é›†ï¼š\n\nC4\nWiki-DPR\n\næœ‰ç›‘ç£æ–‡æœ¬ç”Ÿæˆç›®æ ‡æ•°æ®é›†ï¼š\n\nå¥å­å¯æ¥å—æ€§åˆ¤æ–­ï¼ˆCoLAï¼‰\næƒ…æ„Ÿåˆ†æï¼ˆSST-2ï¼‰\nå¤è¿°/å¥å­ç›¸ä¼¼åº¦ï¼ˆMRPC/STS-B/QQPï¼‰\nè‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆMNLI/QNLI/RTE/CBï¼‰\nå¥å­è¡¥å…¨ï¼ˆCOPAï¼‰\nè¯ä¹‰æ¶ˆæ­§ï¼ˆWICï¼‰\né—®ç­”ç³»ç»Ÿï¼ˆMultiRC/ReCoRD/BoolQï¼‰\nè®­ç»ƒæ–¹æ³•\n\nç ”ç©¶è€…åœ¨æ‘˜è¦ä¸­æŒ‡å‡ºï¼š\n\næœ¬æ–‡é€šè¿‡å°†æ¯ä¸ªè¯­è¨€é—®é¢˜è½¬æ¢ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç³»ç»Ÿæ¯”è¾ƒäº†æ•°åé¡¹è¯­è¨€ç†è§£ä»»åŠ¡ä¸­çš„é¢„è®­ç»ƒç›®æ ‡ã€æ¶æ„ã€æ— æ ‡æ³¨æ•°æ®é›†å’Œè¿ç§»æ–¹æ³•ã€‚\n\nT5æ¡†æ¶æ•´åˆäº†è®ºæ–‡ä¸­ç ”ç©¶çš„å„ç±»è®­ç»ƒæ–¹æ³•ï¼Œè¯¦è§ç ”ç©¶è®ºæ–‡ã€‚\n\nè¯„ä¼°\næµ‹è¯•æ•°æ®ä¸æŒ‡æ ‡\n\nå¼€å‘è€…åœ¨24ä¸ªä»»åŠ¡ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå®Œæ•´ç»“æœå‚è§è®ºæ–‡è¡¨14ã€‚\n\nç¯å¢ƒå½±å“\n\nç¢³æ’æ”¾ä¼°ç®—å¯å‚è€ƒLacosteç­‰äºº(2019)æå‡ºçš„æœºå™¨å­¦ä¹ ç¢³è¶³è¿¹è®¡ç®—å™¨ã€‚\n\nç¡¬ä»¶ç±»å‹ï¼š è°·æ­Œäº‘TPU Pods\nè®­ç»ƒæ—¶é•¿ï¼š å¾…è¡¥å……\näº‘æœåŠ¡å•†ï¼š GCP\nè®¡ç®—åŒºåŸŸï¼š å¾…è¡¥å……\nç¢³æ’æ”¾é‡ï¼š å¾…è¡¥å……\nå¼•ç”¨\n\nBibTeXæ ¼å¼ï¼š\n\n@article{2020t5,\n  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {140},\n  pages   = {1-67},\n  url     = {http://jmlr.org/papers/v21/20-074.html}\n}\n\n\nAPAæ ¼å¼ï¼š\n\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). æ¢ç´¢è¿ç§»å­¦ä¹ çš„æé™ï¼šåŸºäºç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨çš„ç ”ç©¶ã€‚ã€Šæœºå™¨å­¦ä¹ ç ”ç©¶æœŸåˆŠã€‹ï¼Œ21(140), 1-67.\næ¨¡å‹å¡ä½œè€…\n\næœ¬æ¨¡å‹å¡ç”±Hugging Faceå›¢é˜Ÿæ’°å†™ã€‚\n\nå¦‚ä½•å¼€å§‹ä½¿ç”¨è¯¥æ¨¡å‹\n\né€šè¿‡ä»¥ä¸‹ä»£ç å¿«é€Ÿä¸Šæ‰‹è¯¥æ¨¡å‹ã€‚\n\nç‚¹å‡»å±•å¼€",
    "tags": "[\"Translation\", \"PyTorch\", \"Transformers\", \"English\", \"Apache License 2.0\", \"c4\", \"summarization\"]"
  },
  {
    "url": "https://gitcode.com/openMind/bloom_7b1",
    "project_name": "bloom_7b1",
    "readme": "Original Text\nBLOOM è¯­è¨€æ¨¡å‹\nå¤§è§„æ¨¡å¼€æ”¾ç§‘å­¦å¼€æ”¾è·å–å¤šè¯­è¨€è¯­è¨€æ¨¡å‹\næ¨¡å‹æ¦‚è¿°\n\nç‰ˆæœ¬ 1.0 / 2022å¹´5æœˆ26æ—¥\n\nç›®å½•\næ¨¡å‹ç»†èŠ‚\nåº”ç”¨åœºæ™¯\nè®­ç»ƒæ•°æ®\né£é™©ä¸é™åˆ¶\nè¯„ä¼°\nå»ºè®®\næœ¯è¯­ä¸è®¡ç®—\næ›´å¤šä¿¡æ¯\næ¨¡å‹æ¦‚è¿°ä½œè€…\nä¿®æ”¹è®°å½•\n\næ·»åŠ äº†ç¤ºä¾‹ä»£ç å¹¶ä¿®æ”¹äº†é“¾æ¥è·¯å¾„\n\næ¨¡å‹ç»†èŠ‚\nå¿«é€Ÿå…¥é—¨\n\nä»¥ä¸‹ä»£ç å±•ç¤ºäº†ä¸bloom_7b1çš„äº¤äº’ç¤ºä¾‹ï¼š\n\nimport torch\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bloom_7b1\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"PyTorch-NPU/bloom_7b1\", trust_remote_code=True, device_map=\"auto\")\n\ninput = \"Give three tips for staying healthy.\"\nprompt = (\"Below is an instrunction that describes a task. \"\n              \"Write a response that appropriately completes the requests\\n\\n\"\n              f\"### Instruction:\\n{input}\\n\\n### Response:\")\ninputs = tokenizer(prompt, return_tensors=\"pt\")\ninputs = inputs.to(model.device)\n\npred = model.generate(**inputs, max_new_tokens=512, repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\nåŸºç¡€ä¿¡æ¯\n\næœ¬èŠ‚ä¸ºä»»ä½•å¸Œæœ›äº†è§£æ¨¡å‹ä¿¡æ¯çš„äººå£«æä¾›ç›¸å…³ä¿¡æ¯ã€‚\n\nç‚¹å‡»å±•å¼€\næŠ€æœ¯è§„æ ¼\n\næœ¬èŠ‚ä¸ºå‚ä¸æ¨¡å‹å¼€å‘çš„äººå‘˜æä¾›ä¿¡æ¯ã€‚\n\nç‚¹å‡»å±•å¼€\nç¯å¢ƒå½±å“\nç‚¹å‡»å±•å¼€\n\nÂ \n\nç”¨é€”\n\næœ¬èŠ‚è®¨è®ºæ¨¡å‹é¢„è®¡çš„ç”¨é€”ï¼Œä»¥åŠæ¨¡å‹çš„å¯é¢„è§ç”¨æˆ·ï¼ˆåŒ…æ‹¬å—æ¨¡å‹å½±å“çš„ç”¨æˆ·ï¼‰ï¼Œå¹¶æè¿°äº†è¶…å‡ºèŒƒå›´æˆ–è¯¯ç”¨æ¨¡å‹çš„æƒ…å†µã€‚å®ƒä¸ºè€ƒè™‘ä½¿ç”¨æ¨¡å‹æˆ–å—æ¨¡å‹å½±å“çš„äººæä¾›ä¿¡æ¯ã€‚\n\nç‚¹å‡»å±•å¼€\n\nÂ \n\nè®­ç»ƒæ•°æ®\n\næœ¬èŠ‚æä¾›äº†è®­ç»ƒæ•°æ®çš„é«˜çº§æ¦‚è¿°ã€‚å¯¹äºä»»ä½•æƒ³è¦äº†è§£æ¨¡å‹å­¦ä¹ åŸºç¡€ä¿¡æ¯çš„äººæ¥è¯´ï¼Œè¿™æ˜¯ç›¸å…³çš„ã€‚\n\nç‚¹å‡»å±•å¼€\n\nÂ \n\né£é™©å’Œå±€é™æ€§\n\næœ¬èŠ‚è¯†åˆ«å¯é¢„è§çš„ä¼¤å®³å’Œè¯¯è§£ã€‚\n\nç‚¹å‡»å±•å¼€\n\nÂ \n\nè¯„ä¼°\n\næœ¬èŠ‚æè¿°è¯„ä¼°åè®®å¹¶æä¾›ç»“æœã€‚\n\nç‚¹å‡»å±•å¼€\n\nÂ \n\nå»ºè®®æªæ–½\n\næœ¬èŠ‚æä¾›å…³äºè­¦å‘Šå’Œæ½œåœ¨ç¼“è§£æªæ–½çš„ä¿¡æ¯ã€‚\n\nç‚¹å‡»å±•å¼€\n\nÂ \n\næœ¯è¯­ä¸è®¡ç®—\n\næœ¬èŠ‚å®šä¹‰äº†å¸¸è§æœ¯è¯­åŠå¦‚ä½•è®¡ç®—åº¦é‡æ ‡å‡†ã€‚\n\nç‚¹å‡»å±•å¼€\n\nÂ \n\næ›´å¤šä¿¡æ¯\nç‚¹å‡»å±•å¼€\n\nÂ \n\næ¨¡å‹å¡ä½œè€…\n\nå¤§è‡´æŒ‰æ—¶é—´é¡ºåºå’Œæ—¶é—´æŠ•å…¥æ’åºã€‚\n\nMargaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos MuÃ±oz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana IliÄ‡, GÃ©rard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"Safetensors\", \"JAX\", \"English\", \"Chinese\", \"Other\", \"bloom\"]"
  },
  {
    "url": "https://gitcode.com/openMind/conformer_ms",
    "project_name": "conformer_ms",
    "readme": "ä»‹ç»\n\nconformeræ˜¯å°†ä¸€ç§transformerå’Œcnnç»“åˆèµ·æ¥ï¼Œå¯¹éŸ³é¢‘åºåˆ—è¿›è¡Œå±€éƒ¨å’Œå…¨å±€ä¾èµ–éƒ½è¿›è¡Œå»ºæ¨¡çš„æ¨¡å‹ã€‚ç›®å‰åŸºäºtransformerå’Œå·ç§¯ç¥ç»ç½‘ç»œcnnçš„æ¨¡å‹åœ¨ASRä¸Šå·²ç»è¾¾åˆ°äº†è¾ƒå¥½çš„æ•ˆæœï¼ŒTransformerèƒ½å¤Ÿæ•è·é•¿åºåˆ—çš„ä¾èµ–å’ŒåŸºäºå†…å®¹çš„å…¨å±€äº¤äº’ä¿¡æ¯ï¼ŒCNNåˆ™èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å±€éƒ¨ç‰¹å¾ï¼Œå› æ­¤é’ˆå¯¹è¯­éŸ³è¯†åˆ«é—®é¢˜æå‡ºäº†å·ç§¯å¢å¼ºçš„transformeræ¨¡å‹ï¼Œç§°ä¸ºconformerï¼Œæ¨¡å‹æ€§èƒ½ä¼˜äºtransformerå’Œcnnã€‚ç›®å‰æä¾›ç‰ˆæœ¬æ”¯æŒåœ¨NPUå’ŒGPUä¸Šä½¿ç”¨conformeræ¨¡å‹åœ¨aishell-1æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒ/æµ‹è¯•å’Œæ¨ç†ã€‚\n\næ¨¡å‹ç»“æ„\n\nConformeræ•´ä½“ç»“æ„åŒ…æ‹¬ï¼šSpecAugã€ConvolutionSubsamplingã€Linearã€Dropoutã€ConformerBlocksÃ—Nï¼Œå¯è§å¦‚ä¸‹ç»“æ„å›¾ã€‚\n\nConformerBlockç»“æ„ï¼ˆNä¸ªè¯¥ç»“æ„ï¼‰ï¼šFeed Forward Moduleã€Multi-Head Self Attention Moduleã€Convolution Moduleã€Feed Forward Moduleã€Layernormã€‚å…¶ä¸­æ¯ä¸ªModuleéƒ½æ˜¯å‰æ¥ä¸€ä¸ªLayernormåæ¥ä¸€ä¸ªDropoutï¼Œä¸”éƒ½æœ‰æ®‹å·®é“¾è¿æ¥ï¼Œæ®‹å·®æ•°æ®ä¸ºè¾“å…¥æ•°æ®æœ¬èº«ã€‚\n\né©¬å¡é¾™ç»“æ„ï¼šå¯ä»¥çœ‹åˆ°ConformerBlockç¥ä¼¼é©¬å¡é¾™ç»“æ„ï¼Œå³ä¸¤ä¸ªä¸€æ ·çš„Feed Forward Moduleä¸­é—´å¤¹äº†Multi-Head Self Attention Moduleå’ŒConvolutionã€‚\n\næ¨¡å‹è¡¨ç°\nFeature info: using fbank feature, cmvn, online speed perturb\nTraining info: lr 0.001, acc_grad 1, 240 epochs, 8 Ascend910\nDecoding info: ctc_weight 0.3, average_num 30\nPerformance result: total_time 11h17min, 8p, using hccl_tools.\nmodel\tdecoding mode\tCER\nconformer\tctc greedy search\t5.05\nconformer\tctc prefix beam search\t5.05\nconformer\tattention decoder\t5.00\nconformer\tattention rescoring\t4.73\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\næœ‰å…³å¦‚ä½•è®­ç»ƒå’Œæ¨ç†æ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹MindAudio GitHub ä»“åº“.",
    "tags": "[\"Automatic Speech Recognition\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/hrnet_ms",
    "project_name": "hrnet_ms",
    "readme": "Original Text\nHRNet\n\nç”¨äºè§†è§‰è¯†åˆ«çš„æ·±åº¦é«˜åˆ†è¾¨ç‡è¡¨ç¤ºå­¦ä¹ \n\nç®€ä»‹\n\né«˜åˆ†è¾¨ç‡è¡¨ç¤ºå¯¹äºä½ç½®æ•æ„Ÿçš„è§†è§‰é—®é¢˜è‡³å…³é‡è¦ï¼Œå¦‚äººä½“å§¿æ€ä¼°è®¡ã€è¯­ä¹‰åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ã€‚ç°æœ‰çš„æœ€å…ˆè¿›çš„æ¡†æ¶é¦–å…ˆé€šè¿‡ç”±é«˜åˆ°ä½åˆ†è¾¨ç‡å·ç§¯ï¼ˆä¾‹å¦‚ ResNetã€VGGNetï¼‰ç»„æˆçš„å­ç½‘ç»œå°†è¾“å…¥å›¾åƒç¼–ç ä¸ºä½åˆ†è¾¨ç‡è¡¨ç¤ºï¼Œç„¶åä»ç¼–ç çš„ä½åˆ†è¾¨ç‡è¡¨ç¤ºä¸­æ¢å¤é«˜åˆ†è¾¨ç‡è¡¨ç¤ºã€‚ç›¸åï¼Œæ‰€æå‡ºçš„ç½‘ç»œï¼Œå‘½åä¸ºé«˜åˆ†è¾¨ç‡ç½‘ç»œï¼ˆHRNetï¼‰ï¼Œåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ä¿æŒé«˜åˆ†è¾¨ç‡è¡¨ç¤ºã€‚å®ƒæœ‰ä¸¤ä¸ªå…³é”®ç‰¹å¾ï¼šï¼ˆiï¼‰å¹¶è¡Œè¿æ¥é«˜åˆ°ä½åˆ†è¾¨ç‡å·ç§¯æµï¼›ï¼ˆiiï¼‰åœ¨ä¸åŒåˆ†è¾¨ç‡ä¹‹é—´åå¤äº¤æ¢ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯å¾—åˆ°çš„è¡¨ç¤ºåœ¨è¯­ä¹‰ä¸Šæ›´ä¸°å¯Œï¼Œåœ¨ç©ºé—´ä¸Šæ›´ç²¾ç¡®ã€‚å®ƒåœ¨å¤šç§åº”ç”¨ä¸­å±•ç¤ºäº† HRNet çš„ä¼˜è¶Šæ€§ï¼ŒåŒ…æ‹¬äººä½“å§¿æ€ä¼°è®¡ã€è¯­ä¹‰åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ï¼Œè¡¨æ˜ HRNet æ˜¯è®¡ç®—æœºè§†è§‰é—®é¢˜çš„æ›´å¼ºåŸºç¡€æ¨¡å‹ã€‚\n\nå›¾ 1. HRNet æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K ä¸Šçš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•°ï¼ˆMï¼‰\té…ç½®\tä¸‹è½½\nhrnet_w32\tD910x8-G\t80.64\t95.44\t41.30\tyaml\tæƒé‡\nhrnet_w48\tD910x8-G\t81.19\t95.69\t77.57\tyaml\tæƒé‡\næ³¨æ„äº‹é¡¹\nç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º {è®¾å¤‡}x{éƒ¨ä»¶}-{MS æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - ä½¿ç”¨ ms å‡½æ•°çš„ pynative æ¨¡å¼ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºåœ¨ 8 ç‰‡ Ascend 910 NPU ä¸Šä½¿ç”¨å›¾æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸Šçš„å‡†ç¡®åº¦æŠ¥å‘Šã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV ä¸­çš„ å®‰è£…è¯´æ˜ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†ä»¥ä¾›æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ä½¿ç”¨ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾é‡ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/hrnet/hrnet_w32_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­åŠ å…¥ --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\nå…³äºæ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨é‡ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…è°ƒæ•´å­¦ä¹ ç‡ä»¥çº¿æ€§é€‚åº”æ–°çš„å…¨å±€æ‰¹é‡å¤§å°ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨æƒ³åœ¨æ— éœ€åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/hrnet/hrnet_w32_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/hrnet/hrnet_w32_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²\n\nä¸ºäº†é«˜æ•ˆåœ°ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹éƒ¨ç½²åœ¨çº¿æ¨ç†æœåŠ¡ï¼Œè¯·å‚è€ƒéƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] ç‹æ™¯ä¸œï¼Œå­™å…‹ï¼Œç¨‹å¤©æ’ï¼Œç­‰. æ·±åº¦é«˜åˆ†è¾¨ç‡è¡¨å¾å­¦ä¹ åœ¨è§†è§‰è¯†åˆ«ä¸­çš„åº”ç”¨[J]. arXiv é¢„å°æœ¬ arXiv:1908.07919, 2019.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/inceptionv3_ms",
    "project_name": "inceptionv3_ms",
    "readme": "Original Text\nInceptionV3\n\nInceptionV3ï¼šé‡æ–°æ€è€ƒè®¡ç®—æœºè§†è§‰çš„åˆå§‹æ¶æ„\n\nç®€ä»‹\n\nInceptionV3æ˜¯GoogLeNetçš„å‡çº§ç‰ˆæœ¬ã€‚V3æœ€é‡è¦çš„æ”¹è¿›ä¹‹ä¸€æ˜¯åˆ†è§£ï¼ˆFactorizationï¼‰ï¼Œå®ƒå°†7x7å·ç§¯åˆ†è§£ä¸ºä¸¤ä¸ªä¸€ç»´å·ç§¯ï¼ˆ1x7å’Œ7x1ï¼‰ï¼Œ3x3å·ç§¯åŒç†ï¼ˆ1x3å’Œ3x1ï¼‰ã€‚è¿™æ ·çš„å¥½å¤„æ˜¯æ—¢èƒ½åŠ é€Ÿè®¡ç®—ï¼ˆèŠ‚çœçš„è®¡ç®—èµ„æºå¯ç”¨äºåŠ æ·±ç½‘ç»œï¼‰ï¼Œåˆèƒ½å°†ä¸€ä¸ªå·ç§¯æ‹†åˆ†ä¸ºä¸¤ä¸ªå·ç§¯ï¼Œè¿›ä¸€æ­¥å¢åŠ ç½‘ç»œæ·±åº¦ï¼Œæå‡ç½‘ç»œçš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç½‘ç»œçš„è¾“å…¥å°ºå¯¸ä»224x224å˜ä¸º299x299ï¼Œå¹¶æ›´ç²¾ç»†åœ°è®¾è®¡äº†35x35/17x17/8x8æ¨¡å—ã€‚æ­¤å¤–ï¼ŒV3è¿˜å¼•å…¥äº†æ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰ï¼Œä½¿æ¨¡å‹æ”¶æ•›æ›´å¿«ï¼Œèµ·åˆ°éƒ¨åˆ†æ­£åˆ™åŒ–çš„ä½œç”¨ï¼Œæœ‰æ•ˆå‡å°‘äº†è¿‡æ‹Ÿåˆé—®é¢˜ã€‚1\n\nå›¾1. InceptionV3æ¶æ„ [1]\n\næ€§èƒ½\n\næˆ‘ä»¬åœ¨ImageNet-1Kä¸Šå¤ç°çš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚\n\næ¨¡å‹\tè®­ç»ƒç¯å¢ƒ\tTop-1å‡†ç¡®ç‡ (%)\tTop-5å‡†ç¡®ç‡ (%)\tå‚æ•°é‡ (M)\té…ç½®æ–‡ä»¶\tæ¨¡å‹ä¸‹è½½\ninception_v3\tD910x8-G\t79.11\t94.40\t27.20\tyaml\tæƒé‡æ–‡ä»¶\nè¯´æ˜\nè®­ç»ƒç¯å¢ƒï¼š{è®¾å¤‡}x{æ•°é‡}-{MindSporeæ¨¡å¼}ï¼Œå…¶ä¸­MindSporeæ¨¡å¼å¯ä»¥æ˜¯Gï¼ˆå›¾æ¨¡å¼ï¼‰æˆ–Fï¼ˆPyNativeæ¨¡å¼é…åˆmså‡½æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼ŒD910x8-Gè¡¨ç¤ºåœ¨8å¼ Ascend 910 NPUä¸Šä½¿ç”¨å›¾æ¨¡å¼è®­ç»ƒã€‚\nTop-1/Top-5å‡†ç¡®ç‡ï¼šåœ¨ImageNet-1KéªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®ç‡ã€‚\nå¿«é€Ÿå¼€å§‹\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒMindCVçš„å®‰è£…æŒ‡å—ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ImageNet-1Kæ•°æ®é›†ç”¨äºæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…ç½®å¯ä»¥è½»æ¾å¤ç°æŠ¥å‘Šç»“æœã€‚å¯¹äºåœ¨å¤šä¸ªAscend 910è®¾å¤‡ä¸Šçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/inceptionv3/inception_v3_ascend.yaml --data_dir /path/to/imagenet\n\n\nè‹¥è„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nåŒç†ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤š GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚é˜… config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹å¤§å°ï¼ˆbatch_size Ã— num_devicesï¼‰æ˜¯é‡è¦è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹å¤§å°ä¸å˜ï¼Œæˆ–æŒ‰æ–°å…¨å±€æ‰¹å¤§å°çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nå•æœºè®­ç»ƒ\n\nå¦‚æœå¸Œæœ›åœ¨è¾ƒå°æ•°æ®é›†ä¸Šä¸ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒæ¥è®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/inceptionv3/inception_v3_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nè¦éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥ä½¿ç”¨ validate.py è„šæœ¬ï¼Œå¹¶é€šè¿‡ --ckpt_path å‚æ•°æŒ‡å®šæ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/inceptionv3/inception_v3_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²æŒ‡å—\n\nè¯·æŸ¥é˜…MindCVä¸­çš„éƒ¨ç½²æ•™ç¨‹è·å–è¯¦ç»†æŒ‡å¯¼ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Szegedy C, Vanhoucke V, Ioffe S, ç­‰. è®¡ç®—æœºè§†è§‰ä¸­Inceptionæ¶æ„çš„é‡æ–°æ€è€ƒ[C]//IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†. 2016: 2818-2826.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/wavegrad_ms",
    "project_name": "wavegrad_ms",
    "readme": "Original Text\nWaveGrad å£°ç å™¨\n\nWaveGrad æ˜¯ä¸€æ¬¾åŸºäºæ‰©æ•£æ¨¡å‹çš„å£°ç å™¨ï¼Œä¸“ä¸ºæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿè®¾è®¡ã€‚\n\næ¦‚è¿°\n\nWaveGrad æ˜¯ç”± Google Brain å›¢é˜Ÿå¼€å‘çš„é«˜æ•ˆä¼˜è´¨ç¥ç»å£°ç å™¨ã€‚å…¶æ¶æ„åŸç†è¯¦è§è®ºæ–‡ã€ŠWaveGrad: æ³¢å½¢ç”Ÿæˆçš„æ¢¯åº¦ä¼°è®¡ã€‹ã€‚ç®€è¨€ä¹‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿­ä»£ä¼˜åŒ–çš„æ–¹å¼ï¼Œå°†å¯¹æ•°å°ºåº¦æ¢…å°”é¢‘è°±å›¾è½¬æ¢ä¸ºæ³¢å½¢ä¿¡å·ã€‚\n\né¢„è®­ç»ƒæ¨¡å‹\næ¨¡å‹\tæ•°æ®é›†\tæ£€æŸ¥ç‚¹\tæ€»æ‰¹æ¬¡å¤§å°\tå¸§æ•°\tæ¢…å°”é¢‘å¸¦æ•°\tç¡¬ä»¶\tMindSpore ç‰ˆæœ¬\nWaveGrad (åŸºç¡€ç‰ˆ)\tLJSpeech-1.1\t100ä¸‡æ­¥\t256\t30\t128\t8å¼ æ˜‡è…¾èŠ¯ç‰‡\t1.9.0\nå¿«é€Ÿå…¥é—¨\n\nå…³äºæ¨¡å‹è®­ç»ƒä¸æ¨ç†çš„è¯¦ç»†æŒ‡å—ï¼Œè¯·å‚é˜… MindAudio GitHub é¡¹ç›®åº“ã€‚\n\nè®¸å¯åè®®\n\nGNU é€šç”¨å…¬å…±è®¸å¯è¯ v2.0",
    "tags": "[\"Text-to-Audio\", \"GNU General Public License v2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/baichuan2_7b_chat_ms",
    "project_name": "baichuan2_7b_chat_ms",
    "readme": "Original Text\nç™¾å·å¤§æ¨¡å‹ 2\nğŸ¦‰GitHub | ğŸ’¬å¾®ä¿¡\nç™¾å·APIç°å·²æ”¯æŒæœç´¢å¢å¼ºä¸192Ké•¿ä¸Šä¸‹æ–‡çª—å£ï¼Œæ–°å¢çŸ¥è¯†åº“æ£€ç´¢åŠŸèƒ½å¹¶é™æ—¶å…è´¹ï¼\nğŸš€ ç™¾å·å¤§æ¨¡å‹åœ¨çº¿å¯¹è¯å¹³å° å·²å…¨é¢å¼€æ”¾å…¬æµ‹ ğŸ‰\næ›´æ–°è¯´æ˜/Modification\n\nä¼˜åŒ–äº†å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ä»£ç ã€‚/ Optimized the quick start code samples.\n\nç›®å½•/Table of Contents\nğŸ“– æ¨¡å‹æ¦‚è§ˆ/Introduction\nâš™ï¸ å¿«é€Ÿå…¥é—¨/Quick Start\nğŸ“Š æ€§èƒ½è¯„ä¼°/Benchmark Evaluation\nğŸ“œ è®¸å¯åè®®/Terms and Conditions\næ¨¡å‹æ¦‚è§ˆ/Introduction\n\nç™¾å·å¤§æ¨¡å‹2æ˜¯ç™¾å·æ™ºèƒ½ç ”å‘çš„æ–°ä¸€ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹ä½“ç³»ï¼ŒåŸºäº 2.6ä¸‡äº¿ Tokensçš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œåœ¨ä¸­è‹±æ–‡æƒå¨åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°åŒè§„æ¨¡æ¨¡å‹çš„æœ€ä¼˜è¡¨ç°ã€‚æœ¬æ¬¡å‘å¸ƒçš„7Bä¸13Bç‰ˆæœ¬æ¶µç›–åŸºç¡€æ¨¡å‹å’Œå¯¹è¯æ¨¡å‹ï¼Œå¹¶æä¾›4bité‡åŒ–ç‰ˆå¯¹è¯æ¨¡å‹ã€‚æ‰€æœ‰ç‰ˆæœ¬ä¸ä»…é¢å‘å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¼€å‘è€…åªéœ€é€šè¿‡é‚®ä»¶ç”³è¯·è·å–å®˜æ–¹å•†ç”¨æˆæƒåï¼Œå³å¯å…è´¹ç”¨äºå•†ä¸šåœºæ™¯ã€‚å…·ä½“ç‰ˆæœ¬ä¿¡æ¯å¦‚ä¸‹ï¼š\n\nBaichuan 2 is the new generation of open-source large language models developed by Baichuan Intelligence Inc.. Trained on 2.6 trillion high-quality tokens, it achieves state-of-the-art performance on authoritative Chinese and English benchmarks. This release includes 7B and 13B base/chat models, with 4bit quantized chat variants. All models are fully accessible for academic research, and commercial use is free upon obtaining an official license via email request. See version details below:\n\n\tåŸºç¡€æ¨¡å‹\tå¯¹è¯æ¨¡å‹\t4bité‡åŒ–å¯¹è¯æ¨¡å‹\n7B\tBaichuan2-7B-Base\tBaichuan2-7B-Chat\tBaichuan2-7B-Chat-4bits\n13B\tBaichuan2-13B-Base\tBaichuan2-13B-Chat\tBaichuan2-13B-Chat-4bits\næ€§èƒ½è¯„ä¼°/Benchmark Evaluation\n\næˆ‘ä»¬åœ¨[é€šç”¨é¢†åŸŸ]ã€æ³•å¾‹ã€åŒ»ç–—ã€[æ•°ç†é€»è¾‘]ã€[ç¼–ç¨‹]åŠå¤šè¯­è¨€ç¿»è¯‘å…­å¤§é¢†åŸŸçš„ä¸­è‹±æ–‡æƒå¨æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¯¦ç»†ç»“æœè¯·å‚é˜…GitHubã€‚\n\nWe conducted comprehensive evaluations across six domains: General, Legal, Medical, Mathematics, Coding, and Multilingual Translation. For full results, visit GitHub.\n\n7Bæ¨¡å‹è¡¨ç°\n\tC-Eval\tMMLU\tCMMLU\té«˜è€ƒ\tAGIEval\tBBH\n\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t3æ¬¡é‡‡æ ·\nGPT-4\t68.40\t83.93\t70.33\t66.15\t63.27\t75.12\nGPT-3.5 Turbo\t51.10\t68.54\t54.06\t47.07\t46.13\t61.59\nLLaMA-7B\t27.10\t35.10\t26.75\t27.81\t28.17\t32.38\nLLaMA2-7B\t28.90\t45.73\t31.38\t25.97\t26.53\t39.16\nMPT-7B\t27.15\t27.93\t26.00\t26.54\t24.83\t35.20\nFalcon-7B\t24.23\t26.03\t25.66\t24.24\t24.10\t28.77\nChatGLM2-6B\t50.20\t45.90\t49.00\t49.44\t45.28\t31.65\n[ç™¾å·-7B]\t42.80\t42.30\t44.02\t36.34\t34.44\t32.48\n[ç™¾å·2-7BåŸºç¡€ç‰ˆ]\t54.00\t54.16\t57.07\t47.47\t42.73\t41.56\n13Bæ¨¡å‹è¡¨ç°\n\tC-Eval\tMMLU\tCMMLU\té«˜è€ƒ\tAGIEval\tBBH\n\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t3æ¬¡é‡‡æ ·\nGPT-4\t68.40\t83.93\t70.33\t66.15\t63.27\t75.12\nGPT-3.5 Turbo\t51.10\t68.54\t54.06\t47.07\t46.13\t61.59\nLLaMA-13B\t28.50\t46.30\t31.15\t28.23\t28.22\t37.89\nLLaMA2-13B\t35.80\t55.09\t37.99\t30.83\t32.29\t46.98\nVicuna-13B\t32.80\t52.00\t36.28\t30.11\t31.55\t43.04\nChinese-Alpaca-Plus-13B\t38.80\t43.90\t33.43\t34.78\t35.46\t28.94\nXVERSE-13B\t53.70\t55.21\t58.44\t44.69\t42.54\t38.06\n[ç™¾å·-13BåŸºç¡€ç‰ˆ]\t52.40\t51.60\t55.30\t49.69\t43.20\t43.01\n[ç™¾å·2-13BåŸºç¡€ç‰ˆ]\t58.10\t59.17\t61.97\t54.33\t48.17\t48.78\nè®­ç»ƒè¿‡ç¨‹å­˜æ¡£/Training Dynamics\n\né™¤æœ€ç»ˆç‰ˆç™¾å·2-7BåŸºç¡€æ¨¡å‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€æ”¾äº†è®­ç»ƒè¿‡ç¨‹ä¸­11ä¸ªä¸­é—´é˜¶æ®µæ¨¡å‹ï¼ˆå¯¹åº”çº¦0.2~2.4ä¸‡äº¿Tokensè®­ç»ƒé‡ï¼‰ä¾›ç ”ç©¶ä½¿ç”¨ï¼ˆè®­ç»ƒå­˜æ¡£ä¸‹è½½ï¼‰ã€‚ä¸‹å›¾å±•ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨C-Evalã€MMLUã€CMMLUä¸‰å¤§æµ‹è¯•é›†ä¸Šçš„è¡¨ç°æ¼”è¿›ï¼š\n\nAlongside the final Baichuan2-7B-Base, we release 11 intermediate checkpoints (trained on ~0.2-2.4T tokens) for research (Checkpoints Download). The graph illustrates their performance evolution on C-Eval, MMLU, and CMMLU benchmarks:\n\nå£°æ˜ä¸åè®®/Terms and Conditions\nå£°æ˜\n\næˆ‘ä»¬éƒ‘é‡å£°æ˜ï¼Œæœ¬å¼€å‘å›¢é˜ŸæœªåŸºäºBaichuan 2æ¨¡å‹å¼€å‘ä»»ä½•åº”ç”¨ç¨‹åºï¼ŒåŒ…æ‹¬ä½†ä¸é™äºiOSã€Androidã€ç½‘é¡µæˆ–å…¶ä»–å¹³å°åº”ç”¨ã€‚æˆ‘ä»¬å¼ºçƒˆæ•¦ä¿ƒæ‰€æœ‰ç”¨æˆ·ä¸å¾—åˆ©ç”¨Baichuan 2æ¨¡å‹ä»äº‹ä»»ä½•å±å®³å›½å®¶å®‰å…¨ã€ç¤¾ä¼šç§©åºæˆ–è¿åæ³•å¾‹æ³•è§„çš„è¡Œä¸ºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¦æ±‚ç”¨æˆ·ä¸å¾—å°†Baichuan 2æ¨¡å‹ç”¨äºæœªç»å®‰å…¨è¯„ä¼°å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬æœŸå¾…æ‰€æœ‰ç”¨æˆ·å…±åŒéµå®ˆè¿™ä¸€å‡†åˆ™ï¼Œç¡®ä¿æŠ€æœ¯å‘å±•å§‹ç»ˆåœ¨è§„èŒƒåˆæ³•çš„è½¨é“ä¸Šå‰è¡Œã€‚\n\nåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å·²ç«­å°½å…¨åŠ›ç¡®ä¿æ•°æ®ä½¿ç”¨çš„åˆè§„æ€§ã€‚ç„¶è€Œé‰´äºæ¨¡å‹ä¸æ•°æ®çš„å¤æ‚æ€§ï¼Œå³ä¾¿æˆ‘ä»¬ä»˜å‡ºå·¨å¤§åŠªåŠ›ï¼Œä»å¯èƒ½å­˜åœ¨ä¸å¯é¢„è§çš„æ½œåœ¨é—®é¢˜ã€‚å› æ­¤ï¼Œå¯¹äºå› ä½¿ç”¨Baichuan 2å¼€æºæ¨¡å‹å¼•å‘çš„ä»»ä½•é—®é¢˜ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é£é™©ã€èˆ†æƒ…é£é™©ï¼Œä»¥åŠæ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“ä½¿ç”¨æ‰€äº§ç”Ÿçš„å„ç±»é£é™©ä¸é—®é¢˜ï¼‰ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•æ³•å¾‹è´£ä»»ã€‚\n\nåè®®\n\nä½¿ç”¨Baichuan 2æ¨¡å‹éœ€åŒæ—¶éµå®ˆApache 2.0è®¸å¯åŠã€ŠBaichuan 2æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹ã€‚è¯¥æ¨¡å‹æ”¯æŒå•†ä¸šç”¨é€”ï¼Œè‹¥æ‚¨è®¡åˆ’å°†Baichuan 2æ¨¡å‹æˆ–å…¶è¡ç”Ÿä½œå“ç”¨äºå•†ä¸šåœºæ™¯ï¼Œéœ€ç¡®ä¿ç¬¦åˆä»¥ä¸‹æ¡ä»¶ï¼š\n\næ‚¨æˆ–å…³è”æ–¹æœåŠ¡/äº§å“çš„æ—¥å‡æ´»è·ƒç”¨æˆ·æ•°ï¼ˆDAUï¼‰æœªè¾¾100ä¸‡\næ‚¨åŠå…³è”æ–¹éè½¯ä»¶æœåŠ¡æä¾›å•†æˆ–äº‘æœåŠ¡æä¾›å•†\nä¸å­˜åœ¨æœªç»ç™¾å·ä¹¦é¢è®¸å¯ï¼Œå°†å•†ç”¨æˆæƒäºŒæ¬¡æˆäºˆç¬¬ä¸‰æ–¹çš„æƒ…å½¢\n\næ»¡è¶³ä¸Šè¿°æ¡ä»¶åï¼Œè¯·å°†ã€ŠBaichuan 2æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹è¦æ±‚çš„ç”³è¯·ææ–™å‘é€è‡³opensource@baichuan-inc.comã€‚é€šè¿‡å®¡æ ¸åï¼Œç™¾å·å°†æˆäºˆæ‚¨éæ’ä»–æ€§ã€å…¨çƒèŒƒå›´ã€ä¸å¯è½¬è®©ã€ä¸å¯åˆ†è®¸å¯ä¸”å¯æ’¤é”€çš„å•†ç”¨ç‰ˆæƒè®¸å¯ã€‚\n\nå¿«é€Ÿå¼€å§‹/Quick Start\nå¾®è°ƒ\næ•°æ®é›†å‡†å¤‡\n\næˆ‘ä»¬ç°æä¾›belle_chat_ramdonæ•°æ®é›†çš„é¢„å¤„ç†ä¸å¾®è°ƒç¤ºä¾‹ï¼Œæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š\n\nbelle_chat_ramdon_10k\n\nè¿è¡Œbelle_preprocess.pyè„šæœ¬å¯å®Œæˆæ•°æ®é¢„å¤„ç†ä¸Mindrecordæ ¼å¼è½¬æ¢ï¼Œè¯¥è„šæœ¬ä¼šå°†å¸¦promptæ¨¡æ¿çš„åŸå§‹æ•°æ®è½¬åŒ–ä¸ºmindrecordæ ¼å¼ã€‚\n\n# è„šæœ¬è·¯å¾„ï¼šexample/dataset/belle_preprocess.py\npython example/dataset/belle_preprocess.py \\\n--input_glob /{path}/belle_chat_ramdon_10k.json \\\n--output_file /{path}/belle_512.mindrecord \\\n--seq_length 512\n\n# å‚æ•°è¯´æ˜\ninput_glob: è¾“å…¥æ•°æ®é›†è·¯å¾„\nmodel_file: è¯è¡¨æ–‡ä»¶è·¯å¾„\noutput_file: è¾“å‡ºæ•°æ®é›†çš„è·¯å¾„å’Œåç§°\nseq_length: ç”Ÿæˆæ•°æ®é›†çš„åºåˆ—é•¿åº¦\n\nTraining\ncd example\nbash msrun.sh \"finetune.py --train_dataset /{path}/belle_512.mindrecord\"\n\nReasoning\nfrom mindspore import set_context\nfrom openmind import pipeline\n\nset_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\",\n                         model='MindSpore-Lab/baichuan2_7b_chat',\n                         model_kwargs={\"use_past\":True},\n                         framework='ms',\n                         trust_remote_code=True)\npipeline_result = pipeline_task(\"<reserved_106>ä½ æ˜¯è°ï¼Ÿ<reserved_107>\", do_sample=False)\nprint(pipeline_result)\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/coat_ms",
    "project_name": "coat_ms",
    "readme": "Original Text\nCoaT\n\nååŒå°ºåº¦å·ç§¯æ³¨æ„åŠ›å›¾åƒå˜æ¢å™¨\n\nå¼•è¨€\n\nååŒå°ºåº¦å·ç§¯æ³¨æ„åŠ›å›¾åƒå˜æ¢å™¨ï¼ˆCoaTï¼‰æ˜¯ä¸€ç§åŸºäºTransformerçš„å›¾åƒåˆ†ç±»å™¨ï¼Œå…·å¤‡ååŒå°ºåº¦å’Œå·ç§¯æ³¨æ„åŠ›æœºåˆ¶ã€‚é¦–å…ˆï¼ŒååŒå°ºåº¦æœºåˆ¶ä¿æŒäº†Transformerç¼–ç å™¨åˆ†æ”¯åœ¨å„ä¸ªå°ºåº¦ä¸Šçš„å®Œæ•´æ€§ï¼ŒåŒæ—¶å…è®¸ä¸åŒå°ºåº¦ä¸Šå­¦åˆ°çš„è¡¨å¾æœ‰æ•ˆåœ°ç›¸äº’é€šä¿¡ã€‚å…¶æ¬¡ï¼Œå·ç§¯æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡åœ¨åˆ†è§£æ³¨æ„åŠ›æ¨¡å—ä¸­å®ç°ç›¸å¯¹ä½ç½®åµŒå…¥å…¬å¼çš„å·ç§¯-likeé«˜æ•ˆå®ç°è€Œè®¾è®¡ã€‚CoaTèµ‹äºˆäº†å›¾åƒTransformerä¸°å¯Œçš„å¤šå°ºåº¦ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ImageNet-1Kæ•°æ®é›†ä¸Šå¤ç°çš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•° (M)\té…ç½®æ–‡ä»¶yaml\tæƒé‡æ–‡ä»¶weights\ncoat_lite_tiny\tD910x8-G\t77.35\t93.43\t5.72\tyaml\tweights\ncoat_lite_mini\tD910x8-G\t78.51\t93.84\t11.01\tyaml\tweights\ncoat_tiny\tD910x8-G\t79.67\t94.88\t5.50\tyaml\tweights\ncoat_mini\tD910x8-G\t81.08\t95.34\t10.34\tyaml\tweights\næ³¨é‡Š\nç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º{è®¾å¤‡}x{å—æ•°}-{MSæ¨¡å¼}ï¼Œå…¶ä¸­MindSporeæ¨¡å¼å¯ä»¥æ˜¯G - å›¾æ¨¡å¼æˆ–F - pynativeæ¨¡å¼ï¼Œå¸¦mså‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-Gè¡¨ç¤ºä½¿ç”¨å›¾æ¨¡å¼åœ¨8ç‰‡Ascend 910 NPUä¸Šè¿›è¡Œè®­ç»ƒã€‚\nTop-1å’ŒTop-5ï¼šåœ¨ImageNet-1KéªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡æŠ¥å‘Šã€‚\nå¿«é€Ÿå¼€å§‹\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒMindCVä¸­çš„å®‰è£…æŒ‡å—ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ImageNet-1Kæ•°æ®é›†ä»¥ä¾›æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ä½¿ç”¨ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ªAscend 910è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/coat/coat_lite_tiny_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»å‘ mpirun å‘½ä»¤ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nåŒæ ·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨æƒ³åœ¨ä¸éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å°æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/coat/coat_lite_tiny_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/coat/coat_lite_tiny_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²\n\nä¸ºäº†é«˜æ•ˆåœ°éƒ¨ç½²åœ¨çº¿æ¨ç†æœåŠ¡å¹¶ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å‚è€ƒéƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] éŸ©å¾·ï¼Œå°¹ç¡•ï¼Œä½•å½¬ç­‰. å¯¹é«˜æ•ˆæ¨¡å‹è®¾è®¡ä¸­é€šé“ç»´åº¦çš„å†æ€è€ƒ[C]//IEEE/CVFè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†. 2021: 732-741.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/deepspeech2_ms",
    "project_name": "deepspeech2_ms",
    "readme": "ä»‹ç»\n\nDeepSpeech2æ˜¯ä¸€ç§é‡‡ç”¨CTCæŸå¤±è®­ç»ƒçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚å®ƒç”¨ç¥ç»ç½‘ç»œå–ä»£äº†æ•´ä¸ªæ‰‹å·¥è®¾è®¡ç»„ä»¶çš„ç®¡é“ï¼Œå¯ä»¥å¤„ç†å„ç§å„æ ·çš„è¯­éŸ³ï¼ŒåŒ…æ‹¬å˜ˆæ‚çš„ç¯å¢ƒã€å£éŸ³å’Œä¸åŒçš„è¯­è¨€ã€‚ç›®å‰æä¾›ç‰ˆæœ¬æ”¯æŒåœ¨NPUå’ŒGPUä¸Šä½¿ç”¨DeepSpeech2æ¨¡å‹åœ¨librispeechæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒ/æµ‹è¯•å’Œæ¨ç†ã€‚\n\næ¨¡å‹ç»“æ„\n\nç›®å‰çš„å¤ç°çš„æ¨¡å‹åŒ…æ‹¬:\n\nä¸¤ä¸ªå·ç§¯å±‚:\né€šé“æ•°ä¸º 32ï¼Œå†…æ ¸å¤§å°ä¸º 41, 11 ï¼Œæ­¥é•¿ä¸º 2, 2\né€šé“æ•°ä¸º 32ï¼Œå†…æ ¸å¤§å°ä¸º 41, 11 ï¼Œæ­¥é•¿ä¸º 2, 1\näº”ä¸ªåŒå‘ LSTM å±‚ï¼ˆå¤§å°ä¸º 1024ï¼‰\nä¸€ä¸ªæŠ•å½±å±‚ã€å¤§å°ä¸ºå­—ç¬¦æ•°åŠ  1ï¼ˆä¸ºCTCç©ºç™½ç¬¦å·)ï¼Œ28ã€‘\næ¨¡å‹è¡¨ç°\næ¨¡å‹\tæœºå™¨\tLM\tTest Clean CER\tTest Clean WER\tå‚æ•°\tæƒé‡\nDeepSpeech2\tD910x8-G\tNo\t3.461\t10.24\tyaml\tweights\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\næœ‰å…³å¦‚ä½•è®­ç»ƒå’Œæ¨ç†æ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹MindAudio GitHub ä»“åº“.",
    "tags": "[\"Automatic Speech Recognition\", \"MIT\"]"
  },
  {
    "url": "https://gitcode.com/openMind/dit_ms",
    "project_name": "dit_ms",
    "readme": "Original Text\nåŸºäºTransformerçš„å¯æ‰©å±•æ‰©æ•£æ¨¡å‹ (DiT)\n\nç®€ä»‹\n\nå…ˆå‰çš„æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼‰é€šå¸¸é‡‡ç”¨U-Netä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œç¼ºä¹å¯æ‰©å±•æ€§ã€‚DiTæ˜¯ä¸€ç§åŸºäºTransformeræ¶æ„çš„æ–°å‹æ‰©æ•£æ¨¡å‹ã€‚ä½œè€…è®¾è®¡äº†æ‰©æ•£Transformerï¼ˆDiTï¼‰ï¼Œéµå¾ªè§†è§‰Transformerï¼ˆViTï¼‰çš„æœ€ä½³å®è·µ[1]ã€‚DiTé€šè¿‡â€œpatchifyâ€å°†è§†è§‰è¾“å…¥ä½œä¸ºè§†è§‰tokenåºåˆ—ï¼Œç„¶åé€šè¿‡ä¸€ç³»åˆ—Transformerå—ï¼ˆDiTå—ï¼‰å¤„ç†è¾“å…¥ã€‚DiTæ¨¡å‹å’ŒDiTå—çš„ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š\n\nå›¾1. DiTå’ŒDiTå—çš„ç»“æ„ã€‚[2]\n\nDiTæ˜¯æ‰©æ•£æ¨¡å‹çš„å¯æ‰©å±•æ¶æ„ã€‚ä½œè€…å‘ç°ï¼Œç½‘ç»œå¤æ‚åº¦ï¼ˆä»¥Gflopsè¡¡é‡ï¼‰ä¸æ ·æœ¬è´¨é‡ï¼ˆä»¥FIDè¡¡é‡ï¼‰ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚æ¢å¥è¯è¯´ï¼ŒDiTæ¨¡å‹è¶Šå¤æ‚ï¼Œå›¾åƒç”Ÿæˆæ•ˆæœè¶Šå¥½ã€‚\n\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\næœ‰å…³å¦‚ä½•è®­ç»ƒå’Œæ¨ç†æ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·å‚é˜…MindOne GitHubä»“åº“ã€‚\n\nç”¨é€”\nç›´æ¥ä½¿ç”¨\n\nè¯¥æ¨¡å‹ä»…ç”¨äºç ”ç©¶ç›®çš„ã€‚å¯èƒ½çš„ç ”ç©¶é¢†åŸŸå’Œä»»åŠ¡åŒ…æ‹¬ï¼š\n\nè‰ºæœ¯ä½œå“çš„ç”Ÿæˆä»¥åŠåœ¨è®¾è®¡å’Œè‰ºæœ¯åˆ›ä½œè¿‡ç¨‹ä¸­çš„åº”ç”¨ã€‚\næ•™è‚²æˆ–åˆ›æ„å·¥å…·ä¸­çš„åº”ç”¨ã€‚\nç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶ã€‚\næ½œåœ¨ç”Ÿæˆæœ‰å®³å†…å®¹çš„æ¨¡å‹çš„å®‰å…¨éƒ¨ç½²ã€‚\næ¢ç´¢å’Œç†è§£ç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§å’Œåè§ã€‚\n\nä»¥ä¸‹æ˜¯æ’é™¤çš„ä½¿ç”¨åœºæ™¯ã€‚\n\nè¶…å‡ºèŒƒå›´çš„ä½¿ç”¨\n\nè¯¥æ¨¡å‹æœªç»è¿‡è®­ç»ƒä»¥ç”ŸæˆçœŸå®æˆ–å‡†ç¡®çš„äººç‰©æˆ–äº‹ä»¶æè¿°ï¼Œå› æ­¤ä½¿ç”¨è¯¥æ¨¡å‹ç”Ÿæˆæ­¤ç±»å†…å®¹è¶…å‡ºäº†å…¶èƒ½åŠ›èŒƒå›´ã€‚\n\nå±€é™æ€§å’Œåè§\nå±€é™æ€§\næ¨¡å‹æœªè¾¾åˆ°å®Œç¾çš„ç…§ç‰‡çœŸå®æ„Ÿã€‚\næ¨¡å‹æ— æ³•æ¸²æŸ“å¯è¯»æ–‡æœ¬ã€‚\næ¨¡å‹åœ¨æ¶‰åŠç»„åˆæ€§çš„æ›´å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚ç”Ÿæˆâ€œä¸€ä¸ªçº¢è‰²ç«‹æ–¹ä½“åœ¨è“è‰²çƒä½“ä¸Šâ€çš„å›¾åƒã€‚\näººè„¸å’Œäººç‰©å¯èƒ½æ— æ³•æ­£ç¡®ç”Ÿæˆã€‚\næ¨¡å‹çš„è‡ªåŠ¨ç¼–ç éƒ¨åˆ†æ˜¯æœ‰æŸçš„ã€‚\nåè§\n\nå°½ç®¡å›¾åƒç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½å¼ºåŒ–æˆ–åŠ å‰§ç¤¾ä¼šåè§ã€‚",
    "tags": "[\"Text-to-Image\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/ecapatdnn_ms",
    "project_name": "ecapatdnn_ms",
    "readme": "è¯´è¯äººéªŒè¯--ä½¿ç”¨ECAPA-TDNNæå–è¯´è¯äººç‰¹è¯\nä»‹ç»\n\nECAPA-TDNNç”±æ¯”åˆ©æ—¶å“¥ç‰¹å¤§å­¦Desplanquesç­‰äººäº2020å¹´æå‡ºï¼Œé€šè¿‡å¼•å…¥SE (squeeze-excitation)æ¨¡å—ä»¥åŠé€šé“æ³¨æ„æœºåˆ¶ï¼Œæ­¤æ¨¡å‹åœ¨å›½é™…å£°çº¹è¯†åˆ«æ¯”èµ›ï¼ˆVoxSRC2020ï¼‰ä¸­å–å¾—äº†ç¬¬ä¸€åçš„æˆç»©ã€‚\n\næ¨¡å‹ç»“æ„\n\né’ˆå¯¹ç›®å‰åŸºäºx-vectorçš„å£°çº¹è¯†åˆ«ç³»ç»Ÿä¸­çš„ä¸€äº›ä¼˜ç¼ºç‚¹ï¼ŒECAPA-TDNNä»ä»¥ä¸‹3ä¸ªæ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼š\n\n1ã€ä¾èµ–äºé€šé“å’Œä¸Šä¸‹æ–‡çš„ç»Ÿè®¡æ± åŒ–\n\n2ã€ä¸€ç»´Squeeze-Excitationï¼ˆæŒ¤å‹æ¿€åŠ±æ¨¡å—ï¼‰Res2Blocks\n\n3ã€å¤šå±‚ç‰¹å¾èšåˆåŠæ±‚å’Œ\n\næ¨¡å‹ç»“æ„å¦‚ä¸‹å›¾ï¼š\n\næ¨¡å‹è¡¨ç°\næ¨¡å‹\tæœºå™¨\ttraning time\tEER with s-norm\tEER s-norm\tå‚æ•°\tæƒé‡\nECAPA-TDNN\tD910x8-G\t24h\t1.50%\t1.70%\tyaml\tweights\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\næœ‰å…³å¦‚ä½•è®­ç»ƒå’Œæ¨ç†æ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹MindAudio GitHub ä»“åº“.",
    "tags": "[\"Audio Classification\", \"MIT\"]"
  },
  {
    "url": "https://gitcode.com/openMind/nasnet_ms",
    "project_name": "nasnet_ms",
    "readme": "Original Text\nNasNet\n\nå­¦ä¹ å¯è¿ç§»æ¶æ„ä»¥å®ç°å¯æ‰©å±•å›¾åƒè¯†åˆ«\n\nå¼•è¨€\n\nç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰åœ¨æ¨¡å‹é…ç½®ä¸Šæ˜¾ç¤ºäº†å…¶çµæ´»æ€§ã€‚é€šè¿‡åœ¨å¸¦æœ‰å·ç§¯å±‚ã€æœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–å±‚çš„æ± ä¸­è¿›è¡Œç¥ç»æ¶æ„æœç´¢ï¼Œé€‰æ‹©äº†æ™®é€šç»†èƒå’Œç¼©å‡ç»†èƒä½œä¸º NasNet çš„ä¸€éƒ¨åˆ†ã€‚å›¾ 1 å±•ç¤ºäº† NasNet åœ¨ ImageNet ä¸Šçš„æ¶æ„ï¼Œè¯¥æ¶æ„ç”±ç¼©å‡ç»†èƒå’Œæ™®é€šç»†èƒå †å è€Œæˆã€‚æ€»ä¹‹ï¼Œä¸ ImageNet-1K æ•°æ®é›†ä¸Šçš„å…ˆå‰æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒNasNet åœ¨å›¾åƒåˆ†ç±»ä¸Šå¯ä»¥å®ç°æ›´å¥½çš„æ¨¡å‹æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰æ›´å°‘çš„æ¨¡å‹å‚æ•°å’Œæ›´ä½çš„è®¡ç®—æˆæœ¬ã€‚[1]\n\nå›¾ 1. Nasnet æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K ä¸Šçš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æŠ¥é“ã€‚\n\næ¨¡å‹\tä¸Šä¸‹æ–‡\tTop-1 (%)\tTop-5 (%)\tParams (M)\té…æ–¹\tä¸‹è½½\nnasnet_a_4x1056\tD910x8-G\t73.65\t91.25\t5.33\tyaml\tweights\nå¤‡æ³¨\nä¸Šä¸‹æ–‡ï¼šè®­ç»ƒä¸Šä¸‹æ–‡è¡¨ç¤ºä¸º {è®¾å¤‡}x{å—}-{MS æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - ä½¿ç”¨ ms å‡½æ•°çš„ pynative æ¨¡å¼ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºä½¿ç”¨å›¾æ¨¡å¼åœ¨ 8 ä¸ª Ascend 910 NPU ä¸Šè¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®åº¦ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV çš„å®‰è£…è¯´æ˜ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†ç”¨äºæ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/nasnet/nasnet_a_4x1056_ascend.yaml --data_dir /path/to/imagenet\n\n\nè‹¥è„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»å‘ mpirun å‘½ä»¤ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\nå…³äºæ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨æƒ³åœ¨ä¸è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/nasnet/nasnet_a_4x1056_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/nasnet/nasnet_a_4x1056_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²æŒ‡å—\n\nè¯·å‚è€ƒ MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Zoph B, Vasudevan V, Shlens J, ç­‰äºº. å­¦ä¹ å¯è¿ç§»æ¶æ„ä»¥å®ç°å¯æ‰©å±•å›¾åƒè¯†åˆ«[C]//IEEE è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†. 2018: 8697-8710.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/baichuan2_13b_chat_ms",
    "project_name": "baichuan2_13b_chat_ms",
    "readme": "Original Text\nç™¾å·å¤§æ¨¡å‹ 2\nğŸ¦‰ä»£ç ä»“åº“ | ğŸ’¬å¾®ä¿¡äº¤æµ\nç™¾å·APIç°å·²æ”¯æŒæœç´¢å¢å¼ºä¸192Ké•¿æ–‡æœ¬çª—å£ï¼Œæ–°å¢çŸ¥è¯†åº“æ£€ç´¢åŠŸèƒ½ï¼Œé™æ—¶å…è´¹ä½“éªŒï¼\nğŸš€ ç™¾å·æ™ºèƒ½å¯¹è¯å¹³å° å·²å…¨é¢å¼€æ”¾å…¬ä¼—è®¿é—® ğŸ‰\næ›´æ–°è¯´æ˜/Modification\n\nä¼˜åŒ–äº†å¿«é€Ÿå…¥é—¨éƒ¨åˆ†çš„ç¤ºä¾‹ä»£ç ã€‚/ Enhanced the quick start demo code.\n\nç›®å½•ç´¢å¼•/Table of Contents\nğŸ“– æ¨¡å‹æ¦‚è§ˆ/Introduction\nâš™ï¸ å¿«é€Ÿå…¥é—¨/Quick Start\nğŸ“Š æ€§èƒ½è¯„ä¼°/Benchmark Evaluation\nğŸ“œ ä½¿ç”¨æ¡æ¬¾/Terms and Conditions\næ¨¡å‹æ¦‚è§ˆ/Introduction\n\nç™¾å·å¤§æ¨¡å‹2æ˜¯ç”±ç™¾å·æ™ºèƒ½ç ”å‘çš„æ–°ä¸€ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹ä½“ç³»ï¼ŒåŸºäº 2.6ä¸‡äº¿ Tokensçš„é«˜è´¨é‡è®­ç»ƒæ•°æ®æ„å»ºï¼Œåœ¨æƒå¨ä¸­è‹±æ–‡åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°åŒè§„æ¨¡æ¨¡å‹çš„æœ€ä½³è¡¨ç°ã€‚æœ¬æ¬¡å‘å¸ƒçš„ç‰ˆæœ¬åŒ…å«7Bå’Œ13Bè§„æ¨¡çš„BaseåŸºç¡€æ¨¡å‹ä¸Chatå¯¹è¯æ¨¡å‹ï¼Œå¹¶ä¸ºChatç‰ˆæœ¬æä¾›4ä½é‡åŒ–æ–¹æ¡ˆã€‚æ‰€æœ‰ç‰ˆæœ¬ä¸ä»…å‘å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¼€å‘è€…åªéœ€é€šè¿‡é‚®ä»¶ç”³è¯·è·å–å®˜æ–¹å•†ä¸šæˆæƒåï¼Œå³å¯å…è´¹ç”¨äºå•†ä¸šåœºæ™¯ã€‚å…·ä½“ç‰ˆæœ¬ä¿¡æ¯ä¸ä¸‹è½½æ–¹å¼å¦‚ä¸‹ï¼š\n\nBaichuan 2 represents the next-generation open-source large language model series developed by Baichuan Intelligence inc.. Trained on a premium corpus of 2.6 trillion tokens, it achieves state-of-the-art performance on authoritative Chinese and English benchmarks for models of comparable scale. This release includes 7B and 13B parameter variants of both Base and Chat models, with additional 4-bit quantized versions for Chat models. All versions are fully accessible for academic research, while commercial use requires only an official license obtained via email request at no cost. Detailed model versions and download links are provided below:\n\n\tåŸºç¡€æ¨¡å‹\tå¯¹è¯æ¨¡å‹\t4ä½é‡åŒ–å¯¹è¯æ¨¡å‹\n7B\t[ç™¾å·2-7B-åŸºç¡€ç‰ˆ]\t[ç™¾å·2-7B-å¯¹è¯ç‰ˆ]\tç™¾å·2-7B-å¯¹è¯ç‰ˆ-4ä½é‡åŒ–\n13B\t[ç™¾å·2-13B-åŸºç¡€ç‰ˆ]\t[ç™¾å·2-13B-å¯¹è¯ç‰ˆ]\tç™¾å·2-13B-å¯¹è¯ç‰ˆ-4ä½é‡åŒ–\nå¿«é€Ÿå…¥é—¨/Quick Start\næ¨¡å‹æ¨ç†\nfrom mindspore import set_context\nfrom openmind import pipeline\n\nset_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\",\n                         model='MindSpore-Lab/baichuan2_13b_chat',\n                         model_kwargs={\"use_past\":True},\n                         framework='ms',\n                         trust_remote_code=True)\npipeline_result = pipeline_task(\"<reserved_106>ä½ æ˜¯è°ï¼Ÿ<reserved_107>\", do_sample=False)\nprint(pipeline_result)\n\nåŸºå‡†æµ‹è¯•ç»“æœ/Benchmark Evaluation\n\næˆ‘ä»¬åœ¨é€šç”¨é¢†åŸŸã€æ³•å¾‹ã€åŒ»ç–—ã€æ•°å­¦ã€ç¼–ç¨‹å’Œå¤šè¯­è¨€ç¿»è¯‘å…­ä¸ªä¸“ä¸šç»´åº¦çš„ä¸­è‹±æ–‡æƒå¨æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå®Œæ•´æµ‹è¯•æ•°æ®è¯¦è§[GitHubä»“åº“]ã€‚\n\nWe have conducted comprehensive evaluations of the model on authoritative Chinese-English datasets across six specialized dimensions: General, Legal, Medical, Mathematics, Code, and Multilingual Translation. For complete testing data, please visit the GitHub repository.\n\n7Bæ¨¡å‹è¡¨ç°\n\tC-Eval\tMMLU\tCMMLU\té«˜è€ƒé¢˜\tAGIEval\tBBH\n\t5è½®æµ‹è¯•\t5è½®æµ‹è¯•\t5è½®æµ‹è¯•\t5è½®æµ‹è¯•\t5è½®æµ‹è¯•\t3è½®æµ‹è¯•\nGPT-4\t68.40\t83.93\t70.33\t66.15\t63.27\t75.12\nGPT-3.5 Turbo\t51.10\t68.54\t54.06\t47.07\t46.13\t61.59\nLLaMA-7B\t27.10\t35.10\t26.75\t27.81\t28.17\t32.38\nLLaMA2-7B\t28.90\t45.73\t31.38\t25.97\t26.53\t39.16\nMPT-7B\t27.15\t27.93\t26.00\t26.54\t24.83\t35.20\nFalcon-7B\t24.23\t26.03\t25.66\t24.24\t24.10\t28.77\nChatGLM2-6B\t50.20\t45.90\t49.00\t49.44\t45.28\t31.65\nBaichuan-7B\t42.80\t42.30\t44.02\t36.34\t34.44\t32.48\nBaichuan2-7B-Base\t54.00\t54.16\t57.07\t47.47\t42.73\t41.56\n13Bæ¨¡å‹è¡¨ç°\n\tC-Eval\tMMLU\tCMMLU\té«˜è€ƒé¢˜\tAGIEval\tBBH\n\t5è½®æµ‹è¯•\t5è½®æµ‹è¯•\t5è½®æµ‹è¯•\t5è½®æµ‹è¯•\t5è½®æµ‹è¯•\t3è½®æµ‹è¯•\nGPT-4\t68.40\t83.93\t70.33\t66.15\t63.27\t75.12\nGPT-3.5 Turbo\t51.10\t68.54\t54.06\t47.07\t46.13\t61.59\nLLaMA-13B\t28.50\t46.30\t31.15\t28.23\t28.22\t37.89\nLLaMA2-13B\t35.80\t55.09\t37.99\t30.83\t32.29\t46.98\nVicuna-13B\t32.80\t52.00\t36.28\t30.11\t31.55\t43.04\nChinese-Alpaca-Plus-13B\t38.80\t43.90\t33.43\t34.78\t35.46\t28.94\nXVERSE-13B\t53.70\t55.21\t58.44\t44.69\t42.54\t38.06\nBaichuan-13B-Base\t52.40\t51.60\t55.30\t49.69\t43.20\t43.01\nBaichuan2-13B-Base\t58.10\t59.17\t61.97\t54.33\t48.17\t48.78\nè®­ç»ƒè¿‡ç¨‹è¿½è¸ª/Training Dynamics\n\né™¤æœ€ç»ˆè®­ç»ƒ2.6ä¸‡äº¿Tokensçš„Baichuan2-7B-Baseæ¨¡å‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€æ”¾äº†è®­ç»ƒè¿‡ç¨‹ä¸­11ä¸ªä¸­é—´ç‰ˆæœ¬ï¼ˆåˆ†åˆ«å¯¹åº”çº¦0.2~2.4ä¸‡äº¿Tokensè®­ç»ƒé‡ï¼‰ä¾›å­¦æœ¯ç ”ç©¶ä½¿ç”¨ï¼ˆè®­ç»ƒä¸­é—´ç‰ˆæœ¬ä¸‹è½½ï¼‰ã€‚ä¸‹å›¾å±•ç¤ºäº†è¿™äº›ä¸­é—´ç‰ˆæœ¬åœ¨C-Evalã€MMLUå’ŒCMMLUä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½æ¼”è¿›æ›²çº¿ï¼š\n\nIn addition to the final Baichuan2-7B-Base model trained on 2.6 trillion tokens, we have open-sourced 11 intermediate versions (corresponding to approximately 0.2~2.4 trillion tokens of training) for academic research (Intermediate Checkpoints Download). The following graph illustrates the performance evolution of these intermediate versions across three benchmarks: C-Eval, MMLU, and CMMLU.\n\nä½¿ç”¨æ¡æ¬¾/Terms and Conditions\nå…è´£å£°æ˜\n\næˆ‘ä»¬éƒ‘é‡å£°æ˜ï¼Œç ”å‘å›¢é˜ŸæœªåŸºäºBaichuan 2æ¨¡å‹å¼€å‘ä»»ä½•ç»ˆç«¯åº”ç”¨ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºiOSã€AndroidåŠç½‘é¡µç«¯åº”ç”¨ï¼‰ã€‚æˆ‘ä»¬ä¸¥æ­£è¦æ±‚æ‰€æœ‰ä½¿ç”¨è€…ä¸å¾—å°†Baichuan 2æ¨¡å‹ç”¨äºä»»ä½•å±å®³å›½å®¶å®‰å…¨æˆ–è¿åæ³•å¾‹æ³•è§„çš„ç”¨é€”ï¼ŒåŒæ—¶ç¦æ­¢å°†è¯¥æ¨¡å‹ç”¨äºæœªç»å®‰å…¨è¯„ä¼°å’Œåˆè§„å¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬æœŸå¾…æ‰€æœ‰ç”¨æˆ·å…±åŒç»´æŠ¤æŠ€æœ¯åˆ›æ–°åœ¨æ³•æ²»æ¡†æ¶ä¸‹çš„å¥åº·å‘å±•ã€‚\n\nåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å·²å°½æœ€å¤§åŠªåŠ›ç¡®ä¿æ•°æ®æ¥æºçš„åˆè§„æ€§ã€‚ä½†ç”±äºæ¨¡å‹ä¸æ•°æ®çš„å¤æ‚æ€§ï¼Œä»å¯èƒ½å­˜åœ¨ä¸å¯é¢„è§çš„æ½œåœ¨é£é™©ã€‚å¯¹äºå› ä½¿ç”¨Baichuan 2å¼€æºæ¨¡å‹å¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é£é™©ã€èˆ†æƒ…é£é™©ï¼Œä»¥åŠæ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¸å½“ä¼ æ’­ç­‰å¼•å‘çš„å„ç±»é£é™©ï¼‰ï¼Œæˆ‘ä»¬æ¦‚ä¸æ‰¿æ‹…ä»»ä½•æ³•å¾‹è´£ä»»ã€‚\n\nWe solemnly declare that our R&D team has not developed any end-user applications (including but not limited to iOS, Android, or web applications) based on Baichuan 2 models. We strictly require all users not to use Baichuan 2 models for any activities that jeopardize national security or violate laws and regulations, and prohibit the use of the models for internet services that have not undergone security assessments and compliance filings. We expect all users to jointly uphold the healthy development of technological innovation within the legal framework.\n\nDuring the model training process, we have made every effort to ensure the compliance of data sources. However, due to the complexity of models and data, there may still be unforeseeable potential risks. We shall not bear any legal responsibility for any issues arising from the use of Baichuan 2 open-source models (including but not limited to data security risks, public opinion risks, and various risks caused by model misguidance, abuse, or improper dissemination).\n\nè®¸å¯åè®®\n\nä½¿ç”¨Baichuan 2æ¨¡å‹éœ€éµå®ˆApache 2.0åŠã€ŠBaichuan 2æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹ã€‚æœ¬æ¨¡å‹æ”¯æŒå•†ä¸šåº”ç”¨ï¼Œè‹¥è®¡åˆ’å°†Baichuan 2æ¨¡å‹æˆ–å…¶è¡ç”Ÿä½œå“ç”¨äºå•†ä¸šç”¨é€”ï¼Œè¯·ç¡®ä¿æ‚¨çš„å®ä½“ç¬¦åˆä»¥ä¸‹æ¡ä»¶ï¼š\n\næ‚¨æˆ–å…³è”æ–¹çš„æœåŠ¡/äº§å“æ—¥å‡æ´»è·ƒç”¨æˆ·(DAU)ä¸è¶³100ä¸‡\næ‚¨æˆ–å…³è”æ–¹éè½¯ä»¶æœåŠ¡æä¾›å•†æˆ–äº‘æœåŠ¡æä¾›å•†\næ‚¨æˆ–å…³è”æ–¹ä¸å­˜åœ¨æœªç»ç™¾å·äº‹å…ˆä¹¦é¢åŒæ„è€Œå°†å•†ä¸šæˆæƒè½¬æˆç¬¬ä¸‰æ–¹çš„å¯èƒ½æ€§\n\næ»¡è¶³ä¸Šè¿°æ¡ä»¶åï¼Œè¯·å°†ã€ŠBaichuan 2æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹è¦æ±‚çš„ç”³è¯·ææ–™å‘é€è‡³opensource@baichuan-inc.comã€‚ç»å®¡æ ¸é€šè¿‡åï¼Œç™¾å·å°†æˆäºˆæ‚¨å…¨çƒèŒƒå›´å†…éæ’ä»–æ€§ã€ä¸å¯è½¬è®©ã€ä¸å¯åˆ†è®¸å¯ä¸”å¯æ’¤é”€çš„å•†ä¸šä½¿ç”¨è®¸å¯ã€‚\n\nThe use of Baichuan 2 models requires compliance with Apache 2.0 and the Community License Agreement for Baichuan 2 Models. This model supports commercial applications. If you plan to use Baichuan 2 models or derivative works for commercial purposes, please ensure your entity meets the following conditions:\n\nThe Daily Active Users (DAU) of your or your affiliate's service/product is below 1 million\nNeither you nor your affiliates are software service providers or cloud service providers\nThere is no possibility for you or your affiliates to sublicense the commercial authorization to third parties without Baichuan's prior written consent\n\nAfter meeting these conditions, please submit the application materials required by the Community License Agreement for Baichuan 2 Models to opensource@baichuan-inc.com. Upon approval, Baichuan will grant you a worldwide, non-exclusive, non-transferable, non-sublicensable, and revocable commercial use license.",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"English\", \"Chinese\", \"Other\"]"
  },
  {
    "url": "https://gitcode.com/openMind/internlm_20b_base_ms",
    "project_name": "internlm_20b_base_ms",
    "readme": "Original Text\n\nä¹¦ç”ŸÂ·æµ¦è¯­ï¼ˆInternLMï¼‰\n\nÂ \nInternLM çƒ­é—¨\nÂ \n\nğŸ’»GitHubä»“åº“ â€¢ ğŸ¤”æäº¤é—®é¢˜\n\nä¿®æ”¹å†…å®¹\n\næ›´æ–°ç¤ºä¾‹ä»£ç éƒ¨åˆ†ã€‚\n\næ¨¡å‹ä»‹ç»\n\nä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤è”åˆå•†æ±¤ç§‘æŠ€ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦å’Œå¤æ—¦å¤§å­¦æ­£å¼æ¨å‡º200äº¿å‚æ•°é¢„è®­ç»ƒæ¨¡å‹InternLM-20Bã€‚InternLM-20Båœ¨è¶…è¿‡2.3T tokensçš„é«˜è´¨é‡è‹±æ–‡ã€ä¸­æ–‡å’Œä»£ç æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚åŒæ—¶ï¼ŒChatç‰ˆæœ¬ç»è¿‡SFTå’ŒRLHFè®­ç»ƒï¼Œèƒ½æ›´ä¼˜è´¨ã€æ›´å®‰å…¨åœ°æ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚\n\næ¨¡å‹ç»“æ„ä¸Šï¼ŒInternLM-20Bé‡‡ç”¨æ›´æ·±çš„60å±‚ç½‘ç»œè®¾è®¡ï¼Œè¶…è¶Šå¸¸è§„7Bå’Œ13Bæ¨¡å‹32æˆ–40å±‚çš„é…ç½®ã€‚åœ¨å‚æ•°é‡å—é™æ—¶ï¼Œå¢åŠ å±‚æ•°å¯æ˜¾è‘—æå‡æ¨¡å‹ç»¼åˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç›¸æ¯”InternLM-7Bï¼ŒInternLM-20Bçš„é¢„è®­ç»ƒæ•°æ®ç»è¿‡æ›´ä¸¥æ ¼æ¸…æ´—ï¼Œå¹¶è¡¥å……äº†å¯Œå«çŸ¥è¯†çš„ä¸“é¡¹æ•°æ®ä»¥å¼ºåŒ–ç†è§£ä¸æ¨ç†èƒ½åŠ›ã€‚å› æ­¤åœ¨è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›æµ‹è¯•â€”â€”ç†è§£ã€æ¨ç†ã€æ•°å­¦å’Œç¼–ç¨‹ç­‰æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚æ€»ä½“è€Œè¨€ï¼ŒInternLM-20Bå…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š\n\nå“è¶Šçš„ç»¼åˆæ€§èƒ½\nå¼ºå¤§çš„å·¥å…·è°ƒç”¨èƒ½åŠ›\næ”¯æŒ16kä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé€šè¿‡æ¨ç†å¤–æ¨å®ç°ï¼‰\næ›´ä¼˜çš„ä»·å€¼å¯¹é½\næ€§èƒ½è¯„ä¼°\n\nåœ¨OpenCompassæå‡ºçš„5å¤§èƒ½åŠ›ç»´åº¦ä¸Šï¼ŒInternLM-20Bè¡¨ç°ä¼˜å¼‚ï¼ˆåŠ ç²—åˆ†æ•°ä»£è¡¨13B-33Bå‚æ•°èŒƒå›´å†…çš„æœ€ä½³è¡¨ç°ï¼‰ã€‚\n\nèƒ½åŠ›ç»´åº¦\tLlama-13B\tLlama2-13B\tBaichuan2-13B\tInternLM-20B\tLlama-33B\tLlama-65B\tLlama2-70B\nè¯­è¨€\t42.5\t47\t47.5\t55\t44.6\t47.1\t51.6\nçŸ¥è¯†\t58.2\t58.3\t48.9\t60.1\t64\t66\t67.7\nç†è§£\t45.5\t50.9\t58.1\t67.3\t50.6\t54.2\t60.8\næ¨ç†\t42.7\t43.6\t44.2\t54.9\t46.4\t49.8\t55\nè€ƒè¯•\t37.3\t45.2\t51.8\t62.5\t47.4\t49.7\t57.3\nç»¼åˆ\t43.8\t47.3\t49.4\t59.2\t48.9\t51.9\t57.4\n\nä¸‹è¡¨å¯¹æ¯”äº†ä¸»æµå¼€æºæ¨¡å‹åœ¨éƒ¨åˆ†å…·æœ‰å½±å“åŠ›å…¸å‹æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚\n\n\tè¯„æµ‹é›†\tLlama-13B\tLlama2-13B\tBaichuan2-13B\tInternLM-20B\tLlama-33B\tLlama-65B\tLlama2-70B\nè€ƒè¯•èƒ½åŠ›\tMMLU\t47.73\t54.99\t59.55\t62.05\t58.73\t63.71\t69.75\n\tC-Eval (éªŒè¯é›†)\t31.83\t41.4\t59.01\t58.8\t37.47\t40.36\t50.13\n\tAGI-Eval\t22.03\t30.93\t37.37\t44.58\t33.53\t33.92\t40.02\nçŸ¥è¯†å‚¨å¤‡\tBoolQ\t78.75\t82.42\t67\t87.46\t84.43\t86.61\t87.74\n\tTriviaQA\t52.47\t59.36\t46.61\t57.26\t66.24\t69.79\t70.71\n\tNaturalQuestions\t20.17\t24.85\t16.32\t25.15\t30.89\t33.41\t34.16\nç†è§£èƒ½åŠ›\tCMRC\t9.26\t31.59\t29.85\t68.78\t14.17\t34.73\t43.74\n\tCSL\t55\t58.75\t63.12\t65.62\t57.5\t59.38\t60\n\tRACE (åˆä¸­)\t53.41\t63.02\t68.94\t86.35\t64.55\t72.35\t81.55\n\tRACE (é«˜ä¸­)\t47.63\t58.86\t67.18\t83.28\t62.61\t68.01\t79.93\n\tXSum\t20.37\t23.37\t25.23\t35.54\t20.55\t19.91\t25.38\næ¨ç†èƒ½åŠ›\tWinoGrande\t64.64\t64.01\t67.32\t69.38\t66.85\t69.38\t69.77\n\tBBH\t37.93\t45.62\t48.98\t52.51\t49.98\t58.38\t64.91\n\tGSM8K\t20.32\t29.57\t52.62\t52.62\t42.3\t54.44\t63.31\n\tPIQA\t79.71\t79.76\t78.07\t80.25\t81.34\t82.15\t82.54\nç¼–ç¨‹èƒ½åŠ›\tHumanEval\t14.02\t18.9\t17.07\t25.61\t17.68\t18.9\t26.22\n\tMBPP\t20.6\t26.8\t30.8\t35.6\t28.4\t33.6\t39.6\n\næ€»ä½“è€Œè¨€ï¼ŒInternLM-20Båœ¨ç»¼åˆèƒ½åŠ›ä¸Šå…¨é¢è¶…è¶Š13Bå‚æ•°èŒƒå›´çš„å¼€æºæ¨¡å‹ï¼Œåœ¨æ¨ç†è¯„æµ‹é›†ä¸Šçš„è¡¨ç°æ¥è¿‘ç”šè‡³è¶…è¿‡Llama-65Bã€‚\n\nå±€é™æ€§è¯´æ˜ï¼š å°½ç®¡æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°½åŠ›ç¡®ä¿æ¨¡å‹å®‰å…¨æ€§ï¼Œå¹¶å¼•å¯¼æ¨¡å‹ç”Ÿæˆç¬¦åˆé“å¾·å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†ç”±äºæ¨¡å‹è§„æ¨¡åŠæ¦‚ç‡ç”ŸæˆèŒƒå¼ï¼Œä»å¯èƒ½äº§ç”Ÿé¢„æœŸå¤–çš„è¾“å‡ºã€‚ä¾‹å¦‚ç”Ÿæˆçš„å›å¤å¯èƒ½å­˜åœ¨åè§ã€æ­§è§†æˆ–å…¶ä»–æœ‰å®³å†…å®¹ã€‚è¯·å‹¿ä¼ æ’­æ­¤ç±»å†…å®¹ã€‚å› ä¼ æ’­æœ‰å®³ä¿¡æ¯å¯¼è‡´çš„ä»»ä½•åæœï¼Œæˆ‘ä»¬æ¦‚ä¸è´Ÿè´£ã€‚\n\nä½¿ç”¨openMind\næ¨ç†\nimport mindspore as ms\nfrom openmind import pipeline\n\nms.set_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\",\n                         model='MindSpore-Lab/internlm_20b_base',\n                         framework='ms',\n                         model_kwargs={\"use_past\": True},\n                         trust_remote_code=True)\n\ntext = \"<s><|User|>:ä½ æ˜¯è°ï¼Ÿ<eoh>\\n<|Bot|>:\"\n\npipeline_result = pipeline_task(text, do_sample=False)\nprint(pipeline_result)\n\nå¼€æºè®¸å¯\n\nä»£ç é‡‡ç”¨ Apache-2.0 è®¸å¯è¯æˆæƒï¼Œæ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¹¶å…è®¸å…è´¹å•†ä¸šä½¿ç”¨ã€‚å¦‚éœ€ç”³è¯·å•†ä¸šæˆæƒï¼Œè¯·å¡«å†™è‹±æ–‡ç”³è¯·è¡¨/ä¸­æ–‡ç”³è¯·è¡¨ã€‚å…¶ä»–é—®é¢˜æˆ–åˆä½œäº‹å®œï¼Œè¯·è”ç³» internlm@pjlab.org.cnã€‚\n\næ›´æ–°è¯´æ˜\n\nä¼˜åŒ–äº†å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ä»£ç ã€‚\n\næ¦‚è¿°\n\nä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤è”åˆå•†æ±¤ç§‘æŠ€ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦åŠå¤æ—¦å¤§å­¦æ­£å¼å‘å¸ƒä¹¦ç”ŸÂ·æµ¦è¯­200äº¿å‚æ•°æ¨¡å‹ InternLM-20Bã€‚è¯¥æ¨¡å‹åŸºäºè¶…è¿‡2.3TåŒ…å«é«˜è´¨é‡ä¸­è‹±æ–‡åŠä»£ç çš„tokenè¿›è¡Œé¢„è®­ç»ƒï¼Œå…¶ä¸­å¯¹è¯ç‰ˆæœ¬è¿˜ç»è¿‡SFTä¸RLHFè®­ç»ƒï¼Œèƒ½æ›´ç²¾å‡†ã€å®‰å…¨åœ°å“åº”ç”¨æˆ·éœ€æ±‚ã€‚\n\nInternLM-20Bé‡‡ç”¨æ·±åº¦æ¶æ„è®¾è®¡ï¼Œ60å±‚çš„ç½‘ç»œç»“æ„è¿œè¶…å¸¸è§„7B/13Bæ¨¡å‹çš„32æˆ–40å±‚é…ç½®ã€‚åœ¨å‚æ•°é‡å—é™æƒ…å†µä¸‹ï¼Œå¢åŠ å±‚æ•°æ˜¾è‘—æå‡äº†æ¨¡å‹ç»¼åˆèƒ½åŠ›ã€‚ç›¸æ¯”InternLM-7Bï¼Œå…¶è®­ç»ƒæ•°æ®ç»è¿‡æ›´ä¸¥æ ¼æ¸…æ´—ï¼Œå¹¶è¡¥å……äº†é«˜çŸ¥è¯†å¯†åº¦ä¸å¼ºåŒ–ç†è§£æ¨ç†çš„ä¸“é¡¹æ•°æ®ï¼Œåœ¨ç†è§£ã€æ¨ç†ã€æ•°å­¦åŠç¼–ç¨‹ç­‰æ ¸å¿ƒèƒ½åŠ›ç»´åº¦å®ç°æ˜¾è‘—çªç ´ã€‚ä¸»è¦ç‰¹æ€§åŒ…æ‹¬ï¼š\n\nå“è¶Šçš„ç»¼åˆæ€§èƒ½è¡¨ç°\nå¼ºå¤§çš„å·¥å…·è°ƒç”¨èƒ½åŠ›\næ”¯æŒ16kä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé€šè¿‡æ¨ç†æ—¶å¤–æ¨å®ç°ï¼‰\næ›´å®Œå–„çš„ä»·å€¼å¯¹é½\næ€§èƒ½è¯„ä¼°\n\nåœ¨OpenCompassäº”å¤§èƒ½åŠ›ç»´åº¦è¯„æµ‹ä¸­ï¼ŒInternLM-20Bè¡¨ç°äº®çœ¼ï¼ˆåŠ ç²—é¡¹ä¸º13B-33Bé‡çº§åŒºé—´æœ€ä½³æˆç»©ï¼‰\n\nèƒ½åŠ›ç»´åº¦\tLlama-13B\tLlama2-13B\tBaichuan2-13B\tInternLM-20B\tLlama-33B\tLlama-65B\tLlama2-70B\nè¯­è¨€\t42.5\t47\t47.5\t55\t44.6\t47.1\t51.6\nçŸ¥è¯†\t58.2\t58.3\t48.9\t60.1\t64\t66\t67.7\nç†è§£\t45.5\t50.9\t58.1\t67.3\t50.6\t54.2\t60.8\næ¨ç†\t42.7\t43.6\t44.2\t54.9\t46.4\t49.8\t55\nå­¦ç§‘\t37.3\t45.2\t51.8\t62.5\t47.4\t49.7\t57.3\nç»¼åˆ\t43.8\t47.3\t49.4\t59.2\t48.9\t51.9\t57.4\n\nä¸‹è¡¨å‘ˆç°InternLM-20Båœ¨ä¸»æµè¯„æµ‹é›†ä¸Šçš„å¯¹æ ‡è¡¨ç°ï¼š\n\n\tè¯„æµ‹é›†\tLlama-13B\tLlama2-13B\tBaichuan2-13B\tInternLM-20B\tLlama-33B\tLlama-65B\tLlama2-70B\nå­¦ç§‘\tMMLU\t47.73\t54.99\t59.55\t62.05\t58.73\t63.71\t69.75\n\tC-Eval (val)\t31.83\t41.4\t59.01\t58.8\t37.47\t40.36\t50.13\n\tAGI-Eval\t22.03\t30.93\t37.37\t44.58\t33.53\t33.92\t40.02\nçŸ¥è¯†\tBoolQ\t78.75\t82.42\t67\t87.46\t84.43\t86.61\t87.74\n\tTriviaQA\t52.47\t59.36\t46.61\t57.26\t66.24\t69.79\t70.71\n\tNaturalQuestions\t20.17\t24.85\t16.32\t25.15\t30.89\t33.41\t34.16\nç†è§£\tCMRC\t9.26\t31.59\t29.85\t68.78\t14.17\t34.73\t43.74\n\tCSL\t55\t58.75\t63.12\t65.62\t57.5\t59.38\t60\n\tRACE (middle)\t53.41\t63.02\t68.94\t86.35\t64.55\t72.35\t81.55\n\tRACE (high)\t47.63\t58.86\t67.18\t83.28\t62.61\t68.01\t79.93\n\tXSum\t20.37\t23.37\t25.23\t35.54\t20.55\t19.91\t25.38\næ¨ç†\tWinoGrande\t64.64\t64.01\t67.32\t69.38\t66.85\t69.38\t69.77\n\tBBH\t37.93\t45.62\t48.98\t52.51\t49.98\t58.38\t64.91\n\tGSM8K\t20.32\t29.57\t52.62\t52.62\t42.3\t54.44\t63.31\n\tPIQA\t79.71\t79.76\t78.07\t80.25\t81.34\t82.15\t82.54\nç¼–ç¨‹\tHumanEval\t14.02\t18.9\t17.07\t25.61\t17.68\t18.9\t26.22\n\tMBPP\t20.6\t26.8\t30.8\t35.6\t28.4\t33.6\t39.6\n\nç»¼åˆæ¥çœ‹ï¼ŒInternLM-20Båœ¨13Bé‡çº§å¼€æºæ¨¡å‹ä¸­å…¨é¢é¢†å…ˆï¼Œéƒ¨åˆ†æ¨ç†èƒ½åŠ›ç”šè‡³åª²ç¾Llama-65Bã€‚\n\né€šè¿‡openMindä½¿ç”¨\næ¨ç†\ncd example\npython inference.py\n\nOpen Source License\n\nThe code in this repository is open-sourced under the Apache-2.0 license. The model weights are fully accessible for academic research, and free commercial use licenses can also be applied for (Application Form). For other inquiries or collaborations, please contact internlm@pjlab.org.cn.",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/qwen_7b_base_ms",
    "project_name": "qwen_7b_base_ms",
    "readme": "",
    "tags": "[]"
  },
  {
    "url": "https://gitcode.com/openMind/yolov3_ms",
    "project_name": "yolov3_ms",
    "readme": "",
    "tags": "[]"
  },
  {
    "url": "https://gitcode.com/openMind/yolov7_ms",
    "project_name": "yolov7_ms",
    "readme": "",
    "tags": "[]"
  },
  {
    "url": "https://gitcode.com/openMind/glm-edge-4b-chat",
    "project_name": "glm-edge-4b-chat",
    "readme": "",
    "tags": "[]"
  },
  {
    "url": "https://gitcode.com/MooYeh/opus-mt-zh-en",
    "project_name": "opus-mt-zh-en",
    "readme": "Original Text\nä¸­æ–‡-è‹±è¯­\nç›®å½•\nç›®å½•\næ¨¡å‹è¯¦ç»†ä¿¡æ¯\nç”¨é€”\nç›´æ¥ä½¿ç”¨\né£é™©ã€å±€é™æ€§å’Œåå·®\nè®­ç»ƒ\nç³»ç»Ÿä¿¡æ¯\nè®­ç»ƒæ•°æ®\né¢„å¤„ç†\nè¯„ä¼°\nç»“æœ\nåŸºå‡†æµ‹è¯•\nå¼•ç”¨ä¿¡æ¯\nå¦‚ä½•å¼€å§‹ä½¿ç”¨è¯¥æ¨¡å‹\næ¨¡å‹è¯¦ç»†ä¿¡æ¯\næ¨¡å‹æè¿°ï¼š\n**å¼€å‘äººå‘˜ï¼š**èµ«å°”è¾›åŸºå¤§å­¦è¯­è¨€æŠ€æœ¯ç ”ç©¶å°ç»„\n**æ¨¡å‹ç±»å‹ï¼š**ç¿»è¯‘\nè¯­è¨€ï¼š\næºè¯­è¨€ï¼šä¸­æ–‡\nç›®æ ‡è¯­è¨€ï¼šè‹±è¯­\n**è®¸å¯è¯ï¼š**CC-BY-4.0\nè·å–æ›´å¤šä¿¡æ¯çš„èµ„æºï¼š\nGitHub ä»“åº“\nç”¨é€”\nç›´æ¥ä½¿ç”¨\n\nè¯¥æ¨¡å‹å¯ç”¨äºç¿»è¯‘å’Œæ–‡æœ¬åˆ°æ–‡æœ¬ç”Ÿæˆã€‚\n\né£é™©ã€å±€é™æ€§å’Œåå·®\n\nå†…å®¹è­¦å‘Šï¼šè¯»è€…åº”æ³¨æ„æœ¬èŠ‚åŒ…å«ä»¤äººä¸å®‰ã€å†’çŠ¯å†…å®¹ï¼Œå¹¶å¯èƒ½ä¼ æ’­å†å²å’Œå½“å‰çš„åˆ»æ¿å°è±¡ã€‚\n\nå¤§é‡ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹ä¸­çš„åå·®å’Œå…¬å¹³æ€§é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œè¯·å‚é˜… Sheng ç­‰äºº (2021) å’Œ Bender ç­‰äºº (2021)ï¼‰ã€‚\n\næœ‰å…³æ­¤æ¨¡å‹æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ OPUS è¯´æ˜æ–‡ä»¶ï¼šä¸­æ–‡-è‹±è¯­\n\nè®­ç»ƒ\nç³»ç»Ÿä¿¡æ¯\nhelsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\ntransformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\nport_machine: brutasse\nport_time: 2020-08-21-14:41\nsrc_multilingual: False\ntgt_multilingual: False\nè®­ç»ƒæ•°æ®\né¢„å¤„ç†\n\né¢„å¤„ç†ï¼šè§„èŒƒåŒ– + SentencePiece (spm32k,spm32k)\n\nref_len: 82826.0\n\næ•°æ®é›†ï¼šopus\n\nä¸‹è½½åŸå§‹æƒé‡ï¼šopus-2020-07-17.zip\n\næµ‹è¯•é›†ç¿»è¯‘ï¼šopus-2020-07-17.test.txt\n\nè¯„ä¼°\nç»“æœ\n\næµ‹è¯•é›†åˆ†æ•°ï¼šopus-2020-07-17.eval.txt\n\nç®€æ´åº¦æƒ©ç½šï¼š0.948\n\nåŸºå‡†æµ‹è¯•\næµ‹è¯•é›†\tBLEU\tchr-F\nTatoeba-test.zho.eng\t36.1\t0.548\nå¼•ç”¨ä¿¡æ¯\n@InProceedings{TiedemannThottingal:EAMT2020,\n  author = {J{\\\"o}rg Tiedemann and Santhosh Thottingal},\n  title = {{OPUS-MT} â€” {B}uilding open translation services for the {W}orld},\n  booktitle = {Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)},\n  year = {2020},\n  address = {Lisbon, Portugal}\n }\n\nå¦‚ä½•å¼€å§‹ä½¿ç”¨è¯¥æ¨¡å‹\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n\n\nå¥½çš„ï¼Œè¯·æä¾›æ‚¨éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ã€‚æˆ‘ä¼šå°½åŠ›å°†å®ƒç¿»è¯‘æˆé€šä¿—ã€ä¸“ä¸šã€ä¼˜é›…ä¸”æµç•…çš„ä¸­æ–‡ï¼ŒåŒæ—¶ä¿æŒåŸå§‹çš„ Markdown æ ¼å¼ã€‚ ğŸ˜Š",
    "tags": "[\"PyTorch\", \"Transformers\", \"English\", \"Chinese\", \"Creative Commons Attribution 4.0\", \"translation\"]"
  },
  {
    "url": "https://gitcode.com/openMind/deepseek-coder-6.7b-instruct",
    "project_name": "deepseek-coder-6.7b-instruct",
    "readme": "Original Text\n\n[ğŸ ä¸»é¡µ] | [ğŸ¤– ä¸DeepSeek Coderå¯¹è¯] | [Discord] | [å¾®ä¿¡]\n\n1. Deepseek Coder ç®€ä»‹\n\nDeepseek Coder æ˜¯ç”±ä¸€ç³»åˆ—ä»£ç è¯­è¨€æ¨¡å‹ç»„æˆï¼Œæ¯ä¸ªæ¨¡å‹å‡åŸºäº 2T token ä»å¤´è®­ç»ƒï¼Œæ•°æ®åŒ…å« 87% ä»£ç åŠ 13% ä¸­è‹±æ–‡è‡ªç„¶è¯­è¨€ã€‚æˆ‘ä»¬æä¾›å¤šç§è§„æ¨¡çš„ä»£ç æ¨¡å‹ï¼Œå‚æ•°é‡ä» 1B åˆ° 33B ä¸ç­‰ã€‚æ¯ä¸ªæ¨¡å‹å‡é‡‡ç”¨ 16K çª—å£é¢„è®­ç»ƒï¼Œå¹¶é¢å¤–å¼•å…¥å¡«ç©ºä»»åŠ¡ï¼Œä»¥æ”¯æŒé¡¹ç›®çº§ä»£ç è¡¥å…¨ä¸å¡«å……éœ€æ±‚ã€‚åœ¨ç¼–ç èƒ½åŠ›æ–¹é¢ï¼ŒDeepseek Coder åœ¨å¤šç§ç¼–ç¨‹è¯­è¨€å’Œå„ç±»åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°å¼€æºä»£ç æ¨¡å‹çš„é¢†å…ˆæ°´å¹³ã€‚\n\næµ·é‡è®­ç»ƒæ•°æ®ï¼šåŸºäº 2T token ä»å¤´è®­ç»ƒï¼Œå…¶ä¸­ 87% ä¸ºä»£ç æ•°æ®ï¼Œ13% ä¸ºä¸­è‹±æ–‡è‡ªç„¶è¯­è¨€æ•°æ®ã€‚\n\nçµæ´»å¯æ‰©å±•ï¼šæä¾› 1.3Bã€5.7Bã€6.7B å’Œ 33B å››ç§å‚æ•°é‡ç‰ˆæœ¬ï¼Œç”¨æˆ·å¯æ ¹æ®éœ€æ±‚è‡ªç”±é€‰æ‹©ã€‚\n\nå“è¶Šæ€§èƒ½è¡¨ç°ï¼šåœ¨ HumanEvalã€MultiPL-Eã€MBPPã€DS-1000 å’Œ APPS ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½é¢†å…ˆäºå½“å‰å…¬å¼€çš„ä»£ç æ¨¡å‹ã€‚\n\nå…ˆè¿›ä»£ç è¡¥å…¨èƒ½åŠ›ï¼šé‡‡ç”¨ 16K ä¸Šä¸‹æ–‡çª—å£å¹¶å¼•å…¥å¡«ç©ºä»»åŠ¡ï¼Œæ”¯æŒé¡¹ç›®çº§ä»£ç è¡¥å…¨ä¸å¡«å……ã€‚\n\n2. æ¨¡å‹æ¦‚è§ˆ\n\ndeepseek-coder-6.7b-instruct æ˜¯åŸºäº deepseek-coder-6.7b-base åˆå§‹åŒ–ï¼Œå¹¶ç»è¿‡ 20 äº¿ token æŒ‡ä»¤æ•°æ®å¾®è°ƒçš„ 67 äº¿å‚æ•°æ¨¡å‹ã€‚\n\nä¸»é¡µï¼šDeepSeek\nä»£ç ä»“åº“ï¼šdeepseek-ai/deepseek-coder\nåœ¨çº¿ä½“éªŒï¼šDeepSeek-Coder\n3. ä½¿ç”¨æŒ‡å—\n\nä»¥ä¸‹å±•ç¤ºæ¨¡å‹çš„ä½¿ç”¨ç¤ºä¾‹ã€‚\n\nå¯¹è¯æ¨¡å‹æ¨ç†\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).npu()\nmessages=[\n    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n\n4. è®¸å¯å£°æ˜\n\næœ¬ä»£ç ä»“åº“é‡‡ç”¨ MIT è®¸å¯åè®®ã€‚DeepSeek Coder æ¨¡å‹çš„ä½¿ç”¨éœ€éµå®ˆã€Šæ¨¡å‹è®¸å¯åè®®ã€‹ã€‚DeepSeek Coder æ”¯æŒå•†ä¸šç”¨é€”ã€‚\n\næ›´å¤šè¯¦æƒ…è¯·å‚é˜… LICENSE-MODELã€‚\n\n5. è”ç³»æˆ‘ä»¬\n\nå¦‚æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·æäº¤ issue æˆ–é€šè¿‡é‚®ä»¶è”ç³»æˆ‘ä»¬ï¼šagi_code@deepseek.comã€‚",
    "tags": "[\"PyTorch\", \"Transformers\", \"Safetensors\", \"Other\"]"
  },
  {
    "url": "https://gitcode.com/openMind/Qwen1.5-1.8b",
    "project_name": "Qwen1.5-1.8b",
    "readme": "Original Text\nQwen1.5-1.8B\nç®€ä»‹\n\nQwen1.5 æ˜¯ Qwen2 çš„æµ‹è¯•ç‰ˆæœ¬ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„çº¯è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æµ·é‡æ•°æ®é¢„è®­ç»ƒè€Œæˆã€‚ç›¸è¾ƒäºæ­¤å‰å‘å¸ƒçš„ Qwen æ¨¡å‹ï¼Œæœ¬ç³»åˆ—ä¸»è¦æå‡åŒ…æ‹¬ï¼š\n\næä¾› 8 ç§è§„æ¨¡æ¨¡å‹ï¼Œæ¶µç›– 0.5Bã€1.8Bã€4Bã€7Bã€14Bã€32B å’Œ 72B ç¨ å¯†æ¨¡å‹ï¼Œä»¥åŠæ¿€æ´»å‚æ•°é‡ä¸º 2.7B çš„ 14B MoE æ··åˆä¸“å®¶æ¨¡å‹ï¼›\nå¯¹è¯æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼›\nåŸºç¡€æ¨¡å‹ä¸å¯¹è¯æ¨¡å‹å‡æ”¯æŒå¤šè¯­è¨€èƒ½åŠ›ï¼›\nå…¨å°ºå¯¸æ¨¡å‹ç¨³å®šæ”¯æŒ 32K ä¸Šä¸‹æ–‡é•¿åº¦ï¼›\næ— éœ€è°ƒç”¨ trust_remote_code å‚æ•°ã€‚\n\næ›´å¤šæŠ€æœ¯ç»†èŠ‚ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„åšå®¢æ–‡ç« ä¸GitHub é¡¹ç›®ã€‚\n\næ¨¡å‹è¯¦æƒ…\n\nQwen1.5 æ˜¯ç”±ä¸åŒå‚æ•°é‡çº§çš„è§£ç å™¨è¯­è¨€æ¨¡å‹æ„æˆçš„ç³»åˆ—ã€‚é’ˆå¯¹æ¯ä¸ªè§„æ¨¡ï¼Œæˆ‘ä»¬åŒæ­¥å‘å¸ƒåŸºç¡€è¯­è¨€æ¨¡å‹å’Œå¯¹é½åçš„å¯¹è¯æ¨¡å‹ã€‚è¯¥ç³»åˆ—åŸºäº Transformer æ¶æ„ï¼Œé‡‡ç”¨ SwiGLU æ¿€æ´»å‡½æ•°ã€æ³¨æ„åŠ› QKV åç½®ã€åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶èåˆæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸å…¨å±€æ³¨æ„åŠ›ç­‰åˆ›æ–°è®¾è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‡çº§äº†åˆ†è¯å™¨ä»¥æ›´å¥½é€‚é…å¤šè‡ªç„¶è¯­è¨€ä¸ä»£ç åœºæ™¯ã€‚å½“å‰æµ‹è¯•ç‰ˆæš‚æœªå¼•å…¥åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆ32B é™¤å¤–ï¼‰åŠæ»‘åŠ¨çª—å£ä¸å…¨å±€æ³¨æ„åŠ›çš„æ··åˆæœºåˆ¶ã€‚\n\nç¯å¢ƒè¦æ±‚\n\nQwen1.5 çš„ä»£ç å·²é›†æˆè‡³æœ€æ–°ç‰ˆ Hugging Face transformers åº“ï¼Œå»ºè®®å®‰è£… transformers>=4.37.0 ç‰ˆæœ¬ï¼Œå¦åˆ™å¯èƒ½é‡åˆ°å¦‚ä¸‹æŠ¥é”™ï¼š\n\nKeyError: 'qwen2'.\n\nä½¿ç”¨è¯´æ˜\n\næˆ‘ä»¬å»ºè®®æ‚¨ä¸è¦ç›´æ¥ä½¿ç”¨åŸºç¡€è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚æ‚¨å¯ä»¥å¯¹è¯¥æ¨¡å‹è¿›è¡Œåç»­è®­ç»ƒï¼Œä¾‹å¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€ç»§ç»­é¢„è®­ç»ƒç­‰ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨è®¤ä¸ºæˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚\n\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Other\", \"pretrained\"]"
  },
  {
    "url": "https://gitcode.com/openMind/Qwen3-8B",
    "project_name": "Qwen3-8B",
    "readme": "Original Text\nQwen3-8B\nQwen3 äº®ç‚¹\n\nQwen3 æ˜¯é€šä¹‰åƒé—®ç³»åˆ—å¤§è¯­è¨€æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬ï¼Œæä¾›äº†ä¸€ç³»åˆ—å¯†é›†å‹å’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ã€‚åŸºäºå¤§è§„æ¨¡è®­ç»ƒï¼ŒQwen3 åœ¨æ¨ç†ã€æŒ‡ä»¤éµå¾ªã€æ™ºèƒ½ä½“èƒ½åŠ›å’Œå¤šè¯­è¨€æ”¯æŒæ–¹é¢å®ç°äº†çªç ´æ€§è¿›å±•ï¼Œä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š\n\nç‹¬ç‰¹æ”¯æŒæ€ç»´æ¨¡å¼ï¼ˆç”¨äºå¤æ‚é€»è¾‘æ¨ç†ã€æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸éæ€ç»´æ¨¡å¼ï¼ˆç”¨äºé«˜æ•ˆé€šç”¨å¯¹è¯ï¼‰åœ¨å•ä¸€æ¨¡å‹å†…æ— ç¼åˆ‡æ¢ï¼Œç¡®ä¿å„ç±»åœºæ™¯ä¸‹çš„æœ€ä¼˜è¡¨ç°ã€‚\næ˜¾è‘—å¢å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨æ•°å­¦ã€ä»£ç ç”Ÿæˆå’Œå¸¸è¯†é€»è¾‘æ¨ç†æ–¹é¢è¶…è¶Šå‰ä»£ QwQï¼ˆæ€ç»´æ¨¡å¼ï¼‰å’Œ Qwen2.5 æŒ‡ä»¤æ¨¡å‹ï¼ˆéæ€ç»´æ¨¡å¼ï¼‰ã€‚\nå“è¶Šçš„äººç±»åå¥½å¯¹é½ï¼Œåœ¨åˆ›æ„å†™ä½œã€è§’è‰²æ‰®æ¼”ã€å¤šè½®å¯¹è¯å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢è¡¨ç°çªå‡ºï¼Œæä¾›æ›´è‡ªç„¶ã€å¼•äººå…¥èƒœçš„å¯¹è¯ä½“éªŒã€‚\nä¸“ä¸šçš„æ™ºèƒ½ä½“èƒ½åŠ›ï¼Œå¯åœ¨æ€ç»´ä¸éæ€ç»´æ¨¡å¼ä¸‹ç²¾å‡†æ•´åˆå¤–éƒ¨å·¥å…·ï¼Œåœ¨å¼€æºæ¨¡å‹çš„å¤æ‚æ™ºèƒ½ä½“ä»»åŠ¡ä¸­è¡¨ç°é¢†å…ˆã€‚\næ”¯æŒ 100 å¤šç§è¯­è¨€å’Œæ–¹è¨€ï¼Œå…·å¤‡å¼ºå¤§çš„å¤šè¯­è¨€æŒ‡ä»¤éµå¾ªå’Œç¿»è¯‘èƒ½åŠ›ã€‚\næ¨¡å‹æ¦‚è§ˆ\n\nQwen3-8B å…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š\n\nç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹\nè®­ç»ƒé˜¶æ®µï¼šé¢„è®­ç»ƒä¸åè®­ç»ƒ\nå‚æ•°é‡ï¼š82 äº¿\néåµŒå…¥å‚æ•°é‡ï¼š69.5 äº¿\nå±‚æ•°ï¼š36\næ³¨æ„åŠ›å¤´æ•°ï¼ˆGQAï¼‰ï¼šæŸ¥è¯¢å¤´ 32ï¼Œé”®å€¼å¤´ 8\nä¸Šä¸‹æ–‡é•¿åº¦ï¼šåŸç”Ÿæ”¯æŒ 32,768 tokensï¼Œé€šè¿‡ YaRN æ‰©å±•è‡³ 131,072 tokensã€‚\n\næ›´å¤šç»†èŠ‚ï¼ŒåŒ…æ‹¬åŸºå‡†è¯„ä¼°ã€ç¡¬ä»¶éœ€æ±‚å’Œæ¨ç†æ€§èƒ½ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„åšå®¢ã€GitHub å’Œæ–‡æ¡£ã€‚\n\nå¿«é€Ÿå¼€å§‹\n\nQwen3 çš„ä»£ç å·²é›†æˆè‡³æœ€æ–°ç‰ˆ Hugging Face transformersï¼Œå»ºè®®ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ã€‚\n\nè‹¥ä½¿ç”¨ transformers<4.51.0ï¼Œæ‚¨å°†é‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š\n\nKeyError: 'qwen3'\n\n\nä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•åŸºäºç»™å®šè¾“å…¥ä½¿ç”¨è¯¥æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-8B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n\n\néƒ¨ç½²æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ sglang>=0.4.6.post1 æˆ– vllm>=0.8.5 åˆ›å»ºå…¼å®¹ OpenAI çš„ API æœåŠ¡ç«¯ç‚¹ï¼š\n\nSGLang æ–¹å¼ï¼š\npython -m sglang.launch_server --model-path Qwen/Qwen3-8B --reasoning-parser qwen3\n\nvLLM æ–¹å¼ï¼š\nvllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1\n\n\nå¯¹äºæœ¬åœ°ä½¿ç”¨ï¼ŒOllamaã€LMStudioã€MLX-LMã€llama.cpp å’Œ KTransformers ç­‰åº”ç”¨ä¹Ÿå·²æ”¯æŒ Qwen3 æ¨¡å‹ã€‚\n\næ€ç»´æ¨¡å¼ä¸éæ€ç»´æ¨¡å¼åˆ‡æ¢\n\nTip\n\nenable_thinking å¼€å…³åŒæ ·é€‚ç”¨äº SGLang å’Œ vLLM åˆ›å»ºçš„ API æœåŠ¡ã€‚ å…·ä½“ä½¿ç”¨æ–¹æ³•è¯·å‚é˜… SGLang å’Œ vLLM çš„å®˜æ–¹æ–‡æ¡£ã€‚\n\nenable_thinking=Trueï¼ˆé»˜è®¤å¯ç”¨ï¼‰\n\nQwen3 é»˜è®¤å¼€å¯æ€ç»´æ¨ç†èƒ½åŠ›ï¼ˆä¸ QwQ-32B ç±»ä¼¼ï¼‰ï¼Œè¿™æ„å‘³ç€æ¨¡å‹ä¼šè¿ç”¨æ¨ç†èƒ½åŠ›æå‡ç”Ÿæˆå†…å®¹è´¨é‡ã€‚ä¾‹å¦‚åœ¨ tokenizer.apply_chat_template ä¸­æ˜¾å¼è®¾ç½® enable_thinking=True æˆ–ä¿æŒé»˜è®¤å€¼æ—¶ï¼Œæ¨¡å‹å°†è¿›å…¥æ€ç»´æ¨¡å¼ã€‚\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n\n\nåœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ä¼šç”ŸæˆåŒ…å«åœ¨ <think>...</think> å—ä¸­çš„æ€è€ƒå†…å®¹ï¼Œéšåè¾“å‡ºæœ€ç»ˆå“åº”ã€‚\n\n[!æ³¨æ„] æ€è€ƒæ¨¡å¼æ¨èä½¿ç”¨å‚æ•° Temperature=0.6ã€TopP=0.95ã€TopK=20 å’Œ MinP=0ï¼ˆå³ generation_config.json ä¸­çš„é»˜è®¤é…ç½®ï¼‰ã€‚åˆ‡å‹¿ä½¿ç”¨è´ªå¿ƒè§£ç ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™å’Œæ— é™å¾ªç¯ã€‚æ›´è¯¦ç»†çš„æŒ‡å¯¼è¯·å‚é˜…æœ€ä½³å®è·µç« èŠ‚ã€‚\n\nenable_thinking=False\n\næˆ‘ä»¬æä¾›äº†å¼ºåˆ¶å…³é—­æ¨¡å‹æ€è€ƒè¡Œä¸ºçš„ç¡¬å¼€å…³ï¼Œä½¿å…¶åŠŸèƒ½ä¸ä¹‹å‰çš„ Qwen2.5-Instruct æ¨¡å‹ä¿æŒä¸€è‡´ã€‚è¯¥æ¨¡å¼åœ¨éœ€è¦ç¦ç”¨æ€è€ƒä»¥æå‡æ•ˆç‡çš„åœºæ™¯ä¸­å°¤ä¸ºå®ç”¨ã€‚\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n\n\nåœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ä¸ä¼šç”Ÿæˆä»»ä½•æ€è€ƒå†…å®¹ï¼Œä¹Ÿä¸ä¼šåŒ…å« <think>...</think> åŒºå—ã€‚\n\n[!æ³¨æ„] å¯¹äºéæ€è€ƒæ¨¡å¼ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ Temperature=0.7ã€TopP=0.8ã€TopK=20 å’Œ MinP=0ã€‚æ›´è¯¦ç»†çš„æŒ‡å¯¼ï¼Œè¯·å‚é˜…æœ€ä½³å®è·µéƒ¨åˆ†ã€‚\n\né«˜çº§ç”¨æ³•ï¼šé€šè¿‡ç”¨æˆ·è¾“å…¥åœ¨æ€è€ƒä¸éæ€è€ƒæ¨¡å¼é—´åˆ‡æ¢\n\næˆ‘ä»¬æä¾›äº†ä¸€ç§è½¯åˆ‡æ¢æœºåˆ¶ï¼Œå½“ enable_thinking=True æ—¶ï¼Œå…è®¸ç”¨æˆ·åŠ¨æ€æ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºã€‚å…·ä½“æ¥è¯´ï¼Œæ‚¨å¯ä»¥åœ¨ç”¨æˆ·æç¤ºæˆ–ç³»ç»Ÿæ¶ˆæ¯ä¸­æ·»åŠ  /think å’Œ /no_thinkï¼Œä»¥é€è½®åˆ‡æ¢æ¨¡å‹çš„æ€è€ƒæ¨¡å¼ã€‚åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œæ¨¡å‹å°†éµå¾ªæœ€è¿‘çš„æŒ‡ä»¤ã€‚\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªå¤šè½®å¯¹è¯çš„ç¤ºä¾‹ï¼š\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-8B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n\n\n[!æ³¨æ„] å‡ºäº API å…¼å®¹æ€§è€ƒè™‘ï¼Œå½“ enable_thinking=True æ—¶ï¼Œæ— è®ºç”¨æˆ·ä½¿ç”¨ /think è¿˜æ˜¯ /no_thinkï¼Œæ¨¡å‹å§‹ç»ˆä¼šè¾“å‡ºä¸€ä¸ªç”¨ <think>...</think> åŒ…è£¹çš„åŒºå—ã€‚ä½†è‹¥æ€è€ƒåŠŸèƒ½è¢«ç¦ç”¨ï¼Œè¯¥åŒºå—å†…å®¹å¯èƒ½ä¸ºç©ºã€‚ å½“ enable_thinking=False æ—¶ï¼Œè½¯å¼€å…³å°†å¤±æ•ˆã€‚æ— è®ºç”¨æˆ·è¾“å…¥ä»»ä½• /think æˆ– /no_think æ ‡ç­¾ï¼Œæ¨¡å‹éƒ½ä¸ä¼šç”Ÿæˆæ€è€ƒå†…å®¹ï¼Œä¹Ÿä¸ä¼šåŒ…å« <think>...</think> åŒºå—ã€‚\n\næ™ºèƒ½ä½“åº”ç”¨\n\nQwen3 åœ¨å·¥å…·è°ƒç”¨èƒ½åŠ›ä¸Šè¡¨ç°å“è¶Šã€‚æˆ‘ä»¬æ¨èä½¿ç”¨ Qwen-Agent æ¥å……åˆ†å‘æŒ¥ Qwen3 çš„æ™ºèƒ½ä½“èƒ½åŠ›ã€‚Qwen-Agent å†…éƒ¨å°è£…äº†å·¥å…·è°ƒç”¨æ¨¡æ¿å’Œå·¥å…·è°ƒç”¨è§£æå™¨ï¼Œå¯å¤§å¹…é™ä½ç¼–ç å¤æ‚åº¦ã€‚\n\næ‚¨å¯ä»¥é€šè¿‡ MCP é…ç½®æ–‡ä»¶å®šä¹‰å¯ç”¨å·¥å…·ï¼Œä½¿ç”¨ Qwen-Agent çš„é›†æˆå·¥å…·ï¼Œæˆ–è‡ªè¡Œé›†æˆå…¶ä»–å·¥å…·ã€‚\n\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-8B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n\né•¿æ–‡æœ¬å¤„ç†\n\nQwen3 åŸç”Ÿæ”¯æŒ 32,768 tokens çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚å¯¹äºè¾“å…¥è¾“å‡ºæ€»é•¿åº¦è¿œè¶…è¯¥é™åˆ¶çš„å¯¹è¯åœºæ™¯ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ RoPE æ’å€¼æŠ€æœ¯æ¥é«˜æ•ˆå¤„ç†é•¿æ–‡æœ¬ã€‚é€šè¿‡ YaRN æ–¹æ³•ï¼Œæˆ‘ä»¬å·²éªŒè¯æ¨¡å‹åœ¨ 131,072 tokens ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚\n\nç›®å‰ï¼Œtransformers å’Œ llama.cpp ç­‰æœ¬åœ°æ¨ç†æ¡†æ¶ï¼Œä»¥åŠ vllm å’Œ sglang ç­‰éƒ¨ç½²æ¡†æ¶å‡å·²æ”¯æŒ YaRNã€‚å¯¹äºå·²æ”¯æŒè¯¥æŠ€æœ¯çš„æ¡†æ¶ï¼Œé€šå¸¸æœ‰ä¸¤ç§å¯ç”¨æ–¹å¼ï¼š\n\nä¿®æ”¹æ¨¡å‹æ–‡ä»¶ï¼š\nåœ¨ config.json ä¸­æ·»åŠ  rope_scaling å­—æ®µï¼š\n\n{\n    ...,\n    \"rope_scaling\": {\n        \"rope_type\": \"yarn\",\n        \"factor\": 4.0,\n        \"original_max_position_embeddings\": 32768\n    }\n}\n\n\nè‹¥ä½¿ç”¨ llama.cppï¼Œä¿®æ”¹åéœ€é‡æ–°ç”Ÿæˆ GGUF æ–‡ä»¶ã€‚\n\nå‘½ä»¤è¡Œä¼ å‚ï¼š\n\nvllm ç”¨æˆ·å¯æ‰§è¡Œï¼š\nvllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072  \n\nsglang ç”¨æˆ·å¯æ‰§è¡Œï¼š\npython -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\n\nllama.cpp çš„ llama-server ç”¨æˆ·å¯æ‰§è¡Œï¼š\nllama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n\n\nImportant\n\nè‹¥å‡ºç°ä»¥ä¸‹è­¦å‘Šï¼š\n\nUnrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\n\n\nè¯·å‡çº§ transformers>=4.51.0ã€‚\n\nNote\n\nå½“å‰ä¸»æµå¼€æºæ¡†æ¶å‡é‡‡ç”¨é™æ€ YaRN å®ç°ï¼Œå³æ— è®ºè¾“å…¥é•¿åº¦å¦‚ä½•ï¼Œç¼©æ”¾å› å­å§‹ç»ˆä¿æŒä¸å˜ï¼Œè¿™å¯èƒ½å½±å“çŸ­æ–‡æœ¬åœºæ™¯çš„æ€§èƒ½è¡¨ç°ã€‚\nå»ºè®®ä»…åœ¨éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶æ·»åŠ  rope_scaling é…ç½®ï¼Œå¹¶æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´ factor å‚æ•°ã€‚ä¾‹å¦‚ï¼Œè‹¥å…¸å‹ä¸Šä¸‹æ–‡é•¿åº¦ä¸º 65,536 tokensï¼Œæ›´æ¨èè®¾ç½® factor ä¸º 2.0ã€‚\n\nNote\n\nconfig.json ä¸­é»˜è®¤çš„ max_position_embeddings ä¸º 40,960ï¼Œè¯¥è®¾è®¡é¢„ç•™äº† 32,768 tokens ç”¨äºè¾“å‡ºå’Œ 8,192 tokens ç”¨äºå¸¸è§„æç¤ºï¼Œè¶³ä»¥è¦†ç›–å¤§å¤šæ•°çŸ­æ–‡æœ¬å¤„ç†åœºæ™¯ã€‚è‹¥å¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦æœªè¶…è¿‡ 32,768 tokensï¼Œä¸å»ºè®®å¯ç”¨ YaRNï¼Œä»¥å…æ½œåœ¨å½±å“æ¨¡å‹æ€§èƒ½ã€‚\n\nTip\n\né˜¿é‡Œäº‘æ¨¡å‹æœåŠ¡å¹³å°ï¼ˆModel Studioï¼‰æä¾›çš„ç«¯ç‚¹é»˜è®¤æ”¯æŒåŠ¨æ€ YaRNï¼Œæ— éœ€é¢å¤–é…ç½®ã€‚\n\næœ€ä½³å®è·µ\n\nä¸ºè·å¾—æœ€ä¼˜æ€§èƒ½ï¼Œæˆ‘ä»¬æ¨èä»¥ä¸‹è®¾ç½®ï¼š\n\né‡‡æ ·å‚æ•°ï¼š\n\næ€è€ƒæ¨¡å¼ï¼ˆenable_thinking=Trueï¼‰ï¼šå»ºè®® Temperature=0.6ã€TopP=0.95ã€TopK=20 å’Œ MinP=0ã€‚åˆ‡å‹¿ä½¿ç”¨è´ªå¿ƒè§£ç ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™å’Œæ— é™é‡å¤ã€‚\néæ€è€ƒæ¨¡å¼ï¼ˆenable_thinking=Falseï¼‰ï¼šæ¨è Temperature=0.7ã€TopP=0.8ã€TopK=20 å’Œ MinP=0ã€‚\nå¯¹äºæ”¯æŒæ¡†æ¶ï¼Œå¯å°† presence_penalty å‚æ•°è®¾ä¸º 0 è‡³ 2 ä»¥ç¼“è§£æ— é™é‡å¤é—®é¢˜ï¼Œä½†è¿‡é«˜å€¼å¯èƒ½å¶å°”å¯¼è‡´è¯­è¨€æ··æ‚å’Œè½»å¾®æ€§èƒ½ä¸‹é™ã€‚\n\nå……è¶³è¾“å‡ºé•¿åº¦ï¼š\n\nå¸¸è§„æŸ¥è¯¢å»ºè®®è¾“å‡ºé•¿åº¦è®¾ä¸º 32,768 tokensã€‚\né’ˆå¯¹æ•°å­¦ç«èµ›ã€ç¼–ç¨‹ç«èµ›ç­‰é«˜å¤æ‚åº¦é—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œå»ºè®®å°†æœ€å¤§è¾“å‡ºé•¿åº¦è®¾ä¸º 38,912 tokensï¼Œä¸ºæ¨¡å‹æä¾›å……åˆ†ç”Ÿæˆç©ºé—´ä»¥è¾“å‡ºè¯¦å°½è§£ç­”ï¼Œä»è€Œæå‡æ•´ä½“è¡¨ç°ã€‚\n\næ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ï¼š\nåŸºå‡†æµ‹è¯•æ—¶æ¨èé€šè¿‡æç¤ºè¯è§„èŒƒè¾“å‡ºæ ¼å¼ï¼š\n\næ•°å­¦é¢˜ï¼šæç¤ºè¯ä¸­åŠ å…¥â€œè¯·é€æ­¥æ¨ç†ï¼Œå¹¶å°†æœ€ç»ˆç­”æ¡ˆç”¨ \\boxed{} åŒ…è£¹â€ã€‚\né€‰æ‹©é¢˜ï¼šæç¤ºè¯ä¸­åŠ å…¥ JSON ç»“æ„è§„èŒƒè¾“å‡ºï¼Œä¾‹å¦‚ï¼šâ€œè¯·åœ¨ answer å­—æ®µä¸­ä»…å¡«å†™é€‰é¡¹å­—æ¯ï¼Œå¦‚ \"answer\": \"C\"â€ã€‚\n\nå†å²å¯¹è¯ä¸åŒ…å«æ€è€ƒå†…å®¹ï¼š\nåœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œå†å²æ¨¡å‹è¾“å‡ºåº”ä»…åŒ…å«æœ€ç»ˆå›ç­”éƒ¨åˆ†ï¼Œæ— éœ€ä¿ç•™æ€è€ƒè¿‡ç¨‹ã€‚è¯¥é€»è¾‘å·²é€šè¿‡ Jinja2 èŠå¤©æ¨¡æ¿å®ç°ã€‚å¯¹äºæœªç›´æ¥ä½¿ç”¨è¯¥æ¨¡æ¿çš„æ¡†æ¶ï¼Œéœ€å¼€å‘è€…è‡ªè¡Œç¡®ä¿éµå¾ªæ­¤æœ€ä½³å®è·µã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚\n\n@misc{qwen3,\n    title  = {Qwen3},\n    url    = {https://qwenlm.github.io/blog/qwen3/},\n    author = {Qwen Team},\n    month  = {April},\n    year   = {2025}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/Step-Audio",
    "project_name": "Step-Audio",
    "readme": "Original Text\n\nä¸­æ–‡Â  ï½œ Â EnglishÂ  ï½œ Â æ—¥æœ¬èª\n\n\n\n\nStep-Audio\n\n â€‚  â€‚\n â€‚  â€‚\n â€‚  â€‚\nğŸ”¥ğŸ”¥ğŸ”¥ æœ€æ–°åŠ¨æ€!!\n2025å¹´2æœˆ17æ—¥: ğŸ‘‹ å‘å¸ƒæ¨ç†ä»£ç ä¸æ¨¡å‹æƒé‡ï¼ŒåŒ…å«Step-Audio-Chatã€Step-Audio-TTS-3BåŠStep-Audio-Tokenizerã€‚\n2025å¹´2æœˆ17æ—¥: ğŸ‘‹ å‘å¸ƒå¤šè½®è¯­éŸ³äº¤äº’è¯„æµ‹åŸºå‡†StepEval-Audio-360ã€‚\n2025å¹´2æœˆ17æ—¥: ğŸ‘‹ å‘å¸ƒæŠ€æœ¯æŠ¥å‘ŠStep-Audio-Reportã€‚\nä»£ç ä¼˜åŒ–\nåœ¨requirements.txtä¸­å¢åŠ æ˜‡è…¾NPUç›¸å…³ä¾èµ–é¡¹ï¼›\nè°ƒæ•´tts_inference.pyä»¥é€‚é…æ˜‡è…¾NPUï¼›\nä¼˜åŒ–Step-1æ¨¡å‹çš„StepRMSNormç±»ï¼Œæ›¿æ¢ä¸ºæ˜‡è…¾èåˆç®—å­ã€‚\n1. é¡¹ç›®æ¦‚è¿°\n\nStep-Audioæ˜¯ä¸šç•Œé¦–ä¸ªé›†è¯­éŸ³ç†è§£ä¸ç”Ÿæˆæ§åˆ¶äºä¸€ä½“çš„äº§å“çº§å¼€æºå®æ—¶è¯­éŸ³äº¤äº’ç³»ç»Ÿï¼Œæ”¯æŒå¤šè¯­è¨€å¯¹è¯ï¼ˆå¦‚ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥è¯­ï¼‰ã€æƒ…æ„Ÿè¡¨è¾¾ï¼ˆå¦‚å–œæ‚¦ã€æ‚²ä¼¤ï¼‰ã€æ–¹è¨€ï¼ˆå¦‚ç²¤è¯­ã€å››å·è¯ï¼‰ï¼Œå¯ç²¾å‡†è°ƒèŠ‚è¯­é€Ÿä¸éŸµå¾‹é£æ ¼ï¼Œå¹¶æ”¯æŒRAPå’Œå“¼å”±åŠŸèƒ½ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯çªç ´ä½“ç°åœ¨å››å¤§åˆ›æ–°ç‚¹ï¼š\n\nåƒäº¿çº§å¤šæ¨¡æ€æ¨¡å‹ï¼šå•ä¸€æ¨¡å‹å®ç°è¯­éŸ³è¯†åˆ«ã€è¯­ä¹‰ç†è§£ã€å¯¹è¯äº¤äº’ã€è¯­éŸ³å…‹éš†ã€è¯­éŸ³ç”Ÿæˆç­‰å…¨æµç¨‹åŠŸèƒ½ï¼Œå¼€æº1300äº¿å‚æ•°å¤šæ¨¡æ€æ¨¡å‹Step-Audio-Chatã€‚\n\næ™ºèƒ½æ•°æ®ç”Ÿæˆä½“ç³»ï¼šåŸºäº130Bæ¨¡å‹çªç ´ä¼ ç»ŸTTSå¯¹äººå·¥é‡‡é›†æ•°æ®çš„ä¾èµ–ï¼Œç”Ÿæˆé«˜è´¨é‡åˆæˆéŸ³é¢‘æ•°æ®ï¼ŒåŒæ­¥å¼€æºé¦–ä¸ªåŸºäºå¤§è§„æ¨¡åˆæˆæ•°æ®è®­ç»ƒã€æ”¯æŒRAPå’Œå“¼å”±çš„å¢å¼ºç‰ˆè¯­éŸ³åˆæˆæ¨¡å‹Step-Audio-TTS-3Bã€‚\n\nç²¾ç»†åŒ–è¯­éŸ³è°ƒæ§ï¼šæ”¯æŒå¤šç§æƒ…æ„Ÿï¼ˆå¦‚æ„¤æ€’ã€å–œæ‚¦ã€æ‚²ä¼¤ï¼‰ã€æ–¹è¨€ï¼ˆåŒ…æ‹¬ç²¤è¯­ã€å››å·è¯ç­‰ï¼‰åŠæ­Œå”±æ¨¡å¼ï¼ˆå«RAPã€æ¸…å”±å“¼é¸£ï¼‰çš„ç²¾å‡†æ§åˆ¶ï¼Œæ»¡è¶³å¤šæ ·åŒ–è¯­éŸ³ç”Ÿæˆéœ€æ±‚ã€‚\n\næ‰©å±•å·¥å…·ç”Ÿæ€ï¼šé€šè¿‡ToolCallæœºåˆ¶ä¸è§’è‰²æ‰®æ¼”å¢å¼ºï¼Œæ˜¾è‘—æå‡åœ¨æ™ºèƒ½ä½“ä¸å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°åŠ›ã€‚\n\n2. ç³»ç»Ÿæ¶æ„\n\nStep-Audioé‡‡ç”¨Linguistic tokenizerï¼ˆç ç‡16.7Hzï¼Œç æœ¬1024ï¼‰ä¸Semantic tokenizerï¼ˆç ç‡25Hzï¼Œç æœ¬4096ï¼‰å¹¶è¡Œçš„åŒç æœ¬ç¼–ç æ–¹æ¡ˆï¼Œé€šè¿‡2:3æ—¶åºäº¤é”™ç­–ç•¥å®ç°ç‰¹å¾èåˆã€‚åŸºäº130Bå‚æ•°çš„åŸºç¡€æ¨¡å‹ï¼ˆStep-1ï¼‰è¿›è¡ŒéŸ³é¢‘è¯­å¢ƒåŒ–é¢„è®­ç»ƒä¸å®šå‘å¾®è°ƒï¼Œæ„å»ºäº†å¼ºå¤§çš„è·¨æ¨¡æ€è¯­éŸ³ç†è§£èƒ½åŠ›ã€‚å®æ—¶éŸ³é¢‘ç”Ÿæˆé‡‡ç”¨æ··åˆè§£ç æ¶æ„ï¼Œç»“åˆæµåŒ¹é…ï¼ˆflow matchingï¼‰ä¸ç¥ç»å£°ç æŠ€æœ¯ã€‚ \n\n2.1 åˆ†è¯å™¨\n\né€šè¿‡tokençº§äº¤é”™å®ç°Linguistic tokenä¸Semantic tokençš„é«˜æ•ˆæ•´åˆã€‚Linguistic tokenizeré‡‡ç”¨1024ç æœ¬ï¼ˆ16.7Hzç ç‡ï¼‰ï¼ŒSemantic tokenizeré‡‡ç”¨4096å¤§å®¹é‡ç æœ¬ï¼ˆ25Hzç ç‡ï¼‰æ•æ‰å£°å­¦ç»†èŠ‚ã€‚é€šè¿‡2:3æ—¶åºå¯¹é½ç­–ç•¥â€”â€”æ¯2ä¸ªLinguistic tokenå¯¹åº”3ä¸ªSemantic tokenå½¢æˆæ—¶åºæ˜ å°„ã€‚\n\n2.2 è¯­è¨€æ¨¡å‹\n\nä¸ºå¢å¼ºè¯­éŸ³ä¿¡æ¯å¤„ç†èƒ½åŠ›å¹¶å®ç°éŸ³æ–‡ç²¾å‡†å¯¹é½ï¼Œæˆ‘ä»¬åœ¨1300äº¿å‚æ•°çš„æ–‡æœ¬å¤§æ¨¡å‹Step-1åŸºç¡€ä¸Šè¿›è¡Œäº†éŸ³é¢‘æŒç»­é¢„è®­ç»ƒã€‚\n\n2.3 è¯­éŸ³è§£ç å™¨\n\nå°†ç¦»æ•£æ ‡è®°è½¬æ¢ä¸ºè¿ç»­è¯­éŸ³ä¿¡å·ï¼ŒåŒ…å«30äº¿å‚æ•°è¯­è¨€æ¨¡å‹ã€æµåŒ¹é…æ¨¡å‹å’Œæ¢…å°”é¢‘è°±å£°ç å™¨ã€‚é‡‡ç”¨åŒç äº¤é”™è®­ç»ƒæŠ€æœ¯ä¼˜åŒ–è¯­éŸ³æ¸…æ™°åº¦ä¸è‡ªç„¶åº¦ï¼Œç¡®ä¿è¯­ä¹‰ä¸å£°å­¦ç‰¹å¾çš„æ— ç¼èåˆã€‚\n\n2.4 å®æ—¶æ¨ç†ç®¡çº¿\n\næ ¸å¿ƒæ§åˆ¶æ¨¡å—ï¼ˆControllerï¼‰ç®¡ç†çŠ¶æ€è½¬æ¢ä¸å“åº”ç”Ÿæˆï¼Œåè°ƒå››å¤§å­ç³»ç»Ÿï¼š\n\nè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰ï¼šå®æ—¶æ•æ‰è¯­éŸ³èµ·æ­¢\næµå¼éŸ³é¢‘åˆ†è¯å™¨ï¼šå®æ—¶å¤„ç†éŸ³é¢‘æµ\nStep-Audioå¤šæ¨¡æ€å¼•æ“ï¼šç”Ÿæˆèåˆå“åº”\nä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šåŠ¨æ€ç»´æŠ¤å¯¹è¯çŠ¶æ€ \n2.5 è®­ç»ƒä¼˜åŒ–\n\né’ˆå¯¹ASRä¸TTSä»»åŠ¡è¿›è¡Œä¸“é¡¹ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚åœ¨AQTAä»»åŠ¡ä¸­é‡‡ç”¨å¤šæ ·åŒ–é«˜è´¨é‡æ•°æ®é›†ï¼Œç»“åˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æå‡å“åº”è´¨é‡ï¼Œå®ç°æƒ…æ„Ÿã€è¯­é€Ÿã€æ–¹è¨€åŠéŸµå¾‹çš„ç²¾ç»†æ§åˆ¶ã€‚ \n\n3. æ¨¡å‹è·å–\n3.1 Huggingface\næ¨¡å‹\tä¸‹è½½é“¾æ¥\nStep-Audio-Tokenizer\tğŸ¤—huggingface\nStep-Audio-Chat\tğŸ¤—huggingface\nStep-Audio-TTS-3B\tğŸ¤—huggingface\n3.2 Modelscope\næ¨¡å‹\tä¸‹è½½é“¾æ¥\nStep-Audio-Tokenizer\tmodelscope\nStep-Audio-Chat\tmodelscope\nStep-Audio-TTS-3B\tmodelscope\n4. ä½¿ç”¨æŒ‡å—\nğŸ“œ 4.1 è¿è¡Œè¦æ±‚\n\nå•æ‰¹æ¬¡æ¨ç†ï¼ˆbatch size=1ï¼‰çš„ç¡¬ä»¶éœ€æ±‚:\n\næ¨¡å‹\té‡‡æ ·ç‡\tæœ€ä½GPUæ˜¾å­˜\nStep-Audio-Tokenizer\t41.6Hz\t1.5GB\nStep-Audio-Chat\t41.6Hz\t265GB\nStep-Audio-TTS-3B\t41.6Hz\t8GB\nç¡¬ä»¶è¦æ±‚ï¼š\néœ€é…å¤‡NVIDIA CUDAæ˜¾å¡\næµ‹è¯•ç¯å¢ƒï¼š4Ã—80GBæ˜¾å­˜çš„A800æ˜¾å¡\næ¨èé…ç½®ï¼š4Ã—80GBæ˜¾å­˜çš„A800/H800æ˜¾å¡ç»„\næ“ä½œç³»ç»Ÿï¼šLinux\nğŸ”§ 4.2 ç¯å¢ƒé…ç½®\nPython >= 3.10.0 (æ¨èAnacondaæˆ–Miniconda)\nPyTorch >= 2.3-cu121\nCUDA Toolkit\n\nNPUç¯å¢ƒé…ç½®è¯·å‚è€ƒæ˜‡è…¾å®˜æ–¹cann-toolkitå®‰è£…æŒ‡å—\n\ngit clone https://github.com/stepfun-ai/Step-Audio.git\nconda create -n stepaudio python=3.10\nconda activate stepaudio\n\ncd Step-Audio\npip install -r requirements.txt\n\ngit lfs install\ngit clone https://huggingface.co/stepfun-ai/Step-Audio-Tokenizer\ngit clone https://huggingface.co/stepfun-ai/Step-Audio-Chat\ngit clone https://huggingface.co/stepfun-ai/Step-Audio-TTS-3B\n\n\n\nä¸‹è½½æ¨¡å‹åï¼Œwhere_you_download_dir ç›®å½•åº”åŒ…å«å¦‚ä¸‹ç»“æ„ï¼š\n\nwhere_you_download_dir\nâ”œâ”€â”€ Step-Audio-Tokenizer\nâ”œâ”€â”€ Step-Audio-Chat\nâ”œâ”€â”€ Step-Audio-TTS-3B\n\nğŸš€ 4.3 æ¨ç†è„šæœ¬\nç¦»çº¿æ¨ç†\n\næ”¯æŒç«¯åˆ°ç«¯çš„éŸ³é¢‘/æ–‡æœ¬è¾“å…¥ä¸éŸ³é¢‘/æ–‡æœ¬è¾“å‡ºæ¨ç†æµç¨‹ã€‚\n\npython offline_inference.py --model-path where_you_download_dir\n\nSpeech Synthesis Inference\n\nPerform speech synthesis using the default voice or clone a new voice for synthesis.\n\npython tts_inference.py --model-path where_you_download_dir --output-path where_you_save_audio_dir --synthesis-type use_tts_or_clone\n\n\nThe clone mode requires a timbre information dictionary in the following format:\n\n{\n    \"speaker\": \"speaker id\",\n    \"prompt_text\": \"content of prompt wav\",\n    \"wav_path\": \"prompt wav path\"\n}\n\nLaunch Web Demo\n\nStart a local server for online inference. Assuming you have 4 GPUs and all model downloads are complete.\n\npython app.py --model-path where_you_download_dir\n\n5. åŸºå‡†æµ‹è¯•\n5.1 è¯­éŸ³è¯†åˆ«æ€§èƒ½\n\téšå±‚ç‰¹å¾å»ºæ¨¡\tç¦»æ•£æ ‡è®°å»ºæ¨¡\n\tWhisper Large-v3\tQwen2-Audio\tMinMo\tLUCY\tMoshi\tGLM-4-voice Base\tGLM-4-voice Chat\tStep-Audio Pretrain\tStep-Audio-Chat\nAishell-1\t5.14\t1.53\t-\t2.4\t-\t2.46\t226.47\t0.87\t1.95\nAishell-2 ios\t4.76\t3.06\t2.69\t-\t-\t-\t211.3\t2.91\t3.57\nWenetspeech test-net\t9.68\t7.72\t6.64\t8.78\t-\t-\t146.05\t7.62\t8.75\nWenet test-meeting\t18.54\t8.4\t7.6\t10.42\t-\t-\t140.82\t7.78\t9.52\nLibrispeech test-clean\t1.9\t1.6\t1.6\t3.36\t5.7\t2.82\t75.39\t2.36\t3.11\nLibrispeech test-other\t3.65\t3.6\t3.82\t8.05\t-\t7.66\t80.3\t6.32\t8.44\nAVG\t7.28\t4.32\t-\t-\t-\t-\t146.74\t4.64\t5.89\n5.2 è¯­éŸ³åˆæˆè¡¨ç°\n5.2.1 å†…å®¹ä¸€è‡´æ€§å¯¹æ¯”ï¼ˆCER/WERï¼‰\næ¨¡å‹\tä¸­æ–‡æµ‹è¯•é›†\tè‹±æ–‡æµ‹è¯•é›†\nå­—é”™è¯¯ç‡(%) â†“\tè¯é”™è¯¯ç‡(%) â†“\nGLM-4-Voice\t2.19\t2.91\nMinMo\t2.48\t2.90\nStep-Audio\t1.53\t2.71\n5.2.2 SEEDæµ‹è¯•é›†è¡¨ç°\nStepAudio-TTS-3B-Single é‡‡ç”¨åŒç æœ¬ä¸»å¹²ç½‘ç»œä¸å•ç æœ¬å£°ç å™¨çš„ç»„åˆæ¶æ„\næ¨¡å‹\tä¸­æ–‡æµ‹è¯•é›†\tè‹±æ–‡æµ‹è¯•é›†\nå­—é”™è¯¯ç‡(%) â†“\tç›¸ä¼¼åº¦ â†‘\tè¯é”™è¯¯ç‡(%) â†“\tç›¸ä¼¼åº¦ â†‘\nFireRedTTS\t1.51\t0.630\t3.82\t0.460\nMaskGCT\t2.27\t0.774\t2.62\t0.774\nCosyVoice\t3.63\t0.775\t4.29\t0.699\nCosyVoice 2\t1.45\t0.806\t2.57\t0.736\nCosyVoice 2-S\t1.45\t0.812\t2.38\t0.743\nStep-Audio-TTS-3B-Single\t1.37\t0.802\t2.52\t0.704\nStep-Audio-TTS-3B\t1.31\t0.733\t2.31\t0.660\nStep-Audio-TTS\t1.17\t0.73\t2.0\t0.660\n5.2.3 åŒç æœ¬é‡æ„æ€§èƒ½å¯¹æ¯”\n\nï¼ˆæ³¨ï¼šæ­¤å¤„ä¿ç•™åŸå§‹æŠ€æœ¯æœ¯è¯­\"åŒç æœ¬é‡åˆæˆ\"ä»¥ä¿æŒä¸“ä¸šæ€§ï¼‰\n\nToken\tä¸­æ–‡æµ‹è¯•é›†\tè‹±æ–‡æµ‹è¯•é›†\nå­—é”™è¯¯ç‡ (%) â†“\tç›¸ä¼¼åº¦å¾—åˆ† â†‘\tè¯é”™è¯¯ç‡ (%) â†“\tç›¸ä¼¼åº¦å¾—åˆ† â†‘\nçœŸå®æ–‡æœ¬\t0.972\t-\t2.156\t-\nCosyVoice\t2.857\t0.849\t4.519\t0.807\nStep-Audio-TTS-3B\t2.192\t0.784\t3.585\t0.742\n5.3 è¯­éŸ³å¯¹è¯\n\næˆ‘ä»¬æ¨å‡ºå…¨æ–°åŸºå‡†æµ‹è¯•StepEval-Audio-360ï¼Œè¯¥æ•°æ®é›†åŒ…å«137ä¸ªçœŸå®ç”¨æˆ·å¤šè½®ä¸­æ–‡å¯¹è¯æ ·æœ¬ï¼Œç³»ç»Ÿè¯„ä¼°ç”Ÿæˆå¼è¯­éŸ³ç³»ç»Ÿåœ¨ä¹å¤§ç»´åº¦çš„è¡¨ç°ï¼šæŒ‡ä»¤éµå¾ªã€è¯­ä¹‰ç†è§£ã€é€»è¾‘æ¨ç†ã€è§’è‰²æ‰®æ¼”ã€åˆ›ä½œèƒ½åŠ›ã€æ­Œå”±æ¼”ç»ã€å¤šè¯­è¨€æ”¯æŒã€æƒ…æ„Ÿè°ƒæ§åŠæ¸¸æˆäº¤äº’ã€‚\n\n5.3.1 StepEval-Audio-360\nå¤§æ¨¡å‹è¯„ä¼°æŒ‡æ ‡(GPT-4o)\nStepEval-Audio-360è¯­éŸ³å¯¹è¯åŸºç¡€èƒ½åŠ›å¯¹æ¯”\næ¨¡å‹\täº‹å®å‡†ç¡®ç‡ (% â†‘)\tå†…å®¹ç›¸å…³åº¦ (% â†‘)\tå¯¹è¯è¯„åˆ† â†‘\nGLM4-Voice\t54.7\t66.4\t3.49\nQwen2-Audio\t22.6\t26.3\t2.27\nMoshi*\t1.0\t0\t1.49\nStep-Audio-Chat\t66.4\t75.2\t4.11\næ³¨ï¼šæ ‡\"*\"æ•°æ®ä»…ä¾›å‚è€ƒ\nèƒ½åŠ›é›·è¾¾å›¾(äººå·¥è¯„ä¼°)\n5.3.2 å…¬å¼€æµ‹è¯•é›†\næ¨¡å‹\tLlamaé—®ç­”\tç½‘é¡µé—®ç­”\tTriviaQA*\tå¤æ‚æ¨ç†\tæ±‰è¯­æ°´å¹³\nGLM4-Voice\t64.7\t32.2\t39.1\t66.0\t74.0\nMoshi\t62.3\t26.6\t22.8\t-\t-\nFreeze-Omni\t72.0\t44.7\t53.9\t-\t-\nLUCY\t59.7\t29.3\t27.0\t-\t-\nMinMo\t78.9\t55.0\t48.3\t-\t-\nQwen2-Audio\t52.0\t27.0\t37.3\t54.0\t-\nStep-Audio-Chat\t81.0\t75.1\t58.0\t74.0\t86.0\næ³¨ï¼šTriviaQAæ•°æ®é›†æ ‡\"*\"ç»“æœä»…ä¾›å‚è€ƒ\n5.3.3 è¯­éŸ³æŒ‡ä»¤éµå¾ª\nç±»åˆ«\tæŒ‡ä»¤éµå¾ªåº¦\téŸ³é¢‘è´¨é‡\nGLM-4-Voice\tStep-Audio\tGLM-4-Voice\tStep-Audio\nå¤šè¯­è¨€\t1.9\t3.8\t2.9\t3.3\nè§’è‰²æ‰®æ¼”\t3.8\t4.2\t3.2\t3.6\næ­Œå”±/RAP\t2.1\t2.4\t2.4\t4\nè¯­éŸ³æ§åˆ¶\t3.6\t4.4\t3.3\t4.1\n6. Online Engine\n\nThe online version of Step-Audio is accessible via the Yuewen app, where you can also discover some delightful sample demonstrations.\n\n7. Examples\nVoice Cloning\nRole\tPrompt WAV\tCloned WAV\nYu Qian\tGoogle Drive\nAudio File\tGoogle Drive\nAudio File\nLi Xueqin\tGoogle Drive\nAudio File\tGoogle Drive\nAudio File\nSpeed Control\nPrompt\tResponse\nHuman: Say a tongue twister\nAssistant: åƒè‘¡è„ä¸åè‘¡è„çš®ï¼Œä¸åƒè‘¡è„å€’åè‘¡è„çš®\nHuman: Hey, can you say it faster?\tGoogle Drive\nAudio File\nHuman: Say a tongue twister\nAssistant: åƒè‘¡è„ä¸åè‘¡è„çš®ï¼Œä¸åƒè‘¡è„å€’åè‘¡è„çš®\nHuman: Hey, can you say it faster?\nAssistant: åƒè‘¡è„ä¸åè‘¡è„çš®ï¼Œä¸åƒè‘¡è„å€’åè‘¡è„çš®\nHuman: Uh, can you say it super slowly this time?\tGoogle Drive\nAudio File\nHigh EQ (Emotion & Tone Control)\nPrompt\tResponse\nHuman: Your tone isnâ€™t cute or playful at all. Can you try being moreæ’’å¨‡ and å–èŒ?\tGoogle Drive\nAudio File\nHuman: What should I do? I feel like my life is a failure.\tGoogle Drive\nAudio File\nHuman: Xiao Yue, youâ€™re seriously amazing.\tGoogle Drive\nAudio File\nMultilingual (e.g., Chinese, English, Japanese)\nPrompt\tResponse\nHuman: What did the speaker mean when they said, it's raining cats and dogs?\nAssistant: When they say \"It's raining cats and dogs,\" it just means it's raining really hard. The speaker isn't literally saying cats and dogs are falling from the sky! It's just a fun way to describe heavy rain.\tGoogle Drive\nAudio File\nHuman: ã“ã‚“ã«ã¡ã¯ã€‚ï¼ˆHelloï¼‰\nAssistantï¼šã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹æ‰‹ä¼ã„ã¾ã—ã‚‡ã†ã‹ï¼Ÿï¼ˆHello! How can I help you?ï¼‰\tGoogle Drive\nAudio File\nRap & Vocal\nPrompt\tResponse\nHuman: Perform a rap\tGoogle Drive\nAudio File\n8. Acknowledgments\n\nParts of this project's code are sourced from:\n\nCosyVoice\nTransformers\nFunASR\n\nWe extend our gratitude to all the above open-source projects for their contributions!\n\n9. License\n\nThe weights of Step-Audio models are governed by the respective licenses specified in Step-Audio-Chat, Step-Audio-Tokenizer, and Step-Audio-TTS-3B.\n\nThe code in this repository adheres to the Apache 2.0 license.\n\n10. Citation\n@misc{huang2025stepaudiounifiedunderstandinggeneration,\n      title={Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction}, \n      author={Ailin Huang and Boyong Wu and Bruce Wang and Chao Yan and Chen Hu and Chengli Feng and Fei Tian and Feiyu Shen and Jingbei Li and Mingrui Chen and Peng Liu and Ruihang Miao and Wang You and Xi Chen and Xuerui Yang and Yechang Huang and Yuxiang Zhang and Zheng Gong and Zixin Zhang and Brian Li and Changyi Wan and Hanpeng Hu and Ranchen Ming and Song Yuan and Xuelin Zhang and Yu Zhou and Bingxin Li and Buyun Ma and Kang An and Wei Ji and Wen Li and Xuan Wen and Yuankai Ma and Yuanwei Liang and Yun Mou and Bahtiyar Ahmidi and Bin Wang and Bo Li and Changxin Miao and Chen Xu and Chengting Feng and Chenrun Wang and Dapeng Shi and Deshan Sun and Dingyuan Hu and Dula Sai and Enle Liu and Guanzhe Huang and Gulin Yan and Heng Wang and Haonan Jia and Haoyang Zhang and Jiahao Gong and Jianchang Wu and Jiahong Liu and Jianjian Sun and Jiangjie Zhen and Jie Feng and Jie Wu and Jiaoren Wu and Jie Yang and Jinguo Wang and Jingyang Zhang and Junzhe Lin and Kaixiang Li and Lei Xia and Li Zhou and Longlong Gu and Mei Chen and Menglin Wu and Ming Li and Mingxiao Li and Mingyao Liang and Na Wang and Nie Hao and Qiling Wu and Qinyuan Tan and Shaoliang Pang and Shiliang Yang and Shuli Gao and Siqi Liu and Sitong Liu and Tiancheng Cao and Tianyu Wang and Wenjin Deng and Wenqing He and Wen Sun and Xin Han and Xiaomin Deng and Xiaojia Liu and Xu Zhao and Yanan Wei and Yanbo Yu and Yang Cao and Yangguang Li and Yangzhen Ma and Yanming Xu and Yaqiang Shi and Yilei Wang and Yinmin Zhong and Yu Luo and Yuanwei Lu and Yuhe Yin and Yuting Yan and Yuxiang Yang and Zhe Xie and Zheng Ge and Zheng Sun and Zhewei Huang and Zhichao Chang and Zidong Yang and Zili Zhang and Binxing Jiao and Daxin Jiang and Heung-Yeung Shum and Jiansheng Chen and Jing Li and Shuchang Zhou and Xiangyu Zhang and Xinhao Zhang and Yibo Zhu},\n      year={2025},\n      eprint={2502.11946},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2502.11946}, \n}\n\næ˜Ÿæ ‡å†å²",
    "tags": "[\"TensorFlow\", \"Transformers\", \"ONNX\", \"timm\", \"Fairseq\", \"Safetensors\", \"CosyVoice\", \"Datasets\", \"cn\", \"Alibaba\", \"FunASR\", \"Paraformer\", \"INTERSPEECH 2022\"]"
  },
  {
    "url": "https://gitcode.com/openMind/Meta-Llama-3.1-8B-Instruct",
    "project_name": "Meta-Llama-3.1-8B-Instruct",
    "readme": "",
    "tags": "[]"
  },
  {
    "url": "https://gitcode.com/openMind/gemma-3-1b-it",
    "project_name": "gemma-3-1b-it",
    "readme": "Gemma 3 model card\n\nModel Page: Gemma\n\nResources and Technical Documentation:\n\nGemma 3 Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\n\nTerms of Use: Terms\n\nAuthors: Google DeepMind\n\nModel Information\n\nSummary description and brief definition of inputs and outputs.\n\nDescription\n\nGemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n\nInputs and outputs\n\nInput:\n\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens each\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and 32K tokens for the 1B size\n\nOutput:\n\nGenerated text in response to the input, such as an answer to a question, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\nUsage\n\nBelow, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0.\n\n$ pip install -U transformers\n\n\nThen, copy the snippet from the section that is relevant for your use case.\n\nRunning with the pipeline API\n\nWith instruction-tuned models, you need to use chat templates to process our inputs first. Then, you can pass it to the pipeline.\n\nDownload model weights to local directory.\n\ngit clone https://gitcode.com/openMind/gemma-3-1b-it.git\n\n\nUse transformers for inference.\n\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\"text-generation\", model=\"gemma-3-1b-it\", device=\"npu\", torch_dtype=torch.bfloat16)\n\nmessages = [\n    [\n        {\n            \"role\": \"system\",\n            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem for China.\"},]\n        },\n    ],\n]\n\noutput = pipe(messages, max_new_tokens=50)\nprint(output)\n\nRunning the model on a single / multi NPU\nfrom transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\nimport torch\n\nmodel_id = \"gemma-3-1b-it\"\n\n# NPU do not support bitsandbytes\nmodel = Gemma3ForCausalLM.from_pretrained(model_id).eval()\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    [\n        {\n            \"role\": \"system\",\n            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem for Beijing.\"},]\n        },\n    ],\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device).to(torch.bfloat16)\n\n\nwith torch.inference_mode():\n    outputs = model.generate(**inputs, max_new_tokens=64)\n\noutputs = tokenizer.batch_decode(outputs)\n\nCitation\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n\nModel Data\n\nData used for model training and how the data was processed.\n\nTraining Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 14 trillion tokens, the 12B model was trained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and 1B with 2 trillion tokens. Here are the key components:\n\nWeb Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful multimodal model that can handle a wide variety of different tasks and data formats.\n\nData Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training data:\n\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in line with our policies.\nImplementation Information\n\nDetails about the model internals.\n\nHardware\n\nGemma was trained using Tensor Processing Unit (TPU) hardware (TPUv4p, TPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant computational power. TPUs, designed specifically for matrix operations common in machine learning, offer several advantages in this domain:\n\nPerformance: TPUs are specifically designed to handle the massive computations involved in training VLMs. They can speed up training considerably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory, allowing for the handling of large models and batch sizes during training. This can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable solution for handling the growing complexity of large foundation models. You can distribute training across multiple TPU devices for faster and more efficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more cost-effective solution for training large models compared to CPU-based infrastructure, especially when considering the time and resources saved due to faster training.\nThese advantages are aligned with Google's commitments to operate sustainably.\nSoftware\n\nTraining was done using JAX and ML Pathways.\n\nJAX allows researchers to take advantage of the latest generation of hardware, including TPUs, for faster and more efficient training of large models. ML Pathways is Google's latest effort to build artificially intelligent systems capable of generalizing across multiple tasks. This is specially suitable for foundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the paper about the Gemini family of models; \"the 'single controller' programming model of Jax and Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow.\"\n\nEvaluation\n\nModel evaluation metrics and results.\n\nBenchmark Results\n\nThese models were evaluated against a large collection of different datasets and metrics to cover different aspects of text generation:\n\nReasoning and factuality\nBenchmark\tMetric\tGemma 3 PT 1B\tGemma 3 PT 4B\tGemma 3 PT 12B\tGemma 3 PT 27B\nHellaSwag\t10-shot\t62.3\t77.2\t84.2\t85.6\nBoolQ\t0-shot\t63.2\t72.3\t78.8\t82.4\nPIQA\t0-shot\t73.8\t79.6\t81.8\t83.3\nSocialIQA\t0-shot\t48.9\t51.9\t53.4\t54.9\nTriviaQA\t5-shot\t39.8\t65.8\t78.2\t85.5\nNatural Questions\t5-shot\t9.48\t20.0\t31.4\t36.1\nARC-c\t25-shot\t38.4\t56.2\t68.9\t70.6\nARC-e\t0-shot\t73.0\t82.4\t88.3\t89.0\nWinoGrande\t5-shot\t58.2\t64.7\t74.3\t78.8\nBIG-Bench Hard\tfew-shot\t28.4\t50.9\t72.6\t77.7\nDROP\t1-shot\t42.4\t60.1\t72.2\t77.2\nSTEM and code\nBenchmark\tMetric\tGemma 3 PT 4B\tGemma 3 PT 12B\tGemma 3 PT 27B\nMMLU\t5-shot\t59.6\t74.5\t78.6\nMMLU (Pro COT)\t5-shot\t29.2\t45.3\t52.2\nAGIEval\t3-5-shot\t42.1\t57.4\t66.2\nMATH\t4-shot\t24.2\t43.3\t50.0\nGSM8K\t8-shot\t38.4\t71.0\t82.6\nGPQA\t5-shot\t15.0\t25.4\t24.3\nMBPP\t3-shot\t46.0\t60.4\t65.6\nHumanEval\t0-shot\t36.0\t45.7\t48.8\nMultilingual\nBenchmark\tGemma 3 PT 1B\tGemma 3 PT 4B\tGemma 3 PT 12B\tGemma 3 PT 27B\nMGSM\t2.04\t34.7\t64.3\t74.3\nGlobal-MMLU-Lite\t24.9\t57.0\t69.4\t75.7\nWMT24++ (ChrF)\t36.7\t48.4\t53.9\t55.7\nFloRes\t29.5\t39.2\t46.0\t48.8\nXQuAD (all)\t43.9\t68.0\t74.5\t76.8\nECLeKTic\t4.69\t11.0\t17.2\t24.4\nIndicGenBench\t41.4\t57.2\t61.7\t63.4\nMultimodal\nBenchmark\tGemma 3 PT 4B\tGemma 3 PT 12B\tGemma 3 PT 27B\nCOCOcap\t102\t111\t116\nDocVQA (val)\t72.8\t82.3\t85.6\nInfoVQA (val)\t44.1\t54.8\t59.4\nMMMU (pt)\t39.2\t50.3\t56.1\nTextVQA (val)\t58.9\t66.5\t68.6\nRealWorldQA\t45.5\t52.2\t53.9\nReMI\t27.3\t38.5\t44.8\nAI2D\t63.2\t75.2\t79.0\nChartQA\t63.6\t74.7\t76.3\nVQAv2\t63.9\t71.2\t72.9\nBLINK\t38.0\t35.9\t39.6\nOKVQA\t51.0\t58.7\t60.2\nTallyQA\t42.5\t51.8\t54.3\nSpatialSense VQA\t50.9\t60.0\t59.4\nCountBenchQA\t26.1\t17.8\t68.0\nEthics and Safety\n\nEthics and safety evaluation approach and results.\n\nEvaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming testing of relevant content policies. Red-teaming was conducted by a number of different teams, each with different goals and human evaluation metrics. These models were evaluated against a number of different categories relevant to ethics and safety, including:\n\nChild Safety: Evaluation of text-to-text and image to text prompts covering child safety policies, including child sexual abuse and exploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts covering safety policies including, harassment, violence and gore, and hate speech.\nRepresentational Harms: Evaluation of text-to-text and image to text prompts covering safety policies including bias, stereotyping, and harmful associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance evaluations\" which are our 'arms-length' internal evaluations for responsibility governance decision making. They are conducted separately from the model development team, to inform decision making about release. High level findings are fed back to the model team, but prompt sets are held-out to prevent overfitting and preserve the results' ability to inform decision making. Assurance evaluation results are reported to our Responsibility & Safety Council as part of release review.\n\nEvaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of child safety, content safety, and representational harms relative to previous Gemma models. All testing was conducted without safety filters to evaluate the model capabilities and behaviors. For both text-to-text and image-to-text, and across all model sizes, the model produced minimal policy violations, and showed significant improvements over previous Gemma models' performance with respect to ungrounded inferences. A limitation of our evaluations was they included only English language prompts.\n\nUsage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\nIntended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development.\n\nContent Creation and Communication\nText Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus, research papers, or reports.\nImage Data Extraction: These models can be used to extract, interpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These models can serve as a foundation for researchers to experiment with VLM and NLP techniques, develop algorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas the model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging.\nA model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical concerns. In creating an open model, we have carefully considered the following:\n\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the Responsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share innovation by making VLM technology accessible to developers and researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\nPerpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content safety are essential. Developers are encouraged to exercise caution and implement appropriate content safety safeguards based on their specific product policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of VLMs. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in the Gemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of certain personal information and other sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques.\nBenefits\n\nAt the time of release, this family of models provides high-performance open vision-language model implementations designed from the ground up for responsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models have shown to provide superior performance to other, comparably-sized open model alternatives.",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"Safetensors\", \"Gemma Terms of Use\", \"arxiv:28 papers\"]"
  },
  {
    "url": "https://gitcode.com/openMind/QwQ-32B",
    "project_name": "QwQ-32B",
    "readme": "Original Text\nQwQ-32B\nç®€ä»‹\n\nQwQ æ˜¯ Qwen ç³»åˆ—ä¸­çš„æ¨ç†æ¨¡å‹ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼ŒQwQ è¿™ç§èƒ½å¤Ÿæ€è€ƒå’Œæ¨ç†çš„æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯éš¾åº¦è¾ƒé«˜çš„ä»»åŠ¡ä¸Šï¼Œå¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½ã€‚QwQ-32B æ˜¯ä¸€æ¬¾ä¸­å‹æ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿä¸å½“å‰æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ç›¸åª²ç¾ï¼Œä¾‹å¦‚ DeepSeek-R1ã€o1-miniã€‚\n\næ­¤ä»“åº“åŒ…å« QwQ 32B æ¨¡å‹ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š\n\nç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹\nè®­ç»ƒé˜¶æ®µï¼šé¢„è®­ç»ƒä¸åè®­ç»ƒï¼ˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼‰\næ¶æ„ï¼štransformersï¼ŒåŒ…å« RoPEã€SwiGLUã€RMSNorm å’Œæ³¨æ„åŠ› QKV åç½®\nå‚æ•°é‡ï¼š32.5B\néåµŒå…¥å‚æ•°é‡ï¼š31.0B\nå±‚æ•°ï¼š64\næ³¨æ„åŠ›å¤´æ•°ï¼ˆGQAï¼‰ï¼šQ ä¸º 40ï¼ŒKV ä¸º 8\nä¸Šä¸‹æ–‡é•¿åº¦ï¼šå®Œæ•´çš„ 131,072 ä»¤ç‰Œ\nå¯¹äºè¶…è¿‡ 8,192 ä»¤ç‰Œé•¿åº¦çš„æç¤ºï¼Œæ‚¨å¿…é¡»æŒ‰ç…§æœ¬èŠ‚ä¸­çš„è¯´æ˜å¯ç”¨ YaRNã€‚\n\næ³¨æ„ï¼š ä¸ºäº†è·å¾—æœ€ä½³ä½“éªŒï¼Œè¯·åœ¨éƒ¨ç½² QwQ æ¨¡å‹å‰æŸ¥é˜…ä½¿ç”¨æŒ‡å—ã€‚\n\næ‚¨å¯ä»¥å°è¯•æˆ‘ä»¬çš„æ¼”ç¤ºæˆ–é€šè¿‡QwenChatè®¿é—® QwQ æ¨¡å‹ã€‚\n\næ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„åšå®¢ã€GitHubä»¥åŠæ–‡æ¡£ã€‚\n\nç¯å¢ƒè¦æ±‚\n\nQwQ åŸºäº Qwen2.5ï¼Œå…¶ä»£ç å·²åŒ…å«åœ¨æœ€æ–°çš„ Hugging face transformers ä¸­ã€‚æˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨ transformers çš„æœ€æ–°ç‰ˆæœ¬ã€‚\n\nä½¿ç”¨ transformers<4.37.0ï¼Œæ‚¨ä¼šé‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š\n\nKeyError: 'qwen2'\n\nå¿«é€Ÿå…¥é—¨\n\nè¿™é‡Œæä¾›äº†ä¸€ä¸ªä½¿ç”¨ apply_chat_template çš„ä»£ç ç‰‡æ®µï¼Œå‘æ‚¨å±•ç¤ºå¦‚ä½•åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹ä»¥åŠå¦‚ä½•ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/QwQ-32B\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"How many r's are in the word \\\"strawberry\\\"\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\n\nä½¿ç”¨æŒ‡å—\n\nä¸ºäº†å®ç°æœ€ä½³æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®ä»¥ä¸‹è®¾ç½®ï¼š\n\nå¼ºåˆ¶è¾“å‡ºæ€è€ƒå†…å®¹ï¼šç¡®ä¿æ¨¡å‹ä»¥ \"<æ€è€ƒ>\\n\" å¼€å¤´ï¼Œä»¥é˜²æ­¢ç”Ÿæˆç©ºç™½çš„æ€è€ƒå†…å®¹ï¼Œè¿™å¯èƒ½é™ä½è¾“å‡ºè´¨é‡ã€‚å¦‚æœæ‚¨ä½¿ç”¨ apply_chat_template å¹¶è®¾ç½® add_generation_prompt=Trueï¼Œè¿™å·²ç»è‡ªåŠ¨å®ç°ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´å“åº”å¼€å¤´ç¼ºå°‘ <æ€è€ƒ> æ ‡ç­¾ã€‚è¿™æ˜¯æ­£å¸¸è¡Œä¸ºã€‚\n\né‡‡æ ·å‚æ•°ï¼š\n\nä½¿ç”¨ Temperature=0.6ï¼ŒTopP=0.95ï¼ŒMinP=0 ä»£æ›¿è´ªå¿ƒè§£ç ï¼Œä»¥é¿å…æ— ä¼‘æ­¢çš„é‡å¤ã€‚\nä½¿ç”¨ TopK åœ¨20åˆ°40ä¹‹é—´è¿‡æ»¤æ‰ç½•è§æ ‡è®°çš„å‡ºç°ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè¾“å‡ºçš„å¤šæ ·æ€§ã€‚\nå¯¹äºæ”¯æŒçš„æ¡†æ¶ï¼Œæ‚¨å¯ä»¥å°† presence_penalty å‚æ•°è°ƒæ•´ä¸º0åˆ°2ä¹‹é—´ï¼Œä»¥å‡å°‘æ— ä¼‘æ­¢çš„é‡å¤ã€‚ç„¶è€Œï¼Œä½¿ç”¨è¾ƒé«˜çš„å€¼å¯èƒ½ä¼šå¯¼è‡´å¶å°”çš„è¯­è¨€æ··æ·†å’Œæ€§èƒ½ç•¥å¾®ä¸‹é™ã€‚\n\nå†å²è®°å½•ä¸­æ— æ€è€ƒå†…å®¹ï¼šåœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œå†å²æ¨¡å‹è¾“å‡ºåº”ä»…åŒ…æ‹¬æœ€ç»ˆè¾“å‡ºéƒ¨åˆ†ï¼Œæ— éœ€åŒ…å«æ€è€ƒå†…å®¹ã€‚è¿™ä¸€ç‰¹æ€§å·²åœ¨ apply_chat_template ä¸­å®ç°ã€‚\n\næ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ï¼šåœ¨åŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨æç¤ºæ¥æ ‡å‡†åŒ–æ¨¡å‹è¾“å‡ºã€‚\n\næ•°å­¦é—®é¢˜ï¼šåœ¨æç¤ºä¸­åŒ…å« \"è¯·é€æ­¥æ¨ç†ï¼Œå¹¶å°†æœ€ç»ˆç­”æ¡ˆç½®äº \\boxed{} å†…ã€‚\"\né€‰æ‹©é¢˜ï¼šåœ¨æç¤ºä¸­æ·»åŠ ä»¥ä¸‹JSONç»“æ„æ¥æ ‡å‡†åŒ–å“åº”ï¼š \"è¯·åœ¨ answer å­—æ®µä¸­æ˜¾ç¤ºæ‚¨çš„é€‰æ‹©ï¼Œåªéœ€é€‰æ‹©å­—æ¯ï¼Œä¾‹å¦‚ï¼Œ\\\"answer\\\": \\\"C\\\"ã€‚\"\n\nå¤„ç†é•¿è¾“å…¥ï¼šå¯¹äºè¶…è¿‡8,192ä¸ªä»¤ç‰Œçš„è¾“å…¥ï¼Œå¯ç”¨ YaRN ä»¥æ”¹å–„æ¨¡å‹æœ‰æ•ˆæ•è·é•¿åºåˆ—ä¿¡æ¯çš„èƒ½åŠ›ã€‚\n\nå¯¹äºæ”¯æŒçš„æ¡†æ¶ï¼Œæ‚¨å¯ä»¥å°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ° config.json ä¸­ä»¥å¯ç”¨ YaRNï¼š\n\n{\n...,\n\"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n}\n}\n\n\nå¯¹äºéƒ¨ç½²ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ vLLMã€‚å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ vLLMï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ æ–‡æ¡£ äº†è§£ä½¿ç”¨æ–¹æ³•ã€‚ ç›®å‰ï¼ŒvLLM åªæ”¯æŒé™æ€ YARNï¼Œè¿™æ„å‘³ç€æ— è®ºè¾“å…¥é•¿åº¦å¦‚ä½•ï¼Œç¼©æ”¾å› å­éƒ½ä¿æŒä¸å˜ï¼Œå¯èƒ½å¯¹è¾ƒçŸ­çš„æ–‡æœ¬æ€§èƒ½äº§ç”Ÿå½±å“ã€‚ æˆ‘ä»¬å»ºè®®ä»…åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶æ‰æ·»åŠ  rope_scaling é…ç½®ã€‚\n\nè¯„ä¼°ä¸æ€§èƒ½\n\nè¯¦ç»†çš„è¯„ä¼°ç»“æœæŠ¥å‘Šåœ¨ ğŸ“‘åšå®¢ ä¸­ã€‚\n\næœ‰å…³ GPU å†…å­˜éœ€æ±‚å’Œç›¸åº”çš„ååé‡ç»“æœï¼Œè¯·å‚è§ æ­¤å¤„ã€‚\n\nå¼•ç”¨\n\nå¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©ï¼Œè¯·éšæ—¶ç»™æˆ‘ä»¬å¼•ç”¨ã€‚\n\n@misc{qwq32b,\n    title = {QwQ-32B: Embracing the Power of Reinforcement Learning},\n    url = {https://qwenlm.github.io/blog/qwq-32b/},\n    author = {Qwen Team},\n    month = {March},\n    year = {2025}\n}\n\n@article{qwen2.5,\n      title={Qwen2.5 Technical Report}, \n      author={An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},\n      journal={arXiv preprint arXiv:2412.15115},\n      year={2024}\n}\n\n\nå½“ç„¶ï¼Œæˆ‘ä¼šå°½åŠ›æ»¡è¶³æ‚¨çš„è¦æ±‚ã€‚ä¸è¿‡ï¼Œæ‚¨ä¼¼ä¹å¿˜è®°æä¾›éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ã€‚è¯·æä¾›åŸæ–‡ï¼Œæˆ‘å°†ä¼šå°†å…¶ç¿»è¯‘æˆä¸­æ–‡ï¼ŒåŒæ—¶ä¿æŒ Markdown æ ¼å¼å’Œæ‚¨æ‰€è¦æ±‚çš„å…¶ä»–æ ‡å‡†ã€‚",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Apache License 2.0\", \"chat\"]"
  },
  {
    "url": "https://gitcode.com/openMind/swin2SR_classical_sr_x2_64",
    "project_name": "swin2SR_classical_sr_x2_64",
    "readme": "Original Text\nSwin2SRæ¨¡å‹ï¼ˆå›¾åƒè¶…åˆ†è¾¨ç‡ï¼‰\n\nSwin2SRæ¨¡å‹èƒ½å°†å›¾åƒæ”¾å¤§è‡³2å€ã€‚è¯¥æ¨¡å‹åœ¨è®ºæ–‡ Swin2SR: SwinV2 Transformerç”¨äºå‹ç¼©å›¾åƒè¶…åˆ†è¾¨ç‡ä¸æ¢å¤ ä¸­ç”±Condeç­‰äººæå‡ºï¼Œå¹¶åœ¨ è¿™ä¸ªä»“åº“ ä¸­é¦–æ¬¡å‘å¸ƒã€‚\n\né¢„å®šåº”ç”¨åœºæ™¯\n\næœ¬æ¨¡å‹é€‚ç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡å¤„ç†ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\nè¯·å‚é˜… æ–‡æ¡£ã€‚",
    "tags": "[\"PyTorch\", \"Transformers\", \"Safetensors\", \"Apache License 2.0\", \"image-to-image\", \"vision\"]"
  },
  {
    "url": "https://gitcode.com/openMind/Qwen3-0.6B",
    "project_name": "Qwen3-0.6B",
    "readme": "Original Text\nQwen3-0.6B\nQwen3 äº®ç‚¹\n\nQwen3 æ˜¯é€šä¹‰åƒé—®ç³»åˆ—æœ€æ–°ä¸€ä»£å¤§è¯­è¨€æ¨¡å‹ï¼Œæä¾›å…¨ç³»åˆ—ç¨ å¯†ä¸æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ã€‚åŸºäºå¤§è§„æ¨¡è®­ç»ƒï¼ŒQwen3 åœ¨æ¨ç†ã€æŒ‡ä»¤éµå¾ªã€æ™ºèƒ½ä½“èƒ½åŠ›å’Œå¤šè¯­è¨€æ”¯æŒæ–¹é¢å®ç°çªç ´æ€§è¿›å±•ï¼Œä¸»è¦ç‰¹æ€§åŒ…æ‹¬ï¼š\n\nç‹¬æœ‰æ€ç»´æ¨¡å¼åˆ‡æ¢åŠŸèƒ½ï¼šåŒä¸€æ¨¡å‹å†…å¯æ— ç¼åˆ‡æ¢æ€ç»´æ¨¡å¼ï¼ˆé€‚ç”¨äºå¤æ‚é€»è¾‘æ¨ç†ã€æ•°å­¦ä¸ä»£ç åœºæ™¯ï¼‰ä¸éæ€ç»´æ¨¡å¼ï¼ˆé€‚ç”¨äºé«˜æ•ˆé€šç”¨å¯¹è¯ï¼‰ï¼Œç¡®ä¿å„ç±»åœºæ™¯æœ€ä¼˜è¡¨ç°ã€‚\næ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ï¼šåœ¨æ•°å­¦ã€ä»£ç ç”Ÿæˆä¸å¸¸è¯†é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­ï¼Œå…¨é¢è¶…è¶Šå‰ä»£ QwQï¼ˆæ€ç»´æ¨¡å¼ï¼‰ä¸ Qwen2.5 æŒ‡ä»¤æ¨¡å‹ï¼ˆéæ€ç»´æ¨¡å¼ï¼‰ã€‚\nå“è¶Šçš„äººç±»åå¥½å¯¹é½ï¼šåœ¨åˆ›æ„å†™ä½œã€è§’è‰²æ‰®æ¼”ã€å¤šè½®å¯¹è¯åŠæŒ‡ä»¤éµå¾ªæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæä¾›æ›´è‡ªç„¶ã€ç”ŸåŠ¨ä¸”æ²‰æµ¸å¼çš„å¯¹è¯ä½“éªŒã€‚\nä¸“ä¸šæ™ºèƒ½ä½“èƒ½åŠ›ï¼šæ”¯æŒæ€ç»´/éæ€ç»´æ¨¡å¼ä¸‹ç²¾å‡†è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œåœ¨å¤æ‚æ™ºèƒ½ä½“ä»»åŠ¡ä¸­è¾¾åˆ°å¼€æºæ¨¡å‹é¢†å…ˆæ°´å¹³ã€‚\næ”¯æŒè¶…100ç§è¯­è¨€ä¸æ–¹è¨€ï¼Œå…·å¤‡å¼ºå¤§çš„å¤šè¯­è¨€æŒ‡ä»¤ç†è§£ä¸ç¿»è¯‘èƒ½åŠ›ã€‚\næ¨¡å‹æ¦‚è§ˆ\n\nQwen3-0.6B æ ¸å¿ƒå‚æ•°å¦‚ä¸‹ï¼š\n\nç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹\nè®­ç»ƒé˜¶æ®µï¼šé¢„è®­ç»ƒ & åè®­ç»ƒ\nå‚æ•°é‡ï¼š0.6B\néåµŒå…¥å‚æ•°é‡ï¼š0.44B\nå±‚æ•°ï¼š28\næ³¨æ„åŠ›å¤´æ•°ï¼ˆGQAï¼‰ï¼šæŸ¥è¯¢å¤´16ä¸ªï¼Œé”®å€¼å¤´8ä¸ª\nä¸Šä¸‹æ–‡é•¿åº¦ï¼š32,768\n\nå®Œæ•´æŠ€æœ¯ç»†èŠ‚ï¼ˆå«åŸºå‡†æµ‹è¯•ã€ç¡¬ä»¶éœ€æ±‚ä¸æ¨ç†æ€§èƒ½ï¼‰è¯·å‚é˜…æˆ‘ä»¬çš„åšå®¢ã€GitHubåŠæ–‡æ¡£ã€‚\n\nTip\n\nè‹¥å‡ºç°ä¸¥é‡é‡å¤ç”Ÿæˆç°è±¡ï¼Œè¯·å‚è€ƒæœ€ä½³å®è·µç« èŠ‚è°ƒæ•´é‡‡æ ·å‚æ•°ï¼Œå»ºè®®å°†presence_penaltyè®¾ä¸º1.5ã€‚\n\nå¿«é€Ÿå¼€å§‹\n\nQwen3 ä»£ç å·²é›†æˆè‡³æœ€æ–°ç‰ˆ Hugging Face transformersï¼Œæ¨èä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ã€‚\n\nè‹¥ä½¿ç”¨transformers<4.51.0ï¼Œå°†è§¦å‘ä»¥ä¸‹é”™è¯¯ï¼š\n\nKeyError: 'qwen3'\n\n\nä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•åŸºäºç»™å®šè¾“å…¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n\n\nåœ¨éƒ¨ç½²æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ sglang>=0.4.6.post1 æˆ– vllm>=0.8.5 æ¥åˆ›å»ºå…¼å®¹ OpenAI çš„ API ç«¯ç‚¹ï¼š\n\nSGLangï¼š\npython -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --reasoning-parser qwen3\n\nvLLMï¼š\nvllm serve Qwen/Qwen3-0.6B --enable-reasoning --reasoning-parser deepseek_r1\n\n\nå¯¹äºæœ¬åœ°ä½¿ç”¨ï¼ŒOllamaã€LMStudioã€MLX-LMã€llama.cpp å’Œ KTransformers ç­‰åº”ç”¨ä¹Ÿå·²æ”¯æŒ Qwen3ã€‚\n\næ€ç»´æ¨¡å¼ä¸éæ€ç»´æ¨¡å¼åˆ‡æ¢\n\nTip\n\nenable_thinking å¼€å…³åŒæ ·é€‚ç”¨äº SGLang å’Œ vLLM åˆ›å»ºçš„ APIã€‚ å…·ä½“ä½¿ç”¨æ–¹æ³•è¯·å‚é˜… SGLang å’Œ vLLM çš„æ–‡æ¡£è¯´æ˜ã€‚\n\nenable_thinking=True\n\né»˜è®¤æƒ…å†µä¸‹ï¼ŒQwen3 å¯ç”¨äº†æ€ç»´åŠŸèƒ½ï¼Œç±»ä¼¼äº QwQ-32Bã€‚è¿™æ„å‘³ç€æ¨¡å‹ä¼šåˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›æ¥æå‡ç”Ÿæˆå›ç­”çš„è´¨é‡ã€‚ä¾‹å¦‚ï¼Œå½“æ˜ç¡®è®¾ç½® enable_thinking=True æˆ–åœ¨ tokenizer.apply_chat_template ä¸­ä¿ç•™é»˜è®¤å€¼æ—¶ï¼Œæ¨¡å‹å°†è¿›å…¥æ€ç»´æ¨¡å¼ã€‚\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n\n\nåœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹å°†ç”ŸæˆåŒ…è£¹åœ¨<think>...</think>åŒºå—ä¸­çš„æ€è€ƒå†…å®¹ï¼Œéšåè¾“å‡ºæœ€ç»ˆå“åº”ã€‚\n\n[!æ³¨æ„] å¯ç”¨æ€è€ƒæ¨¡å¼æ—¶ï¼Œè¯·ä½¿ç”¨Temperature=0.6ã€TopP=0.95ã€TopK=20ä»¥åŠMinP=0ï¼ˆå³generation_config.jsonä¸­çš„é»˜è®¤é…ç½®ï¼‰ã€‚åˆ‡å‹¿ä½¿ç”¨è´ªå©ªè§£ç ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ä¸æ— é™å¾ªç¯ã€‚æ›´è¯¦ç»†çš„æŒ‡å¯¼è¯·å‚é˜…æœ€ä½³å®è·µç« èŠ‚ã€‚\n\nenable_thinking=False\n\næˆ‘ä»¬æä¾›äº†å¼ºåˆ¶å…³é—­æ¨¡å‹æ€è€ƒè¡Œä¸ºçš„ç¡¬å¼€å…³ï¼Œä½¿å…¶åŠŸèƒ½ä¸å…ˆå‰å‘å¸ƒçš„Qwen2.5-Instructæ¨¡å‹ä¿æŒä¸€è‡´ã€‚è¯¥æ¨¡å¼åœ¨éœ€è¦ç¦ç”¨æ€è€ƒä»¥æå‡æ•ˆç‡çš„åœºæ™¯ä¸­å°¤ä¸ºå®ç”¨ã€‚\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n\n\nåœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ä¸ä¼šç”Ÿæˆä»»ä½•æ€è€ƒå†…å®¹ï¼Œä¹Ÿä¸ä¼šåŒ…å« <think>...</think> åŒºå—ã€‚\n\n[!æ³¨æ„] å¯¹äºéæ€è€ƒæ¨¡å¼ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ Temperature=0.7ã€TopP=0.8ã€TopK=20 å’Œ MinP=0ã€‚æ›´è¯¦ç»†çš„æŒ‡å¯¼ï¼Œè¯·å‚é˜…æœ€ä½³å®è·µéƒ¨åˆ†ã€‚\n\né«˜çº§ç”¨æ³•ï¼šé€šè¿‡ç”¨æˆ·è¾“å…¥åœ¨æ€è€ƒä¸éæ€è€ƒæ¨¡å¼é—´åˆ‡æ¢\n\næˆ‘ä»¬æä¾›äº†ä¸€ç§è½¯åˆ‡æ¢æœºåˆ¶ï¼Œå½“ enable_thinking=True æ—¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡åŠ¨æ€æ§åˆ¶æ¥è°ƒæ•´æ¨¡å‹è¡Œä¸ºã€‚å…·ä½“æ¥è¯´ï¼Œæ‚¨å¯ä»¥åœ¨ç”¨æˆ·æç¤ºæˆ–ç³»ç»Ÿæ¶ˆæ¯ä¸­æ·»åŠ  /think å’Œ /no_thinkï¼Œä»¥é€è½®åˆ‡æ¢æ¨¡å‹çš„æ€è€ƒæ¨¡å¼ã€‚åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œæ¨¡å‹å°†éµå¾ªæœ€è¿‘çš„æŒ‡ä»¤ã€‚\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªå¤šè½®å¯¹è¯çš„ç¤ºä¾‹ï¼š\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-0.6B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n\n\n[!æ³¨æ„] å‡ºäº API å…¼å®¹æ€§è€ƒè™‘ï¼Œå½“ enable_thinking=True æ—¶ï¼Œæ— è®ºç”¨æˆ·ä½¿ç”¨ /think è¿˜æ˜¯ /no_thinkï¼Œæ¨¡å‹å§‹ç»ˆä¼šè¾“å‡ºç”¨ <think>...</think> åŒ…è£¹çš„åŒºå—ã€‚ä½†è‹¥æ€è€ƒåŠŸèƒ½è¢«ç¦ç”¨ï¼Œè¯¥åŒºå—å†…çš„å†…å®¹å¯èƒ½ä¸ºç©ºã€‚ å½“ enable_thinking=False æ—¶ï¼Œè½¯å¼€å…³å°†å¤±æ•ˆã€‚æ— è®ºç”¨æˆ·è¾“å…¥ä»»ä½• /think æˆ– /no_think æ ‡ç­¾ï¼Œæ¨¡å‹éƒ½ä¸ä¼šç”Ÿæˆæ€è€ƒå†…å®¹ï¼Œä¹Ÿä¸ä¼šåŒ…å« <think>...</think> åŒºå—ã€‚\n\næ™ºèƒ½ä½“åº”ç”¨\n\nQwen3 åœ¨å·¥å…·è°ƒç”¨èƒ½åŠ›ä¸Šè¡¨ç°å“è¶Šã€‚æˆ‘ä»¬æ¨èä½¿ç”¨ Qwen-Agent æ¥å……åˆ†å‘æŒ¥ Qwen3 çš„æ™ºèƒ½ä½“èƒ½åŠ›ã€‚Qwen-Agent å†…éƒ¨å°è£…äº†å·¥å…·è°ƒç”¨æ¨¡æ¿å’Œå·¥å…·è°ƒç”¨è§£æå™¨ï¼Œå¯å¤§å¹…é™ä½ç¼–ç å¤æ‚åº¦ã€‚\n\næ‚¨å¯ä»¥é€šè¿‡ MCP é…ç½®æ–‡ä»¶å®šä¹‰å¯ç”¨å·¥å…·ï¼Œä½¿ç”¨ Qwen-Agent çš„é›†æˆå·¥å…·ï¼Œæˆ–è‡ªè¡Œé›†æˆå…¶ä»–å·¥å…·ã€‚\n\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-0.6B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n\næœ€ä½³å®è·µ\n\nä¸ºäº†è·å¾—æœ€ä½³æ€§èƒ½ï¼Œæˆ‘ä»¬æ¨èä»¥ä¸‹è®¾ç½®ï¼š\n\né‡‡æ ·å‚æ•°ï¼š\n\nåœ¨æ€è€ƒæ¨¡å¼ï¼ˆenable_thinking=Trueï¼‰ä¸‹ï¼Œå»ºè®®ä½¿ç”¨ Temperature=0.6ã€TopP=0.95ã€TopK=20 å’Œ MinP=0ã€‚åˆ‡å‹¿ä½¿ç”¨è´ªå¿ƒè§£ç ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™å’Œæ— é™é‡å¤ã€‚\nåœ¨éæ€è€ƒæ¨¡å¼ï¼ˆenable_thinking=Falseï¼‰ä¸‹ï¼Œå»ºè®®ä½¿ç”¨ Temperature=0.7ã€TopP=0.8ã€TopK=20 å’Œ MinP=0ã€‚\nå¯¹äºæ”¯æŒçš„æ¡†æ¶ï¼Œå¯å°† presence_penalty å‚æ•°è°ƒæ•´è‡³ 0 åˆ° 2 ä¹‹é—´ï¼Œä»¥å‡å°‘æ— é™é‡å¤ã€‚ä½†è¿‡é«˜çš„å€¼å¯èƒ½å¯¼è‡´å¶å‘çš„è¯­è¨€æ··æ‚æˆ–æ¨¡å‹æ€§èƒ½è½»å¾®ä¸‹é™ã€‚\n\nå……è¶³çš„è¾“å‡ºé•¿åº¦ï¼š\n\nå¯¹äºå¤§å¤šæ•°æŸ¥è¯¢ï¼Œå»ºè®®å°†è¾“å‡ºé•¿åº¦è®¾ä¸º 32,768 tokensã€‚\nåœ¨æ•°å­¦æˆ–ç¼–ç¨‹ç«èµ›ç­‰é«˜å¤æ‚åº¦é—®é¢˜çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå»ºè®®å°†æœ€å¤§è¾“å‡ºé•¿åº¦è®¾ä¸º 38,912 tokensã€‚è¿™èƒ½ä¸ºæ¨¡å‹æä¾›å……è¶³ç©ºé—´ç”Ÿæˆè¯¦ç»†ä¸”å…¨é¢çš„å›ç­”ï¼Œä»è€Œæå‡æ•´ä½“è¡¨ç°ã€‚\n\næ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ï¼š\n\næ•°å­¦é—®é¢˜ï¼šåœ¨æç¤ºè¯ä¸­åŠ å…¥â€œè¯·é€æ­¥æ¨ç†ï¼Œå¹¶å°†æœ€ç»ˆç­”æ¡ˆç½®äº \\boxed{} ä¸­ã€‚â€\né€‰æ‹©é¢˜ï¼šåœ¨æç¤ºè¯ä¸­æ·»åŠ ä»¥ä¸‹ JSON ç»“æ„ä»¥æ ‡å‡†åŒ–å›ç­”ï¼šâ€œè¯·åœ¨ answer å­—æ®µä¸­ä»…å¡«å†™é€‰é¡¹å­—æ¯ï¼Œä¾‹å¦‚ \"answer\": \"C\"ã€‚â€\n\nå†å²è®°å½•ä¸­ä¸åŒ…å«æ€è€ƒå†…å®¹ï¼š\n\nåœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œå†å²æ¨¡å‹è¾“å‡ºåº”ä»…åŒ…å«æœ€ç»ˆå›ç­”éƒ¨åˆ†ï¼Œæ— éœ€åŒ…å«æ€è€ƒå†…å®¹ã€‚è¯¥åŠŸèƒ½å·²åœ¨æä¾›çš„ Jinja2 èŠå¤©æ¨¡æ¿ä¸­å®ç°ã€‚è‹¥æ¡†æ¶æœªç›´æ¥ä½¿ç”¨è¯¥æ¨¡æ¿ï¼Œå¼€å‘è€…éœ€è‡ªè¡Œç¡®ä¿éµå¾ªæ­¤å®è·µã€‚\nå¼•ç”¨\n\nå¦‚æœæ‚¨è®¤ä¸ºæˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚\n\n@misc{qwen3,\n    title  = {Qwen3},\n    url    = {https://qwenlm.github.io/blog/qwen3/},\n    author = {Qwen Team},\n    month  = {April},\n    year   = {2025}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/AquilaChat-7b",
    "project_name": "AquilaChat-7b",
    "readme": "æ‚Ÿé“Â·å¤©é¹°ï¼ˆAquilaï¼‰\n\næ‚Ÿé“Â·å¤©é¹°ï¼ˆAquilaï¼‰ è¯­è¨€å¤§æ¨¡å‹æ˜¯é¦–ä¸ªå…·å¤‡ä¸­è‹±åŒè¯­çŸ¥è¯†ã€æ”¯æŒå•†ç”¨è®¸å¯åè®®ã€å›½å†…æ•°æ®åˆè§„éœ€æ±‚çš„å¼€æºè¯­è¨€å¤§æ¨¡å‹ã€‚\n\nğŸŒŸ æ”¯æŒå¼€æºå•†ç”¨è®¸å¯ã€‚Aquilaç³»åˆ—æ¨¡å‹çš„æºä»£ç åŸºäº Apache 2.0 åè®®ï¼Œæ¨¡å‹æƒé‡åŸºäºã€Šæ™ºæºAquilaç³»åˆ—æ¨¡å‹è®¸å¯åè®®ã€‹ï¼Œä½¿ç”¨è€…åœ¨æ»¡è¶³è®¸å¯é™åˆ¶çš„æƒ…å†µä¸‹ï¼Œå¯ç”¨äºå•†ä¸šç›®çš„ã€‚\nâœï¸ å…·å¤‡ä¸­è‹±æ–‡çŸ¥è¯†ã€‚Aquilaç³»åˆ—æ¨¡å‹åœ¨ä¸­è‹±æ–‡é«˜è´¨é‡è¯­æ–™åŸºç¡€ä¸Šä» 0 å¼€å§‹è®­ç»ƒï¼Œä¸­æ–‡è¯­æ–™çº¦å  40%ï¼Œä¿è¯æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µå°±å¼€å§‹ç§¯ç´¯åŸç”Ÿçš„ä¸­æ–‡ä¸–ç•ŒçŸ¥è¯†ï¼Œè€Œéç¿»è¯‘è€Œæ¥çš„çŸ¥è¯†ã€‚\nğŸ‘®â€â™€ï¸ç¬¦åˆå›½å†…æ•°æ®åˆè§„éœ€æ±‚ã€‚Aquilaç³»åˆ—æ¨¡å‹çš„ä¸­æ–‡è¯­æ–™æ¥è‡ªæ™ºæºå¤šå¹´ç§¯ç´¯çš„ä¸­æ–‡æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ¥è‡ª1ä¸‡å¤šä¸ªç«™æºçš„ä¸­æ–‡äº’è”ç½‘æ•°æ®ï¼ˆå…¶ä¸­99%ä»¥ä¸Šä¸ºå›½å†…ç«™æºï¼‰ï¼Œä»¥åŠè·å¾—å›½å†…æƒå¨æœºæ„æ”¯æŒçš„é«˜è´¨é‡ä¸­æ–‡æ–‡çŒ®æ•°æ®ã€ä¸­æ–‡ä¹¦ç±æ•°æ®ç­‰ã€‚æˆ‘ä»¬ä»åœ¨æŒç»­ç§¯ç´¯é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œå¹¶æºæºä¸æ–­åŠ å…¥AquilaåŸºç¡€æ¨¡å‹åç»­è®­ç»ƒä¸­ã€‚\nğŸ¯æŒç»­è¿­ä»£ï¼ŒæŒç»­å¼€æºå¼€æ”¾ã€‚æˆ‘ä»¬å°†ä¸æ–­å®Œå–„è®­ç»ƒæ•°æ®ã€ä¼˜åŒ–è®­ç»ƒæ–¹æ³•ã€æå‡æ¨¡å‹æ€§èƒ½ï¼Œåœ¨æ›´ä¼˜ç§€çš„åŸºç¡€æ¨¡å‹åŸºåº§ä¸Šï¼ŒåŸ¹è‚²æç¹å¶èŒ‚çš„â€œæ¨¡å‹æ ‘â€ï¼ŒæŒç»­å¼€æºå¼€æ”¾æ›´æ–°çš„ç‰ˆæœ¬ã€‚\n\næ‚Ÿé“ Â· å¤©é¹° Aquila æ¨¡å‹çš„æ›´å¤šç»†èŠ‚å°†åœ¨å®˜æ–¹æŠ€æœ¯æŠ¥å‘Šä¸­å‘ˆç°ã€‚è¯·å…³æ³¨å®˜æ–¹æ¸ é“æ›´æ–°ã€‚åŒ…æ‹¬ FlagAI GitHubä»“åº“ï¼ŒFlagAI çŸ¥ä¹è´¦å·ã€FlagAI å®˜æ–¹æŠ€æœ¯äº¤æµç¾¤ã€æ™ºæºç ”ç©¶é™¢å¾®ä¿¡å…¬ä¼—å·ã€æ™ºæºç¤¾åŒºå¾®ä¿¡å…¬ä¼—å·ã€‚\n\næ¨¡å‹\tæ¨¡å‹ç±»å‹\tç®€ä»‹\tçŠ¶æ€\tè®­ç»ƒæ‰€ç”¨æ˜¾å¡\nAquila-7B\tåŸºç¡€æ¨¡å‹ï¼Œ70äº¿å‚æ•°\tAquila åŸºç¡€æ¨¡å‹åœ¨æŠ€æœ¯ä¸Šç»§æ‰¿äº† GPT-3ã€LLaMA ç­‰çš„æ¶æ„è®¾è®¡ä¼˜ç‚¹ï¼Œæ›¿æ¢äº†ä¸€æ‰¹æ›´é«˜æ•ˆçš„åº•å±‚ç®—å­å®ç°ã€é‡æ–°è®¾è®¡å®ç°äº†ä¸­è‹±åŒè¯­çš„ tokenizerï¼Œå‡çº§äº† BMTrain å¹¶è¡Œè®­ç»ƒæ–¹æ³•ï¼Œå®ç°äº†æ¯” Magtron+DeepSpeed ZeRO-2 å°†è¿‘ï¼˜å€çš„è®­ç»ƒæ•ˆç‡ã€‚\tå·²å‘å¸ƒ\tNvidia-A100\nAquila-33B\tåŸºç¡€æ¨¡å‹ï¼Œ330äº¿å‚æ•°\tåŒä¸Š\tæ•¬è¯·æœŸå¾…\tNvidia-A100\nAquilaChat-7B\tSFT modelï¼ŒåŸºäº Aquila-7B è¿›è¡Œå¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ \tAquilaChat å¯¹è¯æ¨¡å‹æ”¯æŒæµç•…çš„æ–‡æœ¬å¯¹è¯åŠå¤šç§è¯­è¨€ç±»ç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡å®šä¹‰å¯æ‰©å±•çš„ç‰¹æ®ŠæŒ‡ä»¤è§„èŒƒï¼Œå®ç° AquilaChatå¯¹å…¶å®ƒæ¨¡å‹å’Œå·¥å…·çš„è°ƒç”¨ï¼Œä¸”æ˜“äºæ‰©å±•ã€‚\n\nä¾‹å¦‚ï¼Œè°ƒç”¨æ™ºæºå¼€æºçš„ AltDiffusion å¤šè¯­è¨€æ–‡å›¾ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°äº†æµç•…çš„æ–‡å›¾ç”Ÿæˆèƒ½åŠ›ã€‚é…åˆæ™ºæº InstructFace å¤šæ­¥å¯æ§æ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œè½»æ¾å®ç°å¯¹äººè„¸å›¾åƒçš„å¤šæ­¥å¯æ§ç¼–è¾‘ã€‚\tå·²å‘å¸ƒ\tNvidia-A100\nAquilaChat-33B\tSFT modelï¼ŒåŸºäº Aquila-33B è¿›è¡Œå¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ \tåŒä¸Š\tæ•¬è¯·æœŸå¾…\tNvidia-A100\nAquilaCode-7B-NV\tåŸºç¡€æ¨¡å‹ï¼Œâ€œæ–‡æœ¬-ä»£ç â€ç”Ÿæˆæ¨¡å‹ï¼ŒåŸºäº Aquila-7Bç»§ç»­é¢„è®­ç»ƒï¼Œåœ¨è‹±ä¼Ÿè¾¾èŠ¯ç‰‡å®Œæˆè®­ç»ƒ\tAquilaCode-7B ä»¥å°æ•°æ®é›†ã€å°å‚æ•°é‡ï¼Œå®ç°é«˜æ€§èƒ½ï¼Œæ˜¯ç›®å‰æ”¯æŒä¸­è‹±åŒè¯­çš„ã€æ€§èƒ½æœ€å¥½çš„å¼€æºä»£ç æ¨¡å‹ï¼Œç»è¿‡äº†é«˜è´¨é‡è¿‡æ»¤ã€ä½¿ç”¨æœ‰åˆè§„å¼€æºè®¸å¯çš„è®­ç»ƒä»£ç æ•°æ®è¿›è¡Œè®­ç»ƒã€‚\n\nAquilaCode-7B åˆ†åˆ«åœ¨è‹±ä¼Ÿè¾¾å’Œå›½äº§èŠ¯ç‰‡ä¸Šå®Œæˆäº†ä»£ç æ¨¡å‹çš„è®­ç»ƒã€‚\tgithubå·²å‘å¸ƒ\tNvidia-A100\nAquilaCode-7B-TS\tåŸºç¡€æ¨¡å‹ï¼Œâ€œæ–‡æœ¬-ä»£ç â€ç”Ÿæˆæ¨¡å‹ï¼ŒåŸºäº Aquila-7Bç»§ç»­é¢„è®­ç»ƒï¼Œåœ¨å¤©æ•°æ™ºèŠ¯èŠ¯ç‰‡ä¸Šå®Œæˆè®­ç»ƒ\tåŒä¸Š\tgithubå·²å‘å¸ƒ\tTianshu-BI-V100\n\næ‚Ÿé“Â·å¤©é¹°Aquilaç³»åˆ—æ¨¡å‹å°†æŒç»­å¼€æºæ›´ä¼˜ç‰ˆæœ¬ã€‚\n\n2023/07/14 ï¼šå¼€æº v0.8\nAquila-7B-01 md5: b14329f7314c05dd79d44b2838c315aa\nAquila-7B-02 md5: 88aa286283c7b7dd78c0fbb7fae6327d\nAquilaChat-7B-01 md5: 0a77901af35d3e5ed16eeafa622e2173\nAquilaChat-7B-02 md5: 6e84423fe2837c79c0ced6817c316bd4\n\nAquilaChat-7B v0.8 åœ¨ FlagEval å¤§æ¨¡å‹è¯„æµ‹ä¸­ï¼ˆ â€œä¸»è§‚+å®¢è§‚â€ï¼‰ç›¸æ¯”0.7çš„ç‰ˆæœ¬æ•´ä½“ç¨æœ‰æï¼Œå…¶åœ¨Chinese-MMLUä¸Šæå‡10%å·¦å³ã€‚è¯¦ç»†è¯„æµ‹ç»“æœè¯·é€šè¿‡ http://flageval.baai.ac.cn ç½‘ç«™æŸ¥çœ‹ï¼Œ å†å²ç‰ˆæœ¬å˜æ›´è¯¦æƒ…è§ï¼šå˜æ›´æ—¥å¿— ã€‚\n\nå¿«é€Ÿå¼€å§‹ä½¿ç”¨ AquilaChat-7B å¯¹è¯æ¨¡å‹\nä½¿ç”¨æ–¹å¼/How to use\n1. æ¨ç†/Inference\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ndevice = torch.device(\"cuda:1\")\n\nmodel_info = \"BAAI/AquilaChat-7B\"\ntokenizer = AutoTokenizer.from_pretrained(model_info, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_info, trust_remote_code=True)\nmodel.eval()\nmodel.to(device)\n\ntext = \"è¯·ç»™å‡º10ä¸ªè¦åˆ°åŒ—äº¬æ—…æ¸¸çš„ç†ç”±ã€‚\"\n\ntokens = tokenizer.encode_plus(text)['input_ids'][:-1]\n\ntokens = torch.tensor(tokens)[None,].to(device)\n\n\nwith torch.no_grad():\n    out = model.generate(tokens, do_sample=True, max_length=512, eos_token_id=100007)[0]\n\n    out = tokenizer.decode(out.cpu().numpy().tolist())\n    if \"###\" in out:\n        special_index = out.index(\"###\")\n        out = out[: special_index]\n\n    if \"[UNK]\" in out:\n        special_index = out.index(\"[UNK]\")\n        out = out[:special_index]\n\n    if \"</s>\" in out:\n        special_index = out.index(\"</s>\")\n        out = out[: special_index]\n\n    if len(out) > 0 and out[0] == \" \":\n        out = out[1:]\n    print(out)\n\nè¯ä¹¦/License\n\nAquilaChatç³»åˆ—å¼€æºæ¨¡å‹ä½¿ç”¨ æ™ºæºAquilaç³»åˆ—æ¨¡å‹è®¸å¯åè®®",
    "tags": "[\"PyTorch\", \"Transformers\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/DeepSeek-R1-Distill-Qwen-1.5B",
    "project_name": "DeepSeek-R1-Distill-Qwen-1.5B",
    "readme": "Original Text\nDeepSeek-R1\n  \n  \n\nè®ºæ–‡é“¾æ¥ğŸ‘ï¸\n\n1. ç®€ä»‹\n\næˆ‘ä»¬æ¨å‡ºäº†ç¬¬ä¸€ä»£æ¨ç†æ¨¡å‹â€”â€”DeepSeek-R1-Zero å’Œ DeepSeek-R1ã€‚\nDeepSeek-R1-Zero æ˜¯ä¸€ç§é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè€Œæˆçš„æ¨¡å‹ï¼Œæ— éœ€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºå‰æœŸæ­¥éª¤ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šå±•ç°äº†å“è¶Šæ€§èƒ½ã€‚\né€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ŒDeepSeek-R1-Zero è‡ªç„¶æ¶Œç°å‡ºè®¸å¤šå¼ºå¤§è€Œæœ‰è¶£çš„æ¨ç†è¡Œä¸ºã€‚\nç„¶è€Œï¼ŒDeepSeek-R1-Zero ä¹Ÿé¢ä¸´è¯¸å¦‚æ— é™é‡å¤ã€å¯è¯»æ€§å·®å’Œè¯­è¨€æ··æ‚ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜å¹¶è¿›ä¸€æ­¥æå‡æ¨ç†æ€§èƒ½ï¼Œ\næˆ‘ä»¬æ¨å‡ºäº† DeepSeek-R1ï¼Œå®ƒåœ¨å¼ºåŒ–å­¦ä¹ å‰å¼•å…¥äº†å†·å¯åŠ¨æ•°æ®ã€‚\nDeepSeek-R1 åœ¨æ•°å­¦ã€ä»£ç å’Œæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ OpenAI-o1 ç›¸å½“ã€‚\nä¸ºæ”¯æŒç ”ç©¶ç¤¾åŒºï¼Œæˆ‘ä»¬å¼€æºäº† DeepSeek-R1-Zeroã€DeepSeek-R1ï¼Œä»¥åŠåŸºäº Llama å’Œ Qwen ä» DeepSeek-R1 è’¸é¦å‡ºçš„å…­æ¬¾å¯†é›†æ¨¡å‹ã€‚å…¶ä¸­ï¼ŒDeepSeek-R1-Distill-Qwen-32B åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Š OpenAI-o1-miniï¼Œä¸ºå¯†é›†æ¨¡å‹åˆ›ä¸‹äº†æ–°çš„æ€§èƒ½æ ‡æ†ã€‚\n\næ³¨æ„ï¼šåœ¨æœ¬åœ°è¿è¡Œ DeepSeek-R1 ç³»åˆ—æ¨¡å‹å‰ï¼Œå»ºè®®æŸ¥é˜… ä½¿ç”¨å»ºè®® éƒ¨åˆ†ã€‚\n\n2. æ¨¡å‹æ¦‚è§ˆ\n\nåè®­ç»ƒï¼šåŸºäºåŸºç¡€æ¨¡å‹çš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ \n\næˆ‘ä»¬ç›´æ¥å¯¹åŸºç¡€æ¨¡å‹åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæ— éœ€ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºå‰æœŸæ­¥éª¤ã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿæ¢ç´¢é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä»¥è§£å†³å¤æ‚é—®é¢˜ï¼Œä»è€Œå¼€å‘å‡º DeepSeek-R1-Zeroã€‚DeepSeek-R1-Zero å±•ç°äº†è‡ªæˆ‘éªŒè¯ã€åæ€å’Œç”Ÿæˆé•¿é“¾å¼æ€ç»´çš„èƒ½åŠ›ï¼Œä¸ºç ”ç©¶ç¤¾åŒºæ ‘ç«‹äº†é‡è¦é‡Œç¨‹ç¢‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯é¦–ä¸ªéªŒè¯çº¯å¼ºåŒ–å­¦ä¹ ï¼ˆæ— éœ€ SFTï¼‰å¯æ¿€åŠ±å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…¬å¼€ç ”ç©¶ï¼Œä¸ºæœªæ¥ç›¸å…³é¢†åŸŸçš„çªç ´é“ºå¹³äº†é“è·¯ã€‚\n\næˆ‘ä»¬ä»‹ç»äº†å¼€å‘ DeepSeek-R1 çš„æµç¨‹ã€‚è¯¥æµç¨‹åŒ…å«ä¸¤ä¸ªå¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œæ—¨åœ¨å‘ç°æ›´ä¼˜çš„æ¨ç†æ¨¡å¼å¹¶ä¸äººç±»åå¥½å¯¹é½ï¼Œä»¥åŠä¸¤ä¸ªç›‘ç£å¾®è°ƒé˜¶æ®µï¼Œä½œä¸ºæ¨¡å‹æ¨ç†å’Œéæ¨ç†èƒ½åŠ›çš„ç§å­ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸€æµç¨‹å°†é€šè¿‡æ‰“é€ æ›´ä¼˜æ¨¡å‹é€ ç¦è¡Œä¸šã€‚\n\nè’¸é¦ï¼šå°æ¨¡å‹ä¹Ÿèƒ½å¼ºå¤§\n\næˆ‘ä»¬è¯æ˜ï¼Œå¤§æ¨¡å‹çš„æ¨ç†æ¨¡å¼å¯è¢«è’¸é¦è‡³å°æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¼˜äºåœ¨å°æ¨¡å‹ä¸Šé€šè¿‡å¼ºåŒ–å­¦ä¹ å‘ç°çš„æ¨ç†æ¨¡å¼ã€‚å¼€æºçš„ DeepSeek-R1 åŠå…¶ API å°†åŠ©åŠ›ç ”ç©¶ç¤¾åŒºæœªæ¥è’¸é¦å‡ºæ›´ä¼˜çš„å°æ¨¡å‹ã€‚\nåˆ©ç”¨ DeepSeek-R1 ç”Ÿæˆçš„æ¨ç†æ•°æ®ï¼Œæˆ‘ä»¬å¯¹ç ”ç©¶ç¤¾åŒºå¹¿æ³›ä½¿ç”¨çš„å‡ æ¬¾å¯†é›†æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè’¸é¦åçš„å°å‹å¯†é›†æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æˆ‘ä»¬å‘ç¤¾åŒºå¼€æºäº†åŸºäº Qwen2.5 å’Œ Llama3 ç³»åˆ—çš„ 1.5Bã€7Bã€8Bã€14Bã€32B å’Œ 70B è’¸é¦æ¨¡å‹ã€‚\n3. æ¨¡å‹ä¸‹è½½\nDeepSeek-R1 æ¨¡å‹\næ¨¡å‹\tæ€»å‚æ•°é‡\tæ¿€æ´»å‚æ•°é‡\tä¸Šä¸‹æ–‡é•¿åº¦\tä¸‹è½½é“¾æ¥\nDeepSeek-R1-Zero\t671B\t37B\t128K\tğŸ¤— HuggingFace\nDeepSeek-R1\t671B\t37B\t128K\tğŸ¤— HuggingFace\n\nDeepSeek-R1-Zero å’Œ DeepSeek-R1 åŸºäº DeepSeek-V3-Base è®­ç»ƒè€Œæˆã€‚\nå…³äºæ¨¡å‹æ¶æ„çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è€ƒ DeepSeek-V3 ä»“åº“ã€‚\n\nDeepSeek-R1-Distill æ¨¡å‹\næ¨¡å‹\tåŸºç¡€æ¨¡å‹\tä¸‹è½½é“¾æ¥\nDeepSeek-R1-Distill-Qwen-1.5B\tQwen2.5-Math-1.5B\tğŸ¤— HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\tQwen2.5-Math-7B\tğŸ¤— HuggingFace\nDeepSeek-R1-Distill-Llama-8B\tLlama-3.1-8B\tğŸ¤— HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\tQwen2.5-14B\tğŸ¤— HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\tQwen2.5-32B\tğŸ¤— HuggingFace\nDeepSeek-R1-Distill-Llama-70B\tLlama-3.3-70B-Instruct\tğŸ¤— HuggingFace\n\nDeepSeek-R1-Distill æ¨¡å‹åŸºäºå¼€æºæ¨¡å‹å¾®è°ƒè€Œæˆï¼Œä½¿ç”¨äº† DeepSeek-R1 ç”Ÿæˆçš„æ ·æœ¬ã€‚\næˆ‘ä»¬ç•¥å¾®è°ƒæ•´äº†å®ƒä»¬çš„é…ç½®å’Œåˆ†è¯å™¨ã€‚è¯·ä½¿ç”¨æˆ‘ä»¬çš„è®¾ç½®è¿è¡Œè¿™äº›æ¨¡å‹ã€‚\n\n4. è¯„æµ‹ç»“æœ\nDeepSeek-R1 è¯„æµ‹\n\næˆ‘ä»¬æ‰€æœ‰æ¨¡å‹çš„ç”Ÿæˆé•¿åº¦ä¸Šé™å‡è®¾ä¸º 32,768 tokensã€‚å¯¹äºéœ€è¦é‡‡æ ·çš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¸©åº¦å‚æ•° \n0.60.6\n0.6ã€top-p å€¼ \n0.950.95\n0.95ï¼Œå¹¶ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆ 64 ä¸ªå“åº”ä»¥ä¼°ç®— pass@1 æŒ‡æ ‡ã€‚\n\nç±»åˆ«\tåŸºå‡†æµ‹è¯•ï¼ˆæŒ‡æ ‡ï¼‰\tClaude-3.5-Sonnet-1022\tGPT-4o 0513\tDeepSeek V3\tOpenAI o1-mini\tOpenAI o1-1217\tDeepSeek R1\n\tæ¶æ„\t-\t-\tMoE\t-\t-\tMoE\n\tæ¿€æ´»å‚æ•°é‡\t-\t-\t37B\t-\t-\t37B\n\tæ€»å‚æ•°é‡\t-\t-\t671B\t-\t-\t671B\nè‹±æ–‡èƒ½åŠ›\tMMLU (Pass@1)\t88.3\t87.2\t88.5\t85.2\t91.8\t90.8\n\tMMLU-Redux (EM)\t88.9\t88.0\t89.1\t86.7\t-\t92.9\n\tMMLU-Pro (EM)\t78.0\t72.6\t75.9\t80.3\t-\t84.0\n\tDROP (3-shot F1)\t88.3\t83.7\t91.6\t83.9\t90.2\t92.2\n\tIF-Eval (Prompt Strict)\t86.5\t84.3\t86.1\t84.8\t-\t83.3\n\tGPQA-Diamond (Pass@1)\t65.0\t49.9\t59.1\t60.0\t75.7\t71.5\n\tSimpleQA (Correct)\t28.4\t38.2\t24.9\t7.0\t47.0\t30.1\n\tFRAMES (Acc.)\t72.5\t80.5\t73.3\t76.9\t-\t82.5\n\tAlpacaEval2.0 (LC-winrate)\t52.0\t51.1\t70.0\t57.8\t-\t87.6\n\tArenaHard (GPT-4-1106)\t85.2\t80.4\t85.5\t92.0\t-\t92.3\nç¼–ç¨‹èƒ½åŠ›\tLiveCodeBench (Pass@1-COT)\t33.8\t34.2\t-\t53.8\t63.4\t65.9\n\tCodeforces (Percentile)\t20.3\t23.6\t58.7\t93.4\t96.6\t96.3\n\tCodeforces (Rating)\t717\t759\t1134\t1820\t2061\t2029\n\tSWE Verified (Resolved)\t50.8\t38.8\t42.0\t41.6\t48.9\t49.2\n\tAider-Polyglot (Acc.)\t45.3\t16.0\t49.6\t32.9\t61.7\t53.3\næ•°å­¦èƒ½åŠ›\tAIME 2024 (Pass@1)\t16.0\t9.3\t39.2\t63.6\t79.2\t79.8\n\tMATH-500 (Pass@1)\t78.3\t74.6\t90.2\t90.0\t96.4\t97.3\n\tCNMO 2024 (Pass@1)\t13.1\t10.8\t43.2\t67.6\t-\t78.8\nä¸­æ–‡èƒ½åŠ›\tCLUEWSC (EM)\t85.4\t87.9\t90.9\t89.9\t-\t92.8\n\tC-Eval (EM)\t76.7\t76.0\t86.5\t68.9\t-\t91.8\n\tC-SimpleQA (Correct)\t55.4\t58.7\t68.0\t40.3\t-\t63.7\nè’¸é¦æ¨¡å‹è¯„æµ‹\næ¨¡å‹\tAIME 2024 pass@1\tAIME 2024 cons@64\tMATH-500 pass@1\tGPQA Diamond pass@1\tLiveCodeBench pass@1\tCodeForces rating\nGPT-4o-0513\t9.3\t13.4\t74.6\t49.9\t32.9\t759\nClaude-3.5-Sonnet-1022\t16.0\t26.7\t78.3\t65.0\t38.9\t717\no1-mini\t63.6\t80.0\t90.0\t60.0\t53.8\t1820\nQwQ-32B-Preview\t44.0\t60.0\t90.6\t54.5\t41.9\t1316\nDeepSeek-R1-Distill-Qwen-1.5B\t28.9\t52.7\t83.9\t33.8\t16.9\t954\nDeepSeek-R1-Distill-Qwen-7B\t55.5\t83.3\t92.8\t49.1\t37.6\t1189\nDeepSeek-R1-Distill-Qwen-14B\t69.7\t80.0\t93.9\t59.1\t53.1\t1481\nDeepSeek-R1-Distill-Qwen-32B\t72.6\t83.3\t94.3\t62.1\t57.2\t1691\nDeepSeek-R1-Distill-Llama-8B\t50.4\t80.0\t89.1\t49.0\t39.6\t1205\nDeepSeek-R1-Distill-Llama-70B\t70.0\t86.7\t94.5\t65.2\t57.5\t1633\n5. èŠå¤©ç½‘ç«™ä¸ API å¹³å°\n\næ‚¨å¯ä»¥åœ¨ DeepSeek å®˜æ–¹ç½‘ç«™ chat.deepseek.com ä¸ DeepSeek-R1 å¯¹è¯ï¼Œå¹¶å¼€å¯ \"DeepThink\" åŠŸèƒ½ã€‚\n\næˆ‘ä»¬è¿˜æä¾› OpenAI å…¼å®¹çš„ API æœåŠ¡ï¼Œè®¿é—® platform.deepseek.com å³å¯ä½¿ç”¨ã€‚\n\n6. æœ¬åœ°è¿è¡ŒæŒ‡å—\nDeepSeek-R1 æ¨¡å‹\n\nè¯·è®¿é—® DeepSeek-V3 ä»“åº“ï¼Œè·å–æœ¬åœ°è¿è¡Œ DeepSeek-R1 çš„æ›´å¤šä¿¡æ¯ã€‚\n\næ³¨æ„ï¼šHugging Face Transformers å°šæœªç›´æ¥æ”¯æŒè¯¥æ¨¡å‹ã€‚\n\nDeepSeek-R1-Distill æ¨¡å‹\n\nDeepSeek-R1-Distill æ¨¡å‹çš„ä½¿ç”¨æ–¹å¼ä¸ Qwen æˆ– Llama æ¨¡å‹ç›¸åŒã€‚\n\nä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ vLLM å¿«é€Ÿå¯åŠ¨æœåŠ¡ï¼š\n\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n\n\næ‚¨è¿˜å¯ä»¥è½»æ¾ä½¿ç”¨ SGLang å¯åŠ¨æœåŠ¡\n\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n\nä½¿ç”¨å»ºè®®\n\nä¸ºå……åˆ†å‘æŒ¥ DeepSeek-R1 ç³»åˆ—æ¨¡å‹çš„é¢„æœŸæ€§èƒ½ï¼ˆåŒ…æ‹¬åŸºå‡†æµ‹è¯•ï¼‰ï¼Œæˆ‘ä»¬å»ºè®®éµå¾ªä»¥ä¸‹é…ç½®ï¼š\n\nå°†æ¸©åº¦å‚æ•°è®¾ç½®åœ¨ 0.5-0.7 ä¹‹é—´ï¼ˆæ¨è 0.6ï¼‰ï¼Œä»¥é¿å…è¾“å‡ºæ— é™é‡å¤æˆ–é€»è¾‘æ··ä¹±ã€‚\nè¯·å‹¿æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼Œæ‰€æœ‰æŒ‡ä»¤åº”åŒ…å«åœ¨ç”¨æˆ·æç¤ºä¸­ã€‚\nå¤„ç†æ•°å­¦é—®é¢˜æ—¶ï¼Œå»ºè®®åœ¨æç¤ºä¸­åŠ å…¥æŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼šâ€œè¯·é€æ­¥æ¨ç†ï¼Œå¹¶å°†æœ€ç»ˆç­”æ¡ˆç½®äº \\boxed{} ä¸­ã€‚â€\nè¯„ä¼°æ¨¡å‹æ€§èƒ½æ—¶ï¼Œå»ºè®®è¿›è¡Œå¤šæ¬¡æµ‹è¯•å¹¶å–å¹³å‡å€¼ã€‚\n\næ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç° DeepSeek-R1 ç³»åˆ—æ¨¡å‹åœ¨å“åº”æŸäº›æŸ¥è¯¢æ—¶å¯èƒ½è·³è¿‡æ€è€ƒæ¨¡å¼ï¼ˆå³è¾“å‡ºâ€œ<think>\\n\\n</think>â€ï¼‰ï¼Œè¿™å¯èƒ½å½±å“æ¨¡å‹è¡¨ç°ã€‚\nä¸ºç¡®ä¿æ¨¡å‹å……åˆ†æ¨ç†ï¼Œå»ºè®®å¼ºåˆ¶æ¨¡å‹åœ¨æ¯æ¬¡è¾“å‡ºæ—¶ä»¥â€œ<think>\\nâ€å¼€å¤´ã€‚\n\n7. è®¸å¯å£°æ˜\n\næœ¬ä»£ç ä»“åº“åŠæ¨¡å‹æƒé‡éµå¾ª MIT è®¸å¯è¯ã€‚\nDeepSeek-R1 ç³»åˆ—æ”¯æŒå•†ä¸šç”¨é€”ï¼Œå…è®¸ä»»ä½•ä¿®æ”¹å’Œè¡ç”Ÿä½œå“ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºç”¨äºè®­ç»ƒå…¶ä»–å¤§æ¨¡å‹çš„è’¸é¦è¡Œä¸ºï¼‰ã€‚è¯·æ³¨æ„ï¼š\n\nDeepSeek-R1-Distill-Qwen-1.5Bã€DeepSeek-R1-Distill-Qwen-7Bã€DeepSeek-R1-Distill-Qwen-14B å’Œ DeepSeek-R1-Distill-Qwen-32B è¡ç”Ÿè‡ª Qwen-2.5 ç³»åˆ—ï¼Œå…¶åŸå§‹è®¸å¯ä¸º Apache 2.0 è®¸å¯è¯ï¼Œåç» DeepSeek-R1 ç²¾é€‰çš„ 80 ä¸‡æ ·æœ¬å¾®è°ƒã€‚\nDeepSeek-R1-Distill-Llama-8B è¡ç”Ÿè‡ª Llama3.1-8B-Baseï¼ŒåŸå§‹è®¸å¯ä¸º llama3.1 è®¸å¯è¯ã€‚\nDeepSeek-R1-Distill-Llama-70B è¡ç”Ÿè‡ª Llama3.3-70B-Instructï¼ŒåŸå§‹è®¸å¯ä¸º llama3.3 è®¸å¯è¯ã€‚\n8. å¼•ç”¨å£°æ˜\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n\n9. è”ç³»æˆ‘ä»¬\n\nå¦‚æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·æäº¤é—®é¢˜æˆ–é€šè¿‡é‚®ä»¶ service@deepseek.com ä¸æˆ‘ä»¬å–å¾—è”ç³»ã€‚",
    "tags": "[\"Transformers\", \"Safetensors\", \"MIT\"]"
  },
  {
    "url": "https://gitcode.com/openMind/deepseek-coder-33b-instruct",
    "project_name": "deepseek-coder-33b-instruct",
    "readme": "Original Text\n\n[ğŸ ä¸»é¡µ] | [ğŸ¤– ä¸DeepSeek Coderå¯¹è¯] | [Discord] | [å¾®ä¿¡]\n\n1. Deepseek Coder ç®€ä»‹\n\nDeepseek Coder ç”±ä¸€ç³»åˆ—ä»£ç å¤§æ¨¡å‹ç»„æˆï¼Œæ¯ä¸ªæ¨¡å‹å‡ä»é›¶å¼€å§‹åœ¨ 2T tokens ä¸Šè®­ç»ƒè€Œæˆï¼Œå…¶ä¸­ä»£ç å æ¯” 87%ï¼Œä¸­è‹±æ–‡è‡ªç„¶è¯­è¨€å æ¯” 13%ã€‚æˆ‘ä»¬æä¾›äº†ä» 1B åˆ° 33B ä¸åŒè§„æ¨¡çš„ä»£ç æ¨¡å‹ã€‚æ¯ä¸ªæ¨¡å‹å‡é‡‡ç”¨ 16K çª—å£å’Œé¢å¤–çš„å¡«ç©ºä»»åŠ¡è¿›è¡Œé¡¹ç›®çº§ä»£ç è¯­æ–™é¢„è®­ç»ƒï¼Œä»¥æ”¯æŒé¡¹ç›®çº§ä»£ç è¡¥å…¨å’Œå¡«å……ã€‚åœ¨ç¼–ç¨‹èƒ½åŠ›æ–¹é¢ï¼ŒDeepseek Coder åœ¨å¤šç§ç¼–ç¨‹è¯­è¨€å’Œå„ç±»åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†å¼€æºä»£ç æ¨¡å‹çš„é¢†å…ˆæ°´å¹³ã€‚\n\næµ·é‡è®­ç»ƒæ•°æ®ï¼šä»é›¶å¼€å§‹è®­ç»ƒ 2T tokensï¼ŒåŒ…å« 87% ä»£ç å’Œ 13% ä¸­è‹±æ–‡è¯­è¨€æ•°æ®ã€‚\n\né«˜åº¦çµæ´»å¯æ‰©å±•ï¼šæä¾› 1.3Bã€5.7Bã€6.7B å’Œ 33B ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼Œç”¨æˆ·å¯æ ¹æ®éœ€æ±‚é€‰æ‹©æœ€é€‚åˆçš„é…ç½®ã€‚\n\nå“è¶Šçš„æ¨¡å‹æ€§èƒ½ï¼šåœ¨ HumanEvalã€MultiPL-Eã€MBPPã€DS-1000 å’Œ APPS ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½è¡¨ç°ä¼˜äºç°æœ‰å¼€æºä»£ç æ¨¡å‹ã€‚\n\nå…ˆè¿›çš„ä»£ç è¡¥å…¨èƒ½åŠ›ï¼šé‡‡ç”¨ 16K çª—å£å’Œå¡«ç©ºä»»åŠ¡è®¾è®¡ï¼Œæ”¯æŒé¡¹ç›®çº§ä»£ç è¡¥å…¨ä¸å¡«å……ä»»åŠ¡ã€‚\n\n2. æ¨¡å‹æ¦‚è§ˆ\n\ndeepseek-coder-33b-instruct æ˜¯åŸºäº deepseek-coder-33b-base åˆå§‹åŒ–çš„ 33B å‚æ•°æ¨¡å‹ï¼Œå¹¶åœ¨ 2B tokens çš„æŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚\n\nä¸»é¡µï¼šDeepSeek\nä»£ç ä»“åº“ï¼šdeepseek-ai/deepseek-coder\nä¸DeepSeek Coderå¯¹è¯ï¼šDeepSeek-Coder\n3. ä½¿ç”¨æŒ‡å—\n\nä»¥ä¸‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚\n\nå¯¹è¯æ¨¡å‹æ¨ç†\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).npu()\nmessages=[\n    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n\n4. è®¸å¯è¯å£°æ˜\n\næœ¬ä»£ç ä»“åº“é‡‡ç”¨ MIT å¼€æºè®¸å¯è¯æˆæƒã€‚DeepSeek Coder æ¨¡å‹çš„ä½¿ç”¨éœ€éµå®ˆã€Šæ¨¡å‹è®¸å¯åè®®ã€‹ï¼Œè¯¥æ¨¡å‹æ”¯æŒå•†ä¸šç”¨é€”ã€‚\n\nè¯¦è§ MODEL è®¸å¯è¯ è·å–å®Œæ•´æ¡æ¬¾ã€‚\n\n5. è”ç³»æˆ‘ä»¬\n\nå¦‚æœ‰ä»»ä½•ç–‘é—®ï¼Œæ¬¢è¿æäº¤ issue æˆ–å‘é€é‚®ä»¶è‡³ agi_code@deepseek.com å’¨è¯¢ã€‚",
    "tags": "[\"PyTorch\", \"Transformers\", \"Safetensors\", \"Other\"]"
  },
  {
    "url": "https://gitcode.com/openMind/Aquila-7B",
    "project_name": "Aquila-7B",
    "readme": "æ‚Ÿé“Â·å¤©é¹°ï¼ˆAquilaï¼‰\n\næ‚Ÿé“Â·å¤©é¹°ï¼ˆAquilaï¼‰ è¯­è¨€å¤§æ¨¡å‹æ˜¯é¦–ä¸ªå…·å¤‡ä¸­è‹±åŒè¯­çŸ¥è¯†ã€æ”¯æŒå•†ç”¨è®¸å¯åè®®ã€å›½å†…æ•°æ®åˆè§„éœ€æ±‚çš„å¼€æºè¯­è¨€å¤§æ¨¡å‹ã€‚\n\nğŸŒŸ æ”¯æŒå¼€æºå•†ç”¨è®¸å¯ã€‚Aquilaç³»åˆ—æ¨¡å‹çš„æºä»£ç åŸºäº Apache 2.0 åè®®ï¼Œæ¨¡å‹æƒé‡åŸºäºã€Šæ™ºæºAquilaç³»åˆ—æ¨¡å‹è®¸å¯åè®®ã€‹ï¼Œä½¿ç”¨è€…åœ¨æ»¡è¶³è®¸å¯é™åˆ¶çš„æƒ…å†µä¸‹ï¼Œå¯ç”¨äºå•†ä¸šç›®çš„ã€‚\nâœï¸ å…·å¤‡ä¸­è‹±æ–‡çŸ¥è¯†ã€‚Aquilaç³»åˆ—æ¨¡å‹åœ¨ä¸­è‹±æ–‡é«˜è´¨é‡è¯­æ–™åŸºç¡€ä¸Šä» 0 å¼€å§‹è®­ç»ƒï¼Œä¸­æ–‡è¯­æ–™çº¦å  40%ï¼Œä¿è¯æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µå°±å¼€å§‹ç§¯ç´¯åŸç”Ÿçš„ä¸­æ–‡ä¸–ç•ŒçŸ¥è¯†ï¼Œè€Œéç¿»è¯‘è€Œæ¥çš„çŸ¥è¯†ã€‚\nğŸ‘®â€â™€ï¸ç¬¦åˆå›½å†…æ•°æ®åˆè§„éœ€æ±‚ã€‚Aquilaç³»åˆ—æ¨¡å‹çš„ä¸­æ–‡è¯­æ–™æ¥è‡ªæ™ºæºå¤šå¹´ç§¯ç´¯çš„ä¸­æ–‡æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ¥è‡ª1ä¸‡å¤šä¸ªç«™æºçš„ä¸­æ–‡äº’è”ç½‘æ•°æ®ï¼ˆå…¶ä¸­99%ä»¥ä¸Šä¸ºå›½å†…ç«™æºï¼‰ï¼Œä»¥åŠè·å¾—å›½å†…æƒå¨æœºæ„æ”¯æŒçš„é«˜è´¨é‡ä¸­æ–‡æ–‡çŒ®æ•°æ®ã€ä¸­æ–‡ä¹¦ç±æ•°æ®ç­‰ã€‚æˆ‘ä»¬ä»åœ¨æŒç»­ç§¯ç´¯é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œå¹¶æºæºä¸æ–­åŠ å…¥AquilaåŸºç¡€æ¨¡å‹åç»­è®­ç»ƒä¸­ã€‚\nğŸ¯æŒç»­è¿­ä»£ï¼ŒæŒç»­å¼€æºå¼€æ”¾ã€‚æˆ‘ä»¬å°†ä¸æ–­å®Œå–„è®­ç»ƒæ•°æ®ã€ä¼˜åŒ–è®­ç»ƒæ–¹æ³•ã€æå‡æ¨¡å‹æ€§èƒ½ï¼Œåœ¨æ›´ä¼˜ç§€çš„åŸºç¡€æ¨¡å‹åŸºåº§ä¸Šï¼ŒåŸ¹è‚²æç¹å¶èŒ‚çš„â€œæ¨¡å‹æ ‘â€ï¼ŒæŒç»­å¼€æºå¼€æ”¾æ›´æ–°çš„ç‰ˆæœ¬ã€‚\n\næ‚Ÿé“ Â· å¤©é¹° Aquila æ¨¡å‹çš„æ›´å¤šç»†èŠ‚å°†åœ¨å®˜æ–¹æŠ€æœ¯æŠ¥å‘Šä¸­å‘ˆç°ã€‚è¯·å…³æ³¨å®˜æ–¹æ¸ é“æ›´æ–°ã€‚åŒ…æ‹¬ FlagAI GitHubä»“åº“ï¼ŒFlagAI çŸ¥ä¹è´¦å·ã€FlagAI å®˜æ–¹æŠ€æœ¯äº¤æµç¾¤ã€æ™ºæºç ”ç©¶é™¢å¾®ä¿¡å…¬ä¼—å·ã€æ™ºæºç¤¾åŒºå¾®ä¿¡å…¬ä¼—å·ã€‚\n\næ¨¡å‹\tæ¨¡å‹ç±»å‹\tç®€ä»‹\tçŠ¶æ€\tè®­ç»ƒæ‰€ç”¨æ˜¾å¡\nAquila-7B\tåŸºç¡€æ¨¡å‹ï¼Œ70äº¿å‚æ•°\tAquila åŸºç¡€æ¨¡å‹åœ¨æŠ€æœ¯ä¸Šç»§æ‰¿äº† GPT-3ã€LLaMA ç­‰çš„æ¶æ„è®¾è®¡ä¼˜ç‚¹ï¼Œæ›¿æ¢äº†ä¸€æ‰¹æ›´é«˜æ•ˆçš„åº•å±‚ç®—å­å®ç°ã€é‡æ–°è®¾è®¡å®ç°äº†ä¸­è‹±åŒè¯­çš„ tokenizerï¼Œå‡çº§äº† BMTrain å¹¶è¡Œè®­ç»ƒæ–¹æ³•ï¼Œå®ç°äº†æ¯” Magtron+DeepSpeed ZeRO-2 å°†è¿‘ï¼˜å€çš„è®­ç»ƒæ•ˆç‡ã€‚\tå·²å‘å¸ƒ\tNvidia-A100\nAquila-33B\tåŸºç¡€æ¨¡å‹ï¼Œ330äº¿å‚æ•°\tåŒä¸Š\tæ•¬è¯·æœŸå¾…\tNvidia-A100\nAquilaChat-7B\tSFT modelï¼ŒåŸºäº Aquila-7B è¿›è¡Œå¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ \tAquilaChat å¯¹è¯æ¨¡å‹æ”¯æŒæµç•…çš„æ–‡æœ¬å¯¹è¯åŠå¤šç§è¯­è¨€ç±»ç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡å®šä¹‰å¯æ‰©å±•çš„ç‰¹æ®ŠæŒ‡ä»¤è§„èŒƒï¼Œå®ç° AquilaChatå¯¹å…¶å®ƒæ¨¡å‹å’Œå·¥å…·çš„è°ƒç”¨ï¼Œä¸”æ˜“äºæ‰©å±•ã€‚\n\nä¾‹å¦‚ï¼Œè°ƒç”¨æ™ºæºå¼€æºçš„ AltDiffusion å¤šè¯­è¨€æ–‡å›¾ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°äº†æµç•…çš„æ–‡å›¾ç”Ÿæˆèƒ½åŠ›ã€‚é…åˆæ™ºæº InstructFace å¤šæ­¥å¯æ§æ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œè½»æ¾å®ç°å¯¹äººè„¸å›¾åƒçš„å¤šæ­¥å¯æ§ç¼–è¾‘ã€‚\tå·²å‘å¸ƒ\tNvidia-A100\nAquilaChat-33B\tSFT modelï¼ŒåŸºäº Aquila-33B è¿›è¡Œå¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ \tåŒä¸Š\tæ•¬è¯·æœŸå¾…\tNvidia-A100\nAquilaCode-7B-NV\tåŸºç¡€æ¨¡å‹ï¼Œâ€œæ–‡æœ¬-ä»£ç â€ç”Ÿæˆæ¨¡å‹ï¼ŒåŸºäº Aquila-7Bç»§ç»­é¢„è®­ç»ƒï¼Œåœ¨è‹±ä¼Ÿè¾¾èŠ¯ç‰‡å®Œæˆè®­ç»ƒ\tAquilaCode-7B ä»¥å°æ•°æ®é›†ã€å°å‚æ•°é‡ï¼Œå®ç°é«˜æ€§èƒ½ï¼Œæ˜¯ç›®å‰æ”¯æŒä¸­è‹±åŒè¯­çš„ã€æ€§èƒ½æœ€å¥½çš„å¼€æºä»£ç æ¨¡å‹ï¼Œç»è¿‡äº†é«˜è´¨é‡è¿‡æ»¤ã€ä½¿ç”¨æœ‰åˆè§„å¼€æºè®¸å¯çš„è®­ç»ƒä»£ç æ•°æ®è¿›è¡Œè®­ç»ƒã€‚\n\nAquilaCode-7B åˆ†åˆ«åœ¨è‹±ä¼Ÿè¾¾å’Œå›½äº§èŠ¯ç‰‡ä¸Šå®Œæˆäº†ä»£ç æ¨¡å‹çš„è®­ç»ƒã€‚\tgithubå·²å‘å¸ƒ\tNvidia-A100\nAquilaCode-7B-TS\tåŸºç¡€æ¨¡å‹ï¼Œâ€œæ–‡æœ¬-ä»£ç â€ç”Ÿæˆæ¨¡å‹ï¼ŒåŸºäº Aquila-7Bç»§ç»­é¢„è®­ç»ƒï¼Œåœ¨å¤©æ•°æ™ºèŠ¯èŠ¯ç‰‡ä¸Šå®Œæˆè®­ç»ƒ\tåŒä¸Š\tgithubå·²å‘å¸ƒ\tTianshu-BI-V100\n\næ‚Ÿé“Â·å¤©é¹°Aquilaç³»åˆ—æ¨¡å‹å°†æŒç»­å¼€æºæ›´ä¼˜ç‰ˆæœ¬ã€‚\n\n2023/08/15 ï¼šå¼€æº v0.10\nAquila-7B-01 md5: 4279db72e68df1a0705ecc8d4c7be3db\nAquila-7B-02 md5: 621f8ce4c8deebe1635f5a09aa4b80f2\nAquilaChat-7B-01 md5: 22b22ffaed51388ce23f8e328a9b6a18\nAquilaChat-7B-02 md5: 6e84423fe2837c79c0ced6817c316bd4\n\nAquila-7B æ–°ç‰ˆæœ¬æ¨¡å‹åœ¨ FlagEval å¤§æ¨¡å‹è¯„æµ‹ä¸­ï¼ˆ â€œå®¢è§‚â€ï¼‰ç›¸æ¯”ä¸Šä¸€ç‰ˆåœ¨TruthfulQAä¸­æå‡9.09%ã€‚è¯¦ç»†è¯„æµ‹ç»“æœè¯·é€šè¿‡ http://flageval.baai.ac.cn ç½‘ç«™æŸ¥çœ‹ï¼Œ å†å²ç‰ˆæœ¬å˜æ›´è¯¦æƒ…è§ï¼šå˜æ›´æ—¥å¿— ã€‚\n\nå¿«é€Ÿå¼€å§‹ä½¿ç”¨ Aquila-7B\nä½¿ç”¨æ–¹å¼/How to use\n1. æ¨ç†/Inference\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom cyg_conversation import covert_prompt_to_input_ids_with_history\n\ntokenizer = AutoTokenizer.from_pretrained(\"BAAI/AquilaChat-7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"BAAI/AquilaChat-7B\")\nmodel.eval()\nmodel.to(\"cuda:0\")\nvocab = tokenizer.vocab\nprint(len(vocab))\n\ntext = \"è¯·ç»™å‡º10ä¸ªè¦åˆ°åŒ—äº¬æ—…æ¸¸çš„ç†ç”±ã€‚\"\n\ntokens = covert_prompt_to_input_ids_with_history(text, history=[], tokenizer=tokenizer, max_token=512)\n\ntokens = torch.tensor(tokens)[None,].to(\"npu:0\")\n\n\nwith torch.no_grad():\n    out = model.generate(tokens, do_sample=True, max_length=512, eos_token_id=100007)[0]\n\n    out = tokenizer.decode(out.cpu().numpy().tolist())\n\n    print(out)\n\nè¯ä¹¦/License\n\n`Aquilaç³»åˆ—å¼€æºæ¨¡å‹ä½¿ç”¨ æ™ºæºAquilaç³»åˆ—æ¨¡å‹è®¸å¯åè®®",
    "tags": "[\"PyTorch\", \"Transformers\", \"Other\"]"
  },
  {
    "url": "https://gitcode.com/openMind/Janus-Pro-1B",
    "project_name": "Janus-Pro-1B",
    "readme": "Original Text\n1. å¼•è¨€\n\nJanus-Pro æ˜¯ä¸€ç§æ–°é¢–çš„è‡ªå›å½’æ¡†æ¶ï¼Œå®ƒç»Ÿä¸€äº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡å°†è§†è§‰ç¼–ç è§£è€¦ä¸ºç‹¬ç«‹çš„è·¯å¾„ï¼ŒåŒæ—¶ä¾ç„¶ä½¿ç”¨å•ä¸ªã€ç»Ÿä¸€çš„å˜æ¢å™¨æ¶æ„è¿›è¡Œå¤„ç†ï¼Œä»è€Œå…‹æœäº†å…ˆå‰æ–¹æ³•çš„å±€é™æ€§ã€‚è¿™ç§è§£è€¦ä¸ä»…å‡è½»äº†è§†è§‰ç¼–ç å™¨åœ¨ç†è§£å’Œç”Ÿæˆä¸­çš„è§’è‰²å†²çªï¼Œè¿˜å¢å¼ºäº†æ¡†æ¶çš„çµæ´»æ€§ã€‚Janus-Pro è¶…è¶Šäº†ä¹‹å‰çš„ç»Ÿä¸€æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šä¸ç‰¹å®šä»»åŠ¡æ¨¡å‹ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚Janus-Pro çš„ç®€æ´æ€§ã€é«˜åº¦çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ä½¿å…¶æˆä¸ºä¸‹ä¸€ä»£ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„å¼ºæœ‰åŠ›çš„å€™é€‰è€…ã€‚\n\nGithub ä»“åº“\n\n2. æ¨¡å‹æ¦‚è¿°\n\nJanus-Pro æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç†è§£å’Œç”Ÿæˆå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œå®ƒä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆè§£è€¦äº†è§†è§‰ç¼–ç ã€‚Janus-Pro åŸºäºDeepSeek-LLM-1.5b-base/DeepSeek-LLM-7b-baseæ„å»ºã€‚\n\nå¯¹äºå¤šæ¨¡æ€ç†è§£ï¼Œå®ƒä½¿ç”¨ SigLIP-L ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œæ”¯æŒ 384 x 384 å›¾åƒè¾“å…¥ã€‚å¯¹äºå›¾åƒç”Ÿæˆï¼ŒJanus-Pro ä½¿ç”¨æ¥è‡ª è¿™é‡Œ çš„æ ‡è®°å™¨ï¼Œå…¶é™é‡‡æ ·ç‡ä¸º 16ã€‚\n\n3. å¿«é€Ÿå¼€å§‹\n\nè¯·å‚è€ƒ Github ä»“åº“\n\n4. è®¸å¯\n\næœ¬ä»£ç åº“éµå¾ª MIT è®¸å¯ã€‚ä½¿ç”¨ Janus-Pro æ¨¡å‹éœ€éµå®ˆ DeepSeek æ¨¡å‹è®¸å¯ã€‚\n\n5. å¼•ç”¨\n@article{chen2025janus,\n  title={Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling},\n  author={Chen, Xiaokang and Wu, Zhiyu and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong},\n  journal={arXiv preprint arXiv:2501.17811},\n  year={2025}\n}\n\n6. è”ç³»æ–¹å¼\n\nå¦‚æœæ‚¨æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·æå‡ºä¸€ä¸ªè®®é¢˜æˆ–é€šè¿‡ service@deepseek.com ä¸æˆ‘ä»¬è”ç³»ã€‚",
    "tags": "[\"PyTorch\", \"Transformers\", \"MIT\", \"unified-model\", \"muiltimodal\", \"text-to-image\"]"
  },
  {
    "url": "https://gitcode.com/Ascend_AI4S/prot_bert",
    "project_name": "prot_bert",
    "readme": "Original Text\nProtBert æ¨¡å‹\n\nåŸºäºè›‹ç™½è´¨åºåˆ—é‡‡ç”¨æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ç›®æ ‡è®­ç»ƒçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ¨¡å‹ç”±æ­¤è®ºæ–‡é¦–æ¬¡æå‡ºï¼Œå¹¶å‘å¸ƒäºæ­¤ä»£ç åº“ã€‚æœ¬æ¨¡å‹ä»…é’ˆå¯¹å¤§å†™å­—æ¯è¡¨ç¤ºçš„æ°¨åŸºé…¸è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨æ—¶éœ€ç¡®ä¿è¾“å…¥åºåˆ—å…¨éƒ¨ä¸ºå¤§å†™å½¢å¼ã€‚\n\næ¨¡å‹æè¿°\n\nProtBert åŸºäº Bert æ¶æ„ï¼Œé€šè¿‡è‡ªç›‘ç£æ–¹å¼åœ¨å¤§é‡è›‹ç™½è´¨åºåˆ—è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚è¿™æ„å‘³ç€å…¶è®­ç»ƒä»…ä¾èµ–äºåŸå§‹è›‹ç™½è´¨åºåˆ—æ•°æ®ï¼Œæ— éœ€ä»»ä½•äººå·¥æ ‡æ³¨ï¼ˆå› è€Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨å…¬å¼€æ•°æ®èµ„æºï¼‰ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ä»åºåˆ—ä¸­ç”Ÿæˆè¾“å…¥ä¸æ ‡ç­¾ã€‚\n\nä¸åŸå§‹ Bert ç‰ˆæœ¬çš„å…³é”®åŒºåˆ«åœ¨äºåºåˆ—å¤„ç†æ–¹å¼ï¼šæœ¬æ¨¡å‹å°†æ¯æ¡åºåˆ—è§†ä¸ºç‹¬ç«‹æ–‡æ¡£ï¼Œå› æ­¤æœªé‡‡ç”¨ä¸‹ä¸€å¥é¢„æµ‹ä»»åŠ¡ã€‚æ©ç ç­–ç•¥éµå¾ªåŸå§‹ Bert è®­ç»ƒæ–¹æ³•ï¼Œéšæœºé®ç›–è¾“å…¥åºåˆ—ä¸­15%çš„æ°¨åŸºé…¸ã€‚\n\næœ€ç»ˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æå–çš„ç‰¹å¾è¯å®ï¼šä»…é€šè¿‡æ— æ ‡æ³¨æ•°æ®ï¼ˆçº¯è›‹ç™½è´¨åºåˆ—ï¼‰è·å–çš„è¯­è¨€æ¨¡å‹åµŒå…¥ï¼Œèƒ½å¤Ÿæ•æ‰å†³å®šè›‹ç™½è´¨ç©ºé—´æ„è±¡çš„å…³é”®ç”Ÿç‰©ç‰©ç†ç‰¹æ€§ã€‚è¿™æ„å‘³ç€æ¨¡å‹é€šè¿‡å­¦ä¹ è›‹ç™½è´¨åºåˆ—å®ç°äº†å¯¹ç”Ÿå‘½è¯­è¨€éƒ¨åˆ†è¯­æ³•è§„åˆ™çš„è§£æã€‚\n\nåº”ç”¨åœºæ™¯ä¸é™åˆ¶\n\næœ¬æ¨¡å‹é€‚ç”¨äºè›‹ç™½è´¨ç‰¹å¾æå–æˆ–ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒã€‚éƒ¨åˆ†ä»»åŠ¡åœºæ™¯ä¸­ï¼Œå¾®è°ƒæ¨¡å‹æ¯”ç›´æ¥ç”¨ä½œç‰¹å¾æå–å™¨å¯è·å¾—æ›´é«˜å‡†ç¡®ç‡ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\næ‚¨å¯ä»¥ç›´æ¥é€šè¿‡æ©ç è¯­è¨€å»ºæ¨¡ç®¡é“è°ƒç”¨è¯¥æ¨¡å‹ï¼š\n\n>>> from transformers import BertForMaskedLM, BertTokenizer, pipeline\n>>> tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n>>> model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\")\n>>> unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n>>> unmasker('D L I P T S S K L V V [MASK] D T S L Q V K K A F F A L V T')\n\n[{'score': 0.11088453233242035,\n  'sequence': '[CLS] D L I P T S S K L V V L D T S L Q V K K A F F A L V T [SEP]',\n  'token': 5,\n  'token_str': 'L'},\n {'score': 0.08402521163225174,\n  'sequence': '[CLS] D L I P T S S K L V V S D T S L Q V K K A F F A L V T [SEP]',\n  'token': 10,\n  'token_str': 'S'},\n {'score': 0.07328339666128159,\n  'sequence': '[CLS] D L I P T S S K L V V V D T S L Q V K K A F F A L V T [SEP]',\n  'token': 8,\n  'token_str': 'V'},\n {'score': 0.06921856850385666,\n  'sequence': '[CLS] D L I P T S S K L V V K D T S L Q V K K A F F A L V T [SEP]',\n  'token': 12,\n  'token_str': 'K'},\n {'score': 0.06382402777671814,\n  'sequence': '[CLS] D L I P T S S K L V V I D T S L Q V K K A F F A L V T [SEP]',\n  'token': 11,\n  'token_str': 'I'}]\n\n\nä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨PyTorchä¸­ä½¿ç”¨è¯¥æ¨¡å‹è·å–ç»™å®šè›‹ç™½è´¨åºåˆ—ç‰¹å¾çš„æ–¹æ³•ï¼š\n\nfrom transformers import BertModel, BertTokenizer\nimport re\ntokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\nmodel = BertModel.from_pretrained(\"Rostlab/prot_bert\")\nsequence_Example = \"A E T C Z A O\"\nsequence_Example = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\nencoded_input = tokenizer(sequence_Example, return_tensors='pt')\noutput = model(**encoded_input)\n\nè®­ç»ƒæ•°æ®\n\nProtBertæ¨¡å‹åŸºäºUniRef100æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«2.17äº¿æ¡è›‹ç™½è´¨åºåˆ—ã€‚\n\nè®­ç»ƒæµç¨‹\næ•°æ®é¢„å¤„ç†\n\nè›‹ç™½è´¨åºåˆ—ç»Ÿä¸€è½¬æ¢ä¸ºå¤§å†™å­—æ¯ï¼Œå¹¶ä½¿ç”¨å•ä¸ªç©ºæ ¼è¿›è¡Œåˆ†è¯ï¼Œè¯æ±‡è¡¨å¤§å°ä¸º21ã€‚ç¨€æœ‰æ°¨åŸºé…¸\"U,Z,O,B\"è¢«æ˜ å°„ä¸º\"X\"å­—ç¬¦ã€‚ æ¨¡å‹çš„è¾“å…¥æ ¼å¼å¦‚ä¸‹ï¼š\n\n[CLS] Protein Sequence A [SEP] Protein Sequence B [SEP]\n\n\næ­¤å¤–ï¼Œæ¯æ¡è›‹ç™½è´¨åºåˆ—å‡è¢«è§†ä¸ºç‹¬ç«‹æ–‡æ¡£ã€‚é¢„å¤„ç†æ­¥éª¤æ‰§è¡Œäº†ä¸¤æ¬¡ï¼šä¸€æ¬¡é’ˆå¯¹ç»„åˆé•¿åº¦ï¼ˆä¸¤æ¡åºåˆ—ï¼‰å°äº512ä¸ªæ°¨åŸºé…¸çš„æƒ…å†µï¼Œå¦ä¸€æ¬¡é’ˆå¯¹ç»„åˆé•¿åº¦ï¼ˆä¸¤æ¡åºåˆ—ï¼‰å°äº2048ä¸ªæ°¨åŸºé…¸çš„æƒ…å†µã€‚\n\nå„åºåˆ—çš„æ©ç å¤„ç†ç»†èŠ‚éµå¾ªåŸå§‹Bertæ¨¡å‹è§„èŒƒå¦‚ä¸‹ï¼š\n\n15%çš„æ°¨åŸºé…¸è¢«æ©ç æ ‡è®°\nå…¶ä¸­80%çš„æƒ…å†µï¼Œè¢«æ©ç çš„æ°¨åŸºé…¸æ›¿æ¢ä¸º[MASK]æ ‡è®°\n10%çš„æƒ…å†µï¼Œè¢«æ©ç çš„æ°¨åŸºé…¸æ›¿æ¢ä¸ºéšæœºé€‰å–çš„ä¸åŒæ°¨åŸºé…¸\nå‰©ä½™10%çš„æƒ…å†µï¼Œè¢«æ©ç çš„æ°¨åŸºé…¸ä¿æŒåŸçŠ¶\né¢„è®­ç»ƒè¿‡ç¨‹\n\næ¨¡å‹åœ¨å•å°TPU Pod V3-512ä¸Šå®Œæˆäº†æ€»è®¡40ä¸‡æ­¥è®­ç»ƒï¼š\n\n30ä¸‡æ­¥é‡‡ç”¨512åºåˆ—é•¿åº¦ï¼ˆæ‰¹å¤§å°1.5ä¸‡ï¼‰\n10ä¸‡æ­¥é‡‡ç”¨2048åºåˆ—é•¿åº¦ï¼ˆæ‰¹å¤§å°2500ï¼‰ ä½¿ç”¨Lambä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡0.002ï¼Œæƒé‡è¡°å‡0.01ï¼Œå‰4ä¸‡æ­¥è¿›è¡Œå­¦ä¹ ç‡é¢„çƒ­ï¼Œä¹‹åçº¿æ€§è¡°å‡ã€‚\nè¯„ä¼°ç»“æœ\n\nåœ¨ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒåï¼Œæ¨¡å‹å–å¾—å¦‚ä¸‹è¡¨ç°ï¼š\n\næµ‹è¯•ç»“æœï¼š\n\nä»»åŠ¡/æ•°æ®é›†\täºŒçº§ç»“æ„ï¼ˆ3æ€ï¼‰\täºŒçº§ç»“æ„ï¼ˆ8æ€ï¼‰\täºšç»†èƒå®šä½\tè†œè›‹ç™½æ£€æµ‹\nCASP12\t75\t63\t\t\nTS115\t83\t72\t\t\nCB513\t81\t66\t\t\nDeepLoc\t\t\t79\t91\nBibTeXæ¡ç›®ä¸å¼•ç”¨ä¿¡æ¯\n@article {Elnaggar2020.07.12.199554,\n\tauthor = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard},\n\ttitle = {ProtTrans: Towards Cracking the Language of Life{\\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing},\n\telocation-id = {2020.07.12.199554},\n\tyear = {2020},\n\tdoi = {10.1101/2020.07.12.199554},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tabstract = {Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive language models (Transformer-XL, XLNet) and two auto-encoder models (Bert, Albert) on data from UniRef and BFD containing up to 393 billion amino acids (words) from 2.1 billion protein sequences (22- and 112 times the entire English Wikipedia). The LMs were trained on the Summit supercomputer at Oak Ridge National Laboratory (ORNL), using 936 nodes (total 5616 GPUs) and one TPU Pod (V3-512 or V3-1024). We validated the advantage of up-scaling LMs to larger models supported by bigger data by predicting secondary structure (3-states: Q3=76-84, 8 states: Q8=65-73), sub-cellular localization for 10 cellular compartments (Q10=74) and whether a protein is membrane-bound or water-soluble (Q2=89). Dimensionality reduction revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein shape. This implied learning some of the grammar of the language of life realized in protein sequences. The successful up-scaling of protein LMs through HPC to larger data sets slightly reduced the gap between models trained on evolutionary information and LMs. Availability ProtTrans: \\&lt;a href=\"https://github.com/agemagician/ProtTrans\"\\&gt;https://github.com/agemagician/ProtTrans\\&lt;/a\\&gt;Competing Interest StatementThe authors have declared no competing interest.},\n\tURL = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554},\n\teprint = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554.full.pdf},\n\tjournal = {bioRxiv}\n}\n\n\nç”± Ahmed Elnaggar/@Elnaggar_AI åˆ›å»º | é¢†è‹±",
    "tags": "[\"PyTorch\", \"TensorFlow\", \"Transformers\", \"Uniref100\", \"protein language model\", \"protein\"]"
  },
  {
    "url": "https://gitcode.com/MooYeh/blip-image-captioning-large",
    "project_name": "blip-image-captioning-large",
    "readme": "Original Text\nBLIPï¼šç”¨äºç»Ÿä¸€è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆçš„è¯­è¨€-å›¾åƒé¢„è®­ç»ƒå¯åŠ¨æ¡†æ¶\n\nåŸºäºCOCOæ•°æ®é›†è¿›è¡Œå›¾åƒæ ‡æ³¨é¢„è®­ç»ƒçš„æ¨¡å‹å¡ç‰‡ - åŸºç¡€æ¶æ„ï¼ˆå¸¦æœ‰ViTå¤§å‹ä¸»å¹²ç½‘ï¼‰ã€‚\n\n\nä»BLIPå®˜æ–¹ä»“åº“æ‹‰å–å›¾ç‰‡\nä¿®æ”¹\n\nä¿®æ”¹ç¤ºä¾‹ä»¥æ”¯æŒopenMindï¼Œå¹¶å¢åŠ NPUæ”¯æŒ\n\nå¤ªé•¿ä¸è¯»\n\nè®ºæ–‡ä½œè€…åœ¨æ‘˜è¦ä¸­å†™é“ï¼š\n\nè§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æé«˜äº†è®¸å¤šè§†è§‰è¯­è¨€ä»»åŠ¡çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹åªåœ¨ç†è§£å‹ä»»åŠ¡æˆ–ç”Ÿæˆå‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæ€§èƒ½æå‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯é€šè¿‡æ‰©å¤§æ•°æ®é›†å®ç°çš„ï¼Œè¯¥æ•°æ®é›†ç”±ä»ç½‘ç»œæ”¶é›†çš„å¸¦å™ªå£°çš„å›¾åƒ-æ–‡æœ¬å¯¹ç»„æˆï¼Œè¿™æ˜¯ä¸€ç§æ¬¡ä¼˜çš„ç›‘ç£æ¥æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BLIPï¼Œä¸€ä¸ªæ–°çš„VLPæ¡†æ¶ï¼Œå®ƒå¯ä»¥çµæ´»åœ°è¿ç§»åˆ°è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚BLIPé€šè¿‡å¼•å¯¼æ ‡æ³¨ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨äº†å¸¦å™ªå£°çš„ç½‘ç»œæ•°æ®ï¼Œæ ‡æ³¨å™¨ç”Ÿæˆåˆæˆæ ‡æ³¨ï¼Œè¿‡æ»¤å™¨ç§»é™¤å™ªå£°æ ‡æ³¨ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¦‚å›¾åƒ-æ–‡æœ¬æ£€ç´¢ï¼ˆå¹³å‡å¬å›ç‡@1æé«˜2.7%ï¼‰ï¼Œå›¾åƒæ ‡æ³¨ï¼ˆCIDEræé«˜2.8%ï¼‰ï¼Œä»¥åŠVQAï¼ˆVQAå¾—åˆ†æé«˜1.6%ï¼‰ã€‚BLIPåœ¨ç›´æ¥è¿ç§»åˆ°è§†é¢‘è¯­è¨€ä»»åŠ¡çš„é›¶æ ·æœ¬æ–¹å¼ä¸‹ä¹Ÿå±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒã€‚\n\nä½¿ç”¨\n\næ‚¨å¯ä»¥ä½¿ç”¨æ­¤æ¨¡å‹è¿›è¡Œæ¡ä»¶å’Œæ— æ¡ä»¶çš„å›¾åƒæ ‡æ³¨\n\nä½¿ç”¨PyTorchæ¨¡å‹\nåœ¨CPUä¸Šè¿è¡Œæ¨¡å‹\nç‚¹å‡»å±•å¼€",
    "tags": "[\"Image-to-Text\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Safetensors\", \"BSD 3-Clause New or Revised\", \"image-captioning\"]"
  },
  {
    "url": "https://gitcode.com/MooYeh/stable-diffusion-xl-base-1_0",
    "project_name": "stable-diffusion-xl-base-1_0",
    "readme": "Original Text\nSD-XL 1.0-base æ¨¡å‹å¡ç‰‡\n\nä¿®æ”¹\nä¿®æ”¹ç¤ºä¾‹ä»¥æ”¯æŒ openMind å¹¶å¢åŠ  npu æ”¯æŒ\næ¨¡å‹\n\nSDXL ç”±ä¸€ä¸ªç”¨äºæ½œåœ¨æ‰©æ•£çš„ ä¸“å®¶é›†æˆ ç®¡é“ç»„æˆï¼š é¦–å…ˆï¼ŒåŸºç¡€æ¨¡å‹ç”¨äºç”Ÿæˆï¼ˆå¸¦å™ªå£°çš„ï¼‰æ½œåœ¨å˜é‡ï¼Œç„¶åä½¿ç”¨ä¸€ä¸ªä¸“é—¨çš„ç»†åŒ–æ¨¡å‹ï¼ˆå¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼šhttps://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/ï¼‰è¿›ä¸€æ­¥å¤„ç†ï¼Œä»¥å®Œæˆæœ€ç»ˆçš„é™å™ªæ­¥éª¤ã€‚ è¯·æ³¨æ„ï¼ŒåŸºç¡€æ¨¡å‹å¯ä»¥ä½œä¸ºç‹¬ç«‹æ¨¡å—ä½¿ç”¨ã€‚\n\næˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ä»¥ä¸‹ä¸¤é˜¶æ®µç®¡é“ï¼š é¦–å…ˆï¼ŒåŸºç¡€æ¨¡å‹ç”¨äºç”Ÿæˆæ‰€éœ€è¾“å‡ºå°ºå¯¸çš„æ½œåœ¨å˜é‡ã€‚ åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä¸“é—¨çš„é«˜åˆ†è¾¨ç‡æ¨¡å‹ï¼Œå¹¶å¯¹ç¬¬ä¸€æ­¥éª¤ç”Ÿæˆçš„æ½œåœ¨å˜é‡åº”ç”¨åä¸º SDEditï¼ˆhttps://arxiv.org/abs/2108.01073ï¼Œä¹Ÿç§°ä¸º \"img2img\"ï¼‰çš„æŠ€æœ¯ï¼Œä½¿ç”¨ç›¸åŒçš„æç¤ºã€‚ è¿™ç§æŠ€æœ¯æ¯”ç¬¬ä¸€ç§ç¨æ…¢ï¼Œå› ä¸ºå®ƒéœ€è¦æ›´å¤šçš„å‡½æ•°è¯„ä¼°ã€‚\n\næºä»£ç å¯åœ¨ https://github.com/Stability-AI/generative-models ä¸Šæ‰¾åˆ°ã€‚\n\næ¨¡å‹æè¿°\nå¼€å‘è€…ï¼š Stability AI\næ¨¡å‹ç±»å‹ï¼š åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°å›¾ç‰‡ç”Ÿæˆæ¨¡å‹\nè®¸å¯ï¼š CreativeML Open RAIL++-M License\næ¨¡å‹æè¿°ï¼š è¿™æ˜¯ä¸€ä¸ªå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå’Œä¿®æ”¹å›¾ç‰‡çš„æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ä¸ª æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨ä¸¤ä¸ªå›ºå®šçš„é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ï¼ˆOpenCLIP-ViT/G å’Œ CLIP-ViT/Lï¼‰ã€‚\næ›´å¤šä¿¡æ¯èµ„æºï¼š è¯·æŸ¥çœ‹æˆ‘ä»¬çš„ GitHub ä»“åº“ å’Œ SDXL åœ¨ arXiv ä¸Šçš„æŠ¥å‘Šã€‚\næ¨¡å‹æ¥æº\n\nå‡ºäºç ”ç©¶ç›®çš„ï¼Œæˆ‘ä»¬æ¨èæˆ‘ä»¬çš„ generative-models GitHub ä»“åº“ï¼ˆhttps://github.com/Stability-AI/generative-modelsï¼‰ï¼Œå®ƒå®ç°äº†æœ€å—æ¬¢è¿çš„æ‰©æ•£æ¡†æ¶ï¼ˆåŒ…æ‹¬è®­ç»ƒå’Œæ¨ç†ï¼‰ï¼Œå¹¶ä¸”æ–°åŠŸèƒ½å¦‚è’¸é¦å°†åœ¨æœªæ¥æ·»åŠ ã€‚ Clipdrop æä¾›å…è´¹çš„ SDXL æ¨æ–­ã€‚\n\nä»“åº“ï¼š https://github.com/Stability-AI/generative-models\næ¼”ç¤ºï¼š https://clipdrop.co/stable-diffusion\nè¯„ä¼°\n\n ä¸Šè¿°å›¾è¡¨è¯„ä¼°äº†ç”¨æˆ·å¯¹ SDXLï¼ˆæœ‰å’Œæ— ç»†åŒ–ï¼‰ç›¸å¯¹äº SDXL 0.9 å’Œ Stable Diffusion 1.5 ä¸ 2.1 çš„åå¥½ã€‚ SDXL åŸºç¡€æ¨¡å‹æ¯”ä¹‹å‰çš„å˜ä½“è¡¨ç°æ˜¾è‘—æ›´å¥½ï¼Œä¸”ä¸ç»†åŒ–æ¨¡å—ç»“åˆçš„æ¨¡å‹å®ç°äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚\n\nğŸ§¨ æ‰©æ•£å™¨\n\nç¡®ä¿å‡çº§æ‰©æ•£å™¨è‡³ >= 0.19.0ï¼š\n\npip install diffusers --upgrade\n\n\næ­¤å¤–ï¼Œè¯·ç¡®ä¿å®‰è£… transformersã€safetensorsã€accelerateï¼Œä»¥åŠä¸å¯è§çš„æ°´å°æŠ€æœ¯ï¼š\n\npip install invisible_watermark transformers accelerate safetensors\n\n\nè¦ä»…ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼Œæ‚¨å¯ä»¥è¿è¡Œï¼š\n\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"PyTorch-NPU/stable-diffusion-xl-base-1_0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\npipe.to(\"cuda\")\n\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\n\nprompt = \"An astronaut riding a green horse\"\n\nimages = pipe(prompt=prompt).images[0]\n\n\nè¦ä½¿ç”¨æ•´ä¸ªåŸºç¡€+ç²¾ç‚¼å™¨ç®¡é“ä½œä¸ºä¸€ä¸ªä¸“å®¶ç»„åˆä½“ï¼Œæ‚¨å¯ä»¥è¿è¡Œï¼š\n\nfrom diffusers import DiffusionPipeline\nimport torch\n\n# load both base & refiner\nbase = DiffusionPipeline.from_pretrained(\n    \"PyTorch-NPU/stable-diffusion-xl-base-1_0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\nbase.to(\"cuda\")\nrefiner = DiffusionPipeline.from_pretrained(\n    \"PyTorch-NPU/stable-diffusion-xl-base-1_0\",\n    text_encoder_2=base.text_encoder_2,\n    vae=base.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\",\n)\nrefiner.to(\"cuda\")\n\n# Define how many steps and what % of steps to be run on each experts (80/20) here\nn_steps = 40\nhigh_noise_frac = 0.8\n\nprompt = \"A majestic lion jumping from a big stone at night\"\n\n# run both experts\nimage = base(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_end=high_noise_frac,\n    output_type=\"latent\",\n).images\nimage = refiner(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_start=high_noise_frac,\n    image=image,\n).images[0]\n\n\nå½“ä½¿ç”¨ torch >= 2.0 ç‰ˆæœ¬æ—¶ï¼Œé€šè¿‡ä½¿ç”¨ torch.compileï¼Œæ‚¨å¯ä»¥æé«˜æ¨ç†é€Ÿåº¦ 20-30%ã€‚åœ¨è¿è¡Œç®¡çº¿ä¹‹å‰ï¼Œåªéœ€ç®€å•åœ°ç”¨ torch.compile åŒ…è£… unet å³å¯ï¼š\n\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\n\nå¦‚æœæ‚¨å—é™äº GPU VRAMï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨ pipe.enable_model_cpu_offload æ¥å¯ç”¨ CPU å¸è½½ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ .to(\"cuda\")ã€‚\n\n- pipe.to(\"cuda\")\n+ pipe.enable_model_cpu_offload()\n\n\næœ‰å…³å¦‚ä½•ä½¿ç”¨ diffusers ä¸ Stable Diffusion XL çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥é˜…Stable Diffusion XL æ–‡æ¡£ã€‚\n\nOptimum\n\nOptimum æä¾›äº†ä¸€ä¸ªä¸ OpenVINO å’Œ ONNX Runtime å‡å…¼å®¹çš„ Stable Diffusion ç®¡é“ã€‚\n\nOpenVINO\n\nè¦å®‰è£…å¸¦æœ‰ OpenVINO æ‰€éœ€ä¾èµ–çš„ Optimumï¼Œè¯·ï¼š\n\npip install optimum[openvino]\n\n\nè¦åœ¨OpenVINO Runtimeä¸­åŠ è½½OpenVINOæ¨¡å‹å¹¶è¿›è¡Œæ¨ç†ï¼Œæ‚¨éœ€è¦å°†StableDiffusionXLPipelineæ›¿æ¢ä¸ºOptimumçš„OVStableDiffusionXLPipelineã€‚å¦‚æœæ‚¨å¸Œæœ›åŠ è½½ä¸€ä¸ªPyTorchæ¨¡å‹å¹¶åœ¨è¿è¡Œæ—¶å°†å…¶è½¬æ¢ä¸ºOpenVINOæ ¼å¼ï¼Œå¯ä»¥è®¾ç½®export=Trueã€‚\n\n- from diffusers import StableDiffusionXLPipeline\n+ from optimum.intel import OVStableDiffusionXLPipeline\n\nmodel_id = \"PyTorch-NPU/stable-diffusion-xl-base-1_0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]\n\n\næ‚¨å¯ä»¥åœ¨ Optimum çš„å®˜æ–¹æ–‡æ¡£ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ï¼ˆä¾‹å¦‚é™æ€é‡å¡‘å’Œæ¨¡å‹ç¼–è¯‘ï¼‰ã€‚\n\nONNX\n\nè‹¥è¦å®‰è£…åŒ…å« ONNX Runtime æ¨ç†æ‰€éœ€çš„ä¾èµ–é¡¹çš„ Optimumï¼Œè¯·ï¼š\n\npip install optimum[onnxruntime]\n\n\nè¦åŠ è½½ä¸€ä¸ªONNXæ¨¡å‹å¹¶ä½¿ç”¨ONNX Runtimeè¿›è¡Œæ¨ç†ï¼Œæ‚¨éœ€è¦å°†StableDiffusionXLPipelineæ›¿æ¢ä¸ºOptimumçš„ORTStableDiffusionXLPipelineã€‚å¦‚æœæ‚¨å¸Œæœ›åŠ è½½ä¸€ä¸ªPyTorchæ¨¡å‹å¹¶åœ¨è¿è¡Œæ—¶å°†å…¶å³æ—¶è½¬æ¢ä¸ºONNXæ ¼å¼ï¼Œå¯ä»¥å°†exportè®¾ç½®ä¸ºTrueã€‚\n\n- from diffusers import StableDiffusionXLPipeline\n+ from optimum.onnxruntime import ORTStableDiffusionXLPipeline\n\nmodel_id = \"PyTorch-NPU/stable-diffusion-xl-base-1_0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = ORTStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]\n\n\næ‚¨å¯ä»¥åœ¨ optimum çš„æ–‡æ¡£ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚\n\nä½¿ç”¨åœºæ™¯\nç›´æ¥ä½¿ç”¨\n\nè¯¥æ¨¡å‹ä»…ä¾›ç ”ç©¶ç”¨é€”ã€‚å¯èƒ½çš„ç ”ç©¶é¢†åŸŸå’Œä»»åŠ¡åŒ…æ‹¬ï¼š\n\nè‰ºæœ¯ä½œå“çš„ç”ŸæˆåŠåœ¨è®¾è®¡ä¸å…¶ä»–è‰ºæœ¯è¿‡ç¨‹ä¸­çš„åº”ç”¨ã€‚\næ•™è‚²æˆ–åˆ›æ„å·¥å…·ä¸­çš„åº”ç”¨ã€‚\nç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶ã€‚\nå®‰å…¨éƒ¨ç½²å¯èƒ½ç”Ÿæˆæœ‰å®³å†…å®¹çš„æ¨¡å‹ã€‚\næ¢ç©¶å’Œç†è§£ç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§å’Œåè§ã€‚\n\nä»¥ä¸‹æè¿°äº†ç¦æ­¢ä½¿ç”¨çš„åœºæ™¯ã€‚\n\nè¶…å‡ºèŒƒå›´çš„ä½¿ç”¨\n\nè¯¥æ¨¡å‹æœªç»è¿‡è®­ç»ƒä»¥æˆä¸ºäº‹å®æˆ–çœŸå®äººç‰©ã€äº‹ä»¶çš„è¡¨ç¤ºï¼Œå› æ­¤ä½¿ç”¨è¯¥æ¨¡å‹ç”Ÿæˆæ­¤ç±»å†…å®¹è¶…å‡ºäº†è¯¥æ¨¡å‹çš„èƒ½åŠ›èŒƒå›´ã€‚\n\nå±€é™æ€§å’Œåè§\nå±€é™æ€§\næ¨¡å‹æ— æ³•è¾¾åˆ°å®Œç¾çš„ç…§ç‰‡çº§ç°å®ä¸»ä¹‰ã€‚\næ¨¡å‹æ— æ³•æ¸²æŸ“å¯è¯»æ–‡æœ¬ã€‚\næ¨¡å‹åœ¨å¤„ç†æ¶‰åŠç»„åˆæ€§çš„æ›´éš¾ä»»åŠ¡ä¸Šå­˜åœ¨å›°éš¾ï¼Œä¾‹å¦‚æ¸²æŸ“ç¬¦åˆâ€œä¸€ä¸ªçº¢è‰²ç«‹æ–¹ä½“åœ¨è“è‰²çƒä½“ä¸Šâ€çš„å›¾åƒã€‚\näººç‰©è„¸éƒ¨åŠä¸€èˆ¬çš„äººæ¥è¯´å¯èƒ½æ— æ³•æ­£ç¡®ç”Ÿæˆã€‚\næ¨¡å‹çš„è‡ªåŠ¨ç¼–ç éƒ¨åˆ†å­˜åœ¨æŸå¤±ã€‚\nåè§\n\nè™½ç„¶å›¾åƒç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½åŠ å‰§æˆ–å¼ºåŒ–ç¤¾ä¼šåè§ã€‚",
    "tags": "[\"PyTorch\", \"Transformers\", \"Diffusers\", \"ONNX\", \"OpenVINO\", \"Safetensors\", \"Open Rail++-M License\", \"text-to-image\", \"stable-diffusion\"]"
  },
  {
    "url": "https://gitcode.com/openMind/convnextv2_tiny_1k_224",
    "project_name": "convnextv2_tiny_1k_224",
    "readme": "Original Text\nConvNeXt V2ï¼ˆè¿·ä½ å‹æ¨¡å‹ï¼‰\n\nConvNeXt V2 æ¨¡å‹é‡‡ç”¨ FCMAE æ¡†æ¶è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨ 224x224 åˆ†è¾¨ç‡çš„ ImageNet-1K æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¯¥æ¨¡å‹åœ¨ Woo ç­‰äººå‘è¡¨çš„è®ºæ–‡ ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders ä¸­è¢«æå‡ºï¼Œå¹¶åœ¨ è¿™ä¸ªä»“åº“ ä¸­é¦–æ¬¡å‘å¸ƒã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒ ConvNeXT V2 çš„å›¢é˜Ÿå¹¶æœªä¸ºæ­¤æ¨¡å‹ç¼–å†™æ¨¡å‹å¡ç‰‡ï¼Œå› æ­¤æ­¤æ¨¡å‹å¡ç‰‡ç”± Hugging Face å›¢é˜Ÿæ’°å†™ã€‚\n\nä¿®æ”¹\n\nåœ¨åŸå§‹ README ä¸­æ·»åŠ äº† CANN ç‰ˆæœ¬ä¾èµ–æè¿°ï¼Œå¹¶ä¿®æ”¹äº†ç¤ºä¾‹ä»£ç ã€‚\n\næ¨¡å‹æè¿°\n\nConvNeXt V2 æ˜¯ä¸€ä¸ªçº¯å·ç§¯æ¨¡å‹ï¼ˆConvNetï¼‰ï¼Œå®ƒåœ¨ ConvNeXt ä¸­å¼•å…¥äº†å…¨å·ç§¯é®è”½è‡ªç¼–ç å™¨æ¡†æ¶ï¼ˆFCMAEï¼‰å’Œä¸€ç§æ–°çš„å…¨å±€å“åº”å½’ä¸€åŒ–ï¼ˆGRNï¼‰å±‚ã€‚ConvNeXt V2 æ˜¾è‘—æé«˜äº†çº¯å·ç§¯ç½‘ç»œåœ¨å„ç§è¯†åˆ«åŸºå‡†ä¸Šçš„æ€§èƒ½ã€‚\n\né¢„å®šç”¨é€”ä¸é™åˆ¶\n\næ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\nä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨æ­¤æ¨¡å‹å°† COCO 2017 æ•°æ®é›†ä¸­çš„å›¾åƒåˆ†ç±»åˆ° ImageNet çš„ 1,000 ä¸ªç±»åˆ«ä¸­çš„ä¸€ä¸ªï¼š\n\nfrom openmind import AutoImageProcessor\nfrom transformers import ConvNextV2ForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"./cats_image\")\nimage = dataset[\"train\"][\"image\"][0]\n\npreprocessor = AutoImageProcessor.from_pretrained(\"PyTorch-NPU/convnextv2_tiny_1k_224\")\nmodel = ConvNextV2ForImageClassification.from_pretrained(\"PyTorch-NPU/convnextv2_tiny_1k_224\")\n\ninputs = preprocessor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label]),\n\nBibTeXæ¡ç›®ä¸å¼•ç”¨ä¿¡æ¯\n@article{DBLP:journals/corr/abs-2301-00808,\n  author    = {Sanghyun Woo and\n               Shoubhik Debnath and\n               Ronghang Hu and\n               Xinlei Chen and\n               Zhuang Liu and\n               In So Kweon and\n               Saining Xie},\n  title     = {ConvNeXt {V2:} Co-designing and Scaling ConvNets with Masked Autoencoders},\n  journal   = {CoRR},\n  volume    = {abs/2301.00808},\n  year      = {2023},\n  url       = {https://doi.org/10.48550/arXiv.2301.00808},\n  doi       = {10.48550/arXiv.2301.00808},\n  eprinttype = {arXiv},\n  eprint    = {2301.00808},\n  timestamp = {Tue, 10 Jan 2023 15:10:12 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2301-00808.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n\nè¯·æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬ï¼Œè¿™æ ·æˆ‘æ‰èƒ½ä¸ºæ‚¨ç¿»è¯‘æˆä¸­æ–‡ã€‚æˆ‘ä¼šç¡®ä¿ç¿»è¯‘ç»“æœé€šä¿—ã€ä¸“ä¸šã€ä¼˜é›…ä¸”æµç•…ï¼Œå¹¶ä¿æŒåŸå§‹çš„ Markdown æ ¼å¼ã€‚",
    "tags": "[\"Image Classification\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"NeMo\", \"Safetensors\", \"ELM\", \"Apache License 2.0\", \"imagenet-1k\"]"
  },
  {
    "url": "https://gitcode.com/openMind/baichuan_7b",
    "project_name": "baichuan_7b",
    "readme": "BaiChuan\næ¦‚è¿°\n\nBaichuan-7Bæ˜¯ç”±ç™¾å·æ™ºèƒ½å¼€å‘çš„ä¸€ä¸ªå¼€æºçš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ã€‚åŸºäºTransformerç»“æ„ï¼Œåœ¨å¤§çº¦1.2ä¸‡äº¿tokensä¸Šè®­ç»ƒçš„70äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º4096ã€‚åœ¨æ ‡å‡†çš„ä¸­æ–‡å’Œè‹±æ–‡æƒå¨benchmarkï¼ˆC-EVAL/MMLUï¼‰ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚\n\nåœ¨åŒå°ºå¯¸æ¨¡å‹ä¸­Baichuan-7Bè¾¾åˆ°äº†ç›®å‰SOTAçš„æ°´å¹³ã€‚\nBaichuan-7Bä½¿ç”¨è‡ªæœ‰çš„ä¸­è‹±æ–‡åŒè¯­è¯­æ–™è¿›è¡Œè®­ç»ƒï¼Œåœ¨ä¸­æ–‡ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œåœ¨C-Evalè¾¾åˆ°SOTAæ°´å¹³ã€‚\nä¸åŒäºLLaMAå®Œå…¨ç¦æ­¢å•†ä¸šä½¿ç”¨ï¼ŒBaichuan-7Bä½¿ç”¨æ›´å®½æ¾çš„å¼€æºåè®®ï¼Œå…è®¸ç”¨äºå•†ä¸šç›®çš„ã€‚\nå‡†å¤‡è®­ç»ƒç¯å¢ƒ\nå‡†å¤‡ç¯å¢ƒ\n\nå½“å‰æ¨¡å‹æ”¯æŒçš„ PyTorch ç‰ˆæœ¬å’Œå·²çŸ¥ä¸‰æ–¹åº“ä¾èµ–å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚\n\nTorch_Version\tä¸‰æ–¹åº“ä¾èµ–ç‰ˆæœ¬\nPyTorch 2.1\ttransformers==4.37.0 accelerate==0.27.0\n\nç¯å¢ƒå‡†å¤‡æŒ‡å¯¼ã€‚\n\nå‚è€ƒå¿«é€Ÿå…¥é—¨å‡†å¤‡ç¯å¢ƒã€‚\n\nå‡†å¤‡æ•°æ®é›†\n\nBaichuanæ¨¡å‹ä½¿ç”¨alpaca.jsonä½œä¸ºè®­ç»ƒçš„æ•°æ®é›†ã€‚\n\nä½¿ç”¨å»ºè®®\nè®­ç»ƒ\n\nå¦‚ä¸‹æ˜¯ä½¿ç”¨alpaca.jsonæ•°æ®é›†å¯¹æ­¤æ¨¡å‹è¿›è¡Œè®­ç»ƒçš„æ ·ä¾‹ï¼š\n\ntorchrun --nproc_per_node=8 --master_port=27500 examples/alpaca_sft/train_sft.py \\\n    --model_name_or_path \"PyTorch-NPU/baichuan_7b\" \\\n    --data_path ./alpaca_data.json \\\n    --bf16 True \\\n    --output_dir ./test/output \\\n    --max_steps 2000 \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 2000 \\\n    --save_total_limit 1 \\\n    --learning_rate 2e-5 \\\n    --weight_decay 0. \\\n    --warmup_ratio 0.03 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --fsdp \"full_shard auto_wrap\" \\\n    --fsdp_transformer_layer_cls_to_wrap 'DecoderLayer' \n\n\næ¨¡å‹è®­ç»ƒè„šæœ¬å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š\n\n--model_name_or_path                 //æ¨¡å‹è·¯å¾„æˆ–åç§°\n--data_path                          //æ•°æ®é›†è·¯å¾„\n--output_dir                        //è®­ç»ƒè¾“å‡ºè·¯å¾„      \n--max_steps                         //è®­ç»ƒæ­¥æ•°\n\næ¨ç†\n\nå¦‚ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨Baichuanè¿›è¡Œ1-shotæ¨ç†çš„ä»»åŠ¡ï¼Œæ ¹æ®ä½œå“ç»™å‡ºä½œè€…åï¼Œæ­£ç¡®è¾“å‡ºä¸º\"å¤œé›¨å¯„åŒ—->æå•†éš\"ã€‚\n\nimport torch\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\nfrom openmind.utils.import_utils import is_torch_npu_available\n\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/baichuan_7b\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"PyTorch-NPU/baichuan_7b\", device_map=\"npu:0\", trust_remote_code=True)\ninputs = tokenizer('ç™»é¹³é›€æ¥¼->ç‹ä¹‹æ¶£\\nå¤œé›¨å¯„åŒ—->', return_tensors='pt')\ninputs = inputs.to(device)\npred = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\næ¨¡å‹é…ç½®\n\næ•´ä½“æ¨¡å‹åŸºäºæ ‡å‡†çš„Transformerç»“æ„ï¼Œé‡‡ç”¨äº†å’ŒLLaMAä¸€æ ·çš„æ¨¡å‹è®¾è®¡ã€‚\n\nPosition Embeddingï¼šé‡‡ç”¨rotary-embeddingï¼Œæ˜¯ç°é˜¶æ®µè¢«å¤§å¤šæ•°æ¨¡å‹é‡‡ç”¨çš„ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œå…·æœ‰å¾ˆå¥½çš„å¤–æ¨æ€§ã€‚\n\nFeedforward Layerï¼šé‡‡ç”¨SwiGLUï¼ŒFeedforwardå˜åŒ–ä¸º(8/3)å€çš„éšå«å±‚å¤§å°ï¼Œå³11008ã€‚\n\nLayer Normalization: åŸºäºRMSNormçš„Pre-Normalizationã€‚\n\nå…·ä½“å‚æ•°è§ä¸‹è¡¨ï¼š\n\nè¶…å‚\tæ•°å€¼\nn_parameters\t7000559616\nn_layers\t32\nn_heads\t32\nd_model\t4096\nvocab size\t64000\nsequence length\t4096",
    "tags": "[\"PyTorch\", \"Transformers\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/bloom_1b7",
    "project_name": "bloom_1b7",
    "readme": "Original Text\næ™ºè°±æ¸…è¨€LM\nå¤§è§„æ¨¡ç§‘å­¦å¼€æ”¾è®¿é—®å¤šè¯­ç§è¯­è¨€æ¨¡å‹\næ¨¡å‹æ¦‚è¿°\n\nç‰ˆæœ¬ 1.0 / 2022å¹´5æœˆ26æ—¥\n\næ™ºè°±æ¸…è¨€-1b7æ¨¡å‹æ¦‚è¿°\nç›®å½•\næ¨¡å‹è¯¦æƒ…\nåº”ç”¨åœºæ™¯\nåè§ã€é£é™©ä¸é™åˆ¶\nå»ºè®®\nè®­ç»ƒæ•°æ®\nè¯„ä¼°\nç¯å¢ƒå½±å“\næŠ€æœ¯è§„æ ¼\nå¼•ç”¨\næœ¯è¯­ä¸è®¡ç®—\næ›´å¤šä¿¡æ¯\næ¨¡å‹æ¦‚è¿°ä½œè€…\næ¨¡å‹æ¦‚è¿°è”ç³»æ–¹å¼\nä¿®æ”¹\n\næ·»åŠ äº†ç¤ºä¾‹ä»£ç å¹¶ä¿®æ”¹äº†é“¾æ¥è·¯å¾„\n\næ¨¡å‹è¯¦æƒ…\nå¿«é€Ÿå…¥é—¨\n\nä»¥ä¸‹ä»£ç å±•ç¤ºäº†ä¸æ™ºè°±æ¸…è¨€_1b7äº’åŠ¨çš„ä¸€ä¸ªç¤ºä¾‹ï¼š\n\nimport torch\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bloom_1b7\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"PyTorch-NPU/bloom_1b7\", trust_remote_code=True, device_map=\"auto\")\n\ninput = \"Give three tips for staying healthy.\"\nprompt = (\"Below is an instrunction that describes a task. \"\n              \"Write a response that appropriately completes the requests\\n\\n\"\n              f\"### Instruction:\\n{input}\\n\\n### Response:\")\ninputs = tokenizer(prompt, return_tensors=\"pt\")\ninputs = inputs.to(model.device)\n\npred = model.generate(**inputs, max_new_tokens=512, repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\næ¨¡å‹æè¿°\n\næœ¬èŠ‚ä¸ºä»»ä½•å¸Œæœ›äº†è§£æ¨¡å‹ä¿¡æ¯çš„äººå£«æä¾›ä¿¡æ¯ã€‚\n\nå¼€å‘è€…ï¼š BigScience\n\næ‰€æœ‰åˆä½œè€…å‡ä¸ºå¿—æ„¿è€…æˆ–ä¸å…¶é›‡ä¸»æœ‰åè®®ã€‚(å‚ä¸è€…çš„è¿›ä¸€æ­¥ç»†åˆ†å³å°†å…¬å¸ƒ)ã€‚\n\næ¨¡å‹ç±»å‹ï¼š åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹\n\nç‰ˆæœ¬ï¼š 1.0.0\n\næ”¯æŒè¯­è¨€ï¼š å¤šç§ï¼›è¯¦è§è®­ç»ƒæ•°æ®\n\nè®¸å¯è¯ï¼š RAIL License v1.0 (é“¾æ¥)\n\né¢„è®¡å‘å¸ƒæ—¥æœŸï¼š 2022å¹´7æœˆ11æ—¥ï¼Œæ˜ŸæœŸä¸€\n\nèµ„é‡‘æ¥æºï¼š\n\næ³•å›½æ”¿åºœã€‚\n\nHugging Faceã€‚\n\nè´¡çŒ®è€…æ‰€åœ¨ç»„ç»‡ã€‚(ç»„ç»‡çš„è¿›ä¸€æ­¥ç»†åˆ†å³å°†å…¬å¸ƒ)ã€‚\n\nä½¿ç”¨åœºæ™¯\n\næœ¬èŠ‚è®¨è®ºäº†æ¨¡å‹é¢„è®¡çš„ä½¿ç”¨æ–¹å¼ã€å¯é¢„è§çš„æ¨¡å‹ç”¨æˆ·ï¼ˆåŒ…æ‹¬å—æ¨¡å‹å½±å“çš„äººå£«ï¼‰ï¼Œä»¥åŠè¢«è§†ä½œè¶…å‡ºèŒƒå›´æˆ–æ»¥ç”¨æ¨¡å‹çš„ä½¿ç”¨åœºæ™¯ã€‚æœ¬èŠ‚ä¸ºä»»ä½•è€ƒè™‘ä½¿ç”¨æ¨¡å‹æˆ–å—æ¨¡å‹å½±å“çš„äººå£«æä¾›ä¿¡æ¯ã€‚\n\né¢„å®šç”¨é€”\n\nè¯¥æ¨¡å‹çš„åˆ›å»ºæ—¨åœ¨æ¨åŠ¨å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…¬å…±ç ”ç©¶ã€‚LLMæ—¨åœ¨ç”¨äºè¯­è¨€ç”Ÿæˆæˆ–ä½œä¸ºé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œä»¥è¿›ä¸€æ­¥å¾®è°ƒç‰¹å®šä»»åŠ¡ã€‚ä»¥ä¸‹ç”¨ä¾‹å¹¶ä¸å…¨é¢ã€‚\n\nç›´æ¥ä½¿ç”¨\n\næ–‡æœ¬ç”Ÿæˆ\n\næ¢ç´¢ç”±è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è¯­è¨€ç‰¹å¾\n\nç¤ºä¾‹ï¼šClozeæµ‹è¯•ã€åäº‹å®æƒ…å†µã€å¸¦æœ‰æ¡†æ¶è°ƒæ•´çš„ç”Ÿæˆ\nä¸‹æ¸¸ä½¿ç”¨\nåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡åŒ…æ‹¬ï¼šä¿¡æ¯æå–ã€é—®é¢˜å›ç­”ã€æ‘˜è¦\næ»¥ç”¨ä¸è¶…èŒƒå›´ä½¿ç”¨\n\næœ¬èŠ‚è®¨è®ºç”¨æˆ·ä¸åº”å¦‚ä½•ä½¿ç”¨è¯¥æ¨¡å‹ã€‚\n\nè¯·å‚é˜…BLOOMè®¸å¯è¯é™„ä»¶Aä¸­çš„è¯¦ç»†ä½¿ç”¨é™åˆ¶ã€‚ä»¥ä¸‹åˆ—è¡¨å¹¶ä¸å…¨é¢ï¼Œä½†åˆ—å‡ºäº†ä¸€äº›å®¹æ˜“é¢„è§çš„æœ‰é—®é¢˜ä½¿ç”¨åœºæ™¯ã€‚\n\nè¶…èŒƒå›´ä½¿ç”¨\n\nåœ¨é«˜é£é™©ç¯å¢ƒä¸­ä½¿ç”¨è¯¥æ¨¡å‹è¶…å‡ºäº†èŒƒå›´ã€‚è¯¥æ¨¡å‹æœªè®¾è®¡ç”¨äºå…³é”®å†³ç­–æˆ–å¯¹ä¸ªäººç”Ÿæ´»æˆ–ç¦ç¥‰æœ‰å®è´¨å½±å“çš„ä½¿ç”¨åœºæ™¯ã€‚æ¨¡å‹çš„è¾“å‡ºå†…å®¹çœ‹ä¼¼çœŸå®ä½†å¹¶ä¸å‡†ç¡®ã€‚\n\nè¶…èŒƒå›´ä½¿ç”¨åŒ…æ‹¬ï¼š\n\nåœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸã€æ”¿æ²»å’Œæ³•å¾‹é¢†åŸŸæˆ–é‡‘èé¢†åŸŸçš„ä½¿ç”¨\n\nç”¨äºè¯„ä¼°æˆ–è¯„åˆ†ä¸ªäººï¼Œå¦‚å°±ä¸šã€æ•™è‚²æˆ–ä¿¡è´·\n\nå°†æ¨¡å‹åº”ç”¨äºå…³é”®è‡ªåŠ¨å†³ç­–ã€ç”Ÿæˆäº‹å®å†…å®¹ã€åˆ›å»ºå¯é çš„æ‘˜è¦æˆ–ç”Ÿæˆå¿…é¡»å‡†ç¡®é¢„æµ‹çš„å†…å®¹\n\næ»¥ç”¨\n\næ•…æ„ä½¿ç”¨æ¨¡å‹é€ æˆä¼¤å®³ã€ä¾µçŠ¯äººæƒæˆ–å…¶ä»–æ¶æ„æ´»åŠ¨å‡å±äºæ»¥ç”¨è¯¥æ¨¡å‹ã€‚åŒ…æ‹¬ï¼š\n\nç”Ÿæˆåƒåœ¾é‚®ä»¶\n\nåˆ¶é€ è™šå‡ä¿¡æ¯å’Œå½±å“æ“ä½œ\n\nè¯½è°¤å’Œä¸­ä¼¤\n\næå“å’Œæ»¥ç”¨\n\næ¬ºéª—\n\néæˆæƒæ¨¡ä»¿å’Œå†’å……\n\néæˆæƒç›‘è§†\n\næ ¹æ®RAILè®¸å¯è¯ã€ä½¿ç”¨é™åˆ¶çš„è¦æ±‚ï¼Œç”Ÿæˆæœªæ³¨æ˜æ¨¡å‹æ¥æºçš„å†…å®¹\n\né¢„å®šç”¨æˆ·\nç›´æ¥ç”¨æˆ·\n\nä¸€èˆ¬å…¬ä¼—\n\nç ”ç©¶äººå‘˜\n\nå­¦ç”Ÿ\n\næ•™è‚²å·¥ä½œè€…\n\nå·¥ç¨‹å¸ˆ/å¼€å‘è€…\n\néå•†ä¸šå®ä½“\n\nç¤¾åŒºå€¡å¯¼è€…ï¼ŒåŒ…æ‹¬äººæƒå’Œæ°‘æƒç»„ç»‡\n\né—´æ¥ç”¨æˆ·\n\nä½¿ç”¨ç›´æ¥ç”¨æˆ·åˆ›å»ºçš„æ´¾ç”Ÿäº§å“çš„ç”¨æˆ·ï¼Œä¾‹å¦‚ä½¿ç”¨å…·æœ‰é¢„å®šç”¨é€”çš„è½¯ä»¶çš„ç”¨æˆ·\n\nä½¿ç”¨è®¸å¯è¯ä¸­æè¿°çš„æ¨¡å‹æ´¾ç”Ÿäº§å“çš„ç”¨æˆ·\n\nå…¶ä»–å—å½±å“æ–¹ï¼ˆåˆ©ç›Šç›¸å…³è€…ï¼‰\n\nè¢«LLMæåˆ°çš„äººå£«å’Œå›¢ä½“\n\næš´éœ²äºLLMè¾“å‡ºæˆ–åŸºäºLLMçš„å†³ç­–å½±å“ä¸‹çš„äººå£«å’Œå›¢ä½“\n\nå…¶åŸåˆ›ä½œå“è¢«åŒ…å«åœ¨LLMä¸­çš„äººå£«å’Œå›¢ä½“\n\nåè§ã€é£é™©ä¸å±€é™æ€§\n\næœ¬èŠ‚è¯†åˆ«å¯é¢„è§çš„ä¼¤å®³å’Œè¯¯è§£ã€‚\n\næ¨¡å‹å¯èƒ½ä¼šï¼š\n\nè¿‡åº¦å‘ˆç°æŸäº›è§‚ç‚¹è€Œå¿½è§†å…¶ä»–è§‚ç‚¹\n\nåŒ…å«åˆ»æ¿å°è±¡\n\nåŒ…å«ä¸ªäººä¿¡æ¯\n\nç”Ÿæˆï¼š\n\nä»‡æ¨ã€æ»¥ç”¨æˆ–æš´åŠ›è¯­è¨€\n\næ­§è§†æ€§æˆ–åè§æ€§è¯­è¨€\n\nå¯èƒ½ä¸é€‚åˆæ‰€æœ‰åœºåˆçš„å†…å®¹ï¼ŒåŒ…æ‹¬æ€§å†…å®¹\n\nå‡ºç°é”™è¯¯ï¼ŒåŒ…æ‹¬å°†é”™è¯¯ä¿¡æ¯å½“ä½œäº‹å®ç”Ÿæˆ\n\nç”Ÿæˆä¸ç›¸å…³æˆ–é‡å¤çš„è¾“å‡º\n\næ¨èæªæ–½\n\næœ¬èŠ‚æä¾›æœ‰å…³è­¦å‘Šå’Œæ½œåœ¨ç¼“è§£æªæ–½çš„ä¿¡æ¯ã€‚\n\né—´æ¥ç”¨æˆ·åº”çŸ¥é“ä»–ä»¬å¤„ç†çš„å†…å®¹æ˜¯ç”±LLMåˆ›å»ºçš„ã€‚\n\nç”¨æˆ·åº”äº†è§£é£é™©ä¸å±€é™æ€§ï¼Œå¹¶åœ¨å¿…è¦æ—¶åŒ…æ‹¬é€‚å½“çš„å¹´é¾„å£°æ˜æˆ–æ‹¦æˆªç•Œé¢ã€‚\n\nä½¿ç”¨LLMé¢„è®­ç»ƒçš„æ¨¡å‹åº”åŒ…æ‹¬æ›´æ–°çš„æ¨¡å‹å¡ã€‚\n\nä½¿ç”¨æ¨¡å‹çš„ç”¨æˆ·åº”æä¾›å—å½±å“æ–¹åé¦ˆçš„æœºåˆ¶ï¼Œä¾‹å¦‚æä¾›ç”¨äºè¯„è®ºçš„ç”µå­é‚®ä»¶åœ°å€ã€‚\n\nè®­ç»ƒæ•°æ®\n\næœ¬èŠ‚ä¸ºå¸Œæœ›äº†è§£æ¨¡å‹å­¦ä¹ åŸºç¡€ä¿¡æ¯çš„äººå£«æä¾›è®­ç»ƒæ•°æ®çš„é«˜çº§æ¦‚è¿°ã€‚\n\næ¯ä¸ªæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯æä¾›åœ¨å•ç‹¬çš„æ•°æ®å¡ä¸­ã€‚\n\nè®­ç»ƒæ•°æ®åŒ…æ‹¬ï¼š\n\n45ç§è‡ªç„¶è¯­è¨€\n\n12ç§ç¼–ç¨‹è¯­è¨€\n\nåœ¨1.5TBçš„é¢„å¤„ç†æ–‡æœ¬ä¸­ï¼Œè½¬æ¢ä¸º3500äº¿ä¸ªå”¯ä¸€ä»¤ç‰Œï¼ˆè¯¦è§åˆ†è¯å™¨éƒ¨åˆ†ï¼‰ã€‚\n\nè¯­è¨€\n\né¥¼å›¾æ˜¾ç¤ºäº†è®­ç»ƒæ•°æ®ä¸­è¯­è¨€åˆ†å¸ƒã€‚\n\nä¸‹è¡¨æ˜¾ç¤ºäº†å°¼æ—¥å°”-åˆšæœè¯­æ—å’Œå°æ¬§è¯­ç³»è¯­è¨€åœ¨è®­ç»ƒæ•°æ®ä¸­çš„è¿›ä¸€æ­¥åˆ†å¸ƒã€‚\n\nå°¼æ—¥å°”-åˆšæœè¯­æ—\tç™¾åˆ†æ¯”\t\tå°æ¬§è¯­ç³»\tç™¾åˆ†æ¯”\nChi Tumbuka\t0.00002\t\tAssamese\t0.01\nKikuyu\t0.00004\t\tOdia\t0.04\nBambara\t0.00004\t\tGujarati\t0.04\nAkan\t0.00007\t\tMarathi\t0.05\nXitsonga\t0.00007\t\tPunjabi\t0.05\nSesotho\t0.00007\t\tKannada\t0.06\nChi Chewa\t0.0001\t\tNepali\t0.07\nSetswana\t0.0002\t\tTelugu\t0.09\nNorthern Sotho\t0.0002\t\tMalayalam\t0.10\nFon\t0.0002\t\tUrdu\t0.10\nKirundi\t0.0003\t\tTamil\t0.20\nWolof\t0.0004\t\tBengali\t0.50\nKuganda\t0.0004\t\tHindi\t0.70\nChi Shona\t0.001\t\t\t\nIsi Zulu\t0.001\t\t\t\nIgbo\t0.001\t\t\t\nXhosa\t0.001\t\t\t\nKinyarwanda\t0.003\t\t\t\nYoruba\t0.006\t\t\t\nSwahili\t0.02\t\t\t\n\nä¸‹è¡¨æ˜¾ç¤ºäº†ç¼–ç¨‹è¯­è¨€çš„åˆ†å¸ƒã€‚\n\næ‰©å±•å\tè¯­è¨€\tæ–‡ä»¶æ•°é‡\njava\tJava\t5,407,724\nphp\tPHP\t4,942,186\ncpp\tC++\t2,503,930\npy\tPython\t2,435,072\njs\tJavaScript\t1,905,518\ncs\tC#\t1,577,347\nrb\tRuby\t678,413\ncc\tC++\t443,054\nhpp\tC++\t391,048\nlua\tLua\t352,317\ngo\tGO\t227,763\nts\tTypeScript\t195,254\nC\tC\t134,537\nscala\tScala\t92,052\nhh\tC++\t67,161\nH\tC++\t55,899\ntsx\tTypeScript\t33,107\nrs\tRust\t29,693\nphpt\tPHP\t9,702\nc++\tC++\t1,342\nh++\tC++\t791\nphp3\tPHP\t540\nphps\tPHP\t270\nphp5\tPHP\t166\nphp4\tPHP\t29\nè¯„ä¼°\n\næœ¬èŠ‚ä»‹ç»è¯„ä¼°åè®®ï¼Œå¹¶æä¾›ç»“æœã€‚\n\næŒ‡æ ‡\n\næœ¬èŠ‚æè¿°äº†æ€§èƒ½è®¡ç®—çš„ä¸åŒæ–¹æ³•åŠå…¶åŸå› ã€‚\n\nåŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š\n\næŒ‡æ ‡\té€‰æ‹©åŸå› \nå›°æƒ‘åº¦\tè®­ç»ƒè¿‡ç¨‹ä¸­è¡¡é‡æ¨¡å‹æ”¹è¿›çš„æ ‡å‡†æŒ‡æ ‡\näº¤å‰ç†µ æŸå¤±\tè¯­è¨€æ¨¡å‹çš„æ ‡å‡†ç›®æ ‡ã€‚\n\né’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿˜æœ‰å¤šä¸ªä¸åŒçš„æŒ‡æ ‡ã€‚ (è¯„ä¼°åè®®å®Œæˆåï¼Œå°†æä¾›æ›´å¤šè¯„ä¼°æŒ‡æ ‡ã€‚)\n\nå› ç´ \n\næœ¬èŠ‚åˆ—å‡ºäº†BLOOMæ¨¡å‹çš„ä¸€äº›ä¸åŒæ–¹é¢ã€‚é‡ç‚¹å…³æ³¨å¯èƒ½å¯¼è‡´æ¨¡å‹è¡Œä¸ºé«˜æ–¹å·®çš„é‚£éƒ¨åˆ†ã€‚\n\nè¯­è¨€ï¼Œä¾‹å¦‚è‹±è¯­æˆ–çº¦é²å·´è¯­\n\né¢†åŸŸï¼Œä¾‹å¦‚æ–°é—»ç¨¿æˆ–æ•…äº‹\n\näººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼Œå¦‚æ€§åˆ«æˆ–å›½ç±\n\nç»“æœ\n\nç»“æœåŸºäº å› ç´  å’Œ æŒ‡æ ‡ã€‚\n\nè®­ç»ƒæ—¶è¯„ä¼°ï¼š\n\næˆªè‡³ 2022å¹´5æœˆ25æ—¥ï¼Œ15:00 PSTï¼š\n\nè®­ç»ƒæŸå¤±ï¼š2.0\n\néªŒè¯æŸå¤±ï¼š2.2\n\nå›°æƒ‘åº¦ï¼š8.9\n\nï¼ˆè®­ç»ƒç»“æŸæ—¶å°†æä¾›æ›´å¤šè¯„ä¼°å¾—åˆ†ã€‚ï¼‰\n\n[BLOOM ä¹¦ç±]ï¼šåŸºäºç¤¾åŒºæä¾›çš„æç¤ºï¼Œé˜…è¯»BLOOMç”Ÿæˆçš„æ–‡æœ¬\nç¯å¢ƒå½±å“\n\nè®­ç»ƒè¶…çº§è®¡ç®—æœº Jean Zay (ç½‘ç«™) ä¸»è¦ä½¿ç”¨æ ¸èƒ½ã€‚å…¶äº§ç”Ÿçš„çƒ­é‡è¢«é‡æ–°ç”¨äºä¾›æš–æ ¡å›­ä½å®…ã€‚\n\né¢„ä¼°ç¢³æ’æ”¾é‡ï¼š (è®­ç»ƒå®Œæˆåæä¾›)\n\né¢„ä¼°ç”µåŠ›æ¶ˆè€—ï¼š (è®­ç»ƒå®Œæˆåæä¾›)\n\næŠ€æœ¯è§„æ ¼\n\næœ¬èŠ‚ä¸ºä»äº‹æ¨¡å‹å¼€å‘çš„äººå‘˜æä¾›ä¿¡æ¯ã€‚\n\nè¯·å‚é˜… BLOOM è®­ç»ƒ READMEï¼Œäº†è§£å¤åˆ¶è®­ç»ƒçš„è¯¦ç»†ä¿¡æ¯ã€‚\n\næ¨¡å‹æ¶æ„ï¼š ä» Megatron-LM GPT2 ä¿®æ”¹è€Œæ¥ï¼ˆè§ è®ºæ–‡ï¼ŒBLOOM Megatron ä»£ç )ï¼š\n\nä»…è§£ç å™¨æ¶æ„\n\nå¯¹è¯åµŒå…¥å±‚ï¼ˆStableEmbeddingï¼›è§ ä»£ç ï¼Œè®ºæ–‡ï¼‰åº”ç”¨å±‚å½’ä¸€åŒ–\n\nALiBI ä½ç½®ç¼–ç ï¼ˆè§ è®ºæ–‡ï¼‰ï¼Œé…åˆ GeLU æ¿€æ´»å‡½æ•°\n\næ€»å…± 1,722,408,960 ä¸ªå‚æ•°ï¼š\n\n513,802,240 ä¸ªåµŒå…¥å‚æ•°\n\n24 å±‚ï¼Œ16 ä¸ªæ³¨æ„åŠ›å¤´\n\néšè—å±‚ä¸º 2048 ç»´\n\nä½¿ç”¨çš„åºåˆ—é•¿åº¦ä¸º 2048 ä¸ªæ ‡è®°ï¼ˆè§ BLOOM åˆ†è¯å™¨ï¼Œåˆ†è¯æè¿°ï¼‰\n\nç›®æ ‡å‡½æ•°ï¼š äº¤å‰ç†µï¼Œå–å¹³å‡å€¼ï¼ˆè§ API æ–‡æ¡£ï¼‰ã€‚\n\nè®¡ç®—åŸºç¡€è®¾æ–½ï¼š Jean Zay å…¬å…±è¶…çº§è®¡ç®—æœºï¼Œç”±æ³•å›½æ”¿åºœæä¾›ï¼ˆè§ å…¬å‘Šï¼‰ã€‚\n\nç¡¬ä»¶ï¼š64 å— V100 16/32GB GPUï¼ˆ16 ä¸ªèŠ‚ç‚¹ï¼‰ï¼š\n\næ¯ä¸ªèŠ‚ç‚¹ 4 å— GPU\n\næ¯ä¸ªä»»åŠ¡ 40 ä¸ª CPU\n\næ¯ä¸ªèŠ‚ç‚¹ 1 ä¸ªä»»åŠ¡\n\nCPUï¼šAMD\n\nCPU å†…å­˜ï¼šæ¯ä¸ªèŠ‚ç‚¹ 160GB\n\nGPU å†…å­˜ï¼šæ¯ä¸ªèŠ‚ç‚¹ 64GB æˆ– 128GBï¼ˆå–å†³äºè®­ç»ƒæœŸé—´èŠ‚ç‚¹çš„å¯ç”¨æ€§ï¼‰\n\nèŠ‚ç‚¹é—´è¿æ¥ï¼šOmni-Path æ¶æ„ï¼ˆOPAï¼‰\n\nNCCL é€šä¿¡ç½‘ç»œï¼šä¸€ä¸ªå®Œå…¨ä¸“ç”¨çš„å­ç½‘\n\nç£ç›˜ IO ç½‘ç»œï¼šä¸å…¶ä»–ç±»å‹èŠ‚ç‚¹å…±äº«çš„ç½‘ç»œ\n\nè½¯ä»¶ï¼š\n\nMegatron-DeepSpeed (Github é“¾æ¥)\n\nDeepSpeed (Github é“¾æ¥)\n\nPyTorch (pytorch-1.11 w/ CUDA-11.5ï¼›è§ Github é“¾æ¥)\n\napex (Github é“¾æ¥)\n\nè®­ç»ƒ\n\næ£€æŸ¥ç‚¹å¤§å°ï¼š\n\nFp16 æƒé‡ï¼š2.6GBï¼ˆ# å‚æ•° * 2ï¼‰\n\nå¸¦æœ‰ä¼˜åŒ–å™¨çŠ¶æ€çš„å®Œæ•´æ£€æŸ¥ç‚¹ï¼š--\n\nè®­ç»ƒååé‡ï¼š--\n\nè¿­ä»£æ¬¡æ•°ï¼š1\n\næ—¥æœŸï¼š\n\nå¼€å§‹ï¼š2022å¹´3æœˆ11æ—¥ï¼Œ11:42am PST\n\nç»“æŸï¼š2022å¹´5æœˆ20æ—¥\n\næœåŠ¡å™¨è®­ç»ƒä½ç½®ï¼šæ³•å›½ Ãle-de-France\n\nåˆ†è¯\n\nBLOOM åˆ†è¯å™¨ (é“¾æ¥) æ˜¯ä¸€ä¸ªå­¦ä¹ çš„å­è¯åˆ†è¯å™¨ï¼Œä½¿ç”¨ä»¥ä¸‹æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼š\n\nå­—èŠ‚çº§çš„å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰ç®—æ³•\n\nä¸€ä¸ªç®€å•çš„é¢„åˆ†è¯è§„åˆ™ï¼Œæ— å½’ä¸€åŒ–\n\nè¯æ±‡å¤§å°ä¸º 250,680\n\nå®ƒæ˜¯é€šè¿‡å¯¹è¯­è¨€ä½¿ç”¨ alpha åŠ æƒçš„ä¸€ä¸ªé¢„å…ˆç‰ˆæœ¬çš„è¯­æ–™åº“å­é›†è¿›è¡Œè®­ç»ƒçš„ã€‚\n\nå¼•ç”¨\n\nå¼•ç”¨ä¸ºï¼š BigScience, BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model. å›½é™…ï¼Œ2021å¹´5æœˆ-2022å¹´5æœˆ\n\næœ¯è¯­å’Œè®¡ç®—\n\næœ¬èŠ‚å®šä¹‰å¸¸è§æœ¯è¯­åŠæŒ‡æ ‡çš„è®¡ç®—æ–¹æ³•ã€‚\n\næŸå¤±ï¼š è®¡ç®—æ¨¡å‹æ‰€å­¦å†…å®¹ä¸æ•°æ®æ˜¾ç¤ºçš„â€œçœŸå®å€¼â€ä¹‹é—´çš„å·®å¼‚ã€‚æŸå¤±è¶Šä½ï¼Œæ¨¡å‹æ•ˆæœè¶Šå¥½ã€‚è®­ç»ƒè¿‡ç¨‹æ—¨åœ¨æœ€å°åŒ–æŸå¤±ã€‚\n\nå›°æƒ‘åº¦ï¼š è¿™æ˜¯åŸºäºæ¨¡å‹å¯¹æ–°æ•°æ®çš„æ¦‚ç‡ä¼°è®¡ã€‚å›°æƒ‘åº¦è¶Šä½ï¼Œæ¨¡å‹æ•ˆæœè¶Šå¥½ã€‚å¦‚æœæ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªå°†çœ‹åˆ°çš„æ ‡è®°æ—¶å®Œå…¨æ­£ç¡®ï¼Œé‚£ä¹ˆå›°æƒ‘åº¦å°±æ˜¯ 1ã€‚æ•°å­¦ä¸Šè¿™æ˜¯é€šè¿‡ç†µæ¥è®¡ç®—çš„ã€‚\n\né«˜é£é™©è®¾ç½®ï¼š ä¾‹å¦‚ï¼Œæ¬§ç›Ÿææ¡ˆä¸­çš„â€œé«˜é£é™© AI ç³»ç»Ÿâ€å’Œâ€œä¸å¯æ¥å—é£é™© AI ç³»ç»Ÿâ€ (äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ³•æ¡ˆï¼‰ã€‚\n\nå…³é”®å†³ç­–ï¼š ä¾‹å¦‚ï¼Œç¾å›½ææ¡ˆä¸­çš„ç®—æ³•è´£ä»»æ³•æ¡ˆä¸­å®šä¹‰çš„ å…³é”®å†³ç­–ã€‚\n\näººæƒï¼š åŒ…æ‹¬ ä¸–ç•Œäººæƒå®£è¨€ ä¸­å®šä¹‰çš„æƒåˆ©ã€‚\n\nä¸ªäººæ•°æ®å’Œä¸ªäººä¿¡æ¯ï¼š ä¸ªäººæ•°æ®å’Œä¿¡æ¯çš„å®šä¹‰åœ¨å¤šä¸ªæ•°æ®ä¿æŠ¤æ³•è§„ä¸­æœ‰æ‰€ä¸åŒï¼Œä¾‹å¦‚æ¬§ç›Ÿé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ä¸­çš„â€œä¸ªäººæ•°æ®â€(ä¸ªäººæ•°æ®)ï¼›ä»¥åŠå—éå…±å’Œå›½çš„ ä¸ªäººæ•°æ®ä¿æŠ¤æ³• ä¸­çš„â€œä¸ªäººä¿¡æ¯â€ï¼›ä¸­åäººæ°‘å…±å’Œå›½çš„ ä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•ã€‚\n\næ•æ„Ÿç‰¹å¾ï¼š è¿™åŒ…æ‹¬äººæƒï¼ˆè§ UHDRï¼Œç¬¬2æ¡ï¼‰å’Œä¸ªäººä¿¡æ¯æ³•è§„ï¼ˆè§ GDPRï¼Œç¬¬9æ¡ï¼›ä¸ªäººæ•°æ®ä¿æŠ¤æ³•ï¼Œç¬¬1ç« ï¼‰ä¸­ç‰¹åˆ«ä¿æŠ¤çš„ç±»åˆ«ã€‚\n\næ¬ºè¯ˆï¼š é€šè¿‡åˆ›å»ºç¤¾äº¤åª’ä½“ä¸Šçš„æ­»æœºå™¨äººæˆ–èŠå¤©æœºå™¨äººå‡æ‰®çœŸäººï¼Œæˆ–ç”Ÿæˆæ¶ˆè´¹è€…æœªæ„è¯†åˆ°æ˜¯ç”±æœºå™¨ç”Ÿæˆçš„æ–‡æœ¬ï¼Œæ•…æ„è¯¯å¯¼äººä»¬ç›¸ä¿¡è™šå‡ä¿¡æ¯ã€‚\n\næ›´å¤šä¿¡æ¯\næ•°æ®é›†åˆ›å»º\n\nè¯¦ç»†ä»‹ç»æ•°æ®é›†åˆ›å»ºè¿‡ç¨‹ä¸­çš„è®¾è®¡é€‰æ‹©çš„åšå®¢æ–‡ç« \n\næŠ€æœ¯è§„æ ¼\n\næ¦‚è¿°æ¶æ„ã€è§„æ¨¡ã€å½¢æ€ä»¥åŠé¢„è®­ç»ƒæŒç»­æ—¶é—´é€‰æ‹©è¿‡ç¨‹çš„åšå®¢æ–‡ç« \n\nå…³äºæ¶æ„/ä¼˜åŒ–å™¨çš„æ›´å¤šç»†èŠ‚ï¼šhttps://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml\n\nå…³äºç¡¬ä»¶/å·¥ç¨‹æ–¹é¢çš„åšå®¢æ–‡ç« \n\nåˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒé…ç½®çš„è¯¦ç»†ä¿¡æ¯ï¼šhttps://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml\n\nè®­ç»ƒæœŸé—´æ›´æ–°çš„Tensorboard\n\nå…³äºå¦‚ä½•è¿›è¡Œè®­ç»ƒçš„è§è§£ä»¥åŠè´Ÿé¢ç»“æœçš„è¯´æ˜ï¼šhttps://github.com/bigscience-workshop/bigscience/blob/master/train/lessons-learned.md\n\nå…³äºå·¥ç¨‹å‡†å¤‡è¿‡ç¨‹ä¸­å…‹æœçš„éšœç¢çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬ç¨³å®šæ€§é—®é¢˜ã€è®­ç»ƒååé‡ä¼˜åŒ–ã€ä¼—å¤šæŠ€æœ¯æŠ€å·§å’Œé—®é¢˜ï¼‰ï¼šhttps://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md\n\nåˆæ­¥ç»“æœ\n\nä½¿ç”¨ä¸´æ—¶æ£€æŸ¥ç‚¹è¿›è¡Œçš„åˆæ­¥æç¤ºå®éªŒ\n\næ¨¡å‹å¡ä½œè€…\n\næŒ‰æ—¶é—´é¡ºåºåŠæŠ•å…¥æ—¶é—´å¤šå°‘æ’åˆ—\n\nMargaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos MuÃ±oz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana IliÄ‡, GÃ©rard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff\n\næ¨¡å‹å¡è”ç³»æ–¹å¼\n\nå‘é€é—®é¢˜è‡³ï¼š bigscience-contact@googlegroups.com",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"Safetensors\", \"JAX\", \"English\", \"Chinese\", \"Other\", \"bloom\"]"
  },
  {
    "url": "https://gitcode.com/openMind/bluelm_7b_chat",
    "project_name": "bluelm_7b_chat",
    "readme": "Original Text\nBlueLM\n\nğŸ–¥ GitHub â€¢ ğŸ“œ è®¸å¯è¯ â€¢ ğŸ¯ vivoå¼€å‘è€…å¹³å° â€¢ ğŸ—¨ å¾®ä¿¡äº¤æµç¾¤\n\nä¿®æ”¹è¯´æ˜/Modification\n\nä¼˜åŒ–README.mdä¸­çš„ç¤ºä¾‹ä»£ç ï¼Œæ–°å¢NPUç¡¬ä»¶æ”¯æŒã€‚ Modified examples in README.md to include NPU support.\n\næ¨¡å‹ä»‹ç»/Introduction\n\nBlueLMæ˜¯ç”±vivo AIå…¨çƒç ”ç©¶é™¢è‡ªä¸»ç ”å‘çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚æœ¬æ¬¡å¼€æºç‰ˆæœ¬åŒ…å«7BåŸºç¡€æ¨¡å‹ä¸7Bå¯¹è¯æ¨¡å‹ï¼ŒåŒæ—¶æ¨å‡ºæ”¯æŒ32Ké•¿æ–‡æœ¬å¤„ç†çš„åŸºç¡€æ¨¡å‹å’Œå¯¹è¯æ¨¡å‹ã€‚\n\næµ·é‡ä¼˜è´¨è®­ç»ƒæ•°æ®ï¼šåŸºäº2.6ä¸‡äº¿tokençš„é«˜è´¨é‡å¤šè¯­è¨€è¯­æ–™åº“è®­ç»ƒï¼Œæ¶µç›–ä¸­è‹±æ–‡åŠå°‘é‡æ—¥éŸ©æ•°æ®ã€‚\nå“è¶Šæ€§èƒ½è¡¨ç°ï¼šBlueLM-7B-Chatåœ¨C-Evalå’ŒCMMLUè¯„æµ‹ä¸­å‡å–å¾—åŒå°ºå¯¸å¼€æºæ¨¡å‹é¢†å…ˆæˆç»©ã€‚\nè¶…é•¿ä¸Šä¸‹æ–‡æ”¯æŒï¼šBlueLM-7B-Base-32Kä¸BlueLM-7B-Chat-32Kåœ¨ä¿æŒåŸºç¡€èƒ½åŠ›çš„åŒæ—¶ï¼Œå…·å¤‡32Kçº§åˆ«çš„é•¿æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚\nå¼€æ”¾æˆæƒåè®®ï¼šBlueLMç³»åˆ—æ¨¡å‹æ”¯æŒå­¦æœ¯ç ”ç©¶ä¸å•†ä¸šåº”ç”¨ã€‚\n\nBlueLM is a large-scale open-source language model developed by vivo AI Lab. This release includes both 2K and 32K context length versions for Base and Chat models.\n\nPremium Data Quality: Trained on 2.6 trillion tokens of multilingual corpus with rigorous quality control.\nBenchmark Leadership: BlueLM-7B-Chat achieves top-tier results in C-Eval and CMMLU among comparable open-source models.\nExtended Context: The 32K versions maintain core competencies while significantly expanding context handling capacity.\nCommercial-Friendly: Released under permissive license for both academic and commercial use.\nè¯„æµ‹ç»“æœ/Benchmark Results\n\né‡‡ç”¨OpenCompassç»Ÿä¸€è¯„æµ‹æ¡†æ¶ï¼Œåœ¨C-Evalã€MMLUã€CMMLUã€é«˜è€ƒã€AGIEvalã€BBHã€GSM8Kã€MATHå’ŒHumanEvalç­‰åŸºå‡†æµ‹è¯•ä¸­å…¨é¢è¯„ä¼°æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€æ•°ç†é€»è¾‘ä¸ç¼–ç¨‹èƒ½åŠ›ã€‚\n\nUsing standardized OpenCompass evaluation protocol, we conducted comprehensive assessments across general intelligence, mathematical reasoning and coding capabilities.\n\næ¨¡å‹\tC-Eval\tMMLU\tCMMLU\té«˜è€ƒ\tAGIEval\tBBH\tGSM8K\tMATH\tHumanEval\n\t5-shot\t5-shot\t5-shot\t0-shot\t0-shot\t3-shot\t4-shot\t5-shot\t0-shot\nGPT-4\t69.9\t86.4\t71.2\t72.3\t55.1\t86.7\t91.4\t45.8\t74.4\nChatGPT\t52.5\t70.0\t53.9\t51.1\t39.9\t70.1\t78.2\t28\t73.2\nLLaMA2-7B\t32.5\t45.3\t31.8\t18.9\t21.8\t38.2\t16.7\t3.3\t12.8\nChatGLM2-6B(åŸºåº§)\t51.7\t47.9\t50.0\t-\t-\t33.7\t32.4\t6.5\t-\nBaichuan2-7B\t56.3\t54.7\t57.0\t34.8\t34.6\t41.8\t24.6\t5.4\t17.7\nBlueLM-7B-åŸºåº§\t67.5\t55.2\t66.6\t58.9\t43.4\t41.7\t27.2\t6.2\t18.3\nBlueLM-7B-å¯¹è¯\t72.7\t50.7\t74.2\t48.7\t43.4\t65.6\t51.9\t13.4\t21.3\næ¨ç†éƒ¨ç½²/Inference and Deployment\nimport torch\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name=\"PyTorch-NPU/bluelm_7b_chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\nmodel = model.eval()\n\ninputs = tokenizer(\"[|Human|]:ä¸‰å›½æ¼”ä¹‰çš„ä½œè€…æ˜¯è°ï¼Ÿ[|AI|]:\", return_tensors=\"pt\").to(model.device)\npred = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\n\n\næ›´å¤šä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„ Github ä»“åº“ã€‚\n\nFor more instructions, please refer to our Github Repo.\n\nåè®®/License\n\næœ¬ç¤¾åŒºä»£ç åŸºäº Apache-2.0 åè®®å¼€æºï¼Œä½¿ç”¨ BlueLM æ¨¡å‹æƒé‡éœ€éµå®ˆ vivo_BlueLMæ¨¡å‹è®¸å¯åè®®ã€‚\n\nOur code is licensed under Apache-2.0, and the use of BlueLM model weights is subject to the Community License for BlueLM Model.",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"ELM\", \"English\", \"Chinese\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/albert_xxlarge_v2",
    "project_name": "albert_xxlarge_v2",
    "readme": "Original Text\nä¿®æ”¹\nä¿®æ”¹ç¤ºä¾‹å¹¶å¢åŠ  NPU æ”¯æŒï¼›\nä¿®æ”¹â€œé™åˆ¶ä¸åå·®â€å’Œâ€œé¢„æœŸç”¨é€”åŠé™åˆ¶â€éƒ¨åˆ†ã€‚\nALBERT XXLarge v2\n\nåŸºäºè‹±æ–‡è¯­æ–™ï¼Œé€šè¿‡é®è”½è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚å®ƒé¦–æ¬¡åœ¨ è¿™ç¯‡è®ºæ–‡ä¸­æå‡ºï¼Œå¹¶åœ¨ è¿™ä¸ªä»“åº“ä¸­é¦–æ¬¡å‘å¸ƒã€‚ä¸æ‰€æœ‰ ALBERT æ¨¡å‹ä¸€æ ·ï¼Œæ­¤æ¨¡å‹ä¸åŒºåˆ†å¤§å°å†™ï¼šå®ƒä¸å¯¹è‹±æ–‡å’ŒEnglishè¿›è¡ŒåŒºåˆ†ã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒ ALBERT çš„å›¢é˜Ÿæœªä¸ºæ­¤æ¨¡å‹ç¼–å†™æ¨¡å‹å¡ç‰‡ï¼Œå› æ­¤æ­¤æ¨¡å‹å¡ç‰‡ç”± Hugging Face å›¢é˜Ÿç¼–å†™ã€‚\n\næ¨¡å‹æè¿°\n\nALBERT æ˜¯ä¸€ç§åœ¨å¤§é‡è‹±æ–‡æ•°æ®ä¸Šä»¥è‡ªç›‘ç£æ–¹å¼é¢„è®­ç»ƒçš„å˜æ¢å™¨æ¨¡å‹ã€‚è¿™æ„å‘³ç€å®ƒä»…åœ¨æœªç»äººå·¥æ ‡æ³¨çš„åŸå§‹æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ˆè¿™ä¹Ÿæ˜¯å®ƒèƒ½ä½¿ç”¨å¤§é‡å…¬å¼€å¯ç”¨æ•°æ®çš„åŸå› ï¼‰ï¼Œé€šè¿‡è‡ªåŠ¨æµç¨‹ä»è¿™äº›æ–‡æœ¬ä¸­ç”Ÿæˆè¾“å…¥å’Œæ ‡ç­¾ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå®ƒæ˜¯é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒçš„ï¼š\n\né®è”½è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ï¼šæ¨¡å‹éšæœºé®è”½è¾“å…¥ä¸­çš„15%çš„å•è¯ï¼Œç„¶åå°†æ•´ä¸ªé®è”½çš„å¥å­é€šè¿‡æ¨¡å‹ï¼Œå¹¶é¢„æµ‹é®è”½çš„å•è¯ã€‚è¿™ä¸ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ä¸åŒï¼Œåè€…é€šå¸¸ä¸€æ¬¡çœ‹åˆ°ä¸€ä¸ªå•è¯ï¼Œæˆ–è€…åƒ GPT è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹ï¼Œåœ¨å†…éƒ¨é®è”½æœªæ¥çš„æ ‡è®°ã€‚è¿™ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¥å­çš„åŒå‘è¡¨ç¤ºã€‚\nå¥å­æ’åºé¢„æµ‹ï¼ˆSOPï¼‰ï¼šALBERT ä½¿ç”¨åŸºäºé¢„æµ‹ä¸¤ä¸ªè¿ç»­æ–‡æœ¬ç‰‡æ®µé¡ºåºçš„é¢„è®­ç»ƒæŸå¤±ã€‚\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å­¦ä¹ äº†è‹±è¯­çš„å†…åœ¨è¡¨ç¤ºï¼Œç„¶åå¯ä»¥ç”¨äºæå–å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾ï¼šä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªæ ‡è®°å¥å­çš„æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ ALBERT æ¨¡å‹ç”Ÿæˆçš„ç‰¹å¾ä½œä¸ºè¾“å…¥æ¥è®­ç»ƒä¸€ä¸ªæ ‡å‡†åˆ†ç±»å™¨ã€‚\n\nALBERT çš„ç‰¹ç‚¹æ˜¯å®ƒåœ¨å˜æ¢å™¨ä¸­å…±äº«å…¶å±‚ã€‚å› æ­¤ï¼Œæ‰€æœ‰å±‚éƒ½æœ‰ç›¸åŒçš„æƒé‡ã€‚ä½¿ç”¨é‡å¤å±‚å¯ä»¥å‡å°‘å†…å­˜å ç”¨ï¼Œç„¶è€Œï¼Œè®¡ç®—æˆæœ¬ä»ç„¶ä¸å…·æœ‰ç›¸åŒéšè—å±‚æ•°çš„ BERT ç±»ä¼¼æ¶æ„ç›¸è¿‘ï¼Œå› ä¸ºå®ƒå¿…é¡»éå†ç›¸åŒæ•°é‡çš„ï¼ˆé‡å¤ï¼‰å±‚ã€‚\n\nè¿™æ˜¯ xxlarge æ¨¡å‹çš„ç¬¬äºŒä¸ªç‰ˆæœ¬ã€‚ç‰ˆæœ¬2ç”±äºä¸åŒçš„ä¸¢å¼ƒç‡ã€é¢å¤–çš„è®­ç»ƒæ•°æ®å’Œæ›´é•¿çš„è®­ç»ƒæ—¶é—´è€Œä¸ç‰ˆæœ¬1ä¸åŒã€‚å®ƒåœ¨å‡ ä¹æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡ä¸­éƒ½æœ‰æ›´å¥½çš„ç»“æœã€‚\n\næ­¤æ¨¡å‹å…·æœ‰ä»¥ä¸‹é…ç½®ï¼š\n\n12ä¸ªé‡å¤å±‚\n128ç»´åµŒå…¥ç»´åº¦\n4096ç»´éšè—ç»´åº¦\n64ä¸ªæ³¨æ„åŠ›å¤´\n2.23äº¿ä¸ªå‚æ•°\né¢„æœŸç”¨é€”åŠé™åˆ¶\n\næ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œé®è”½è¯­è¨€å»ºæ¨¡æˆ–ä¸‹ä¸€å¥é¢„æµ‹ï¼Œä½†å®ƒä¸»è¦ç”¨äºåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n\nè¯·æ³¨æ„ï¼Œæ­¤æ¨¡å‹ä¸»è¦æ—¨åœ¨ç”¨äºåœ¨åˆ©ç”¨æ•´ä¸ªï¼ˆå¯èƒ½è¢«é®è”½çš„ï¼‰å¥å­æ¥åšå†³ç­–çš„ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä¾‹å¦‚åºåˆ—åˆ†ç±»ã€æ ‡è®°åˆ†ç±»æˆ–é—®é¢˜å›ç­”ã€‚å¯¹äºå¦‚æ–‡æœ¬ç”Ÿæˆè¿™æ ·çš„ä»»åŠ¡ï¼Œæ‚¨åº”è¯¥è€ƒè™‘åƒ GPT2 è¿™æ ·çš„æ¨¡å‹ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\næ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤æ¨¡å‹é€šè¿‡ä¸€ä¸ªæµç¨‹è¿›è¡Œé®è”½è¯­è¨€å»ºæ¨¡ï¼š\n\nfrom openmind import pipeline\n\nunmasker = pipeline('fill-mask', device_map=\"npu:0\", model='PyTorch-NPU/albert_xxlarge_v2')\nunmasker(\"Hello I'm a [MASK] model.\")\n\n\nå±€é™æ€§ä¸åè§\n\nå°½ç®¡ç”¨äºè¯¥æ¨¡å‹çš„è®­ç»ƒæ•°æ®å¯ä»¥è¢«æè¿°ä¸ºç›¸å¯¹ä¸­ç«‹ï¼Œä½†è¯¥æ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿå¸¦æœ‰åè§çš„é¢„æµ‹ã€‚è¿™ç§åè§ä¹Ÿä¼šå½±å“è¯¥æ¨¡å‹çš„æ‰€æœ‰å¾®è°ƒç‰ˆæœ¬ã€‚\n\nè®­ç»ƒæ•°æ®\n\nALBERT æ¨¡å‹åœ¨ BookCorpus æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å« 11,038 æœ¬æœªå‘è¡¨ä¹¦ç±å’Œ è‹±æ–‡ç»´åŸºç™¾ç§‘ï¼ˆä¸åŒ…æ‹¬åˆ—è¡¨ã€è¡¨æ ¼å’Œæ ‡é¢˜ï¼‰ã€‚\n\nè®­ç»ƒè¿‡ç¨‹\né¢„å¤„ç†\n\næ–‡æœ¬è¢«è½¬æ¢ä¸ºå°å†™ï¼Œå¹¶ä½¿ç”¨ SentencePiece è¿›è¡Œåˆ†è¯ï¼Œè¯æ±‡è¡¨å¤§å°ä¸º 30,000ã€‚æ¨¡å‹çš„è¾“å…¥å½¢å¼å¦‚ä¸‹ï¼š\n\n[CLS] Sentence A [SEP] Sentence B [SEP]\n\nè®­ç»ƒ\n\nALBERT ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹éµå¾ª BERT æ¨¡å‹çš„è®¾ç½®ã€‚\n\nå¯¹äºæ¯ä¸ªå¥å­çš„æ©ç è¿‡ç¨‹ï¼Œå…·ä½“ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n15% çš„ä»¤ç‰Œè¢«æ©ç ã€‚\nåœ¨ 80% çš„æƒ…å†µä¸‹ï¼Œè¢«æ©ç çš„ä»¤ç‰Œè¢«æ›¿æ¢ä¸º `\n@article{DBLP:journals/corr/abs-1909-11942,\n  author    = {Zhenzhong Lan and\n               Mingda Chen and\n               Sebastian Goodman and\n               Kevin Gimpel and\n               Piyush Sharma and\n               Radu Soricut},\n  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language\n               Representations},\n  journal   = {CoRR},\n  volume    = {abs/1909.11942},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1909.11942},\n  archivePrefix = {arXiv},\n  eprint    = {1909.11942},\n  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n\nç‚¹å‡»ä»¥ä¸‹é“¾æ¥æŸ¥çœ‹æ¨¡å‹è¯¦ç»†ä¿¡æ¯ï¼š ",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Safetensors\", \"English\", \"Apache License 2.0\", \"bookcorpus\", \"wikipedia\", \"exbert\"]"
  },
  {
    "url": "https://gitcode.com/openMind/byt5_large",
    "project_name": "byt5_large",
    "readme": "ByT5 - large\n\nPaper: ByT5: Towards a token-free future with pre-trained byte-to-byte models\n\nAuthors: Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel\n\nModification\n\nAdded the CANN version dependency description to the original README and modified the example code.\n\nExample Inference\nfrom openmind import AutoTokenizer\nfrom transformers import T5ForConditionalGeneration\n\ndevice = \"npu:0\"\nmodel_name = \"PyTorch-NPU/byt5_large\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n\nmodel_inputs = tokenizer([\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\",\n                          return_tensors=\"pt\").to(device)\nlabels_dict = tokenizer([\"La vie est comme une boÃ®te de chocolat.\", \"Aujourd'hui c'est lundi.\"], padding=\"longest\",\n                    return_tensors=\"pt\").to(device)\nlabels = labels_dict.input_ids\nloss = model(**model_inputs, labels=labels).loss\nprint()\nprint(\"loss:\", loss.item())\n\n\n\nAbstract\n\nMost widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.",
    "tags": "[\"PyTorch\", \"TensorFlow\", \"Transformers\", \"JAX\", \"English\", \"Apache License 2.0\", \"mc4\"]"
  },
  {
    "url": "https://gitcode.com/openMind/deberta_v3_base",
    "project_name": "deberta_v3_base",
    "readme": "Original Text\nDeBERTaV3ï¼šé€šè¿‡ELECTRAé£æ ¼é¢„è®­ç»ƒä¸æ¢¯åº¦è§£è€¦åµŒå…¥å…±äº«æå‡DeBERTaæ€§èƒ½\n\nDeBERTa é€šè¿‡è§£è€¦æ³¨æ„åŠ›æœºåˆ¶å’Œå¢å¼ºå‹æ©ç è§£ç å™¨æ”¹è¿›äº†BERTä¸RoBERTaæ¨¡å‹ã€‚å‡­å€Ÿè¿™ä¸¤é¡¹åˆ›æ–°ï¼ŒDeBERTaåœ¨80GBè®­ç»ƒæ•°æ®ä¸‹ï¼Œäºå¤šæ•°è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸­è¶…è¶Šäº†RoBERTaçš„è¡¨ç°ã€‚\n\nåœ¨DeBERTa V3ä¸­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨ELECTRAé£æ ¼çš„é¢„è®­ç»ƒç»“åˆæ¢¯åº¦è§£è€¦åµŒå…¥å…±äº«æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ•ˆç‡ã€‚ç›¸è¾ƒäºå‰ä»£ï¼ŒV3ç‰ˆæœ¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°å®ç°äº†è´¨çš„é£è·ƒã€‚æ›´å¤šæŠ€æœ¯ç»†èŠ‚è¯¦è§æˆ‘ä»¬çš„è®ºæ–‡ã€‚\n\næ¬¢è¿è®¿é—®å®˜æ–¹ä»£ç åº“è·å–æ›´å¤šå®ç°ç»†èŠ‚ä¸æ›´æ–°åŠ¨æ€ã€‚\n\nDeBERTa V3åŸºç¡€æ¨¡å‹åŒ…å«12å±‚ç½‘ç»œç»“æ„ï¼Œéšè—å±‚ç»´åº¦ä¸º768ã€‚å…¶ä¸»å¹²å‚æ•°ä»…8600ä¸‡ï¼Œè¯è¡¨è§„æ¨¡è¾¾128Kè¯å…ƒï¼ŒåµŒå…¥å±‚å‚æ•°é‡ä¸º9800ä¸‡ã€‚æœ¬æ¨¡å‹ä½¿ç”¨ä¸DeBERTa V2ç›¸åŒçš„160GBæ•°æ®è¿›è¡Œè®­ç»ƒã€‚\n\nä¿®æ”¹è¯´æ˜\nè°ƒæ•´pipeline_tagä¸frameworksé…ç½®\næ–°å¢npuæ”¯æŒ\nè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡å¾®è°ƒç»“æœ\n\nä»¥ä¸‹å±•ç¤ºSQuAD 2.0å’ŒMNLIä»»åŠ¡çš„å¼€å‘é›†è¡¨ç°ï¼š\n\næ¨¡å‹\tè¯è¡¨é‡(K)\tä¸»å¹²å‚æ•°é‡(M)\tSQuAD 2.0(F1/EM)\tMNLI-m/mm(å‡†ç¡®ç‡)\nRoBERTa-base\t50\t86\t83.7/80.5\t87.6/-\nXLNet-base\t32\t92\t-/80.2\t86.8/-\nELECTRA-base\t30\t86\t-/80.5\t88.8/\nDeBERTa-base\t50\t100\t86.2/83.1\t88.8/88.5\nDeBERTa-v3-base\t128\t86\t88.4/85.4\t90.6/90.7\nDeBERTa-v3-base + SiFT\t128\t86\t-/-\t91.0/-\n\nåŒæ—¶å‘ˆç°SQuAD 1.1/2.0ä¸MNLIä»»åŠ¡å¼€å‘é›†ç»“æœã€‚\n\nå¼•ç”¨å£°æ˜\n\nè‹¥DeBERTaå¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€åŠ©ç›Šï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼š\n\n@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Rust\", \"English\", \"MIT\"]"
  },
  {
    "url": "https://gitcode.com/openMind/falcon_7b",
    "project_name": "falcon_7b",
    "readme": "Original Text\nğŸš€ Falcon-7B\n\nFalcon-7B æ˜¯ç”± TII å¼€å‘çš„å…·æœ‰ 70 äº¿å‚æ•°çš„å› æœè§£ç å™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ç»ç²¾å¿ƒæŒ‘é€‰çš„è¯­æ–™åº“å¢å¼ºçš„ RefinedWeb 1.5ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹éµå¾ª Apache 2.0 è®¸å¯åè®®å¼€æ”¾ä½¿ç”¨ã€‚\n\nä¿®æ”¹\nä¿®æ”¹ç¤ºä¾‹ä»£ç ä»¥å…¼å®¹ OpenMind å¹¶æ·»åŠ å¯¹ npu çš„æ”¯æŒ\nä¸ºä»€ä¹ˆé€‰æ‹© Falcon-7Bï¼Ÿ\nå…¶è¡¨ç°ä¼˜äºåŒç±»å¼€æºæ¨¡å‹ï¼Œè¿™å¾—ç›Šäºåœ¨å¢å¼ºåçš„ RefinedWeb 1.5ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œçš„è®­ç»ƒã€‚\nå…¶æ¶æ„é’ˆå¯¹æ¨ç†è¿›è¡Œäº†ä¼˜åŒ–ï¼Œé‡‡ç”¨äº† FlashAttention (Dao et al., 2022) å’Œ multiquery (Shazeer et al., 2019) æŠ€æœ¯ã€‚\nå®ƒéµå¾ªå®½æ¾çš„ Apache 2.0 è®¸å¯åè®®ï¼Œå…è®¸å•†ç”¨ï¼Œæ— éœ€æ”¯ä»˜ç‰ˆç¨æˆ–å—ä»»ä½•é™åˆ¶ã€‚\n\nâš ï¸ è¿™æ˜¯ä¸€ä¸ªåŸå§‹çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹éœ€è¦è¿›ä¸€æ­¥å¾®è°ƒã€‚\n\nğŸ”¥ å¯»æ‰¾æ›´å¼ºå¤§çš„æ¨¡å‹ï¼Ÿ Falcon-40B æ˜¯ Falcon-7B çš„å¼ºå¤§å‡çº§ç‰ˆï¼\n\nfrom openmind import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\n\nmodel_path= \"PyTorch-NPU/falcon_7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\npipeline = pipeline(\n    \"text-generation\",\n    model=model_path,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\n\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\n\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\n\nğŸ’¥ Falcon LLMs ä½¿ç”¨æ—¶éœ€æ­é… PyTorch 2.0 å’Œ transformers åº“ï¼\n\nè‹¥è¦å®ç° Falcon çš„å¿«é€Ÿæ¨ç†ï¼Œè¯·æŸ¥çœ‹ Text Generation Inferenceï¼\n\næ‚¨éœ€è¦ è‡³å°‘16GBå†…å­˜ æ‰èƒ½è¿…é€Ÿè¿è¡Œ Falcon-7B çš„æ¨ç†ã€‚\n\nFalcon-7B æ¨¡å‹å¡\næ¨¡å‹è¯¦æƒ…\næ¨¡å‹æè¿°\nå¼€å‘è€…ï¼š https://www.tii.aeï¼›\næ¨¡å‹ç±»å‹ï¼š ä»…è§£ç å™¨çš„å› æœæ¨¡å‹ï¼›\nè¯­è¨€ï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰ï¼š è‹±è¯­ã€å¾·è¯­ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­ï¼ˆä»¥åŠæœ‰é™çš„æ„å¤§åˆ©è¯­ã€è‘¡è„ç‰™è¯­ã€æ³¢å…°è¯­ã€è·å…°è¯­ã€ç½—é©¬å°¼äºšè¯­ã€æ·å…‹è¯­å’Œç‘å…¸è¯­èƒ½åŠ›ï¼‰ï¼›\nè®¸å¯ï¼š Apache 2.0ã€‚\næ¨¡å‹æ¥æº\nè®ºæ–‡ï¼š å³å°†æ¨å‡ºã€‚\nä½¿ç”¨åœºæ™¯\nç›´æ¥åº”ç”¨\n\nå¤§å‹è¯­è¨€æ¨¡å‹ç ”ç©¶ï¼›ä½œä¸ºè¿›ä¸€æ­¥ä¸“ä¸šåŒ–å®šåˆ¶å’Œå¾®è°ƒç‰¹å®šç”¨ä¾‹ï¼ˆä¾‹å¦‚ï¼Œæ‘˜è¦ã€æ–‡æœ¬ç”Ÿæˆã€èŠå¤©æœºå™¨äººç­‰ï¼‰çš„åŸºç¡€ã€‚\n\nä¸é€‚ç”¨åœºæ™¯\n\næœªç»å……åˆ†è¯„ä¼°é£é™©å’Œç¼“è§£æªæ–½çš„ç”Ÿäº§ä½¿ç”¨ï¼›ä»»ä½•å¯èƒ½è¢«è®¤ä¸ºæ˜¯ä¸è´Ÿè´£ä»»æˆ–æœ‰å®³çš„ç”¨ä¾‹ã€‚\n\nåè§ã€é£é™©ä¸é™åˆ¶\n\nFalcon-7B ä»…åœ¨è‹±è¯­å’Œæ³•è¯­æ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œæ— æ³•é€‚å½“æ³›åŒ–åˆ°å…¶ä»–è¯­è¨€ã€‚æ­¤å¤–ï¼Œç”±äºå®ƒæ˜¯åœ¨ä»£è¡¨ç½‘ç»œçš„è§„æ¨¡åŒ–è¯­æ–™åº“ä¸Šè®­ç»ƒçš„ï¼Œä¼šæºå¸¦ç½‘ç»œä¸Šå¸¸è§çš„åˆ»æ¿å°è±¡å’Œåè§ã€‚\n\næ¨èæ„è§\n\næˆ‘ä»¬å»ºè®®ä½¿ç”¨ Falcon-7B çš„ç”¨æˆ·é’ˆå¯¹ç‰¹å®šä»»åŠ¡é›†åˆå¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨ä»»ä½•ç”Ÿäº§ä½¿ç”¨ä¸­é‡‡å–é˜²æŠ¤æªæ–½å’Œé€‚å½“é¢„é˜²ã€‚\n\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ­¤æ¨¡å‹\nfrom openmind import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\n\nmodel_path= \"PyTorch-NPU/falcon_7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\npipeline = pipeline(\n    \"text-generation\",\n    model=model_path,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\n\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\n\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\nåŸ¹è®­è¯¦æƒ…\nåŸ¹è®­æ•°æ®\n\nFalcon-7B åœ¨ 1,500B çš„ RefinedWeb ä»¤ç‰Œä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡é«˜è´¨é‡ç­›é€‰å’Œå»é‡çš„ç½‘ç»œæ•°æ®é›†ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡ç²¾é€‰çš„è¯­æ–™åº“å¯¹å…¶è¿›è¡Œäº†å¢å¼ºã€‚æˆ‘ä»¬ç²¾é€‰çš„è¯­æ–™åº“ä¸­çš„é‡è¦ç»„æˆéƒ¨åˆ†å—åˆ° The Pile (Gao et al., 2020) çš„å¯å‘ã€‚\n\næ•°æ®æº\tæ¯”ä¾‹\tä»¤ç‰Œæ•°\tæ¥æº\nRefinedWeb-English\t79%\t1,185B\tæµ·é‡ç½‘é¡µæŠ“å–\nå›¾ä¹¦\t7%\t110B\t\nå¯¹è¯\t6%\t85B\tRedditã€StackOverflowã€HackerNews\nä»£ç \t3%\t45B\t\nRefinedWeb-French\t3%\t45B\tæµ·é‡ç½‘é¡µæŠ“å–\næŠ€æœ¯\t2%\t30B\tarXivã€PubMedã€USPTO ç­‰\n\næ•°æ®ä½¿ç”¨ Falcon-7B/40B åˆ†è¯å™¨è¿›è¡Œäº†åˆ†è¯ã€‚\n\nåŸ¹è®­æµç¨‹\n\nFalcon-7B åœ¨ 384 ä¸ª A100 40GB GPU ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œé‡‡ç”¨ 2D å¹¶è¡Œç­–ç•¥ï¼ˆPP=2ï¼ŒDP=192ï¼‰å¹¶ç»“åˆ ZeROã€‚\n\nåŸ¹è®­è¶…å‚æ•°\nè¶…å‚æ•°\tå€¼\tå¤‡æ³¨\nç²¾åº¦\tbfloat16\t\nä¼˜åŒ–å™¨\tAdamW\t\nå­¦ä¹ ç‡\t6e-4\t4B ä»¤ç‰Œé¢„çƒ­ï¼Œä½™å¼¦è¡°å‡è‡³ 1.2e-5\næƒé‡è¡°å‡\t1e-1\t\nZ-æŸå¤±\t1e-4\t\næ‰¹æ¬¡å¤§å°\t2304\t30B ä»¤ç‰Œé€æ¸å¢åŠ \né€Ÿåº¦ã€å¤§å°ã€æ—¶é—´\n\nè®­ç»ƒäº 2023 å¹´ 3 æœˆåˆå¼€å§‹ï¼Œå¤§çº¦è¿›è¡Œäº†ä¸¤å‘¨æ—¶é—´ã€‚\n\nè¯„ä¼°\n\nè®ºæ–‡å³å°†å‘å¸ƒã€‚\n\næŠ€æœ¯è§„æ ¼\næ¨¡å‹æ¶æ„å’Œç›®æ ‡\n\nFalcon-7B æ˜¯ä¸€ä¸ªä»…åœ¨å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼ˆå³é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œï¼‰ä¸Šè®­ç»ƒçš„å› æœè§£ç å™¨æ¨¡å‹ã€‚\n\nè¯¥æ¶æ„ä¸»è¦å‚è€ƒäº† GPT-3 è®ºæ–‡ (Brown et al., 2020)ï¼Œæœ‰ä»¥ä¸‹ä¸åŒä¹‹å¤„ï¼š\n\nä½ç½®åµŒå…¥ï¼š æ—‹è½¬ (Su et al., 2021)ï¼›\næ³¨æ„åŠ›ï¼š å¤šæŸ¥è¯¢ (Shazeer et al., 2019) å’Œ FlashAttention (Dao et al., 2022)ï¼›\nè§£ç å™¨å—ï¼š å¹¶è¡Œæ³¨æ„åŠ›/MLPï¼Œå¸¦æœ‰å•ä¸€å±‚å½’ä¸€åŒ–ã€‚\nè¶…å‚æ•°\tå€¼\tå¤‡æ³¨\nå±‚æ•°\t32\t\nd_model\t4544\tå¢åŠ ä»¥è¡¥å¿å¤šæŸ¥è¯¢\nhead_dim\t64\tå‡å°‘ä»¥ä¼˜åŒ– FlashAttention\nè¯æ±‡é‡\t65024\t\nåºåˆ—é•¿åº¦\t2048\t\nè®¡ç®—åŸºç¡€è®¾æ–½\nç¡¬ä»¶\n\nFalcon-7B åœ¨ AWS SageMaker ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨ P4d å®ä¾‹ä¸­çš„ 384 ä¸ª A100 40GB GPUã€‚\n\nè½¯ä»¶\n\nFalcon-7B ä½¿ç”¨å®šåˆ¶çš„åˆ†å¸ƒå¼è®­ç»ƒä»£ç åº“ Gigatron è¿›è¡Œè®­ç»ƒã€‚å®ƒç»“åˆäº† ZeRO å’Œé«˜æ€§èƒ½ Triton å†…æ ¸ï¼ˆFlashAttention ç­‰ï¼‰çš„ 3D å¹¶è¡Œæ–¹æ³•ã€‚\n\nå¼•ç”¨\n\nè®ºæ–‡å³å°†å‘å¸ƒ ğŸ˜Šã€‚åœ¨æ­¤æœŸé—´ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä¿¡æ¯è¿›è¡Œå¼•ç”¨ï¼š\n\n@article{falcon40b,\n  title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n  year={2023}\n}\n\n\nè¦äº†è§£æ›´å¤šå…³äºé¢„è®­ç»ƒæ•°æ®é›†çš„ä¿¡æ¯ï¼Œè¯·å‚é˜… ğŸ““ RefinedWebè®ºæ–‡ã€‚\n\n@article{refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n  journal={arXiv preprint arXiv:2306.01116},\n  eprint={2306.01116},\n  eprinttype = {arXiv},\n  url={https://arxiv.org/abs/2306.01116},\n  year={2023}\n}\n\nè®¸å¯è¯\n\nFalcon-7B éµå¾ª Apache 2.0 è®¸å¯è¯å‘å¸ƒã€‚\n\nè”ç³»æ–¹å¼\n\nfalconllm@tii.ae",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"English\", \"Apache License 2.0\", \"tiiuae/falcon-refinedweb\"]"
  },
  {
    "url": "https://gitcode.com/openMind/mobilebert_uncased",
    "project_name": "mobilebert_uncased",
    "readme": "Original Text\nä¿®æ”¹\n\nåœ¨ç¤ºä¾‹ä¸­æ·»åŠ äº† npu æ”¯æŒã€‚\n\nMobileBERTï¼šé€‚ç”¨äºèµ„æºå—é™è®¾å¤‡çš„ç´§å‡‘å‹ä»»åŠ¡æ— å…³ BERT\n\nMobileBERT æ˜¯ BERT_LARGE çš„ç²¾ç®€ç‰ˆæœ¬ï¼ŒåŒæ—¶é…å¤‡äº†ç“¶é¢ˆç»“æ„ï¼Œå¹¶åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸å‰é¦ˆç½‘ç»œä¹‹é—´è¿›è¡Œäº†ç²¾å¿ƒè®¾è®¡çš„å¹³è¡¡ã€‚\n\næ­¤æ£€æŸ¥ç‚¹æ˜¯åŸå§‹çš„ MobileBert ä¼˜åŒ–æ— å¤§å°å†™è‹±æ–‡ç‰ˆæœ¬ï¼š uncased_L-24_H-128_B-512_A-4_F-4_OPT æ£€æŸ¥ç‚¹ã€‚\n\nå¦‚ä½•åœ¨ openmind ä¸­ä½¿ç”¨ MobileBERT\nfrom openmind import is_torch_npu_available, pipeline\n\nmodel_path = \"PyTorch-NPU/mobilebert_uncased\"\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\nfill_mask = pipeline(\"fill-mask\", model=model_path, tokenizer=model_path, device_map=device)\nprint(\n\tfill_mask(f\"As we all know, the sun always {fill_mask.tokenizer.mask_token}.\")\n)\n\n\nä½¿ç”¨ git è¿›è¡Œç‰ˆæœ¬æ§åˆ¶\n\nåœ¨è½¯ä»¶å¼€å‘è¿‡ç¨‹ä¸­ï¼Œç‰ˆæœ¬æ§åˆ¶æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„ç¯èŠ‚ã€‚git æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼Œå¹¿æ³›åº”ç”¨äºä»£ç ç®¡ç†ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„ git å‘½ä»¤åŠå…¶åŠŸèƒ½ï¼š\n\nåˆå§‹åŒ–ä»“åº“\n\nè¦åœ¨æœ¬åœ°åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„ git ä»“åº“ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š\n\ngit init\n\nå…‹éš†ä»“åº“\n\nå¦‚æœä½ æƒ³ä»è¿œç¨‹ä»“åº“å…‹éš†ä»£ç åˆ°æœ¬åœ°ï¼Œå¯ä»¥ä½¿ç”¨ï¼š\n\ngit clone <repository_url>\n\næ·»åŠ æ–‡ä»¶\n\nåœ¨æäº¤æ›´æ”¹ä¹‹å‰ï¼Œä½ éœ€è¦å°†æ–‡ä»¶æ·»åŠ åˆ°æš‚å­˜åŒºï¼š\n\ngit add <file_name>\n\n\næˆ–è€…ï¼Œä½ å¯ä»¥ä¸€æ¬¡æ€§æ·»åŠ æ‰€æœ‰æ›´æ”¹çš„æ–‡ä»¶ï¼š\n\ngit add .\n\næäº¤æ›´æ”¹\n\nå°†æš‚å­˜åŒºçš„æ–‡ä»¶æäº¤åˆ°æœ¬åœ°ä»“åº“ï¼Œå¹¶é™„ä¸Šæäº¤ä¿¡æ¯ï¼š\n\ngit commit -m \"æäº¤ä¿¡æ¯\"\n\næŸ¥çœ‹çŠ¶æ€\n\nä½ å¯ä»¥éšæ—¶æŸ¥çœ‹å½“å‰ä»“åº“çš„çŠ¶æ€ï¼Œäº†è§£å“ªäº›æ–‡ä»¶å·²è¢«ä¿®æ”¹ã€æ·»åŠ æˆ–åˆ é™¤ï¼š\n\ngit status\n\næŸ¥çœ‹æ—¥å¿—\n\næŸ¥çœ‹æäº¤å†å²è®°å½•ï¼š\n\ngit log\n\nåˆ†æ”¯ç®¡ç†\n\nåˆ›å»ºæ–°åˆ†æ”¯ï¼š\n\ngit branch <branch_name>\n\n\nåˆ‡æ¢åˆ°æŒ‡å®šåˆ†æ”¯ï¼š\n\ngit checkout <branch_name>\n\n\nåˆ›å»ºå¹¶åˆ‡æ¢åˆ°æ–°åˆ†æ”¯ï¼š\n\ngit checkout -b <branch_name>\n\nåˆå¹¶åˆ†æ”¯\n\nå°†æŒ‡å®šåˆ†æ”¯çš„æ›´æ”¹åˆå¹¶åˆ°å½“å‰åˆ†æ”¯ï¼š\n\ngit merge <branch_name>\n\næ¨é€åˆ°è¿œç¨‹ä»“åº“\n\nå°†æœ¬åœ°ä»“åº“çš„æ›´æ”¹æ¨é€åˆ°è¿œç¨‹ä»“åº“ï¼š\n\ngit push origin <branch_name>\n\næ‹‰å–è¿œç¨‹ä»“åº“\n\nä»è¿œç¨‹ä»“åº“æ‹‰å–æœ€æ–°çš„æ›´æ”¹åˆ°æœ¬åœ°ï¼š\n\ngit pull origin <branch_name>\n\n\né€šè¿‡è¿™äº›åŸºæœ¬çš„ git å‘½ä»¤ï¼Œä½ å¯ä»¥æœ‰æ•ˆåœ°ç®¡ç†ä»£ç ç‰ˆæœ¬ï¼Œç¡®ä¿å›¢é˜Ÿåä½œçš„é¡ºç•…è¿›è¡Œã€‚",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"English\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/mt5_base",
    "project_name": "mt5_base",
    "readme": "Original Text\n\nGoogle çš„ mT5\n\nmT5 æ˜¯åœ¨ mC4 è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒçš„ï¼Œæ¶µç›– 101 ç§è¯­è¨€ï¼š\n\nAfrikaansã€Albanianã€Amharicã€Arabicã€Armenianã€Azerbaijaniã€Basqueã€Belarusianã€Bengaliã€Bulgarianã€Burmeseã€Catalanã€Cebuanoã€Chichewaã€Chineseã€Corsicanã€Czechã€Danishã€Dutchã€Englishã€Esperantoã€Estonianã€Filipinoã€Finnishã€Frenchã€Galicianã€Georgianã€Germanã€Greekã€Gujaratiã€Haitian Creoleã€Hausaã€Hawaiianã€Hebrewã€Hindiã€Hmongã€Hungarianã€Icelandicã€Igboã€Indonesianã€Irishã€Italianã€Japaneseã€Javaneseã€Kannadaã€Kazakhã€Khmerã€Koreanã€Kurdishã€Kyrgyzã€Laoã€Latinã€Latvianã€Lithuanianã€Luxembourgishã€Macedonianã€Malagasyã€Malayã€Malayalamã€Malteseã€Maoriã€Marathiã€Mongolianã€Nepaliã€Norwegianã€Pashtoã€Persianã€Polishã€Portugueseã€Punjabiã€Romanianã€Russianã€Samoanã€Scottish Gaelicã€Serbianã€Shonaã€Sindhiã€Sinhalaã€Slovakã€Slovenianã€Somaliã€Sothoã€Spanishã€Sundaneseã€Swahiliã€Swedishã€Tajikã€Tamilã€Teluguã€Thaiã€Turkishã€Ukrainianã€Urduã€Uzbekã€Vietnameseã€Welshã€West Frisianã€Xhosaã€Yiddishã€Yorubaã€Zuluã€‚\n\næ³¨æ„ï¼šmT5 ä»…åœ¨ mC4 ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒæœªåŒ…æ‹¬ä»»ä½•æœ‰ç›‘ç£çš„è®­ç»ƒã€‚å› æ­¤ï¼Œæ­¤æ¨¡å‹åœ¨ä½¿ç”¨äºä¸‹æ¸¸ä»»åŠ¡å‰éœ€è¦è¿›ä¸€æ­¥å¾®è°ƒã€‚\n\né¢„è®­ç»ƒæ•°æ®é›†ï¼šmC4\n\nå…¶ä»–ç¤¾åŒºæ£€æŸ¥ç‚¹ï¼šè¿™é‡Œ\n\nè®ºæ–‡ï¼šmT5ï¼šä¸€ç§å¤§è§„æ¨¡å¤šè¯­è¨€é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨\n\nä½œè€…ï¼šLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel\n\næ‘˜è¦\n\næœ€è¿‘çš„â€œæ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨â€ï¼ˆText-to-Text Transfer Transformerï¼ŒT5ï¼‰åˆ©ç”¨ç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼å’Œè§„æ¨¡ï¼Œåœ¨å¤šç§è‹±è¯­è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† mT5ï¼Œè¿™æ˜¯åŸºäº T5 çš„å¤šè¯­è¨€å˜ä½“ï¼Œå®ƒåœ¨ä¸€ä¸ªæ–°çš„ Common Crawl-based æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè¯¥æ•°æ®é›†æ¶µç›–äº† 101 ç§è¯­è¨€ã€‚æˆ‘ä»¬æè¿°äº† mT5 çš„è®¾è®¡å’Œä¿®æ”¹åçš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨è®¸å¤šå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹éƒ½å·²å…¬å¼€å¯ç”¨ã€‚\n\nä½¿ç”¨æ–¹æ³•\nimport os\n\nimport torch\nfrom openmind import AutoTokenizer, is_torch_npu_available\nfrom openmind_hub import snapshot_download\nfrom transformers import MT5ForConditionalGeneration\n\n\nmodel_path = snapshot_download(\"PyTorch-NPU/mt5_base\", revision=\"main\", resume_download=True,\n                                ignore_patterns=[\"*.h5\", \"*.ot\", \"*.msgpack\"])\nif is_torch_npu_available():\n    device = \"npu:0\"\nelif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\nmodel = MT5ForConditionalGeneration.from_pretrained(model_path, device_map=device)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n\noutput = model.generate(input_ids)\nprint(tokenizer.decode(output[0]))\n",
    "tags": "[\"Translation\", \"PyTorch\", \"Transformers\", \"English\", \"Chinese\", \"multilingual\", \"Apache License 2.0\", \"mc4\"]"
  },
  {
    "url": "https://gitcode.com/openMind/open_llama_7b",
    "project_name": "open_llama_7b",
    "readme": "Original Text\nOpenLLaMAï¼šLLaMA çš„å¼€æºå¤ç°\n\nåœ¨æ­¤ä»£ç åº“ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå®½æ¾è®¸å¯çš„å¼€æºå¤ç°ç‰ˆæœ¬â€”â€”Meta AI çš„ LLaMA å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬å‘å¸ƒäº†åŸºäº 1T æ ‡è®°è®­ç»ƒçš„ 7B å’Œ 3B æ¨¡å‹ï¼Œä»¥åŠåŸºäº 600B æ ‡è®°è®­ç»ƒçš„ 13B æ¨¡å‹çš„é¢„è§ˆç‰ˆã€‚æˆ‘ä»¬æä¾›äº†é¢„è®­ç»ƒçš„ OpenLLaMA æ¨¡å‹çš„ PyTorch å’Œ JAX æƒé‡ï¼Œä»¥åŠè¯„ä¼°ç»“æœå’Œä¸åŸå§‹ LLaMA æ¨¡å‹çš„å¯¹æ¯”ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è§ OpenLLaMA é¡¹ç›®ä¸»é¡µã€‚\n\nä¿®æ”¹\n\nä¿®æ”¹äº†ç¤ºä¾‹ä»£ç \n\næƒé‡å‘å¸ƒã€è®¸å¯å’Œä½¿ç”¨\n\næˆ‘ä»¬ä»¥ä¸¤ç§æ ¼å¼å‘å¸ƒæƒé‡ï¼šä¸€ç§æ˜¯ä¸æˆ‘ä»¬çš„ EasyLM æ¡†æ¶ ä¸€èµ·ä½¿ç”¨çš„ EasyLM æ ¼å¼ï¼Œå¦ä¸€ç§æ˜¯ä¸ Hugging Face transformers åº“ä¸€èµ·ä½¿ç”¨çš„ PyTorch æ ¼å¼ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ¡†æ¶ EasyLM å’Œæ£€æŸ¥ç‚¹æƒé‡éƒ½éµå¾ªå®½æ¾çš„ Apache 2.0 è®¸å¯ã€‚\n\nä½¿ç”¨ Hugging Face Transformers åŠ è½½æƒé‡\n\né¢„è§ˆæ£€æŸ¥ç‚¹å¯ä»¥ç›´æ¥ä» Hugging Face Hub åŠ è½½ã€‚è¯·æ³¨æ„ï¼Œç›®å‰å»ºè®®é¿å…ä½¿ç”¨ Hugging Face å¿«é€Ÿåˆ†è¯å™¨ï¼Œå› ä¸ºæˆ‘ä»¬å‘ç°è‡ªåŠ¨è½¬æ¢çš„å¿«é€Ÿåˆ†è¯å™¨æœ‰æ—¶ä¼šäº§ç”Ÿä¸æ­£ç¡®çš„åˆ†è¯ç»“æœã€‚ è¿™å¯ä»¥é€šè¿‡ç›´æ¥ä½¿ç”¨ LlamaTokenizer ç±»æˆ–ä¸º AutoTokenizer ç±»ä¼ é€’ use_fast=False é€‰é¡¹æ¥å®ç°ã€‚ä»¥ä¸‹ä¸ºä½¿ç”¨ç¤ºä¾‹ã€‚\n\nimport torch\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\nmodel_path = 'PyTorch-NPU/open_llama_7b'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float16, device_map='auto',\n)\nprompt = 'Q: What is the largest animal?\\nA:'\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\ninput_ids = input_ids.to(model.device)\ngeneration_output = model.generate(\n    input_ids=input_ids, max_new_tokens=32\n)\nprint(tokenizer.decode(generation_output[0]))\n\n\nå¯¹äºæ›´é«˜çº§çš„ä½¿ç”¨æ–¹æ³•ï¼Œè¯·éµå¾ªTransformers LLaMAæ–‡æ¡£ã€‚\n\nä½¿ç”¨ LM-Eval-Harness è¿›è¡Œè¯„ä¼°\n\næ¨¡å‹å¯ä»¥ä½¿ç”¨LM-Eval-Harnessè¿›è¡Œè¯„ä¼°ã€‚ç„¶è€Œï¼Œç”±äºå‰è¿°çš„åˆ†è¯å™¨é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦é¿å…ä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨ä»¥è·å¾—æ­£ç¡®çš„ç»“æœã€‚è¿™å¯ä»¥é€šè¿‡å‘LM-Eval-Harnessçš„è¿™éƒ¨åˆ†ä¼ é€’use_fast=Falseæ¥å®ç°ï¼Œå¦‚ä¸‹ç¤ºä¾‹æ‰€ç¤ºï¼š\n\ntokenizer = self.AUTO_TOKENIZER_CLASS.from_pretrained(\n    pretrained if tokenizer is None else tokenizer,\n    revision=revision + (\"/\" + subfolder if subfolder is not None else \"\"),\n    use_fast=False\n)\n\nä½¿ç”¨ EasyLM åŠ è½½æƒé‡\n\nåœ¨ä½¿ç”¨ EasyLM æ¡†æ¶ä¸­çš„æƒé‡æ—¶ï¼Œè¯·å‚è€ƒ EasyLM çš„ LLaMA æ–‡æ¡£ã€‚è¯·æ³¨æ„ï¼Œä¸åŸå§‹ LLaMA æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„ OpenLLaMA åˆ†è¯å™¨å’Œæƒé‡å®Œå…¨ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå› æ­¤æ— éœ€è·å–åŸå§‹ LLaMA åˆ†è¯å™¨å’Œæƒé‡ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨äº† BOSï¼ˆå¥é¦–ï¼‰æ ‡è®°ï¼ˆid=1ï¼‰ï¼Œå› æ­¤åœ¨å°‘é‡æ ·æœ¬è¯„ä¼°æ—¶æœ€å¥½æ·»åŠ æ­¤æ ‡è®°ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚\n\næ•°æ®é›†ä¸è®­ç»ƒ\n\næˆ‘ä»¬åœ¨ Together å‘å¸ƒçš„ RedPajama æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ•°æ®é›†æ˜¯ LLaMA è®­ç»ƒæ•°æ®é›†çš„å¤åˆ¶å“ï¼ŒåŒ…å«è¶…è¿‡ 1.2 ä¸‡äº¿ä¸ªæ ‡è®°ã€‚æˆ‘ä»¬éµå¾ªåŸå§‹ LLaMA è®ºæ–‡ä¸­çš„å®Œå…¨ç›¸åŒçš„é¢„å¤„ç†æ­¥éª¤å’Œè®­ç»ƒè¶…å‚æ•°ï¼ŒåŒ…æ‹¬æ¨¡å‹æ¶æ„ã€ä¸Šä¸‹æ–‡é•¿åº¦ã€è®­ç»ƒæ­¥éª¤ã€å­¦ä¹ ç‡è®¡åˆ’ä»¥åŠä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬è®¾ç½®ä¸åŸå§‹è®¾ç½®çš„å”¯ä¸€åŒºåˆ«åœ¨äºæ•°æ®é›†ï¼šOpenLLaMA ä½¿ç”¨ RedPajama æ•°æ®é›†ï¼Œè€ŒåŸå§‹ LLaMA ä½¿ç”¨çš„æ˜¯å…¶ä»–æ•°æ®é›†ã€‚\n\næˆ‘ä»¬åœ¨äº‘ç«¯çš„ TPU-v4s ä¸Šä½¿ç”¨ EasyLM è®­ç»ƒæ¨¡å‹ï¼ŒEasyLM æ˜¯æˆ‘ä»¬å¼€å‘çš„ä¸€ä¸ªåŸºäº JAX çš„è®­ç»ƒæµç¨‹ï¼Œç”¨äºè®­ç»ƒå’Œå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬ç»“åˆä½¿ç”¨äº†å¸¸è§„æ•°æ®å¹¶è¡Œå’Œ å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼ˆä¹Ÿç§°ä¸º ZeRO é˜¶æ®µ 3ï¼‰ï¼Œä»¥å¹³è¡¡è®­ç»ƒååé‡å’Œå†…å­˜ä½¿ç”¨ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ 7B æ¨¡å‹åœ¨å•ä¸ª TPU-v4 èŠ¯ç‰‡ä¸Šå®ç°äº†è¶…è¿‡ 2200 ä¸ªæ ‡è®°/ç§’çš„ååé‡ã€‚\n\nè¯„ä¼°\n\næˆ‘ä»¬ä½¿ç”¨ lm-evaluation-harness å¯¹ OpenLLaMA è¿›è¡Œäº†å¹¿æ³›ä»»åŠ¡çš„è¯„ä¼°ã€‚LLaMA çš„ç»“æœæ˜¯åœ¨ç›¸åŒçš„è¯„ä¼°æŒ‡æ ‡ä¸Šè¿è¡ŒåŸå§‹ LLaMA æ¨¡å‹ç”Ÿæˆçš„ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œæˆ‘ä»¬çš„ LLaMA æ¨¡å‹ç»“æœä¸åŸå§‹ LLaMA è®ºæ–‡ç•¥æœ‰ä¸åŒï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯ç”±äºè¯„ä¼°åè®®ä¸åŒé€ æˆçš„ã€‚ç±»ä¼¼çš„å·®å¼‚å·²åœ¨ lm-evaluation-harness çš„æ­¤é—®é¢˜ä¸­æŠ¥å‘Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨ Pile æ•°æ®é›†ä¸Šè®­ç»ƒçš„ 60B å‚æ•°æ¨¡å‹ GPT-J çš„ç»“æœã€‚\n\nåŸå§‹ LLaMA æ¨¡å‹è®­ç»ƒäº† 1 ä¸‡äº¿ä¸ªæ ‡è®°ï¼ŒGPT-J è®­ç»ƒäº† 5000 äº¿ä¸ªæ ‡è®°ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å±•ç¤ºçš„ç»“æœã€‚OpenLLaMA åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šä¸åŸå§‹ LLaMA å’Œ GPT-J çš„æ€§èƒ½ç›¸å½“ï¼Œå¹¶åœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¿‡äº†å®ƒä»¬ã€‚\n\nä»»åŠ¡/æŒ‡æ ‡\tGPT-J 6B\tLLaMA 7B\tOpenLLaMA 7B\tOpenLLaMA 3B\tOpenLLaMA 13B 600BT\nanli_r1/acc\t0.32\t0.35\t0.33\t0.33\t0.33\nanli_r2/acc\t0.34\t0.34\t0.36\t0.32\t0.35\nanli_r3/acc\t0.35\t0.37\t0.38\t0.35\t0.38\narc_challenge/acc\t0.34\t0.39\t0.37\t0.34\t0.39\narc_challenge/acc_norm\t0.37\t0.41\t0.38\t0.37\t0.42\narc_easy/acc\t0.67\t0.68\t0.72\t0.69\t0.74\narc_easy/acc_norm\t0.62\t0.52\t0.68\t0.65\t0.70\nddboolq/acc\t0.50\t0.56\t0.53\t0.49\t0.71\nhellaswag/acc\t0.36\t0.36\t0.63\t0.43\t0.54\nhellaswag/acc_norm\t0.66\t0.73\t0.72\t0.67\t0.73\nopenbookqa/acc\t0.29\t0.29\t0.30\t0.27\t0.30\nopenbookqa/acc_norm\t0.38\t0.41\t0.40\t0.40\t0.41\npiqa/acc\t0.75\t0.78\t0.76\t0.75\t0.77\npiqa/acc_norm\t0.76\t0.78\t0.77\t0.76\t0.78\nrecord/em\t0.88\t0.91\t0.89\t0.88\t0.90\nrecord/f1\t0.89\t0.91\t0.90\t0.89\t0.90\nrte/acc\t0.54\t0.56\t0.60\t0.58\t0.65\ntruthfulqa_mc/mc1\t0.20\t0.21\t0.23\t0.22\t0.22\ntruthfulqa_mc/mc2\t0.36\t0.34\t0.35\t0.35\t0.35\nwic/acc\t0.50\t0.50\t0.51\t0.48\t0.49\nwinogrande/acc\t0.64\t0.68\t0.67\t0.62\t0.67\nå¹³å‡\t0.51\t0.53\t0.55\t0.52\t0.56\n\nç”±äºæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¿™ä¸¤ä¸ªä»»åŠ¡ä¸Šè¡¨ç°å¼‚å¸¸å‡ºè‰²ï¼Œæˆ‘ä»¬ä»æˆ‘ä»¬çš„åŸºå‡†ä¸­ç§»é™¤äº†ä»»åŠ¡ CB å’Œ WSCã€‚æˆ‘ä»¬æ¨æµ‹è®­ç»ƒé›†ä¸­å¯èƒ½å­˜åœ¨åŸºå‡†æ•°æ®æ±¡æŸ“ã€‚\n\nè”ç³»æ–¹å¼\n\næˆ‘ä»¬éå¸¸æ¬¢è¿ç¤¾åŒºçš„åé¦ˆã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·æå‡ºä¸€ä¸ª issue æˆ–ä¸æˆ‘ä»¬è”ç³»ã€‚\n\nOpenLLaMA ç”± Berkeley AI Research çš„ Xinyang Geng* å’Œ Hao Liu* å¼€å‘ã€‚ *å¹³ç­‰è´¡çŒ®\n\nè‡´è°¢\n\næˆ‘ä»¬æ„Ÿè°¢ Google TPU Research Cloud è®¡åˆ’æä¾›éƒ¨åˆ†è®¡ç®—èµ„æºã€‚æˆ‘ä»¬ç‰¹åˆ«æ„Ÿè°¢ TPU Research Cloud çš„ Jonathan Caton åœ¨ç»„ç»‡è®¡ç®—èµ„æºæ–¹é¢çš„å¸®åŠ©ï¼Œæ„Ÿè°¢ Google Cloud å›¢é˜Ÿçš„ Rafi Witten å’Œ Google JAX å›¢é˜Ÿçš„ James Bradbury åœ¨ä¼˜åŒ–æˆ‘ä»¬è®­ç»ƒååé‡æ–¹é¢çš„å¸®åŠ©ã€‚æˆ‘ä»¬è¿˜æ„Ÿè°¢ Charlie Snellã€Gautier Izacardã€Eric Wallaceã€Lianmin Zheng ä»¥åŠæˆ‘ä»¬çš„ç”¨æˆ·ç¤¾åŒºåœ¨è®¨è®ºå’Œåé¦ˆæ–¹é¢çš„è´¡çŒ®ã€‚ OpenLLaMA 13B æ¨¡å‹æ˜¯åœ¨ä¸ Stability AI çš„åˆä½œä¸­è®­ç»ƒçš„ï¼Œæˆ‘ä»¬æ„Ÿè°¢ Stability AI æä¾›çš„è®¡ç®—èµ„æºã€‚æˆ‘ä»¬ç‰¹åˆ«æ„Ÿè°¢ David Ha å’Œ Shivanshu Purohit åœ¨åè°ƒç‰©æµå’Œæä¾›å·¥ç¨‹æ”¯æŒæ–¹é¢çš„å¸®åŠ©ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\nå¦‚æœæ‚¨åœ¨ç ”ç©¶æˆ–åº”ç”¨ä¸­å‘ç°äº† OpenLLaMA çš„æœ‰ç”¨æ€§ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ BibTeX å¼•ç”¨ï¼š\n\n@software{openlm2023openllama,\n  author = {Geng, Xinyang and Liu, Hao},\n  title = {OpenLLaMA: An Open Reproduction of LLaMA},\n  month = May,\n  year = 2023,\n  url = {https://github.com/openlm-research/open_llama}\n}\n\n\nå½“ç„¶ï¼Œæˆ‘ä¼šæŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚è¯·æ‚¨æä¾›éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬å†…å®¹ã€‚\n\n@software{together2023redpajama,\n  author = {Together Computer},\n  title = {RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset},\n  month = April,\n  year = 2023,\n  url = {https://github.com/togethercomputer/RedPajama-Data}\n}\n\n\nå½“ç„¶ï¼Œæˆ‘ä¼šå°½åŠ›æŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚ä¸è¿‡ï¼Œæ‚¨ä¼¼ä¹å¿˜è®°äº†æä¾›éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ã€‚è¯·æä¾›åŸæ–‡ï¼Œæˆ‘å°†ä¼šæŠŠå®ƒç¿»è¯‘æˆä¸­æ–‡ï¼ŒåŒæ—¶ä¿æŒ Markdown æ ¼å¼å’Œæ‚¨æ‰€è¦æ±‚çš„é£æ ¼ã€‚\n\n@article{touvron2023llama,\n  title={Llama: Open and efficient foundation language models},\n  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},\n  journal={arXiv preprint arXiv:2302.13971},\n  year={2023}\n}\n\n\nè¯·æä¾›éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬å†…å®¹ï¼Œè¿™æ ·æˆ‘æ‰èƒ½è¿›è¡Œç¿»è¯‘ã€‚",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"Apache License 2.0\", \"togethercomputer/RedPajama-Data-1T\"]"
  },
  {
    "url": "https://gitcode.com/openMind/xglm_1.7b",
    "project_name": "xglm_1.7b",
    "readme": "XGLM-1.7B\n\nXGLM-1.7B is a multilingual autoregressive language model (with 1.7 billion parameters) trained on a balanced corpus of a diverse set of languages totaling 500 billion sub-tokens. It was introduced in the paper Few-shot Learning with Multilingual Language Models by Xi Victoria Lin*, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li* (*Equal Contribution). The original implementation was released in this repository.\n\nModification\n\nModify examples in README.md and add npu support.\n\nTraining Data Statistics\n\nThe training data statistics of XGLM-1.7B is shown in the table below.\n\nISO-639-1\tfamily\tname\t# tokens\tratio\tratio w/ lowRes upsampling\nen\tIndo-European\tEnglish\t803526736124\t0.489906\t0.3259\nru\tIndo-European\tRussian\t147791898098\t0.0901079\t0.0602\nzh\tSino-Tibetan\tChinese\t132770494630\t0.0809494\t0.0483\nde\tIndo-European\tGerman\t89223707856\t0.0543992\t0.0363\nes\tIndo-European\tSpanish\t87303083105\t0.0532282\t0.0353\nfr\tIndo-European\tFrench\t77419639775\t0.0472023\t0.0313\nja\tJaponic\tJapanese\t66054364513\t0.040273\t0.0269\nit\tIndo-European\tItalian\t41930465338\t0.0255648\t0.0171\npt\tIndo-European\tPortuguese\t36586032444\t0.0223063\t0.0297\nel\tIndo-European\tGreek (modern)\t28762166159\t0.0175361\t0.0233\nko\tKoreanic\tKorean\t20002244535\t0.0121953\t0.0811\nfi\tUralic\tFinnish\t16804309722\t0.0102455\t0.0681\nid\tAustronesian\tIndonesian\t15423541953\t0.00940365\t0.0125\ntr\tTurkic\tTurkish\t12413166065\t0.00756824\t0.0101\nar\tAfro-Asiatic\tArabic\t12248607345\t0.00746791\t0.0099\nvi\tAustroasiatic\tVietnamese\t11199121869\t0.00682804\t0.0091\nth\tTaiâ€“Kadai\tThai\t10842172807\t0.00661041\t0.044\nbg\tIndo-European\tBulgarian\t9703797869\t0.00591635\t0.0393\nca\tIndo-European\tCatalan\t7075834775\t0.0043141\t0.0287\nhi\tIndo-European\tHindi\t3448390110\t0.00210246\t0.014\net\tUralic\tEstonian\t3286873851\t0.00200399\t0.0133\nbn\tIndo-European\tBengali, Bangla\t1627447450\t0.000992245\t0.0066\nta\tDravidian\tTamil\t1476973397\t0.000900502\t0.006\nur\tIndo-European\tUrdu\t1351891969\t0.000824241\t0.0055\nsw\tNigerâ€“Congo\tSwahili\t907516139\t0.000553307\t0.0037\nte\tDravidian\tTelugu\t689316485\t0.000420272\t0.0028\neu\tLanguage isolate\tBasque\t105304423\t6.42035e-05\t0.0043\nmy\tSino-Tibetan\tBurmese\t101358331\t6.17976e-05\t0.003\nht\tCreole\tHaitian, Haitian Creole\t86584697\t5.27902e-05\t0.0035\nqu\tQuechuan\tQuechua\t3236108\t1.97304e-06\t0.0001\nModel card\n\nFor intended usage of the model, please refer to the model card released by the XGLM-1.7B development team.\n\nExample (COPA)\n\nThe following snippet shows how to evaluate our models (GPT-3 style, zero-shot) on the Choice of Plausible Alternatives (COPA) task, using examples in English, Chinese and Hindi.\n\nimport torch\nimport torch.nn.functional as F\nfrom openmind import is_torch_npu_available, AutoTokenizer\nfrom transformers import XGLMForCausalLM\n \n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\nmodel_name_or_path = 'PyTorch-NPU/xglm_1.7b'\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\nmodel = XGLMForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, device_map=device)\n\ndata_samples = {\n    'en': [\n        {\n            \"premise\": \"I wanted to conserve energy.\",\n            \"choice1\": \"I swept the floor in the unoccupied room.\",\n            \"choice2\": \"I shut off the light in the unoccupied room.\",\n            \"question\": \"effect\",\n            \"label\": \"1\"\n        },\n        {\n            \"premise\": \"The flame on the candle went out.\",\n            \"choice1\": \"I blew on the wick.\",\n            \"choice2\": \"I put a match to the wick.\",\n            \"question\": \"cause\",\n            \"label\": \"0\"\n        }\n    ],\n    'zh': [\n        {\n            \"premise\": \"æˆ‘æƒ³èŠ‚çº¦èƒ½æºã€‚\",\n            \"choice1\": \"æˆ‘åœ¨ç©ºç€çš„æˆ¿é—´é‡Œæ‰«äº†åœ°æ¿ã€‚\",\n            \"choice2\": \"æˆ‘æŠŠç©ºæˆ¿é—´é‡Œçš„ç¯å…³äº†ã€‚\",\n            \"question\": \"effect\",\n            \"label\": \"1\"\n        },\n        {\n            \"premise\": \"èœ¡çƒ›ä¸Šçš„ç«ç„°ç†„ç­äº†ã€‚\",\n            \"choice1\": \"æˆ‘å¹ç­äº†ç¯èŠ¯ã€‚\",\n            \"choice2\": \"æˆ‘æŠŠä¸€æ ¹ç«æŸ´æ”¾åœ¨ç¯èŠ¯ä¸Šã€‚\",\n            \"question\": \"cause\",\n            \"label\": \"0\"\n        }\n    ],\n    'hi': [\n        {\n            \"premise\": \"M te vle konsÃ¨ve enÃ¨ji.\",\n            \"choice1\": \"Mwen te fin baleye chanm lib la.\",\n            \"choice2\": \"Mwen te femen limyÃ¨ nan chanm lib la.\",\n            \"question\": \"effect\",\n            \"label\": \"1\"\n        },\n        {\n            \"premise\": \"Flam bouji a te etenn.\",\n            \"choice1\": \"Mwen te soufle bouji a.\",\n            \"choice2\": \"Mwen te limen mÃ¨ch bouji a.\",\n            \"question\": \"cause\",\n            \"label\": \"0\"\n        }\n    ]\n}\n\n\ndef get_logprobs(prompt, device):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    input_ids, output_ids = inputs[\"input_ids\"], inputs[\"input_ids\"][:, 1:]\n    outputs = model(**inputs, labels=input_ids)\n    logits = outputs.logits\n    logprobs = torch.gather(F.log_softmax(logits, dim=2), 2,\n                            output_ids.unsqueeze(2))\n    return logprobs\n\n\ndef COPA_eval(prompt, alternative1, alternative2, device):\n    lprob1 = get_logprobs(prompt + \"\\n\" + alternative1, device).sum()\n    lprob2 = get_logprobs(prompt + \"\\n\" + alternative2, device).sum()\n    return 0 if lprob1 > lprob2 else 1\n\n\nfor lang in data_samples:\n    for idx, example in enumerate(data_samples[lang]):\n        predict = COPA_eval(example[\"premise\"], example[\"choice1\"],\n                            example[\"choice2\"], device)\n        print(f'{lang}-{idx}', predict, example['label'])\n\n\n",
    "tags": "[\"Text Generation\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"31 languages\", \"MIT\"]"
  },
  {
    "url": "https://gitcode.com/openMind/xlm_roberta_base",
    "project_name": "xlm_roberta_base",
    "readme": "Original Text\nä¿®æ”¹\nä¿®æ”¹ README.md ä¸­çš„ç¤ºä¾‹å¹¶æ·»åŠ  npu æ”¯æŒï¼›\nä¿®æ”¹ README.md ä¸­çš„â€œé¢„æœŸç”¨é€”åŠé™åˆ¶â€éƒ¨åˆ†ã€‚\nXLM-RoBERTaï¼ˆåŸºç¡€æ¨¡å‹ï¼‰\n\nXLM-RoBERTa æ¨¡å‹åœ¨åŒ…å« 100 ç§è¯­è¨€çš„ 2.5TB ç»è¿‡è¿‡æ»¤çš„ CommonCrawl æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚è¯¥æ¨¡å‹ç”± Conneau ç­‰äººåœ¨è®ºæ–‡ Unsupervised Cross-lingual Representation Learning at Scale ä¸­æå‡ºï¼Œå¹¶é¦–æ¬¡åœ¨ è¿™ä¸ªä»“åº“ ä¸­å‘å¸ƒã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒ XLM-RoBERTa çš„å›¢é˜Ÿæ²¡æœ‰ä¸ºæ­¤æ¨¡å‹ç¼–å†™æ¨¡å‹å¡ï¼Œå› æ­¤æ­¤æ¨¡å‹å¡ç”± Hugging Face å›¢é˜Ÿç¼–å†™ã€‚\n\næ¨¡å‹æè¿°\n\nXLM-RoBERTa æ˜¯ RoBERTa çš„å¤šè¯­è¨€ç‰ˆæœ¬ã€‚å®ƒåœ¨åŒ…å« 100 ç§è¯­è¨€çš„ 2.5TB ç»è¿‡è¿‡æ»¤çš„ CommonCrawl æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚\n\nRoBERTa æ˜¯ä¸€ç§åœ¨å¤§å‹è¯­æ–™åº“ä¸­è‡ªç›‘ç£é¢„è®­ç»ƒçš„è½¬æ¢å™¨æ¨¡å‹ã€‚è¿™æ„å‘³ç€å®ƒåªåœ¨å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œé¢„è®­ç»ƒï¼Œè€Œæ²¡æœ‰äººç±»ä»¥ä»»ä½•æ–¹å¼è¿›è¡Œæ ‡æ³¨ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒå¯ä»¥ä½¿ç”¨å¤§é‡å…¬å¼€å¯ç”¨çš„æ•°æ®ï¼‰ä»¥åŠä¸€ä¸ªè‡ªåŠ¨è¿‡ç¨‹ä»è¿™äº›æ–‡æœ¬ç”Ÿæˆè¾“å…¥å’Œæ ‡ç­¾ã€‚\n\næ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒæ˜¯é€šè¿‡æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒçš„ã€‚å¯¹äºä¸€ä¸ªå¥å­ï¼Œæ¨¡å‹éšæœºé®è”½è¾“å…¥ä¸­çš„ 15% çš„å•è¯ï¼Œç„¶åè¿è¡Œæ•´ä¸ªé®è”½çš„å¥å­é€šè¿‡æ¨¡å‹å¹¶é¢„æµ‹é®è”½çš„å•è¯ã€‚è¿™ä¸ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰ä¸åŒï¼Œåè€…é€šå¸¸ä¸€æ¬¡çœ‹åˆ°å•è¯ï¼Œæˆ–è€…åƒ GPT è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹åœ¨å†…éƒ¨é®è”½æœªæ¥çš„æ ‡è®°ã€‚è¿™å…è®¸æ¨¡å‹å­¦ä¹ å¥å­çš„åŒå‘è¡¨ç¤ºã€‚\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å­¦ä¹ äº† 100 ç§è¯­è¨€çš„å†…éƒ¨è¡¨ç¤ºï¼Œç„¶åè¿™äº›è¡¨ç¤ºå¯ä»¥ç”¨äºæå–å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾ï¼šä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªæ ‡è®°å¥å­çš„æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ XLM-RoBERTa æ¨¡å‹ç”Ÿæˆçš„ç‰¹å¾ä½œä¸ºè¾“å…¥æ¥è®­ç»ƒä¸€ä¸ªæ ‡å‡†çš„åˆ†ç±»å™¨ã€‚\n\né¢„æœŸç”¨é€”åŠé™åˆ¶\n\nè¯·æ³¨æ„ï¼Œæ­¤æ¨¡å‹ä¸»è¦æ—¨åœ¨ç”¨äºåœ¨ä»»åŠ¡ä¸Šå¾®è°ƒï¼Œè¿™äº›ä»»åŠ¡éœ€è¦ä½¿ç”¨æ•´ä¸ªå¥å­ï¼ˆå¯èƒ½è¢«é®è”½ï¼‰æ¥åšå‡ºå†³ç­–ï¼Œä¾‹å¦‚åºåˆ—åˆ†ç±»ã€æ ‡è®°åˆ†ç±»æˆ–é—®ç­”ã€‚å¯¹äºå¦‚æ–‡æœ¬ç”Ÿæˆè¿™æ ·çš„ä»»åŠ¡ï¼Œæ‚¨åº”è¯¥è€ƒè™‘ä½¿ç”¨åƒ GPT2 è¿™æ ·çš„æ¨¡å‹ã€‚\n\nä½¿ç”¨æ–¹å¼\n\næ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤æ¨¡å‹ä»¥åŠç”¨äºé®è”½è¯­è¨€å»ºæ¨¡çš„ç®¡é“ï¼š\n\nfrom openmind import pipeline\n\nmodel_path = \"PyTorch-NPU/xlm_roberta_base\"\n\nfill_mask = pipeline(\"fill-mask\", model=model_path, tokenizer=model_path, device_map=\"auto\")\nprint(\n\tfill_mask(f\"As we all know, the sun always {fill_mask.tokenizer.mask_token}.\")\n)\n\nBibTeXæ¡ç›®ä¸å¼•ç”¨ä¿¡æ¯\n@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\\'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n\nç‚¹å‡»æ­¤å¤„ æŸ¥çœ‹  EXBERT æ¨¡å‹ã€‚",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"ONNX\", \"Safetensors\", \"JAX\", \"93 languages\", \"MIT\", \"exbert\"]"
  },
  {
    "url": "https://gitcode.com/openMind/openai_gpt",
    "project_name": "openai_gpt",
    "readme": "Original Text\nOpenAI GPT 1\nç›®å½•\næ¨¡å‹è¯¦æƒ…\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\nç”¨é€”\né£é™©ã€é™åˆ¶ä¸åè§\nè®­ç»ƒ\nè¯„ä¼°\nç¯å¢ƒå½±å“\næŠ€æœ¯è§„æ ¼\nå¼•ç”¨ä¿¡æ¯\næ¨¡å‹å¡ç‰‡ä½œè€…\nä¿®æ”¹\nä¿®æ”¹ç¤ºä¾‹ä»£ç ä»¥æ”¯æŒopenMindï¼Œå¹¶æ·»åŠ NPUæ”¯æŒ\næ¨¡å‹è¯¦æƒ…\n\næ¨¡å‹æè¿°ï¼š openai-gptï¼ˆåˆåâ€œGPT-1â€ï¼‰æ˜¯OpenAIåˆ›å»ºå¹¶å‘å¸ƒçš„ç¬¬ä¸€ä¸ªåŸºäºå˜å‹å™¨æ¶æ„çš„è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ˜¯ä¸€ä¸ªå› æœï¼ˆå•å‘ï¼‰å˜å‹å™¨ï¼Œé€šè¿‡åœ¨å…·æœ‰é•¿è·ç¦»ä¾èµ–æ€§çš„å¤§å‹è¯­æ–™åº“ä¸Šè¿›è¡Œè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒã€‚\n\nå¼€å‘è€…ï¼š Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskeverã€‚è¯·æŸ¥çœ‹ç›¸å…³ç ”ç©¶è®ºæ–‡å’ŒGitHubä»“åº“äº†è§£æ¨¡å‹å¼€å‘è€…å’Œè´¡çŒ®è€…ã€‚\næ¨¡å‹ç±»å‹ï¼š åŸºäºå˜å‹å™¨çš„è¯­è¨€æ¨¡å‹\nè¯­è¨€ï¼š è‹±è¯­\nè®¸å¯ï¼š MITè®¸å¯\nç›¸å…³æ¨¡å‹ï¼š GPT2, GPT2-Medium, GPT2-Large å’Œ GPT2-XL\næ›´å¤šä¿¡æ¯èµ„æºï¼š\nç ”ç©¶è®ºæ–‡\nOpenAIåšå®¢æ–‡ç« \nGitHubä»“åº“\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\nä½¿ç”¨ä»¥ä¸‹ä»£ç å¼€å§‹ä½¿ç”¨æ¨¡å‹ã€‚æ‚¨å¯ä»¥ç›´æ¥å°†æ­¤æ¨¡å‹ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„ç®¡é“ä¸­ã€‚\n\nimport torch\nfrom openmind import pipeline, is_torch_npu_available\n\nif is_torch_npu_available():\n  device = \"npu:0\"\nelse:\n  device = \"cpu\"\n\nmodel_path= \"PyTorch-NPU/openai_gpt\"\n\ngenerator = pipeline('text-generation', model=model_path, device = device)\n\noutput = generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\nprint(f\">>>output={output}\", flush=True)\n\nä½¿ç”¨åœºæ™¯\nç›´æ¥ä½¿ç”¨\n\nè¯¥æ¨¡å‹å¯ç”¨äºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚\n\nä¸‹æ¸¸åº”ç”¨\n\nè¯¥æ¨¡å‹çš„æ½œåœ¨ä¸‹æ¸¸åº”ç”¨åŒ…æ‹¬åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å„ç§ä»»åŠ¡ã€‚åœ¨ç›¸å…³è®ºæ–‡ä¸­ï¼Œæ¨¡å‹å¼€å‘è€…è®¨è®ºäº†æ¨¡å‹åœ¨åŒ…æ‹¬è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰ã€é—®é¢˜å›ç­”ã€è¯­ä¹‰ç›¸ä¼¼åº¦å’Œæ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡ä¸­çš„è¯„ä¼°ã€‚\n\næ»¥ç”¨ä¸è¶Šç•Œä½¿ç”¨\n\nè¯¥æ¨¡å‹æœªç»è¿‡è®­ç»ƒä»¥ç”Ÿæˆäº‹å®æ€§æˆ–çœŸå®çš„äººç‰©æˆ–äº‹ä»¶æè¿°ï¼Œå› æ­¤ä½¿ç”¨è¯¥æ¨¡å‹ç”Ÿæˆæ­¤ç±»å†…å®¹è¶…å‡ºäº†æ¨¡å‹çš„èƒ½åŠ›èŒƒå›´ã€‚\n\né£é™©ã€å±€é™æ€§å’Œåè§\nåè§\n\nå†…å®¹è­¦å‘Šï¼šè¯»è€…åº”å½“çŸ¥æ™“ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆçš„è¯­è¨€å¯èƒ½ä¼šå¯¹æŸäº›äººé€ æˆå›°æ‰°æˆ–å†’çŠ¯ï¼Œå¹¶ä¸”å¯èƒ½ä¼ æ’­å†å²å’Œå½“å‰çš„åˆ»æ¿å°è±¡ã€‚\n\nå·²æœ‰å¤§é‡ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹ä¸­çš„åè§å’Œå…¬å¹³æ€§é—®é¢˜ï¼ˆå‚è§ï¼Œä¾‹å¦‚Sheng et al. (2021)å’ŒBender et al. (2021)ï¼‰ã€‚è¯¥æ¨¡å‹çš„é¢„æµ‹å¯èƒ½åŒ…å«æ¶‰åŠå—ä¿æŠ¤ç±»åˆ«ã€èº«ä»½ç‰¹å¾ä»¥åŠæ•æ„Ÿã€ç¤¾ä¼šå’ŒèŒä¸šç¾¤ä½“çš„ä»¤äººä¸å®‰å’Œæœ‰å®³çš„åˆ»æ¿å°è±¡ã€‚è¿™ç§åè§ä¹Ÿå¯èƒ½å½±å“æ¨¡å‹çš„å¾®è°ƒç‰ˆæœ¬ã€‚ç”¨æˆ·ï¼ˆæ— è®ºæ˜¯ç›´æ¥ç”¨æˆ·è¿˜æ˜¯ä¸‹æ¸¸ç”¨æˆ·ï¼‰éƒ½åº”äº†è§£æ¨¡å‹çš„é£é™©ã€åè§å’Œå±€é™æ€§ã€‚\n\né£é™©å’Œå±€é™æ€§\n\næ¨¡å‹å¼€å‘è€…è¿˜åœ¨åšå®¢æ–‡ç« ä¸­ä»‹ç»äº†æ¨¡å‹çš„é£é™©å’Œå±€é™æ€§ï¼ŒåŒ…æ‹¬ï¼š\n\n**è®¡ç®—è¦æ±‚ï¼š**è®¸å¤šå¤„ç†NLPä»»åŠ¡çš„ä¼ ç»Ÿæ–¹æ³•æ˜¯åœ¨å•ä¸ªGPUä¸Šä»å¤´å¼€å§‹è®­ç»ƒç›¸å¯¹è¾ƒå°çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•éœ€è¦æ˜‚è´µçš„é¢„è®­ç»ƒæ­¥éª¤â€”â€”åœ¨8ä¸ªGPUä¸Šè¿è¡Œ1ä¸ªæœˆã€‚å¹¸è¿çš„æ˜¯ï¼Œè¿™ä¸€æ­¥éª¤åªéœ€è¿›è¡Œä¸€æ¬¡ï¼Œå¹¶ä¸”æˆ‘ä»¬å·²ç»å‘å¸ƒäº†æ¨¡å‹ï¼Œä»¥ä¾¿å…¶ä»–äººå¯ä»¥é¿å…è¿™ä¸€æ­¥éª¤ã€‚å®ƒä¹Ÿæ˜¯ä¸€ä¸ªå¤§å‹æ¨¡å‹ï¼ˆä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼‰ï¼Œå› æ­¤ä½¿ç”¨äº†æ›´å¤šçš„è®¡ç®—å’Œå†…å­˜â€”â€”æˆ‘ä»¬ä½¿ç”¨äº†37å±‚ï¼ˆ12ä¸ªå—ï¼‰çš„Transformeræ¶æ„ï¼Œå¹¶ä¸”åœ¨åºåˆ—é•¿åº¦è¾¾512ä¸ªtokençš„åºåˆ—ä¸Šè¿›è¡Œè®­ç»ƒã€‚å¤§å¤šæ•°å®éªŒéƒ½æ˜¯åœ¨4å’Œ8 GPUç³»ç»Ÿä¸Šè¿›è¡Œçš„ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè¿…é€Ÿå¯¹æ–°ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œè¿™æœ‰åŠ©äºç¼“è§£é¢å¤–çš„èµ„æºè¦æ±‚ã€‚\n**é€šè¿‡æ–‡æœ¬å­¦ä¹ ä¸–ç•Œçš„å±€é™æ€§å’Œåè§ï¼š**äº’è”ç½‘ä¸Šæ˜“äºè·å–çš„ä¹¦ç±å’Œæ–‡æœ¬å¹¶ä¸åŒ…å«å…³äºä¸–ç•Œçš„å®Œæ•´ç”šè‡³å‡†ç¡®çš„ä¿¡æ¯ã€‚æœ€è¿‘çš„ç ”ç©¶ï¼ˆLucy and Gauthier, 2017ï¼‰è¡¨æ˜ï¼ŒæŸäº›ç±»å‹çš„ä¿¡æ¯å¾ˆéš¾ä»…é€šè¿‡æ–‡æœ¬å­¦ä¹ ï¼Œè€Œå…¶ä»–ç ”ç©¶ï¼ˆGururangan et al., 2018ï¼‰åˆ™è¡¨æ˜æ¨¡å‹ä¼šåœ¨æ•°æ®åˆ†å¸ƒä¸­å­¦ä¹ å¹¶åˆ©ç”¨åè§ã€‚\n**ä»ç„¶è„†å¼±çš„æ³›åŒ–ï¼š**å°½ç®¡æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸Šæé«˜äº†æ€§èƒ½ï¼Œä½†å½“å‰çš„æ·±åº¦å­¦ä¹ NLPæ¨¡å‹åœ¨ç³»ç»Ÿæ€§ã€å¯¹æŠ—æ€§æˆ–éåˆ†å¸ƒæ–¹å¼è¯„ä¼°ä¸‹ä»è¡¨ç°å‡ºä»¤äººæƒŠè®¶å’Œåç›´è§‰çš„è¡Œä¸ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å¹¶éä¸å—è¿™äº›é—®é¢˜çš„å›°æ‰°ï¼Œå°½ç®¡æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸€äº›è¿›å±•çš„è¿¹è±¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–‡æœ¬è•´å«çš„çº¯ç¥ç»æ–¹æ³•ä¸Šæ˜¾ç¤ºå‡ºæ”¹è¿›çš„è¯æ±‡é²æ£’æ€§ã€‚åœ¨Glockner et al. (2018)å¼•å…¥çš„æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†83.75%ï¼Œè¡¨ç°ä¸é€šè¿‡WordNetæ•´åˆå¤–éƒ¨çŸ¥è¯†çš„KIMç›¸ä¼¼ã€‚\nè®­ç»ƒ\nè®­ç»ƒæ•°æ®\n\næ¨¡å‹å¼€å‘è€…å†™é“ï¼š\n\næˆ‘ä»¬ä½¿ç”¨BooksCorpusæ•°æ®é›†ï¼ˆZhu et al., 2015ï¼‰æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚å®ƒåŒ…å«è¶…è¿‡7,000æœ¬æœªå‡ºç‰ˆçš„ç‹¬ç‰¹ä¹¦ç±ï¼Œæ¶µç›–äº†å†’é™©ã€å¥‡å¹»å’Œæµªæ¼«ç­‰å¤šç§ç±»å‹ã€‚å…³é”®çš„æ˜¯ï¼Œå®ƒåŒ…å«äº†è¿ç»­çš„é•¿æ–‡æœ¬ç‰‡æ®µï¼Œè¿™ä½¿å¾—ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°é•¿è·ç¦»ä¿¡æ¯ã€‚\n\nè®­ç»ƒè¿‡ç¨‹\n\næ¨¡å‹å¼€å‘è€…å†™é“ï¼š\n\næˆ‘ä»¬æ¨¡å‹çš„è®¾è®¡å¤§éƒ¨åˆ†éµå¾ªåŸå§‹çš„transformerå·¥ä½œ[62]ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ª12å±‚çš„ä»…è§£ç å™¨Transformerï¼Œå¸¦æœ‰æ©ç çš„è‡ªæ³¨æ„åŠ›å¤´ï¼ˆ768ç»´çŠ¶æ€å’Œ12ä¸ªæ³¨æ„åŠ›å¤´ï¼‰ã€‚å¯¹äºä½ç½®-wiseçš„å‰é¦ˆç½‘ç»œï¼Œæˆ‘ä»¬ä½¿ç”¨äº†3072ç»´çš„å†…éƒ¨çŠ¶æ€ã€‚æˆ‘ä»¬ä½¿ç”¨äº†Adamä¼˜åŒ–æ–¹æ¡ˆ[27]ï¼Œæœ€å¤§å­¦ä¹ ç‡ä¸º2.5e-4ã€‚å­¦ä¹ ç‡ä»é›¶å¼€å§‹çº¿æ€§å¢åŠ åˆ°å‰2,000æ¬¡æ›´æ–°ï¼Œç„¶åä½¿ç”¨ä½™å¼¦è°ƒåº¦å‡å°‘åˆ°é›¶ã€‚æˆ‘ä»¬åœ¨64ä¸ªéšæœºé‡‡æ ·çš„è¿ç»­512ä¸ªtokençš„åºåˆ—ç»„æˆçš„minibatchä¸Šè®­ç»ƒäº†100ä¸ªepochã€‚ç”±äºæ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨äº†layernorm[2]ï¼Œä¸€ä¸ªç®€å•çš„æƒé‡åˆå§‹åŒ–N(0, 0.02)å°±è¶³å¤Ÿäº†ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«40,000ä¸ªåˆå¹¶çš„bytepairç¼–ç ï¼ˆBPEï¼‰è¯æ±‡[53]ï¼Œä»¥åŠæ®‹ä½™ã€åµŒå…¥å’Œæ³¨æ„åŠ›ä¸¢å¼ƒç‡ä¸º0.1çš„æ­£åˆ™åŒ–ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº†[37]ä¸­æå‡ºçš„ä¿®æ”¹ç‰ˆçš„L2æ­£åˆ™åŒ–ï¼Œw=0.01åº”ç”¨äºæ‰€æœ‰éåç½®æˆ–å¢ç›Šæƒé‡ã€‚å¯¹äºæ¿€æ´»å‡½æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒï¼ˆGELUï¼‰[18]ã€‚æˆ‘ä»¬ä½¿ç”¨äº†å­¦ä¹ åˆ°çš„ä½ç½®åµŒå…¥ï¼Œè€Œä¸æ˜¯åŸå§‹å·¥ä½œä¸­æå‡ºçš„æ­£å¼¦ç‰ˆæœ¬ã€‚æˆ‘ä»¬ä½¿ç”¨ftfyåº“2æ¸…æ´—BooksCorpusä¸­çš„åŸå§‹æ–‡æœ¬ï¼Œæ ‡å‡†åŒ–ä¸€äº›æ ‡ç‚¹å’Œç©ºç™½ï¼Œå¹¶ä½¿ç”¨spaCyåˆ†è¯å™¨ã€‚\n\nè¿›ä¸€æ­¥ç»†èŠ‚å’Œå¼•ç”¨é“¾æ¥è¯·è§è®ºæ–‡ã€‚\n\nè¯„ä¼°\n\nä»¥ä¸‹è¯„ä¼°ä¿¡æ¯æ‘˜è‡ªç›¸å…³åšå®¢æ–‡ç« ã€‚æ›´å¤šç»†èŠ‚è¯·è§ç›¸å…³è®ºæ–‡ã€‚\n\næµ‹è¯•æ•°æ®ã€å› ç´ å’ŒæŒ‡æ ‡\n\næ¨¡å‹å¼€å‘è€…æŠ¥å‘Šç§°ï¼Œæ¨¡å‹åœ¨ä»¥ä¸‹ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šä½¿ç”¨ä»¥ä¸‹æŒ‡æ ‡è¿›è¡Œäº†è¯„ä¼°ï¼š\n\nä»»åŠ¡ï¼š æ–‡æœ¬è•´å«\n\næ•°æ®é›†ï¼š SNLI, MNLI Matched, MNLI Mismatched, SciTail, QNLI, RTE\næŒ‡æ ‡ï¼š å‡†ç¡®ç‡\n\nä»»åŠ¡ï¼š è¯­ä¹‰ç›¸ä¼¼åº¦\n\næ•°æ®é›†ï¼š STS-B, QQP, MRPC\næŒ‡æ ‡ï¼š å‡†ç¡®ç‡\n\nä»»åŠ¡ï¼š é˜…è¯»ç†è§£\n\næ•°æ®é›†ï¼š RACE\næŒ‡æ ‡ï¼š å‡†ç¡®ç‡\n\nä»»åŠ¡ï¼š å¸¸è§æ¨ç†\n\næ•°æ®é›†ï¼š ROCStories, COPA\næŒ‡æ ‡ï¼š å‡†ç¡®ç‡\n\nä»»åŠ¡ï¼š æƒ…æ„Ÿåˆ†æ\n\næ•°æ®é›†ï¼š SST-2\næŒ‡æ ‡ï¼š å‡†ç¡®ç‡\n\nä»»åŠ¡ï¼š è¯­è¨€å­¦å¯æ¥å—æ€§\n\næ•°æ®é›†ï¼š CoLA\næŒ‡æ ‡ï¼š å‡†ç¡®ç‡\n\nä»»åŠ¡ï¼š å¤šä»»åŠ¡åŸºå‡†\n\næ•°æ®é›†ï¼š GLUE\næŒ‡æ ‡ï¼š å‡†ç¡®ç‡\nç»“æœ\n\nè¯¥æ¨¡å‹åœ¨ä¸è¿›è¡Œä»»ä½•å¾®è°ƒï¼ˆé›¶æ ·æœ¬ï¼‰çš„æƒ…å†µä¸‹å–å¾—äº†ä»¥ä¸‹ç»“æœï¼š\n\nä»»åŠ¡\tTE\tTE\tTE\tTE\tTE\tTE\tSS\tSS\tSS\tRC\tCR\tCR\tSA\tLA\tMTB\nDataset\tSNLI\tMNLI Matched\tMNLI Mismatched\tSciTail\tQNLI\tRTE\tSTS-B\tQQP\tMPRC\tRACE\tROCStories\tCOPA\tSST-2\tCoLA\tGLUE\n\t89.9\t82.1\t81.4\t88.3\t88.1\t56.0\t82.0\t70.3\t82.3\t59.0\t86.5\t78.6\t91.3\t45.4\t72.8\nç¯å¢ƒå½±å“\n\næ¨¡å‹å¼€å‘è€…ä»¬åœ¨æŠ¥å‘Šä¸­æåˆ°ï¼š\n\nè®­ç»ƒæ­¤æ¨¡å‹çš„è®¡ç®—æœºèµ„æºæ€»é‡ä¸º0.96æ‹æµ®ç‚¹è¿ç®—å¤©æ•°ï¼ˆpfs-daysï¼‰ã€‚\n\n8å°P600 GPU * 30å¤© * æ¯å°GPU 12 TFLOPS * 0.33åˆ©ç”¨ç‡ = 0.96 pfs-days\n\nç¢³æ’æ”¾é‡å¯ä»¥é€šè¿‡ä½¿ç”¨Lacosteç­‰äººï¼ˆ2019å¹´ï¼‰æå‡ºçš„æœºå™¨å­¦ä¹ å½±å“è®¡ç®—å™¨æ¥ä¼°ç®—ã€‚\n\nç¡¬ä»¶ç±»å‹ï¼š 8å°P600 GPU\nä½¿ç”¨æ—¶é•¿ï¼š 720å°æ—¶ï¼ˆ30å¤©ï¼‰\näº‘æœåŠ¡æä¾›å•†ï¼š æœªçŸ¥\nè®¡ç®—åŒºåŸŸï¼š æœªçŸ¥\nç¢³æ’æ”¾é‡ï¼š æœªçŸ¥\næŠ€æœ¯è§„æ ¼\n\nè¯¦æƒ…è¯·å‚é˜…ç›¸å…³è®ºæ–‡ï¼Œå…¶ä¸­ä»‹ç»äº†å»ºæ¨¡æ¶æ„ã€ç›®æ ‡ã€è®¡ç®—åŸºç¡€è®¾æ–½å’Œè®­ç»ƒç»†èŠ‚ã€‚\n\nå¼•ç”¨ä¿¡æ¯\n@article{radford2018improving,\n  title={Improving language understanding by generative pre-training},\n  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},\n  year={2018},\n  publisher={OpenAI}\n}\n\n\nAPA: æ‹‰å¾·ç¦å¾·ï¼ŒA.ï¼Œçº³æ‹‰æ–¯å§†æ±‰ï¼ŒK.ï¼Œè¨åˆ©æ›¼æ–¯ï¼ŒT.ï¼Œ& è‹èŒ¨å…‹ç»´ï¼ŒI.ï¼ˆ2018ï¼‰ã€‚é€šè¿‡ç”Ÿæˆé¢„è®­ç»ƒæé«˜è¯­è¨€ç†è§£èƒ½åŠ›ã€‚\n\næ¨¡å‹å¡ä½œè€…\n\nè¿™ä»½æ¨¡å‹å¡ç”±Hugging Faceå›¢é˜Ÿæ’°å†™ã€‚",
    "tags": "[\"Text Generation\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Rust\", \"Safetensors\", \"English\", \"MIT\"]"
  },
  {
    "url": "https://gitcode.com/openMind/mt5_large",
    "project_name": "mt5_large",
    "readme": "Original Text\n\nGoogleçš„mT5\n\nmT5 æ˜¯åœ¨ mC4 è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒçš„ï¼Œæ¶µç›–101ç§è¯­è¨€ï¼š\n\néæ´²è¯­ã€é˜¿å°”å·´å°¼äºšè¯­ã€é˜¿å§†å“ˆæ‹‰è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€äºšç¾å°¼äºšè¯­ã€é˜¿å¡æ‹œç–†è¯­ã€å·´æ–¯å…‹è¯­ã€ç™½ä¿„ç½—æ–¯è¯­ã€å­ŸåŠ æ‹‰è¯­ã€ä¿åŠ åˆ©äºšè¯­ã€ç¼…ç”¸è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€å®¿åŠ¡è¯­ã€å¥‡åˆ‡ç“¦è¯­ã€ä¸­æ–‡ã€ç§‘è¥¿å˜‰è¯­ã€æ·å…‹è¯­ã€ä¸¹éº¦è¯­ã€è·å…°è¯­ã€è‹±è¯­ã€ä¸–ç•Œè¯­ã€çˆ±æ²™å°¼äºšè¯­ã€è²å¾‹å®¾è¯­ã€èŠ¬å…°è¯­ã€æ³•è¯­ã€åŠ åˆ©è¥¿äºšè¯­ã€æ ¼é²å‰äºšè¯­ã€å¾·è¯­ã€å¸Œè…Šè¯­ã€å¤å‰æ‹‰ç‰¹è¯­ã€æµ·åœ°å…‹é‡Œå¥¥å°”è¯­ã€è±ªè¨è¯­ã€å¤å¨å¤·è¯­ã€å¸Œä¼¯æ¥è¯­ã€å°åœ°è¯­ã€è‹—è¯­ã€åŒˆç‰™åˆ©è¯­ã€å†°å²›è¯­ã€ä¼Šåšè¯­ã€å°åº¦å°¼è¥¿äºšè¯­ã€çˆ±å°”å…°è¯­ã€æ„å¤§åˆ©è¯­ã€æ—¥è¯­ã€çˆªå“‡è¯­ã€å¡çº³è¾¾è¯­ã€å“ˆè¨å…‹è¯­ã€é«˜æ£‰è¯­ã€éŸ©è¯­ã€åº“å°”å¾·è¯­ã€å‰å°”å‰æ–¯è¯­ã€è€æŒè¯­ã€æ‹‰ä¸è¯­ã€æ‹‰è„±ç»´äºšè¯­ã€ç«‹é™¶å®›è¯­ã€å¢æ£®å ¡è¯­ã€é©¬å…¶é¡¿è¯­ã€é©¬è¾¾åŠ æ–¯åŠ è¯­ã€é©¬æ¥è¯­ã€é©¬æ‹‰é›…æ‹‰å§†è¯­ã€é©¬è€³ä»–è¯­ã€æ¯›åˆ©è¯­ã€é©¬æ‹‰åœ°è¯­ã€è’™å¤è¯­ã€å°¼æ³Šå°”è¯­ã€æŒªå¨è¯­ã€æ™®ä»€å›¾è¯­ã€æ³¢æ–¯è¯­ã€æ³¢å…°è¯­ã€è‘¡è„ç‰™è¯­ã€æ—é®æ™®è¯­ã€ç½—é©¬å°¼äºšè¯­ã€ä¿„è¯­ã€è¨æ‘©äºšè¯­ã€è‹æ ¼å…°ç›–å°”è¯­ã€å¡å°”ç»´äºšè¯­ã€ç»çº³è¯­ã€ä¿¡å¾·è¯­ã€åƒ§ä¼½ç½—è¯­ã€æ–¯æ´›ä¼å…‹è¯­ã€æ–¯æ´›æ–‡å°¼äºšè¯­ã€ç´¢é©¬é‡Œè¯­ã€ç´¢æ‰˜è¯­ã€è¥¿ç­ç‰™è¯­ã€è‹ä¸¹è¯­ã€æ–¯ç“¦å¸Œé‡Œè¯­ã€ç‘å…¸è¯­ã€å¡”å‰å…‹è¯­ã€æ³°ç±³å°”è¯­ã€æ³°å¢å›ºè¯­ã€æ³°è¯­ã€åœŸè€³å…¶è¯­ã€ä¹Œå…‹å…°è¯­ã€ä¹Œå°”éƒ½è¯­ã€ä¹Œå…¹åˆ«å…‹è¯­ã€è¶Šå—è¯­ã€å¨å°”å£«è¯­ã€è¥¿å¼—é‡Œè¥¿äºšè¯­ã€ç§‘è¨è¯­ã€æ„ç¬¬ç»ªè¯­ã€çº¦é²å·´è¯­ã€ç¥–é²è¯­ã€‚\n\næ³¨æ„ï¼šmT5 åªåœ¨ mC4 ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œæ²¡æœ‰è¿›è¡Œä»»ä½•ç›‘ç£è®­ç»ƒã€‚å› æ­¤ï¼Œåœ¨ç”¨äºä¸‹æ¸¸ä»»åŠ¡ä¹‹å‰ï¼Œæ­¤æ¨¡å‹éœ€è¦è¿›ä¸€æ­¥å¾®è°ƒã€‚\n\né¢„è®­ç»ƒæ•°æ®é›†ï¼šmC4\n\nå…¶ä»–ç¤¾åŒºæ£€æŸ¥ç‚¹ï¼šæ­¤å¤„\n\nè®ºæ–‡ï¼šmT5: ä¸€ä¸ªå¤§è§„æ¨¡å¤šè¯­è¨€é¢„è®­ç»ƒæ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨\n\nä½œè€…ï¼šæ—æŒºé›ªã€è¯ºäºšÂ·åº·æ–¯å¦ç‰¹ã€äºšå½“Â·ç½—ä¼¯èŒ¨ã€ç±³å¸Œå°”Â·å‡¯å‹’ã€æ‹‰ç±³Â·é˜¿å°”-é²ç¦ã€é˜¿è¿ªäºšÂ·è¥¿ä¸¹ç‰¹ã€é˜¿è¿ªäºšÂ·å·´æ‹‰ã€ç§‘æ—Â·æ‹‰æ–å°”\n\næ‘˜è¦\n\næœ€è¿‘çš„â€œæ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨â€ï¼ˆT5ï¼‰åˆ©ç”¨ç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼å’Œè§„æ¨¡ï¼Œåœ¨ä¼—å¤šè‹±è¯­è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† mT5ï¼Œè¿™æ˜¯åŸºäºæ–°çš„ Common Crawl æ•°æ®é›†è¦†ç›–101ç§è¯­è¨€çš„ä¸€ç§å¤šè¯­è¨€ T5 å˜ä½“ã€‚æˆ‘ä»¬æè¿°äº† mT5 çš„è®¾è®¡å’Œä¿®æ”¹åçš„è®­ç»ƒï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨è®¸å¤šå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å‡å·²å…¬å¼€å¯ç”¨ã€‚\n\nä½¿ç”¨æ–¹æ³•\nimport os\n\nimport torch\nfrom openmind import AutoTokenizer, is_torch_npu_available\nfrom openmind_hub import snapshot_download\nfrom transformers import MT5ForConditionalGeneration\n\n\nmodel_path = snapshot_download(\"PyTorch-NPU/mt5_large\", revision=\"main\", resume_download=True,\n                                ignore_patterns=[\"*.h5\", \"*.ot\", \"*.msgpack\"])\nif is_torch_npu_available():\n    device = \"npu:0\"\nelif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\nmodel = MT5ForConditionalGeneration.from_pretrained(model_path, device_map=device)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n\noutput = model.generate(input_ids)\nprint(tokenizer.decode(output[0]))\n\n\nå½“ç„¶ï¼Œæˆ‘ä¼šå°½åŠ›æ»¡è¶³æ‚¨çš„è¦æ±‚ã€‚ä¸è¿‡ï¼Œæ‚¨ä¼¼ä¹å¿˜è®°æä¾›éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ã€‚è¯·æä¾›åŸæ–‡ï¼Œæˆ‘å°†ä¼šå°†å…¶ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒ Markdown æ ¼å¼ï¼Œå¹¶ç¡®ä¿ç¿»è¯‘çš„é€šä¿—æ€§ã€ä¸“ä¸šæ€§ã€ä¼˜é›…æ€§å’Œæµç•…æ€§ã€‚",
    "tags": "[\"Translation\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"JAX\", \"English\", \"Chinese\", \"multilingual\", \"Apache License 2.0\", \"mc4\"]"
  },
  {
    "url": "https://gitcode.com/openMind/mt5_small",
    "project_name": "mt5_small",
    "readme": "Original Text\n\nGoogle çš„ mT5\n\nmT5 æ¨¡å‹åœ¨ mC4 è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–101ç§è¯­è¨€ï¼š\n\néæ´²è¯­ã€é˜¿å°”å·´å°¼äºšè¯­ã€é˜¿å§†å“ˆæ‹‰è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€äºšç¾å°¼äºšè¯­ã€é˜¿å¡æ‹œç–†è¯­ã€å·´æ–¯å…‹è¯­ã€ç™½ä¿„ç½—æ–¯è¯­ã€å­ŸåŠ æ‹‰è¯­ã€ä¿åŠ åˆ©äºšè¯­ã€ç¼…ç”¸è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€å®¿åŠ¡è¯­ã€å¥‡åˆ‡ç“¦è¯­ã€ä¸­æ–‡ã€ç§‘è¥¿å˜‰è¯­ã€æ·å…‹è¯­ã€ä¸¹éº¦è¯­ã€è·å…°è¯­ã€è‹±è¯­ã€ä¸–ç•Œè¯­ã€çˆ±æ²™å°¼äºšè¯­ã€è²å¾‹å®¾è¯­ã€èŠ¬å…°è¯­ã€æ³•è¯­ã€åŠ åˆ©è¥¿äºšè¯­ã€æ ¼é²å‰äºšè¯­ã€å¾·è¯­ã€å¸Œè…Šè¯­ã€å¤å‰æ‹‰ç‰¹è¯­ã€æµ·åœ°å…‹é‡Œå¥¥å°”è¯­ã€è±ªè¨è¯­ã€å¤å¨å¤·è¯­ã€å¸Œä¼¯æ¥è¯­ã€å°åœ°è¯­ã€è‹—æ—ã€åŒˆç‰™åˆ©è¯­ã€å†°å²›è¯­ã€ä¼Šåšè¯­ã€å°åº¦å°¼è¥¿äºšè¯­ã€çˆ±å°”å…°è¯­ã€æ„å¤§åˆ©è¯­ã€æ—¥è¯­ã€çˆªå“‡è¯­ã€å¡çº³è¾¾è¯­ã€å“ˆè¨å…‹è¯­ã€é«˜æ£‰è¯­ã€éŸ©è¯­ã€åº“å°”å¾·è¯­ã€å‰å°”å‰æ–¯è¯­ã€è€æŒè¯­ã€æ‹‰ä¸è¯­ã€æ‹‰è„±ç»´äºšè¯­ã€ç«‹é™¶å®›è¯­ã€å¢æ£®å ¡è¯­ã€é©¬å…¶é¡¿è¯­ã€é©¬è¾¾åŠ æ–¯åŠ è¯­ã€é©¬æ¥è¯­ã€é©¬æ‹‰é›…æ‹‰å§†è¯­ã€é©¬è€³ä»–è¯­ã€æ¯›åˆ©è¯­ã€é©¬æ‹‰åœ°è¯­ã€è’™å¤è¯­ã€å°¼æ³Šå°”è¯­ã€æŒªå¨è¯­ã€æ™®ä»€å›¾è¯­ã€æ³¢æ–¯è¯­ã€æ³¢å…°è¯­ã€è‘¡è„ç‰™è¯­ã€æ—é®æ™®è¯­ã€ç½—é©¬å°¼äºšè¯­ã€ä¿„è¯­ã€è¨æ‘©äºšè¯­ã€è‹æ ¼å…°ç›–å°”è¯­ã€å¡å°”ç»´äºšè¯­ã€ç»çº³è¯­ã€ä¿¡å¾·è¯­ã€åƒ§ä¼½ç½—è¯­ã€æ–¯æ´›ä¼å…‹è¯­ã€æ–¯æ´›æ–‡å°¼äºšè¯­ã€ç´¢é©¬é‡Œè¯­ã€ç´¢æ‰˜è¯­ã€è¥¿ç­ç‰™è¯­ã€è‹ä¸¹è¯­ã€æ–¯ç“¦å¸Œé‡Œè¯­ã€ç‘å…¸è¯­ã€å¡”å‰å…‹è¯­ã€æ³°ç±³å°”è¯­ã€æ³°å¢å›ºè¯­ã€æ³°è¯­ã€åœŸè€³å…¶è¯­ã€ä¹Œå…‹å…°è¯­ã€ä¹Œå°”éƒ½è¯­ã€ä¹Œå…¹åˆ«å…‹è¯­ã€è¶Šå—è¯­ã€å¨å°”å£«è¯­ã€è¥¿å¼—é‡Œè¥¿äºšè¯­ã€ç§‘è¨è¯­ã€æ„ç¬¬ç»ªè¯­ã€çº¦é²å·´è¯­ã€ç¥–é²è¯­ã€‚\n\næ³¨æ„ï¼šmT5 ä»…åœ¨ mC4 ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒæœªåŒ…å«ä»»ä½•ç›‘ç£è®­ç»ƒã€‚å› æ­¤ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä½¿ç”¨ä¹‹å‰ï¼Œæ­¤æ¨¡å‹éœ€è¿›è¡Œå¾®è°ƒã€‚\n\né¢„è®­ç»ƒæ•°æ®é›†ï¼šmC4\n\nå…¶ä»–ç¤¾åŒºæ£€æŸ¥ç‚¹ï¼šæ­¤å¤„\n\nè®ºæ–‡ï¼šmT5: A massively multilingual pre-trained text-to-text transformer\n\nä½œè€…ï¼šLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel\n\næ‘˜è¦\n\næœ€è¿‘æå‡ºçš„â€œæ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å˜å‹å™¨â€ï¼ˆT5ï¼‰åˆ©ç”¨ç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼å’Œè§„æ¨¡ï¼Œåœ¨å¤šç§è‹±è¯­è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† mT5ï¼Œè¿™æ˜¯åŸºäº T5 çš„å¤šè¯­è¨€å˜ä½“ï¼Œåœ¨æ–°çš„ Common Crawl-based æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–101ç§è¯­è¨€ã€‚æˆ‘ä»¬æè¿°äº† mT5 çš„è®¾è®¡åŠä¿®æ”¹åçš„è®­ç»ƒæ–¹å¼ï¼Œå¹¶å±•ç¤ºäº†å®ƒåœ¨è®¸å¤šå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­çš„é¢†å…ˆæ€§èƒ½ã€‚æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å‡å·²å…¬å¼€æä¾›ã€‚\n\nä½¿ç”¨æ–¹æ³•\nimport os\n\nimport torch\nfrom openmind import AutoTokenizer, is_torch_npu_available\nfrom openmind_hub import snapshot_download\nfrom transformers import MT5ForConditionalGeneration\n\n\nmodel_path = snapshot_download(\"PyTorch-NPU/mt5_small\", revision=\"main\", resume_download=True,\n                                ignore_patterns=[\"*.h5\", \"*.ot\", \"*.msgpack\"])\ndevice = \"npu:0\"\n\nmodel = MT5ForConditionalGeneration.from_pretrained(model_path, device_map=device)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n\noutput = model.generate(input_ids)\nprint(tokenizer.decode(output[0]))\n",
    "tags": "[\"Translation\", \"PyTorch\", \"Transformers\", \"English\", \"Chinese\", \"multilingual\", \"Apache License 2.0\", \"mc4\"]"
  },
  {
    "url": "https://gitcode.com/openMind/rembert",
    "project_name": "rembert",
    "readme": "Original Text\nRemBERTï¼ˆç”¨äºåˆ†ç±»ï¼‰\n\nåŸºäº110ç§è¯­è¨€çš„é®è”½è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ç›®æ ‡é¢„è®­ç»ƒçš„RemBERTæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨è®ºæ–‡Rethinking embedding coupling in pre-trained language modelsä¸­æå‡ºã€‚æ¨¡å‹çš„ checkpoints çš„ç›´æ¥å¯¼å‡ºé¦–å…ˆåœ¨æ­¤ä»“åº“ä¸­æä¾›ã€‚ç”±äºæ­¤ç‰ˆæœ¬çš„ checkpoints æ˜¯ä¸ºäº†è¿›è¡Œåˆ†ç±»è€Œè®¾è®¡ï¼Œçœç•¥äº†è¾“å‡ºåµŒå…¥æƒé‡ï¼Œå› æ­¤æ›´ä¸ºè½»é‡ã€‚\n\nä¿®æ”¹\n\næ·»åŠ ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åœ¨openmindä¸­ä½¿ç”¨RemBERTã€‚\n\næ¨¡å‹æè¿°\n\nRemBERTä¸mBERTçš„ä¸»è¦åŒºåˆ«åœ¨äºè¾“å…¥å’Œè¾“å‡ºåµŒå…¥æ²¡æœ‰ç»‘å®šã€‚ç›¸åï¼ŒRemBERTä½¿ç”¨å°çš„è¾“å…¥åµŒå…¥å’Œè¾ƒå¤§çš„è¾“å‡ºåµŒå…¥ã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ›´ä¸ºé«˜æ•ˆï¼Œå› ä¸ºè¾“å‡ºåµŒå…¥ä¼šè¢«ä¸¢å¼ƒã€‚å½“åƒRemBERTé‚£æ ·å°†è¾“å…¥åµŒå…¥çš„å‚æ•°é‡æ–°æŠ•èµ„åˆ°æ ¸å¿ƒæ¨¡å‹ä¸­æ—¶ï¼Œå®ƒä¹Ÿæ›´åŠ å‡†ç¡®ã€‚\n\né¢„æœŸç”¨é€”ä¸é™åˆ¶\n\næ‚¨åº”è¯¥é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®ƒè¢«è®¾è®¡ä¸ºä¸€ç§é€šç”¨æ¨¡å‹ï¼Œç±»ä¼¼äºmBERTã€‚åœ¨æˆ‘ä»¬çš„è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å·²æˆåŠŸå°†è¯¥æ¨¡å‹åº”ç”¨äºåˆ†ç±»ã€é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«ã€è¯æ€§æ ‡æ³¨ç­‰ä»»åŠ¡ã€‚å¯¹äºæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ï¼Œæ‚¨åº”è¯¥è€ƒè™‘ä½¿ç”¨GPT2ç­‰æ¨¡å‹ã€‚\n\nè®­ç»ƒæ•°æ®\n\nRemBERTæ¨¡å‹åœ¨110ç§è¯­è¨€çš„å¤šç§è¯­è¨€ç»´åŸºç™¾ç§‘æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚å®Œæ•´çš„è¯­è¨€åˆ—è¡¨åœ¨æ­¤ä»“åº“ä¸Šã€‚\n\nBibTeXæ¡ç›®å’Œå¼•ç”¨ä¿¡æ¯\n@inproceedings{DBLP:conf/iclr/ChungFTJR21,\n  author    = {Hyung Won Chung and\n               Thibault F{\\'{e}}vry and\n               Henry Tsai and\n               Melvin Johnson and\n               Sebastian Ruder},\n  title     = {Rethinking Embedding Coupling in Pre-trained Language Models},\n  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,\n               Virtual Event, Austria, May 3-7, 2021},\n  publisher = {OpenReview.net},\n  year      = {2021},\n  url       = {https://openreview.net/forum?id=xpFFI\\_NtgpW},\n  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},\n  biburl    = {https://dblp.org/rec/conf/iclr/ChungFTJR21.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\nå¦‚ä½•åœ¨ OpenMind ä¸­ä½¿ç”¨ RemBERT\nimport torch\nfrom openmind import AutoTokenizer, is_torch_npu_available\nfrom transformers import RemBertForSequenceClassification\n\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\nmodel_path=  \"PyTorch-NPU/rembert\"\n\n#æ¨ç†\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = RemBertForSequenceClassification.from_pretrained(model_path).to(device)\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nprint(\">>>predicted_class_id = \", predicted_class_id)\n",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"English\", \"Chinese\", \"multilingual\", \"Apache License 2.0\", \"wikipedia\"]"
  },
  {
    "url": "https://gitcode.com/openMind/albert_base_v2",
    "project_name": "albert_base_v2",
    "readme": "Original Text\nä¿®æ”¹\nä¿®æ”¹ç¤ºä¾‹å¹¶æ·»åŠ  npu æ”¯æŒï¼›\nä¿®æ”¹â€œé™åˆ¶ä¸åå·®â€å’Œâ€œé¢„æœŸç”¨é€”ä¸é™åˆ¶â€éƒ¨åˆ†ã€‚\nALBERT åŸºç¡€ç‰ˆ v2\n\nè¿™æ˜¯ä¸€ç§åŸºäºè‹±æ–‡è¯­è¨€çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ç›®æ ‡ã€‚å®ƒé¦–æ¬¡å‡ºç°åœ¨ this paper å¹¶åœ¨ this repository ä¸­é¦–æ¬¡å‘å¸ƒã€‚ä¸æ‰€æœ‰ ALBERT æ¨¡å‹ä¸€æ ·ï¼Œè¯¥æ¨¡å‹ä¸åŒºåˆ†å¤§å°å†™ï¼šå®ƒä¸å¯¹ english å’Œ English è¿›è¡ŒåŒºåˆ†ã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒ ALBERT çš„å›¢é˜Ÿå¹¶æœªä¸ºè¯¥æ¨¡å‹ç¼–å†™æ¨¡å‹å¡ç‰‡ï¼Œå› æ­¤è¿™ä»½æ¨¡å‹å¡ç‰‡ç”± Hugging Face å›¢é˜Ÿæ’°å†™ã€‚\n\næ¨¡å‹æè¿°\n\nALBERT æ˜¯ä¸€ç§åœ¨å¤§é‡è‹±æ–‡æ•°æ®ä¸Šä»¥è‡ªç›‘ç£æ–¹å¼é¢„è®­ç»ƒçš„å˜æ¢å™¨æ¨¡å‹ã€‚è¿™æ„å‘³ç€å®ƒä»…åœ¨åŸå§‹æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ— éœ€äººå·¥æ ‡æ³¨ï¼ˆè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå¯ä»¥ä½¿ç”¨å¤§é‡å…¬å¼€å¯ç”¨æ•°æ®ï¼‰ï¼Œé€šè¿‡è‡ªåŠ¨è¿‡ç¨‹ä»è¿™äº›æ–‡æœ¬ç”Ÿæˆè¾“å…¥å’Œæ ‡ç­¾ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒæ˜¯ç”¨ä¸¤ä¸ªç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒçš„ï¼š\n\næ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ï¼šæ¨¡å‹éšæœºæ©ç è¾“å…¥ä¸­çš„ 15% çš„è¯è¯­ï¼Œç„¶åå°†æ•´ä¸ªæ©ç å¥å­é€šè¿‡æ¨¡å‹è¿è¡Œï¼Œå¹¶é¢„æµ‹æ©ç çš„è¯è¯­ã€‚è¿™ä¸ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰é€šå¸¸é€ä¸ªæŸ¥çœ‹è¯è¯­ï¼Œæˆ–è€…åƒ GPT è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹å†…éƒ¨æ©ç æœªæ¥æ ‡è®°ä¸åŒã€‚å®ƒä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¥å­çš„åŒå‘è¡¨ç¤ºã€‚\nå¥å­æ’åºé¢„æµ‹ï¼ˆSOPï¼‰ï¼šALBERT ä½¿ç”¨åŸºäºé¢„æµ‹ä¸¤ä¸ªè¿ç»­æ–‡æœ¬ç‰‡æ®µé¡ºåºçš„é¢„è®­ç»ƒæŸå¤±ã€‚\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å­¦ä¹ äº†è‹±è¯­è¯­è¨€çš„å†…éƒ¨è¡¨ç¤ºï¼Œç„¶åå¯ä»¥ç”¨äºæå–å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾ï¼šä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªæ ‡è®°å¥å­çš„æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨ ALBERT æ¨¡å‹ç”Ÿæˆçš„ç‰¹å¾ä½œä¸ºè¾“å…¥æ¥è®­ç»ƒä¸€ä¸ªæ ‡å‡†åˆ†ç±»å™¨ã€‚\n\nALBERT çš„ç‰¹åˆ«ä¹‹å¤„åœ¨äºå®ƒåœ¨ Transformer ä¸­å…±äº«å…¶å±‚ã€‚å› æ­¤ï¼Œæ‰€æœ‰å±‚éƒ½æœ‰ç›¸åŒçš„æƒé‡ã€‚ä½¿ç”¨é‡å¤å±‚å¯ä»¥å‡å°‘å†…å­˜å ç”¨ï¼Œä½†è®¡ç®—æˆæœ¬ä¸å…·æœ‰ç›¸åŒæ•°é‡éšè—å±‚çš„ BERT ç±»ä¼¼æ¶æ„ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒå¿…é¡»éå†ç›¸åŒæ•°é‡çš„ï¼ˆé‡å¤ï¼‰å±‚ã€‚\n\nè¿™æ˜¯åŸºç¡€æ¨¡å‹çš„ç¬¬äºŒä¸ªç‰ˆæœ¬ã€‚ç‰ˆæœ¬ 2 ä¸ç‰ˆæœ¬ 1 çš„ä¸åŒä¹‹å¤„åœ¨äºä¸åŒçš„ä¸¢å¼ƒç‡ã€é¢å¤–çš„è®­ç»ƒæ•°æ®å’Œæ›´é•¿çš„è®­ç»ƒæ—¶é—´ã€‚å®ƒåœ¨å‡ ä¹æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡ä¸­éƒ½æœ‰æ›´å¥½çš„ç»“æœã€‚\n\nè¯¥æ¨¡å‹å…·æœ‰ä»¥ä¸‹é…ç½®ï¼š\n\n12 ä¸ªé‡å¤å±‚\n128 ç»´åµŒå…¥ç»´åº¦\n768 ç»´éšè—ç»´åº¦\n12 ä¸ªæ³¨æ„åŠ›å¤´\n1100 ä¸‡ä¸ªå‚æ•°\né¢„æœŸç”¨é€”ä¸é™åˆ¶\n\næ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡æˆ–ä¸‹ä¸€å¥å­é¢„æµ‹ï¼Œä½†å®ƒä¸»è¦æ—¨åœ¨åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n\nè¯·æ³¨æ„ï¼Œè¯¥æ¨¡å‹ä¸»è¦ç”¨äºåœ¨ä»¥ä¸‹ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼šä½¿ç”¨æ•´ä¸ªå¥å­ï¼ˆå¯èƒ½è¢«æ©ç ï¼‰æ¥åšå‡ºå†³ç­–çš„ä»»åŠ¡ï¼Œå¦‚åºåˆ—åˆ†ç±»ã€æ ‡è®°åˆ†ç±»æˆ–é—®é¢˜å›ç­”ã€‚å¯¹äºæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ï¼Œæ‚¨åº”è¯¥è€ƒè™‘ä½¿ç”¨åƒ GPT2 è¿™æ ·çš„æ¨¡å‹ã€‚\n\nå¦‚ä½•ä½¿ç”¨\n\næ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤æ¨¡å‹ä»¥åŠç”¨äºæ©ç è¯­è¨€å»ºæ¨¡çš„ç®¡é“ï¼š\n\nimport torch\nfrom openmind import pipeline\n\nunmasker = pipeline('fill-mask', device_map=\"npu:0\", model='PyTorch-NPU/albert_base_v2')\nunmasker(\"Hello I'm a [MASK] model.\")\n\nå±€é™æ€§ä¸åè§\n\nå³ä¾¿ç”¨äºè®­ç»ƒæ­¤æ¨¡å‹çš„è®­ç»ƒæ•°æ®å¯ä»¥è¢«æè¿°ä¸ºç›¸å¯¹ä¸­ç«‹ï¼Œæ­¤æ¨¡å‹ä»å¯èƒ½äº§ç”Ÿå¸¦æœ‰åè§çš„é¢„æµ‹ã€‚è¿™ç§åè§ä¹Ÿä¼šå½±å“æ‰€æœ‰å¯¹æ­¤æ¨¡å‹çš„å¾®è°ƒç‰ˆæœ¬ã€‚\n\nè®­ç»ƒæ•°æ®\n\nALBERT æ¨¡å‹åœ¨ BookCorpus æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å« 11,038 æœ¬æœªå‘è¡¨ä¹¦ç±ä»¥åŠ è‹±æ–‡ç»´åŸºç™¾ç§‘ï¼ˆä¸å«åˆ—è¡¨ã€è¡¨æ ¼å’Œæ ‡é¢˜ï¼‰ã€‚\n\nè®­ç»ƒæµç¨‹\né¢„å¤„ç†\n\næ–‡æœ¬ä¼šè¢«è½¬æ¢ä¸ºå°å†™å¹¶ä½¿ç”¨ SentencePiece è¿›è¡Œåˆ†è¯ï¼Œè¯æ±‡é‡è®¾å®šä¸º 30,000ã€‚æ¨¡å‹çš„è¾“å…¥å½¢å¼å¦‚ä¸‹ï¼š\n\n[CLS] Sentence A [SEP] Sentence B [SEP]\n\nè®­ç»ƒ\n\nALBERT ç®—æ³•éµå¾ª BERT çš„è®¾ç½®ã€‚\n\nå¯¹äºæ¯ä¸ªå¥å­çš„æ©ç å¤„ç†ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n15% çš„æ ‡è®°è¢«æ©ç ã€‚\nåœ¨ 80% çš„æƒ…å†µä¸‹ï¼Œè¢«æ©ç çš„æ ‡è®°è¢«æ›¿æ¢ä¸º `\n@article{DBLP:journals/corr/abs-1909-11942,\n  author    = {Zhenzhong Lan and\n               Mingda Chen and\n               Sebastian Goodman and\n               Kevin Gimpel and\n               Piyush Sharma and\n               Radu Soricut},\n  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language\n               Representations},\n  journal   = {CoRR},\n  volume    = {abs/1909.11942},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1909.11942},\n  archivePrefix = {arXiv},\n  eprint    = {1909.11942},\n  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n\nå½“ç„¶ï¼Œè¯·æ‚¨æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬å’ŒMarkdownæ ¼å¼ï¼Œæˆ‘å°†æŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Safetensors\", \"English\", \"Apache License 2.0\", \"bookcorpus\", \"wikipedia\"]"
  },
  {
    "url": "https://gitcode.com/openMind/qwen1.5_7b_chat",
    "project_name": "qwen1.5_7b_chat",
    "readme": "Qwen1.5-7B-Chat\nModification\nModify example code to openMind and add npu support\nIntroduction\n\nQwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:\n\n6 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, and 72B;\nSignificant performance improvement in human preference for chat models;\nMultilingual support of both base and chat models;\nStable support of 32K context length for models of all sizes\nNo need of trust_remote_code.\n\nFor more details, please refer to our blog post and GitHub repo.\n\n\nModel Details\n\nQwen1.5 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes. For the beta version, temporarily we did not include GQA and the mixture of SWA and full attention.\n\nTraining details\n\nWe pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization. However, DPO leads to improvements in human preference evaluation but degradation in benchmark evaluation. In the very near future, we will fix both problems.\n\nRequirements\n\nThe code of Qwen1.5 has been in the latest Hugging face transformers and we advise you to install transformers>=4.37.0, or you might encounter the following error:\n\nKeyError: 'qwen2'\n\nQuickstart\n\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\n\nimport torch\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path= \"PyTorch-NPU/qwen1.5_7b_chat\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(f'>>>response={response}', flush = True)\n\n\nFor quantized models, we advise you to use the GPTQ, AWQ, and GGUF correspondents, namely Qwen1.5-7B-Chat-GPTQ-Int4, Qwen1.5-7B-Chat-GPTQ-Int8, Qwen1.5-7B-Chat-AWQ, and Qwen1.5-7B-Chat-GGUF.\n\nTips\nIf you encounter code switching or other bad cases, we advise you to use our provided hyper-parameters in generation_config.json.\nCitation\n\nIf you find our work helpful, feel free to give us a cite.\n\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"Safetensors\", \"English\", \"Other\", \"chat\"]"
  },
  {
    "url": "https://gitcode.com/openMind/t5_small",
    "project_name": "t5_small",
    "readme": "Model Card for T5 Small\nModification\n\nAdded the CANN version dependency description to the original README and modified the example code.\n\nTable of Contents\nModel Details\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nModel Card Authors\nHow To Get Started With the Model\nModel Details\nModel Description\n\nThe developers of the Text-To-Text Transfer Transformer (T5) write:\n\nWith T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\n\nT5-Small is the checkpoint with 60 million parameters.\n\nDeveloped by: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See associated paper and GitHub repo\nModel type: Language model\nLanguage(s) (NLP): English, French, Romanian, German\nLicense: Apache 2.0\nResources for more information:\nResearch paper\nGoogle's T5 Blog Post\nGitHub Repo\nUses\nDirect Use and Downstream Use\n\nThe developers write in a blog post that the model:\n\nOur text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\n\nSee the blog post and research paper for further details.\n\nOut-of-Scope Use\n\nMore information needed.\n\nBias, Risks, and Limitations\n\nMore information needed.\n\nRecommendations\n\nMore information needed.\n\nTraining Details\nTraining Data\n\nThe model is pre-trained on the Colossal Clean Crawled Corpus (C4), which was developed and released in the context of the same research paper as T5.\n\nThe model was pre-trained on a on a multi-task mixture of unsupervised (1.) and supervised tasks (2.). Thereby, the following datasets were being used for (1.) and (2.):\n\nDatasets used for Unsupervised denoising objective:\nC4\nWiki-DPR\nDatasets used for Supervised text-to-text language modeling objective\nSentence acceptability judgment\nCoLA Warstadt et al., 2018\nSentiment analysis\nSST-2 Socher et al., 2013\nParaphrasing/sentence similarity\nMRPC Dolan and Brockett, 2005\nSTS-B Ceret al., 2017\nQQP Iyer et al., 2017\nNatural language inference\nMNLI Williams et al., 2017\nQNLI Rajpurkar et al.,2016\nRTE Dagan et al., 2005\nCB De Marneff et al., 2019\nSentence completion\nCOPA Roemmele et al., 2011\nWord sense disambiguation\nWIC Pilehvar and Camacho-Collados, 2018\nQuestion answering\nMultiRC Khashabi et al., 2018\nReCoRD Zhang et al., 2018\nBoolQ Clark et al., 2019\nTraining Procedure\n\nIn their abstract, the model developers write:\n\nIn this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks.\n\nThe framework introduced, the T5 framework, involves a training procedure that brings together the approaches studied in the paper. See the research paper for further details.\n\nEvaluation\nTesting Data, Factors & Metrics\n\nThe developers evaluated the model on 24 tasks, see the research paper for full details.\n\nResults\n\nFor full results for T5-small, see the research paper, Table 14.\n\nEnvironmental Impact\n\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\n\nHardware Type: Google Cloud TPU Pods\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\n\nBibTeX:\n\n@article{2020t5,\n  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {140},\n  pages   = {1-67},\n  url     = {http://jmlr.org/papers/v21/20-074.html}\n}\n\n\nAPA:\n\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.\nModel Card Authors\n\nThis model card was written by the team at Hugging Face.\n\nHow to Get Started with the Model\n\nUse the code below to get started with the model.\n\nClick to expand",
    "tags": "[\"Translation\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"ONNX\", \"Rust\", \"Safetensors\", \"JAX\", \"English\", \"Apache License 2.0\", \"c4\"]"
  },
  {
    "url": "https://gitcode.com/openMind/bit_ms",
    "project_name": "bit_ms",
    "readme": "Original Text\nBigTransfer\n\nBig Transfer (BiT): é€šç”¨è§†è§‰è¡¨å¾å­¦ä¹ \n\nç®€ä»‹\n\nåœ¨è®­ç»ƒè§†è§‰ä»»åŠ¡çš„æ·±åº¦ç¥ç»ç½‘ç»œæ—¶ï¼Œè¿ç§»é¢„è®­ç»ƒè¡¨å¾å¯ä»¥æé«˜æ ·æœ¬æ•ˆç‡å¹¶ç®€åŒ–è¶…å‚æ•°è°ƒæ•´ã€‚Big Transfer (BiT) é€šè¿‡ç»“åˆä¸€äº›ç²¾å¿ƒé€‰æ‹©çš„ç»„ä»¶å¹¶ä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•è¿›è¡Œè¿ç§»ï¼Œåœ¨20å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚BiT ä¸ºè®­ç»ƒè¿ç§»æ€§è‰¯å¥½çš„æ¨¡å‹æç‚¼çš„ç»„ä»¶åŒ…æ‹¬ï¼š1) å¤§æ•°æ®é›†ï¼šéšç€æ•°æ®é›†å¤§å°çš„å¢åŠ ï¼ŒBITæ¨¡å‹çš„æœ€ä¼˜æ€§èƒ½ä¹Ÿä¼šå¢åŠ ã€‚2) å¤§æ¶æ„ï¼šä¸ºäº†å……åˆ†åˆ©ç”¨å¤§æ•°æ®é›†ï¼Œéœ€è¦è¶³å¤Ÿå¤§çš„æ¶æ„ã€‚3) é•¿æ—¶é—´é¢„è®­ç»ƒï¼šåœ¨å¤§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒéœ€è¦æ›´å¤šçš„è®­ç»ƒå‘¨æœŸå’Œè®­ç»ƒæ—¶é—´ã€‚4) GroupNorm å’Œ Weight Standardisationï¼šBiT ä½¿ç”¨ GroupNorm å’Œ Weight Standardisation è€Œä¸æ˜¯ BatchNormã€‚å› ä¸ºå½“æ¯ä¸ªåŠ é€Ÿå™¨ä¸Šçš„å›¾åƒæ•°é‡å¤ªå°‘æ—¶ï¼ŒBatchNorm çš„è¡¨ç°ä¼šæ›´å·®ã€‚5) é€šè¿‡BiTå¾®è°ƒï¼Œå³ä½¿è‡ªç„¶å›¾åƒä¸­æ¯ç§ç±»å‹çš„æ ·æœ¬åªæœ‰ä¸€ä¸ªä¹Ÿèƒ½è¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚å‚è€ƒæ–‡çŒ® 1, 2\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K ä¸Šçš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•°(M)\té…ç½®æ–‡ä»¶\tä¸‹è½½é“¾æ¥\nbit_resnet50\tD910x8-G\t76.81\t93.17\t25.55\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nbit_resnet50x3\tD910x8-G\t80.63\t95.12\t217.31\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nbit_resnet101\tD910x8-G\t77.93\t93.75\t44.54\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\næ³¨æ„äº‹é¡¹\nç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º {è®¾å¤‡}x{éƒ¨ä»¶}-{MindSporeæ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - pynative æ¨¡å¼ï¼Œå¸¦æœ‰ ms å‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºä½¿ç”¨å›¾æ¨¡å¼åœ¨8ç‰‡ Ascend 910 NPU ä¸Šè¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®ç‡ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV ä¸­çš„ å®‰è£…è¯´æ˜ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾é‡ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/bit/bit_resnet50_ascend.yaml --data_dir /path/to/imagenet\n\n\nè‹¥è„šæœ¬ç”±æ ¹ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­åŠ å…¥ --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚é˜… config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶ï¼Œçº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨å¸Œæœ›åœ¨æ— éœ€åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/bit/bit_resnet50_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/bit/bit_resnet50_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²æŒ‡å—\n\nè¯·å‚è€ƒMindCVçš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Kolesnikov A, Beyer L, Zhai X, ç­‰. Big transfer (bit): é€šç”¨è§†è§‰è¡¨ç¤ºå­¦ä¹ [C]//æ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®®. Springer, Cham, 2020: 491-507.\n\n[2] BigTransfer (BiT)ï¼šè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æœ€æ–°è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œhttps://blog.tensorflow.org/2020/05/bigtransfer-bit-state-of-art-transfer-learning-computer-vision.html",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/crossvit_ms",
    "project_name": "crossvit_ms",
    "readme": "Original Text\nCrossViT\n\nCrossViTï¼šç”¨äºå›¾åƒåˆ†ç±»çš„äº¤å‰æ³¨æ„åŠ›å¤šå°ºåº¦è§†è§‰è½¬æ¢å™¨\n\nç®€ä»‹\n\nCrossViT æ˜¯ä¸€ç§ä½¿ç”¨åŒåˆ†æ”¯æ¶æ„æå–å›¾åƒåˆ†ç±»å¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºçš„è§†è§‰è½¬æ¢å™¨ã€‚è¯¥æ¶æ„ç»“åˆäº†ä¸åŒå¤§å°çš„å›¾åƒå—ï¼ˆå³å˜æ¢å™¨ä¸­çš„æ ‡è®°ï¼‰ä»¥äº§ç”Ÿæ›´å¼ºçš„è§†è§‰ç‰¹å¾ç”¨äºå›¾åƒåˆ†ç±»ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªä¸åŒè®¡ç®—å¤æ‚åº¦çš„ç‹¬ç«‹åˆ†æ”¯å¤„ç†å°å’Œå¤§å›¾åƒå—æ ‡è®°ï¼Œè¿™äº›æ ‡è®°ä¼šå¤šæ¬¡èåˆä»¥äº’è¡¥å½¼æ­¤ã€‚\n\nèåˆæ˜¯é€šè¿‡ä¸€ä¸ªé«˜æ•ˆçš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—å®ç°çš„ï¼Œåœ¨è¯¥æ¨¡å—ä¸­ï¼Œæ¯ä¸ªå˜æ¢å™¨åˆ†æ”¯åˆ›å»ºä¸€ä¸ªéå—æ ‡è®°ä½œä¸ºä»£ç†ï¼Œé€šè¿‡æ³¨æ„åŠ›ä¸å…¶ä»–åˆ†æ”¯äº¤æ¢ä¿¡æ¯ã€‚è¿™å…è®¸èåˆæ—¶æ³¨æ„åŠ›å›¾çš„çº¿æ€§æ—¶é—´ç”Ÿæˆï¼Œè€ŒéäºŒæ¬¡æ—¶é—´ã€‚[1]\n\nå›¾ 1. CrossViT æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K ä¸Šçš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•° (M)\té…ç½®æ–‡ä»¶[yaml]\tä¸‹è½½[weights]\ncrossvit_9\tD910x8-G\t73.56\t91.79\t8.55\té…ç½®\tæƒé‡\ncrossvit_15\tD910x8-G\t81.08\t95.33\t27.27\té…ç½®\tæƒé‡\ncrossvit_18\tD910x8-G\t81.93\t95.75\t43.27\té…ç½®\tæƒé‡\nå¤‡æ³¨\nç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º {è®¾å¤‡}x{å—æ•°}-{æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - pynative æ¨¡å¼ï¼Œå¸¦æœ‰ ms å‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºä½¿ç”¨å›¾æ¨¡å¼åœ¨ 8 ç‰‡ Ascend 910 NPU ä¸Šè¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®åº¦ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV çš„å®‰è£…æŒ‡å—ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†ä»¥ä¾›æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ä½¿ç”¨ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾é‡ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/crossvit/crossvit_15_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬æ˜¯ç”± root ç”¨æˆ·æ‰§è¡Œçš„ï¼Œå¿…é¡»å‘ mpirun å‘½ä»¤ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nåŒæ ·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨å¸Œæœ›åœ¨ä¸éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/crossvit/crossvit_15_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/crossvit/crossvit_15_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²\n\nè¯·å‚è€ƒ MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Chun-Fu Chen, Quanfu Fan, Rameswar Panda. CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/edgenext_ms",
    "project_name": "edgenext_ms",
    "readme": "Original Text\nEdgeNeXt\n\nEdgeNeXtï¼šé¢å‘ç§»åŠ¨è§†è§‰åº”ç”¨çš„CNN-Transformeré«˜æ•ˆèåˆæ¶æ„\n\nç®€ä»‹\n\nEdgeNeXt æœ‰æ•ˆåœ°ç»“åˆäº† CNN å’Œ Transformer æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæ˜¯ä¸€ç§æ–°å‹çš„é«˜æ•ˆæ··åˆæ¶æ„ã€‚EdgeNeXt å¼•å…¥äº†ä¸€ç§åˆ†è£‚æ·±åº¦å·ç§¯è½¬ç½®æ³¨æ„åŠ›ï¼ˆSDTAï¼‰ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å°†è¾“å…¥å¼ é‡åˆ†æˆå¤šä¸ªé€šé“ç»„ï¼Œå¹¶æ²¿ç€é€šé“ç»´åº¦åˆ©ç”¨æ·±åº¦å·ç§¯å’Œè‡ªæ³¨æ„åŠ›æ¥éšå¼å¢åŠ æ„Ÿå—é‡å¹¶ç¼–ç å¤šå°ºåº¦ç‰¹å¾ã€‚[1]\n\nå›¾ 1. EdgeNeXt æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K æ•°æ®é›†ä¸Šå¤ç°çš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tè®­ç»ƒç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•°é‡ (M)\té…ç½®æ–‡ä»¶\tä¸‹è½½\nedgenext_xx_small\tD910x8-G\t71.02\t89.99\t1.33\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nedgenext_x_small\tD910x8-G\t75.14\t92.50\t2.34\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nedgenext_small\tD910x8-G\t79.15\t94.39\t5.59\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nedgenext_base\tD910x8-G\t82.24\t95.94\t18.51\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nè¯´æ˜\nè®­ç»ƒç¯å¢ƒï¼šè¡¨ç¤ºä¸º {è®¾å¤‡}x{ç‰‡æ•°}-{MindSpore æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - PyNative æ¨¡å¼ï¼Œå¹¶å¸¦æœ‰ ms å‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºä½¿ç”¨å›¾æ¨¡å¼åœ¨ 8 ç‰‡ Ascend 910 NPU ä¸Šè¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV çš„å®‰è£…è¯´æ˜ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†ä»¥ä¾›æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/edgenext/edgenext_small_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹æ¬¡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹æ¬¡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹æ¬¡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨æƒ³åœ¨ä¸éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/edgenext/edgenext_small_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/edgenext/edgenext_small_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²æŒ‡å—\n\nè¯·å‚è€ƒ MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Maaz M, Shaker A, Cholakkal H, ç­‰. EdgeNeXtï¼šä¸ºç§»åŠ¨è§†è§‰åº”ç”¨é«˜æ•ˆèåˆçš„å·ç§¯ç¥ç»ç½‘ç»œ-å˜ä½“æ¶æ„[J]. arXiv é¢„å°æœ¬ arXiv:2206.10589, 2022ã€‚",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/halonet_ms",
    "project_name": "halonet_ms",
    "readme": "HaloNet\n\nScaling Local Self-Attention for Parameter Efficient Visual Backbones\n\nIntroduction\n\nResearchers from Google Research and UC Berkeley have developed a new model of self-attention that can outperform standard baseline models and even high-performance convolutional models.[1]\n\nBlocked Self-Attentionï¼šThe whole input image is divided into multiple blocks and self-attention is applied to each block.However, if only the information inside the block is considered each time, it will inevitably lead to the loss of information.Therefore, before calculating the SA, a haloing operation is performed on each block, i.e., outside of each block, the information of the original image is used to padding a circle, so that the sensory field of each block can be appropriately larger and focus on more information.\n\nFigure 1. Architecture of Blocked Self-Attention [1]\n\nDown Samplingï¼šIn order to reduce the amount of computation, each block is sampled separately, and then attentions are performed on this sampled information to reach the effect of down sampling.\n\nFigure 2. Architecture of Down Sampling [1]\n\nResults\n\nOur reproduced model performance on ImageNet-1K is reported as follows.\n\nModel\tContext\tTop-1 (%)\tTop-5 (%)\tParams (M)\tRecipe\tDownload\nhalonet_50t\tD910X8-G\t79.53\t94.79\t22.79\tyaml\tweights\nNotes\nContext: Training context denoted as {device}x{pieces}-{MS mode}, where mindspore mode can be G - graph mode or F - pynative mode with ms function. For example, D910x8-G is for training on 8 pieces of Ascend 910 NPU using graph mode.\nTop-1 and Top-5: Accuracy reported on the validation set of ImageNet-1K.\nQuick Start\nPreparation\nInstallation\n\nPlease refer to the installation instruction in MindCV.\n\nDataset Preparation\n\nPlease download the ImageNet-1K dataset for model training and validation.\n\nTraining\nDistributed Training\n\nIt is easy to reproduce the reported results with the pre-defined training recipe. For distributed training on multiple Ascend 910 devices, please run\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/halonet/halonet_50t_ascend.yaml  --data_dir /path/to/imagenet\n\n\nIf the script is executed by the root user, the --allow-run-as-root parameter must be added to mpirun.\n\nSimilarly, you can train the model on multiple GPU devices with the above mpirun command.\n\nFor detailed illustration of all hyper-parameters, please refer to config.py.\n\nNote: As the global batch size (batch_size x num_devices) is an important hyper-parameter, it is recommended to keep the global batch size unchanged for reproduction or adjust the learning rate linearly to a new global batch size.\n\nStandalone Training\n\nIf you want to train or finetune the model on a smaller dataset without distributed training, please run:\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/halonet/halonet_50t_ascend.yaml  --data_dir /path/to/dataset --distribute False\n\nValidation\n\nTo validate the accuracy of the trained model, you can use validate.py and parse the checkpoint path with --ckpt_path.\n\npython validate.py -c configs/halonet/halonet_50t_ascend.yaml  --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\nDeployment\n\nPlease refer to the deployment tutorial in MindCV.\n\nReferences\n\n[1] Vaswani A, Ramachandran P, Srinivas A, et al. Scaling local self-attention for parameter efficient visual backbones[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 12894-12904.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/internlm_20b_chat_ms",
    "project_name": "internlm_20b_chat_ms",
    "readme": "Original Text\n\nInternLM\n\nÂ \nInternLM HOT\nÂ \n\nğŸ’»Github ä»“åº“ â€¢ ğŸ¤”é—®é¢˜åé¦ˆ\n\nä¿®æ”¹å†…å®¹\n\nè°ƒæ•´ç¤ºä¾‹ä»£ç éƒ¨åˆ†ã€‚\n\nç®€ä»‹\n\nä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤è”åˆå•†æ±¤ç§‘æŠ€ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦åŠå¤æ—¦å¤§å­¦ï¼Œæ­£å¼æ¨å‡º200äº¿å‚æ•°é¢„è®­ç»ƒæ¨¡å‹InternLM-20Bã€‚InternLM-20Båœ¨è¶…è¿‡2.3Tçš„é«˜è´¨é‡è‹±è¯­ã€ä¸­æ–‡åŠä»£ç æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚æ­¤å¤–ï¼Œå…¶Chatç‰ˆæœ¬è¿˜ç»è¿‡SFTä¸RLHFè®­ç»ƒï¼Œèƒ½æ›´ä¼˜è´¨ã€æ›´å®‰å…¨åœ°æ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚\n\nåœ¨æ¨¡å‹æ¶æ„ä¸Šï¼ŒInternLM-20Bé‡‡ç”¨æ›´æ·±çš„60å±‚è®¾è®¡ï¼Œè¿œè¶…å¸¸è§„7Bä¸13Bæ¨¡å‹çš„32æˆ–40å±‚ã€‚åœ¨å‚æ•°å—é™æ—¶ï¼Œå¢åŠ å±‚æ•°å¯æå‡æ¨¡å‹æ•´ä½“èƒ½åŠ›ã€‚ç›¸æ¯”InternLM-7Bï¼Œå…¶é¢„è®­ç»ƒæ•°æ®ç»è¿‡æ›´ä¸¥æ ¼çš„æ¸…æ´—ï¼Œå¹¶è¡¥å……äº†å¼ºåŒ–çŸ¥è¯†ä¸æ¨ç†èƒ½åŠ›çš„æ•°æ®ï¼Œå› æ­¤åœ¨ç†è§£ã€æ¨ç†ã€æ•°å­¦åŠç¼–ç¨‹ç­‰è€ƒéªŒè¯­è¨€æ¨¡å‹æŠ€æœ¯å®åŠ›çš„ç»´åº¦å‡æœ‰æ˜¾è‘—æå‡ã€‚æ€»ä½“è€Œè¨€ï¼ŒInternLM-20Bå…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n\nå“è¶Šçš„ç»¼åˆæ€§èƒ½\nå¼ºå¤§çš„å·¥å…·è°ƒç”¨èƒ½åŠ›\næ”¯æŒ16kä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé€šè¿‡æ¨ç†å¤–æ¨å®ç°ï¼‰\næ›´ä¼˜çš„ä»·å€¼å¯¹é½\næ€§èƒ½è¯„ä¼°\n\nåœ¨OpenCompassæå‡ºçš„5å¤§èƒ½åŠ›ç»´åº¦ä¸Šï¼ŒInternLM-20Bè¡¨ç°ä¼˜å¼‚ï¼ˆåŠ ç²—åˆ†æ•°ä»£è¡¨13B-33Bå‚æ•°èŒƒå›´å†…çš„æœ€ä½³è¡¨ç°ï¼‰ã€‚\n\nèƒ½åŠ›ç»´åº¦\tLlama-13B\tLlama2-13B\tBaichuan2-13B\tInternLM-20B\tLlama-33B\tLlama-65B\tLlama2-70B\nè¯­è¨€èƒ½åŠ›\t42.5\t47\t47.5\t55\t44.6\t47.1\t51.6\nçŸ¥è¯†å‚¨å¤‡\t58.2\t58.3\t48.9\t60.1\t64\t66\t67.7\nç†è§£èƒ½åŠ›\t45.5\t50.9\t58.1\t67.3\t50.6\t54.2\t60.8\næ¨ç†èƒ½åŠ›\t42.7\t43.6\t44.2\t54.9\t46.4\t49.8\t55\nè€ƒè¯•è¡¨ç°\t37.3\t45.2\t51.8\t62.5\t47.4\t49.7\t57.3\nç»¼åˆå¾—åˆ†\t43.8\t47.3\t49.4\t59.2\t48.9\t51.9\t57.4\n\nä¸‹è¡¨å¯¹æ¯”äº†ä¸»æµå¼€æºæ¨¡å‹åœ¨éƒ¨åˆ†æƒå¨å…¸å‹æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚\n\n\tè¯„æµ‹é›†\tLlama-13B\tLlama2-13B\tBaichuan2-13B\tInternLM-20B\tLlama-33B\tLlama-65B\tLlama2-70B\nè€ƒè¯•è¡¨ç°\tMMLU\t47.73\t54.99\t59.55\t62.05\t58.73\t63.71\t69.75\n\tC-Eval (val)\t31.83\t41.4\t59.01\t58.8\t37.47\t40.36\t50.13\n\tAGI-Eval\t22.03\t30.93\t37.37\t44.58\t33.53\t33.92\t40.02\nçŸ¥è¯†å‚¨å¤‡\tBoolQ\t78.75\t82.42\t67\t87.46\t84.43\t86.61\t87.74\n\tTriviaQA\t52.47\t59.36\t46.61\t57.26\t66.24\t69.79\t70.71\n\tNaturalQuestions\t20.17\t24.85\t16.32\t25.15\t30.89\t33.41\t34.16\nç†è§£èƒ½åŠ›\tCMRC\t9.26\t31.59\t29.85\t68.78\t14.17\t34.73\t43.74\n\tCSL\t55\t58.75\t63.12\t65.62\t57.5\t59.38\t60\n\tRACE (middle)\t53.41\t63.02\t68.94\t86.35\t64.55\t72.35\t81.55\n\tRACE (high)\t47.63\t58.86\t67.18\t83.28\t62.61\t68.01\t79.93\n\tXSum\t20.37\t23.37\t25.23\t35.54\t20.55\t19.91\t25.38\næ¨ç†èƒ½åŠ›\tWinoGrande\t64.64\t64.01\t67.32\t69.38\t66.85\t69.38\t69.77\n\tBBH\t37.93\t45.62\t48.98\t52.51\t49.98\t58.38\t64.91\n\tGSM8K\t20.32\t29.57\t52.62\t52.62\t42.3\t54.44\t63.31\n\tPIQA\t79.71\t79.76\t78.07\t80.25\t81.34\t82.15\t82.54\nç¼–ç¨‹èƒ½åŠ›\tHumanEval\t14.02\t18.9\t17.07\t25.61\t17.68\t18.9\t26.22\n\tMBPP\t20.6\t26.8\t30.8\t35.6\t28.4\t33.6\t39.6\n\næ€»ä½“è€Œè¨€ï¼ŒInternLM-20Båœ¨13Bå‚æ•°èŒƒå›´å†…çš„ç»¼åˆèƒ½åŠ›å…¨é¢è¶…è¶Šå¼€æºæ¨¡å‹ï¼Œåœ¨æ¨ç†è¯„æµ‹é›†ä¸Šçš„è¡¨ç°æ¥è¿‘ç”šè‡³è¶…è¿‡Llama-65Bã€‚\n\nä¸openMindç»“åˆä½¿ç”¨\næ¨ç†\nimport mindspore as ms\nfrom openmind import pipeline\n\nms.set_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\",\n                         model='MindSpore-Lab/internlm_20b_chat',\n                         framework='ms',\n                         model_kwargs={\"use_past\": True},\n                         trust_remote_code=True)\n\ntext = \"<s><|User|>:ä½ æ˜¯è°ï¼Ÿ<eoh>\\n<|Bot|>:\"\n\npipeline_result = pipeline_task(text, do_sample=False)\nprint(pipeline_result)\n\nå¼€æºè®¸å¯\n\nä»£ç åŸºäº Apache-2.0 åè®®å¼€æºï¼Œæ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¹¶å…è®¸å…è´¹å•†ä¸šä½¿ç”¨ã€‚å¦‚éœ€ç”³è¯·å•†ä¸šæˆæƒï¼Œè¯·å¡«å†™ç”³è¯·è¡¨ï¼ˆè‹±æ–‡ï¼‰/Application Form (Chinese)ã€‚å…¶ä»–é—®é¢˜æˆ–åˆä½œéœ€æ±‚ï¼Œè¯·è”ç³» internlm@pjlab.org.cnã€‚\n\næ›´æ–°è¯´æ˜\n\nä¼˜åŒ–äº†å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ä»£ç ã€‚\n\næ¨¡å‹ç®€ä»‹\n\nä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤è”åˆå•†æ±¤ç§‘æŠ€ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦åŠå¤æ—¦å¤§å­¦æ­£å¼å‘å¸ƒä¹¦ç”ŸÂ·æµ¦è¯­ 200 äº¿å‚æ•°ç‰ˆæœ¬ InternLM-20Bã€‚è¯¥æ¨¡å‹åœ¨è¶…è¿‡ 2.3T åŒ…å«é«˜è´¨é‡ä¸­è‹±æ–‡åŠä»£ç çš„ token æ•°æ®ä¸Šå®Œæˆé¢„è®­ç»ƒï¼Œå…¶å¯¹è¯ç‰ˆæœ¬ï¼ˆChatï¼‰è¿›ä¸€æ­¥ç»è¿‡ SFT ä¸ RLHF è®­ç»ƒï¼Œèƒ½æ›´ç²¾å‡†ã€å®‰å…¨åœ°å“åº”ç”¨æˆ·éœ€æ±‚ã€‚\n\nInternLM-20B é‡‡ç”¨æ·±åº¦æ¶æ„è®¾è®¡ï¼Œå±‚æ•°è¾¾ 60 å±‚ï¼Œè¿œè¶…å¸¸è§„ 7B/13B æ¨¡å‹çš„ 32 æˆ– 40 å±‚ç»“æ„ã€‚åœ¨å‚æ•°é‡å—é™æ¡ä»¶ä¸‹ï¼Œå¢åŠ å±‚æ•°æ˜¾è‘—æå‡äº†æ¨¡å‹ç»¼åˆèƒ½åŠ›ã€‚ç›¸æ¯” InternLM-7Bï¼Œ20B ç‰ˆæœ¬é‡‡ç”¨æ›´ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—æµç¨‹ï¼Œå¹¶æ–°å¢é«˜çŸ¥è¯†å¯†åº¦ä¸å¼ºåŒ–ç†è§£æ¨ç†çš„è®­ç»ƒæ•°æ®ï¼Œåœ¨è¯­è¨€ç†è§£ã€é€»è¾‘æ¨ç†ã€æ•°å­¦è®¡ç®—ã€ç¼–ç¨‹èƒ½åŠ›ç­‰æ ¸å¿ƒç»´åº¦å®ç°çªç ´æ€§æå‡ã€‚æ ¸å¿ƒä¼˜åŠ¿åŒ…æ‹¬ï¼š\n\nå“è¶Šçš„ç»¼åˆæ€§èƒ½è¡¨ç°\nå¼ºå¤§çš„å·¥å…·è°ƒç”¨èƒ½åŠ›\næ”¯æŒ 16k ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé€šè¿‡æ¨ç†æ—¶å¤–æ¨æŠ€æœ¯å®ç°ï¼‰\næ›´ç¬¦åˆäººç±»ä»·å€¼è§‚çš„å¯¹é½èƒ½åŠ›\næ€§èƒ½è¯„æµ‹\n\nåœ¨ OpenCompass äº”å¤§èƒ½åŠ›ç»´åº¦è¯„æµ‹ä¸­ï¼ŒInternLM-20B è¡¨ç°äº®çœ¼ï¼ˆåŠ ç²—é¡¹ä¸º 13B-33B é‡çº§èŒƒå›´å†…çš„æœ€ä½³æˆç»©ï¼‰ï¼š\n\nèƒ½åŠ›ç»´åº¦\tLlama-13B\tLlama2-13B\tBaichuan2-13B\tInternLM-20B\tLlama-33B\tLlama-65B\tLlama2-70B\nè¯­è¨€\t42.5\t47\t47.5\t55\t44.6\t47.1\t51.6\nçŸ¥è¯†\t58.2\t58.3\t48.9\t60.1\t64\t66\t67.7\nç†è§£\t45.5\t50.9\t58.1\t67.3\t50.6\t54.2\t60.8\næ¨ç†\t42.7\t43.6\t44.2\t54.9\t46.4\t49.8\t55\nå­¦ç§‘\t37.3\t45.2\t51.8\t62.5\t47.4\t49.7\t57.3\næ€»å¹³å‡\t43.8\t47.3\t49.4\t59.2\t48.9\t51.9\t57.4\n\nä¸‹è¡¨å±•ç¤º InternLM-20B åœ¨ç»å…¸è¯„æµ‹é›†ä¸Šä¸ä¸»æµå¼€æºæ¨¡å‹çš„å¯¹æ¯”è¡¨ç°ï¼š\n\n\tè¯„æµ‹é›†\tLlama-13B\tLlama2-13B\tBaichuan2-13B\tInternLM-20B\tLlama-33B\tLlama-65B\tLlama2-70B\nå­¦ç§‘\tMMLU\t47.73\t54.99\t59.55\t62.05\t58.73\t63.71\t69.75\n\tC-Eval (val)\t31.83\t41.4\t59.01\t58.8\t37.47\t40.36\t50.13\n\tAGI-Eval\t22.03\t30.93\t37.37\t44.58\t33.53\t33.92\t40.02\nçŸ¥è¯†\tBoolQ\t78.75\t82.42\t67\t87.46\t84.43\t86.61\t87.74\n\tTriviaQA\t52.47\t59.36\t46.61\t57.26\t66.24\t69.79\t70.71\n\tNaturalQuestions\t20.17\t24.85\t16.32\t25.15\t30.89\t33.41\t34.16\nç†è§£\tCMRC\t9.26\t31.59\t29.85\t68.78\t14.17\t34.73\t43.74\n\tCSL\t55\t58.75\t63.12\t65.62\t57.5\t59.38\t60\n\tRACE (middle)\t53.41\t63.02\t68.94\t86.35\t64.55\t72.35\t81.55\n\tRACE (high)\t47.63\t58.86\t67.18\t83.28\t62.61\t68.01\t79.93\n\tXSum\t20.37\t23.37\t25.23\t35.54\t20.55\t19.91\t25.38\næ¨ç†\tWinoGrande\t64.64\t64.01\t67.32\t69.38\t66.85\t69.38\t69.77\n\tBBH\t37.93\t45.62\t48.98\t52.51\t49.98\t58.38\t64.91\n\tGSM8K\t20.32\t29.57\t52.62\t52.62\t42.3\t54.44\t63.31\n\tPIQA\t79.71\t79.76\t78.07\t80.25\t81.34\t82.15\t82.54\nç¼–ç¨‹\tHumanEval\t14.02\t18.9\t17.07\t25.61\t17.68\t18.9\t26.22\n\tMBPP\t20.6\t26.8\t30.8\t35.6\t28.4\t33.6\t39.6\n\nInternLM-20B ç»¼åˆæ€§èƒ½å…¨é¢è¶…è¶Š 13B é‡çº§å¼€æºæ¨¡å‹ï¼Œéƒ¨åˆ†æ¨ç†èƒ½åŠ›æ¥è¿‘ç”šè‡³ä¼˜äº Llama-65Bã€‚\n\né€šè¿‡ OpenMind ä½¿ç”¨\næ¨ç†\ncd example\npython inference.py\n\nOpen Source License\n\nThe code in this repository is open-sourced under the Apache-2.0 license. The model weights are fully accessible for academic research, and free commercial use authorization can also be applied for (Application Form). For other inquiries or collaborations, please contact internlm@pjlab.org.cn.",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/latte_ms",
    "project_name": "latte_ms",
    "readme": "Latte: Latent Diffusion Transformer for Video Generation\nExample 1\tExample 2\tExample 3\n\t\t\n\nFigure 2. The generated videos of the pretrained model converted from the torch checkpoint.\n\nIntroduction\n\nLatte [1] is a novel Latent Diffusion Transformer designed for video generation. It is built based on DiT (a diffusion transformer model for image generation). For introduction of DiT [2], please refer to README of DiT.\n\nLatte first uses a VAE (Variational AutoEncoder) to compress the video data into a latent space, and then extracts spatial-temporal tokens given the latent codes. Similar to DiT, it stacks multiple transformer blocks to model the video diffusion in the latent space. How to design the spatial and temporal blocks becomes a major question.\n\nThrough experiments and analysis, they found the best practice is structure (a) in the image below. It stacks spatial blocks and temporal blocks alternately to model spatial attentions and temporal attentions in turns.\n\nFigure 1. The Structure of Latte and Latte transformer blocks. [1]\n\nSimilar to DiT, Latte supports un-conditional video generation and class-labels-conditioned video generation. In addition, it supports to generate videos given text captions.\n\nEvaluation Results\n\nThe training speed of the experiments with 256x256 image size is summarized in the following table:\n\nCards\tRecompute\tDataset Sink mode\tEmbedding Cache\tTrain. imgs/s\n1\tOFF\tON\tOFF\t62.3\n1\tON\tON\tON\t93.6\n4\tON\tON\tON\t368.3\nHow to Get Started with the Model\n\nFor information on how to train and infer with the model, please have a look at MindOne GitHub Repository.\n\nUses\nDirect Use\n\nThe model is intended for research purposes only. Possible research areas and tasks include\n\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\n\nExcluded uses are described below.\n\nOut-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to â€œA red cube on top of a blue sphereâ€\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nBias\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.",
    "tags": "[\"Text-to-Image\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/yolov5_ms",
    "project_name": "yolov5_ms",
    "readme": "Original Text\nYOLOv5\nå¼•è¨€\n\nYOLOv5æ˜¯ä¸€ç³»åˆ—åœ¨COCOæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„å¯¹è±¡æ£€æµ‹æ¶æ„å’Œæ¨¡å‹ï¼Œä»£è¡¨äº†Ultralyticså¯¹æœªæ¥è§†è§‰AIæ–¹æ³•çš„å¼€æºç ”ç©¶ï¼Œæ±²å–äº†æ•°åƒå°æ—¶çš„ç ”ç©¶å’Œå¼€å‘ç»éªŒä¸­æ‰€å­¦åˆ°çš„æ•™è®­å’Œæœ€ä½³å®è·µã€‚\n\nè¯„ä¼°ç»“æœ\nåç§°\tè§„æ¨¡\tæ¶æ„\tä¸Šä¸‹æ–‡\tå›¾åƒå°ºå¯¸\tæ•°æ®é›†\tç›’å­mAP (%)\tå‚æ•°\tFLOPs\té…ç½®æ–‡ä»¶\tä¸‹è½½\nYOLOv5\tN\tP5\tD910x8-G\t640\tMS COCO 2017\t27.3\t1.9M\t4.5G\tyaml\tæƒé‡\nYOLOv5\tS\tP5\tD910x8-G\t640\tMS COCO 2017\t37.6\t7.2M\t16.5G\tyaml\tæƒé‡\nYOLOv5\tM\tP5\tD910x8-G\t640\tMS COCO 2017\t44.9\t21.2M\t49.0G\tyaml\tæƒé‡\nYOLOv5\tL\tP5\tD910x8-G\t640\tMS COCO 2017\t48.5\t46.5M\t109.1G\tyaml\tæƒé‡\nYOLOv5\tX\tP5\tD910x8-G\t640\tMS COCO 2017\t50.5\t86.7M\t205.7G\tyaml\tæƒé‡\n\n\næ³¨é‡Š\nä¸Šä¸‹æ–‡ï¼šè®­ç»ƒä¸Šä¸‹æ–‡è¡¨ç¤ºä¸º{è®¾å¤‡}x{ç‰‡æ•°}-{MSæ¨¡å¼}ï¼Œå…¶ä¸­mindsporeæ¨¡å¼å¯ä»¥æ˜¯G - å›¾å½¢æ¨¡å¼æˆ–F - pynativeæ¨¡å¼ï¼Œå¸¦mså‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-Gä»£è¡¨åœ¨8ç‰‡Ascend 910 NPUä¸Šä½¿ç”¨å›¾å½¢æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚\nç›’å­mAPï¼šåœ¨éªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®åº¦ã€‚\næˆ‘ä»¬å‚è€ƒäº†å®˜æ–¹çš„ YOLOV5 æ¥é‡ç°P5ç³»åˆ—æ¨¡å‹ï¼Œå·®å¼‚å¦‚ä¸‹ï¼š\næˆ‘ä»¬ä½¿ç”¨8x NPU(Ascend910)è¿›è¡Œè®­ç»ƒï¼Œå•NPUçš„æ‰¹é‡å¤§å°ä¸º32ã€‚è¿™ä¸å®˜æ–¹ä»£ç æœ‰æ‰€ä¸åŒã€‚\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\næœ‰å…³å¦‚ä½•ä½¿ç”¨è¯¥æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œæ¨ç†çš„ä¿¡æ¯ï¼Œè¯·å‚é˜…MindYOLO GitHub ä»£ç åº“ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Jocher Glenn. YOLOv5ç‰ˆæœ¬v6.1å‘å¸ƒã€‚https://github.com/ultralytics/yolov5/releases/tag/v6.1, 2022.",
    "tags": "[\"Object Detection\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/cmt_ms",
    "project_name": "cmt_ms",
    "readme": "Original Text\nCMTï¼šå·ç§¯ç¥ç»ç½‘ç»œé‡è§è§†è§‰å˜æ¢å™¨\n\nCMT: å·ç§¯ç¥ç»ç½‘ç»œé‡è§è§†è§‰å˜æ¢å™¨\n\nå¼•è¨€\n\nCMTæ˜¯ä¸€ç§å……åˆ†åˆ©ç”¨CNNä¸å˜æ¢å™¨çš„ä¼˜åŠ¿çš„æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰é•¿è·ç¦»ä¾èµ–å¹¶æå–å±€éƒ¨ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†é™ä½è®¡ç®—æˆæœ¬ï¼Œæ­¤æ–¹æ³•ä½¿ç”¨äº†è½»é‡çº§çš„MHSAï¼ˆå¤šå¤´è‡ªæ³¨æ„åŠ›ï¼‰å’Œç±»ä¼¼äºMobileNetçš„æ·±åº¦å·ç§¯ä¸é€ç‚¹å·ç§¯ã€‚é€šè¿‡ç»“åˆè¿™äº›éƒ¨åˆ†ï¼ŒCMTåœ¨ImageNet-1Kæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ImageNet-1Kæ•°æ®é›†ä¸Šå¤ç°çš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tè®­ç»ƒç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•°é‡(M)\té…ç½®æ–¹æ¡ˆ\tä¸‹è½½\ncmt_small\tD910x8-G\t83.24\t96.41\t26.09\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nè¯´æ˜\nè®­ç»ƒç¯å¢ƒï¼šè¡¨ç¤ºä¸º{è®¾å¤‡}x{å—æ•°}-{MindSporeæ¨¡å¼}çš„è®­ç»ƒç¯å¢ƒï¼Œå…¶ä¸­MindSporeæ¨¡å¼å¯ä»¥æ˜¯G - å›¾æ¨¡å¼æˆ–F - åŸç”Ÿæ¨¡å¼ï¼ˆä½¿ç”¨mså‡½æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼ŒD910x8-Gè¡¨ç¤ºåœ¨8å—Ascend 910 NPUä¸Šä½¿ç”¨å›¾æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚\nTop-1å’ŒTop-5ï¼šåœ¨ImageNet-1KéªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®åº¦ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒMindCVçš„å®‰è£…è¯´æ˜ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ImageNet-1Kæ•°æ®é›†ä»¥ä¾›æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šå°Ascend 910è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/cmt/cmt_small_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­åŠ å…¥ --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\nå…³äºæ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…è°ƒæ•´å­¦ä¹ ç‡ï¼Œä½¿å…¶çº¿æ€§é€‚åº”æ–°çš„å…¨å±€æ‰¹é‡å¤§å°ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨å¸Œæœ›åœ¨æ— éœ€åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/cmt/cmt_small_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/cmt/cmt_small_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²\n\nè¯·å‚è€ƒéƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] éƒ­Jï¼ŒéŸ©Kï¼Œå´Hï¼Œç­‰. Cmt: å·ç§¯ç¥ç»ç½‘ç»œä¸è§†è§‰å˜æ¢å™¨ç›¸é‡[C]//IEEE/CVFè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ã€‚2022: 12175-12185ã€‚",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/codellama_34b_ms",
    "project_name": "codellama_34b_ms",
    "readme": "Code Llama\n\nCode Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 34 billion parameters. This is the repository for the base 34B version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding. Links to other models can be found in the index at the bottom.\n\nModel Use\n\nTo use this model, please make sure to install transformers:\n\npip install transformers.git accelerate\n\n\nModel capabilities:\n\nCode completion.\nInfilling.\nInstructions / chat.\nPython specialist.\n>>> from openmind import AutoTokenizer, AutoModelForCausalLM\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openmind/glm2_6b_ms\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"openmind/glm2_6b_ms\")\n>>> queries = [\"ä½ å¥½\", \"è¯·ä»‹ç»ä¸€ä¸‹æ­å·\"]\n>>> history = []\n>>> for query in queries:\n>>>     prompt = tokenizer.build_prompt(query, history=history) \n>>>     input_id = tokenizer.encode(prompt)\n>>>     output = model.generate([input_id], do_sample=False)\n>>>     response = tokenizer.decode(output)\n>>>     print(response)\n>>>     history += [(query, response)]\nresponse1:ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\nresponse2:æ­å·æ˜¯ä¸­å›½æµ™æ±Ÿçœçœä¼šï¼Œä½äºæµ™æ±Ÿçœä¸œå—éƒ¨ï¼Œåœ°å¤„æµ™æ±ŸçœåŒ—éƒ¨ï¼Œä¸œä¸´ä¸œæµ·ï¼Œå—æ¥ç¦å»ºçœï¼ŒåŒ—ä¸æ±Ÿè‹çœæ¯—é‚»ï¼Œæ˜¯ä¸­å›½è‘—åçš„æ—…æ¸¸åŸå¸‚ä¹‹ä¸€ã€‚\\n\\næ­å·æœ‰ç€æ‚ ä¹…çš„å†å²å’Œæ–‡åŒ–ï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€ï¼Œè‡ªä¸œæ™‹ä»¥æ¥ï¼Œæ­å·ä¸€ç›´æ˜¯æ”¿æ²»ã€ç»æµã€æ–‡åŒ–å’Œäº¤é€šä¸­å¿ƒã€‚å®‹ä»£ï¼Œæ­å·æˆä¸ºç¹åçš„å•†ä¸šåŸå¸‚ï¼Œè¢«èª‰ä¸ºâ€œä¸œå—ç¬¬ä¸€é‡é•‡â€ã€‚æ˜æ¸…æ—¶æœŸï¼Œæ­å·æˆä¸ºå…¨å›½è‘—åçš„â€œäººé—´å¤©å ‚â€ï¼Œå¸å¼•äº†å¤§é‡å•†äººã€æ–‡äººã€å®˜å‘˜å‰æ¥è§‚å…‰ã€äº¤æµã€‚\\n\\nå¦‚ä»Šï¼Œæ­å·å·²æˆä¸ºä¸­å›½çš„è‘—åæ—…æ¸¸åŸå¸‚ä¹‹ä¸€ï¼Œè¢«èª‰ä¸ºâ€œä¸­å›½æœ€å…·é­…åŠ›çš„åŸå¸‚â€ä¹‹ä¸€ã€‚æ­å·æ‹¥æœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿï¼Œå¦‚è¥¿æ¹–ã€çµéšå¯ºã€åƒå²›æ¹–ã€å®‹åŸç­‰ï¼Œå¸å¼•äº†å¤§é‡å›½å†…å¤–æ¸¸å®¢å‰æ¥è§‚å…‰æ—…æ¸¸ã€‚æ­¤å¤–ï¼Œæ­å·è¿˜æœ‰ç€å‘è¾¾çš„ç»æµå’Œä¼˜ç¾çš„è‡ªç„¶ç¯å¢ƒï¼Œè¢«èª‰ä¸ºâ€œå±±æ°´ç”²å¤©ä¸‹â€ã€‚\n\nModel Details\n\n*Note: Use of this model is governed by the Meta license. Meta developed and publicly released the Code Llama family of large language models (LLMs).\n\nModel Developers Meta\n\nVariations Code Llama comes in three model sizes, and three variants:\n\nCode Llama: base models designed for general code synthesis and understanding\nCode Llama - Python: designed specifically for Python\nCode Llama - Instruct: for instruction following and safer deployment\n\nAll variants are available in sizes of 7B, 13B and 34B parameters.\n\nThis repository contains the base version of the 34B parameters model.\n\nInput Models input text only.\n\nOutput Models generate text only.\n\nModel Architecture Code Llama is an auto-regressive language model that uses an optimized transformer architecture.\n\nModel Dates Code Llama and its variants have been trained between January 2023 and July 2023.\n\nStatus This is a static model trained on an offline dataset. Future versions of Code Llama - Instruct will be released as we improve model safety with community feedback.\n\nLicense A custom commercial license is available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\n\nResearch Paper More information can be found in the paper \"Code Llama: Open Foundation Models for Code\" or its arXiv page.\n\nIntended Use\n\nIntended Use Cases Code Llama and its variants is intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.\n\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.\n\nHardware and Software\n\nTraining Factors We used custom training libraries. The training and fine-tuning of the released models have been performed Metaâ€™s Research Super Cluster.\n\nCarbon Footprint In aggregate, training all 9 Code Llama models required 400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 65.3 tCO2eq, 100% of which were offset by Metaâ€™s sustainability program.\n\nTraining Data\n\nAll experiments reported here and the released models have been trained and fine-tuned using the same data as Llama 2 with different weights (see Section 2 and Table 1 in the research paper for details).\n\nEvaluation Results\n\nSee evaluations for the main models and detailed ablations in Section 3 and safety evaluations in Section 4 of the research paper.\n\nEthical Considerations and Limitations\n\nCode Llama and its variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Code Llamaâ€™s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Code Llama, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available available at https://ai.meta.com/llama/responsible-use-guide.",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/convit_ms",
    "project_name": "convit_ms",
    "readme": "ConViT\n\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n\nIntroduction\n\nConViT combines the strengths of convolutional architectures and Vision Transformers (ViTs). ConViT introduces gated positional self-attention (GPSA), a form of positional self-attention that can be equipped with a â€œsoftâ€ convolutional inductive bias. ConViT initializes the GPSA layers to mimic the locality of convolutional layers, then gives each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. ConViT, outperforms the DeiT (Touvron et al., 2020) on ImageNet, while offering a much improved sample efficiency.[1]\n\nFigure 1. Architecture of ConViT [1]\n\nResults\n\nOur reproduced model performance on ImageNet-1K is reported as follows.\n\nModel\tContext\tTop-1 (%)\tTop-5 (%)\tParams (M)\tRecipe\tDownload\nconvit_tiny\tD910x8-G\t73.66\t91.72\t5.71\tyaml\tweights\nconvit_tiny_plus\tD910x8-G\t77.00\t93.60\t9.97\tyaml\tweights\nconvit_small\tD910x8-G\t81.63\t95.59\t27.78\tyaml\tweights\nconvit_small_plus\tD910x8-G\t81.80\t95.42\t48.98\tyaml\tweights\nconvit_base\tD910x8-G\t82.10\t95.52\t86.54\tyaml\tweights\nconvit_base_plus\tD910x8-G\t81.96\t95.04\t153.13\tyaml\tweights\nNotes\nContext: Training context denoted as {device}x{pieces}-{MS mode}, where mindspore mode can be G - graph mode or F - pynative mode with ms function. For example, D910x8-G is for training on 8 pieces of Ascend 910 NPU using graph mode.\nTop-1 and Top-5: Accuracy reported on the validation set of ImageNet-1K.\nQuick Start\nPreparation\nInstallation\n\nPlease refer to the installation instruction in MindCV.\n\nDataset Preparation\n\nPlease download the ImageNet-1K dataset for model training and validation.\n\nTraining\nDistributed Training\n\nIt is easy to reproduce the reported results with the pre-defined training recipe. For distributed training on multiple Ascend 910 devices, please run\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/convit/convit_tiny_ascend.yaml --data_dir /path/to/imagenet\n\n\nIf the script is executed by the root user, the --allow-run-as-root parameter must be added to mpirun.\n\nSimilarly, you can train the model on multiple GPU devices with the above mpirun command.\n\nFor detailed illustration of all hyper-parameters, please refer to config.py.\n\nNote: As the global batch size (batch_size x num_devices) is an important hyper-parameter, it is recommended to keep the global batch size unchanged for reproduction or adjust the learning rate linearly to a new global batch size.\n\nStandalone Training\n\nIf you want to train or finetune the model on a smaller dataset without distributed training, please run:\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/convit/convit_tiny_ascend.yaml --data_dir /path/to/dataset --distribute False\n\nValidation\n\nTo validate the accuracy of the trained model, you can use validate.py and parse the checkpoint path with --ckpt_path.\n\npython validate.py -c configs/convit/convit_tiny_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\nDeployment\n\nPlease refer to the deployment tutorial in MindCV.\n\nReferences\n\n[1] dâ€™Ascoli S, Touvron H, Leavitt M L, et al. Convit: Improving vision transformers with soft convolutional inductive biases[C]//International Conference on Machine Learning. PMLR, 2021: 2286-2296.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/ghostnet_ms",
    "project_name": "ghostnet_ms",
    "readme": "Original Text\nGhostNet\n\nGhostNet: ä»¥ä½æˆæœ¬æ“ä½œè·å–æ›´å¤šç‰¹å¾\n\nç®€ä»‹\n\nç‰¹å¾å›¾å†—ä½™æ˜¯æˆåŠŸå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ä¸€ä¸ªé‡è¦ç‰¹æ€§ï¼Œä½†åœ¨ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡ä¸­å´å¾ˆå°‘è¢«ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„Ghostæ¨¡å—ï¼Œä»¥ä½æˆæœ¬æ“ä½œç”Ÿæˆæ›´å¤šçš„ç‰¹å¾å›¾ã€‚åŸºäºä¸€ç»„å†…åœ¨ç‰¹å¾å›¾ï¼Œä½œè€…åº”ç”¨äº†ä¸€ç³»åˆ—ä½æˆæœ¬çº¿æ€§å˜æ¢æ¥ç”Ÿæˆè®¸å¤šå¹½çµç‰¹å¾å›¾ï¼Œè¿™äº›ç‰¹å¾å›¾å¯ä»¥å®Œå…¨æ­ç¤ºå†…åœ¨ç‰¹å¾ä¸‹çš„ä¿¡æ¯ã€‚æå‡ºçš„Ghostæ¨¡å—å¯ä»¥ä½œä¸ºå³æ’å³ç”¨çš„ç»„ä»¶æ¥å‡çº§ç°æœ‰çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚è®¾è®¡Ghostç“¶é¢ˆä»¥å †å Ghostæ¨¡å—ï¼Œä»è€Œå¯ä»¥è½»æ¾æ„å»ºè½»é‡çº§çš„GhostNetã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGhostæ¨¡å—æ˜¯åŸºçº¿æ¨¡å‹ä¸­å·ç§¯å±‚çš„ä»¤äººå°è±¡æ·±åˆ»çš„é€‰æ‹©ï¼ŒGhostNetåœ¨ImageNet ILSVRC-2012åˆ†ç±»æ•°æ®é›†ä¸Šä»¥ç±»ä¼¼çš„è®¡ç®—æˆæœ¬å®ç°äº†æ¯”MobileNetV3æ›´é«˜çš„è¯†åˆ«æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œ75.7%çš„å‰1åå‡†ç¡®åº¦ï¼‰ã€‚[1]\n\nå›¾ 1. GhostNetæ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ImageNet-1Kä¸Šå¤ç°çš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tç¯å¢ƒ\tå‰1åå‡†ç¡®åº¦ (%)\tå‰5åå‡†ç¡®åº¦ (%)\tå‚æ•°é‡ (M)\té…ç½®æ–‡ä»¶\tä¸‹è½½åœ°å€\nghostnet_050\tD910x8-G\t66.03\t86.64\t2.60\tyaml\tæƒé‡æ–‡ä»¶\nghostnet_100\tD910x8-G\t73.78\t91.66\t5.20\tyaml\tæƒé‡æ–‡ä»¶\nghostnet_130\tD910x8-G\t75.50\t92.56\t7.39\tyaml\tæƒé‡æ–‡ä»¶\næ³¨é‡Š\nç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º{è®¾å¤‡}x{å—æ•°}-{MindSporeæ¨¡å¼}ï¼Œå…¶ä¸­MindSporeæ¨¡å¼å¯ä»¥æ˜¯G - å›¾æ¨¡å¼æˆ–F - pynativeæ¨¡å¼ï¼Œä½¿ç”¨mså‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-Gè¡¨ç¤ºåœ¨8å—Ascend 910 NPUä¸Šä½¿ç”¨å›¾æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚\nå‰1åå’Œå‰5åï¼šåœ¨ImageNet-1KéªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®åº¦ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚é˜…MindCVä¸­çš„å®‰è£…æŒ‡å—ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ImageNet-1Kæ•°æ®é›†ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ªAscend 910è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/ghostnet/ghostnet_100_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹æ¬¡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹æ¬¡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹æ¬¡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨å¸Œæœ›åœ¨æ— éœ€åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/ghostnet/ghostnet_100_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/ghostnet/ghostnet_100_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²æŒ‡å—\n\nè¯·å‚é˜… MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] éŸ©åº·, ç‹å²©, ç”°å¥‡, ç­‰. Ghostnetï¼šä»ä½æˆæœ¬æ“ä½œä¸­è·å¾—æ›´å¤šç‰¹å¾[C]// IEEE/CVF è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†. 2020: 1580-1589.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/mixnet_ms",
    "project_name": "mixnet_ms",
    "readme": "MixNet\n\nMixConv: Mixed Depthwise Convolutional Kernels\n\nIntroduction\n\nDepthwise convolution is becoming increasingly popular in modern efficient ConvNets, but its kernel size is often overlooked. In this paper, the authors systematically study the impact of different kernel sizes, and observe that combining the benefits of multiple kernel sizes can lead to better accuracy and efficiency. Based on this observation, the authors propose a new mixed depthwise convolution (MixConv), which naturally mixes up multiple kernel sizes in a single convolution. As a simple drop-in replacement of vanilla depthwise convolution, our MixConv improves the accuracy and efficiency for existing MobileNets on both ImageNet classification and COCO object detection.[1]\n\nFigure 1. Architecture of MixNet [1]\n\nResults\n\nOur reproduced model performance on ImageNet-1K is reported as follows.\n\nModel\tContext\tTop-1 (%)\tTop-5 (%)\tParams (M)\tRecipe\tDownload\nmixnet_s\tD910x8-G\t75.52\t92.52\t4.17\tyaml\tweights\nmixnet_m\tD910x8-G\t76.64\t93.05\t5.06\tyaml\tweights\nmixnet_l\tD910x8-G\t78.73\t94.31\t7.38\tyaml\tweights\nNotes\nContext: Training context denoted as {device}x{pieces}-{MS mode}, where mindspore mode can be G - graph mode or F - pynative mode with ms function. For example, D910x8-G is for training on 8 pieces of Ascend 910 NPU using graph mode.\nTop-1 and Top-5: Accuracy reported on the validation set of ImageNet-1K.\nQuick Start\nPreparation\nInstallation\n\nPlease refer to the installation instruction in MindCV.\n\nDataset Preparation\n\nPlease download the ImageNet-1K dataset for model training and validation.\n\nTraining\nDistributed Training\n\nIt is easy to reproduce the reported results with the pre-defined training recipe. For distributed training on multiple Ascend 910 devices, please run\n\n# distrubted training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/mixnet/mixnet_s_ascend.yaml --data_dir /path/to/imagenet\n\n\nIf the script is executed by the root user, the --allow-run-as-root parameter must be added to mpirun.\n\nSimilarly, you can train the model on multiple GPU devices with the above mpirun command.\n\nFor detailed illustration of all hyper-parameters, please refer to config.py.\n\nNote: As the global batch size (batch_size x num_devices) is an important hyper-parameter, it is recommended to keep the global batch size unchanged for reproduction or adjust the learning rate linearly to a new global batch size.\n\nStandalone Training\n\nIf you want to train or finetune the model on a smaller dataset without distributed training, please run:\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/mixnet/mixnet_s_ascend.yaml --data_dir /path/to/dataset --distribute False\n\nValidation\n\nTo validate the accuracy of the trained model, you can use validate.py and parse the checkpoint path with --ckpt_path.\n\npython validate.py -c configs/mixnet/mixnet_s_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\nDeployment\n\nPlease refer to the deployment tutorial in MindCV.\n\nReferences\n\n[1] Tan M, Le Q V. Mixconv: Mixed depthwise convolutional kernels[J]. arXiv preprint arXiv:1907.09595, 2019.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/pangu-draw-v3_ms",
    "project_name": "pangu-draw-v3_ms",
    "readme": "PanGu Draw 3.0 Model Card\n  \n\nFig1: \"ä¸€å¹…ä¸­å›½æ°´å¢¨ç”»ï¼šä¸€å¶è½»èˆŸæ¼‚æ³Šåœ¨æ³¢å…‰ç²¼ç²¼çš„æ¹–é¢ä¸Šï¼ŒèˆŸä¸Šçš„äººæ­£åœ¨é¥®é…’æ”¾æ­Œ\"\nFig2: \"ååœ¨æµ·è¾¹çœ‹æµ·æµªçš„å°‘å¹´ï¼Œé»„æ˜\"\nFig3: \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n\n\n\nIntroduction\n\nThis folder contains PanGu Draw 3.0 models implemented with MindSpore.\n\nModel Description\nDeveloped by: MindSpore lab\nFramework: MindSpore\nModel type: text-to-image generative model\nLicense: apache-2.0\nModel Description: This is a model that can be used to generate mages based on text prompts.\nResources for more information: Check out the GitHub Repository.\nHow to Get Started with the Model\n\nFor information on how to train and infer with the model, please have a look at MindOne GitHub Repository.\n\nUses\nDirect Use\n\nThe model is intended for research purposes only. Possible research areas and tasks include\n\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\n\nExcluded uses are described below.\n\nOut-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to â€œA red cube on top of a blue sphereâ€\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nBias\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.",
    "tags": "[\"Text-to-Image\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/pit_ms",
    "project_name": "pit_ms",
    "readme": "Original Text\nPiT\n\nPiTï¼šé‡æ–°æ€è€ƒè§†è§‰å˜æ¢å™¨çš„ç©ºé—´ç»´åº¦\n\nç®€ä»‹\n\nPiTï¼ˆåŸºäºæ± åŒ–çš„è§†è§‰å˜æ¢å™¨ï¼‰æ˜¯ Byeongho Heo åœ¨ 2021 å¹´æå‡ºçš„å¯¹è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹çš„æ”¹è¿›ã€‚PiT åœ¨ ViT æ¨¡å‹çš„åŸºç¡€ä¸Šå¢åŠ äº†æ± åŒ–å±‚ï¼Œä½¿å¾—æ¯ä¸€å±‚çš„ç©ºé—´ç»´åº¦åƒ CNN ä¸€æ ·å‡å°‘ï¼Œè€Œä¸æ˜¯åƒ ViT é‚£æ ·å¯¹æ‰€æœ‰å±‚ä½¿ç”¨ç›¸åŒçš„ç©ºé—´ç»´åº¦ã€‚PiT å®ç°äº†æ¯” ViT æ›´å¥½çš„æ¨¡å‹èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚[1]\n\nå›¾ 1. PiT æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K ä¸Šçš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tè®­ç»ƒç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•°é‡ (M)\té…ç½®\tä¸‹è½½\npit_ti\tD910x8-G\t72.96\t91.33\t4.85\t[é…ç½®æ–‡ä»¶](https://openmind.cn/models/MindSpore-Lab/pit/blob/main/configs/pit/pit_ti_ascend.yaml)\t[æƒé‡æ–‡ä»¶](https://openmind.cn/models/MindSpore-Lab/pit/blob/main/pit_ti-e647a593.ckpt)\npit_xs\tD910x8-G\t78.41\t94.06\t10.61\t[é…ç½®æ–‡ä»¶](https://openmind.cn/models/MindSpore-Lab/pit/blob/main/configs/pit/pit_xs_ascend.yaml)\t[æƒé‡æ–‡ä»¶](https://openmind.cn/models/MindSpore-Lab/pit/blob/main/pit_xs-fea0d37e.ckpt)\npit_s\tD910x8-G\t80.56\t94.80\t23.46\t[é…ç½®æ–‡ä»¶](https://openmind.cn/models/MindSpore-Lab/pit/blob/main/configs/pit/pit_s_ascend.yaml)\t[æƒé‡æ–‡ä»¶](https://openmind.cn/models/MindSpore-Lab/pit/blob/main/pit_s-3c1ba36f.ckpt)\npit_b\tD910x8-G\t81.87\t95.04\t73.76\t[é…ç½®æ–‡ä»¶](https://openmind.cn/models/MindSpore-Lab/pit/blob/main/configs/pit/pit_b_ascend.yaml)\t[æƒé‡æ–‡ä»¶](https://openmind.cn/models/MindSpore-Lab/pit/blob/main/pit_b-2411c9b6.ckpt)\nå¤‡æ³¨\nè®­ç»ƒç¯å¢ƒï¼šè¡¨ç¤ºä¸º {è®¾å¤‡}x{å—æ•°}-{MindSpore æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - pynative æ¨¡å¼ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºåœ¨ 8 å— Ascend 910 NPU ä¸Šä½¿ç”¨å›¾æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV çš„å®‰è£…æŒ‡å—ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†ä»¥ä¾›æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/pit/pit_xs_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­åŠ å…¥ --allow-run-as-root å‚æ•°ã€‚\n\nåŒæ ·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\nå…³äºæ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨é‡ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨å¸Œæœ›åœ¨ä¸éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å°æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/pit/pit_xs_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/pit/pit_xs_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²\n\nè¯·å‚è€ƒ MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Heo B, Yun S, Han D, ç­‰äºº. é‡æ–°æ€è€ƒè§†è§‰å˜å‹å™¨çš„ç©ºé—´ç»´åº¦[C]//IEEE/CVFå›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®è®ºæ–‡é›†ã€‚2021: 11936-11945ã€‚",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/stable-diffusion-v2_ms",
    "project_name": "stable-diffusion-v2_ms",
    "readme": "Stable Diffusion v2 Model Card\n\nIntroduction\n\nThis repository integrates state-of-the-art Stable Diffusion models including SD2.0 base and its derivatives, supporting various generation tasks and pipelines based on MindSpore.\n\nModel Description\nDeveloped by: MindSpore lab based the work of Stability AI\nFramework: MindSpore\nModel type: Diffusion-based text-to-image generative model\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses two fixed, pretrained text encoders (OpenCLIP-ViT/G).\nResources for more information: Check out the GitHub Repository.\nModel Details\n\nWe currently provide the following checkpoints:\n\nsd_v2_base-57526ee4.ckpt: 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5. 850k steps at resolution 512x512 on the same dataset with resolution >= 512x512.\nsd_v2_768_v-e12e3a9b.ckpt: Resumed from sd_v2_base-57526ee4.ckpt and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on a 768x768 subset of our dataset.\nsd_v2_depth-186e18a0.ckpt: Resumed from sd_v2_base-57526ee4.ckpt and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning. The additional input channels of the U-Net which process this extra information were zero-initialized.\nsd_v2_inpaint-f694d5cf.ckpt: Resumed from sd_v2_base-57526ee4.ckpt and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning. The additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the 1.5-inpainting checkpoint.\nHow to Get Started with the Model\n\nFor information on how to train and infer with the model, please have a look at MindOne GitHub Repository.\n\nUses\nDirect Use\n\nThe model is intended for research purposes only. Possible research areas and tasks include\n\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\n\nExcluded uses are described below.\n\nOut-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to â€œA red cube on top of a blue sphereâ€\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nBias\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.",
    "tags": "[\"Text-to-Image\", \"CreativeML OpenRAIL-M\"]"
  },
  {
    "url": "https://gitcode.com/openMind/videocomposer_ms",
    "project_name": "videocomposer_ms",
    "readme": "Introduction\n\nMindSpore implementation & optimization of VideoComposer: Compositional Video Synthesis with Motion Controllability.\n\nVideoComposer Architecture\n\nGallery\nDifferent conditions to videos\n\n\nCondition: image depth\nText input: \"A black swan swam in the water\"\n\n\n\n\nCondition: local image\nText input: \"A black swan swam in the water\"\n\n\n\n\nCondition: mask\nText input: \"A black swan swam in the water\"\n\n\n\n\nCondition: motion\nText input: \"A black swan swam in the water\"\n\n\n\n\nCondition: sketch\nText input: \"A black swan swam in the water\"\n\nMotion transfer to videos\n\nText input: \"Beneath Van Gogh's Starry Sky\"\n\n\n\n\nText input: \"A beautiful big silver moon on the water\"\n\n\n\n\nText input: \"A sunflower in a field of flowers\"\n\nSingle sketch to videos with style\n Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  \n\nStyle image Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Text input: \"Red-backed Shrike lanius collurio\"\n\nSingle sketch to videos without style\n\nText input: \"A little bird is standing on a branch\"\n\nImage depth to videos without style\n\nText input: \"Ironman is fighting against the enemy, big fire in the background, photorealistic\"\n\nImage depth to videos with style\n Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  \n\nStyle image Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Text input: \"Van Gogh played tennis under the stars\"\n\n\n\nModel Description\nDeveloped by: MindSpore lab based the work of Stability AI\nFramework: MindSpore\nModel type: Compositional Video Synthesis with Motion Controllability\nLicense: CreativeML Open RAIL++-M License\nModel Description: The pursuit of controllability as a higher standard of visual content creation has yielded remarkable progress in customizable image synthesis. However, achieving controllable video synthesis remains challenging due to the large variation of temporal dynamics and the requirement of cross-frame temporal consistency. Based on the paradigm of compositional generation, this work presents VideoCompoer that allows users to flexibly compose a video with textual conditions, spatial conditions, and more importantly temporal conditions. Specifically, considering the characteristic of video data, we introduce the motion vector from compressed videos as an explicit control signal to provide guidance regarding temporal dynamics. In addition, we develop a Spatio-Temporal Condition encoder (STC-encoder) that serves as a unified interface to effectively incorporate the spatial and temporal relations of sequential inputs, with which the model could make better use of temporal conditions and hence achieve higher inter-frame consistency. Extensive experimental results suggest that VideoCompoer is able to control the spatial and temporal patterns simultaneously within a synthesized video in various forms, such as text description, sketch sequence, reference video, or even simply hand-crafted motions. ditions and hence achieve higher inter-frame consistency.\nResources for more information: Check out the GitHub Repository.\nEvaluation Results\nTraining\n\nThe training performance for exp02-motion transfer is as follows.\n\nNPU\t** Num. Cards**\tDataset\tBatch size\t** Performance (ms/step)**\n910*\t1x8\tWebVid\t1\t~950\n910*\t8x8\tWebVid\t1\t~1100\nInference\n\nThe video generation speed is as follows.\n\nNPU\t** Framework **\t** Sampler **\t** Steps **\t** Performance (s/trial)**\n910*\tMindSpore-2.2(20230907)\tDDIM\t50\t12\n910*\tMindSpore-Lite-2.2(20230907)\tDDIM\t50\t11.6\n\nNote that with MindSpore-Lite, the graph compilation time is eliminated.\n\nHow to Get Started with the Model\n\nFor information on how to train and infer with the model, please have a look at MindOne GitHub Repository.\n\nUses\nDirect Use\n\nThe model is intended for research purposes only. Possible research areas and tasks include\n\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\n\nExcluded uses are described below.\n\nOut-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to â€œA red cube on top of a blue sphereâ€\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nBias\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.",
    "tags": "[\"Text-to-Image\", \"OpenCLIP\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/yolov8_ms",
    "project_name": "yolov8_ms",
    "readme": "Original Text\nYOLOv8\nç®€ä»‹\n\nUltralytics YOLOv8 æ˜¯ç”± Ultralytics å¼€å‘çš„ä¸€æ¬¾å‰æ²¿ã€æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ¨¡å‹ï¼Œå®ƒåŸºäºå…ˆå‰ YOLO ç‰ˆæœ¬çš„æˆåŠŸï¼Œå¹¶å¼•å…¥äº†æ–°çš„åŠŸèƒ½å’Œæ”¹è¿›ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½å’Œçµæ´»æ€§ã€‚YOLOv8 æ—¨åœ¨å¿«é€Ÿã€å‡†ç¡®ä¸”æ˜“äºä½¿ç”¨ï¼Œæ˜¯å¤„ç†å„ç§ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²å’Œå›¾åƒåˆ†ç±»ä»»åŠ¡çš„ç†æƒ³é€‰æ‹©ã€‚\n\nè¯„ä¼°ç»“æœ\næ£€æµ‹\nåç§°\tè§„æ¨¡\tæ¶æ„\tä¸Šä¸‹æ–‡\tå›¾åƒå°ºå¯¸\tæ•°æ®é›†\tBox mAP (%)\tå‚æ•°\tFLOPs\té…ç½®æ–‡ä»¶\tä¸‹è½½é“¾æ¥\nYOLOv8\tN\tP5\tD910x8-G\t640\tMS COCO 2017\t37.2\t3.2M\t8.7G\tyaml\tweights\nYOLOv8\tS\tP5\tD910x8-G\t640\tMS COCO 2017\t44.6\t11.2M\t28.6G\tyaml\tweights\nYOLOv8\tM\tP5\tD910x8-G\t640\tMS COCO 2017\t50.5\t25.9M\t78.9G\tyaml\tweights\nYOLOv8\tL\tP5\tD910x8-G\t640\tMS COCO 2017\t52.8\t43.7M\t165.2G\tyaml\tweights\nYOLOv8\tX\tP5\tD910x8-G\t640\tMS COCO 2017\t53.7\t68.2M\t257.8G\tyaml\tweights\nåˆ†å‰²\nåç§°\tè§„æ¨¡\tæ¶æ„\tä¸Šä¸‹æ–‡\tå›¾åƒå°ºå¯¸\tæ•°æ®é›†\tBox mAP (%)\tMask mAP (%)\tå‚æ•°\tFLOPs\té…ç½®æ–‡ä»¶\tä¸‹è½½é“¾æ¥\nYOLOv8-seg\tX\tP5\tD910x8-G\t640\tMS COCO 2017\t52.5\t42.9\t71.8M\t344.1G\tyaml\tweights\nå¤‡æ³¨\nä¸Šä¸‹æ–‡ï¼šè®­ç»ƒä¸Šä¸‹æ–‡è¡¨ç¤ºä¸º {è®¾å¤‡}x{æ•°é‡}-{MindSporeæ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - ä½¿ç”¨ ms å‡½æ•°çš„ PyNative æ¨¡å¼ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºåœ¨ 8 å— Ascend 910 NPU ä¸Šä½¿ç”¨å›¾æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚\nBox mAPï¼šåœ¨éªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®ç‡ã€‚\næˆ‘ä»¬å‚è€ƒå®˜æ–¹ YOLOV8 æ¥å¤ç° P5 ç³»åˆ—æ¨¡å‹ã€‚\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\næœ‰å…³å¦‚ä½•è®­ç»ƒå’Œæ¨ç†æ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·å‚é˜… MindYOLO GitHub ä»“åº“ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Jocher Glenn. Ultralytics YOLOv8. https://github.com/ultralytics/ultralytics, 2023.",
    "tags": "[\"Object Detection\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/baichuan2_7b_base_ms",
    "project_name": "baichuan2_7b_base_ms",
    "readme": "Original Text\nç™¾å·å¤§æ¨¡å‹2\nğŸ¦‰GitHub | ğŸ’¬å¾®ä¿¡\nç™¾å·APIç°å·²æ”¯æŒæœç´¢å¢å¼ºä¸192Ké•¿ä¸Šä¸‹æ–‡çª—å£ï¼Œæ–°å¢çŸ¥è¯†åº“æ£€ç´¢åŠŸèƒ½å¹¶é™æ—¶å…è´¹å¼€æ”¾ï¼\nğŸš€ ç™¾å·å¤§æ¨¡å‹å¯¹è¯å¹³å° å·²æ­£å¼é¢å‘å…¬ä¼—å¼€æ”¾ ğŸ‰\nä¿®æ”¹è¯´æ˜/Modification\n\nä¼˜åŒ–äº†å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ä»£ç ã€‚/ Optimized the quick start code samples.\n\nç›®å½•/Table of Contents\nğŸ“– æ¨¡å‹æ¦‚è¿°/Introduction\nâš™ï¸ å¿«é€Ÿå…¥é—¨/Quick Start\nğŸ“Š æ€§èƒ½è¯„ä¼°/Benchmark Evaluation\nğŸ“œ ä½¿ç”¨æ¡æ¬¾/Terms and Conditions\næ¨¡å‹æ¦‚è¿°/Introduction\n\nç™¾å·å¤§æ¨¡å‹2æ˜¯ç™¾å·æ™ºèƒ½æ¨å‡ºçš„æ–°ä¸€ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº2.6ä¸‡äº¿é«˜è´¨é‡è®­ç»ƒè¯­æ–™ï¼Œåœ¨æƒå¨ä¸­è‹±æ–‡åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°åŒè§„æ¨¡æœ€ä½³è¡¨ç°ã€‚æœ¬æ¬¡å‘å¸ƒåŒ…å«7Bã€13Bçš„åŸºç¡€ç‰ˆä¸å¯¹è¯ç‰ˆï¼Œå¹¶æä¾›å¯¹è¯ç‰ˆ4ä½é‡åŒ–æ¨¡å‹ã€‚æ‰€æœ‰ç‰ˆæœ¬ä¸ä»…å‘å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¼€å‘è€…åªéœ€é€šè¿‡é‚®ä»¶ç”³è¯·è·å–å®˜æ–¹å•†ç”¨æˆæƒåï¼Œå³å¯å…è´¹ç”¨äºå•†ä¸šåœºæ™¯ã€‚å…·ä½“ç‰ˆæœ¬ä¸ä¸‹è½½é“¾æ¥å¦‚ä¸‹ï¼š\n\nBaichuan 2 is the new generation of open-source large language models developed by Baichuan Intelligence Inc.. Trained on 2.6 trillion high-quality tokens, it achieves state-of-the-art performance on authoritative Chinese and English benchmarks. This release includes 7B and 13B versions of both base and chat models, with 4-bit quantized chat variants. All models are fully accessible for academic research, and commercial use is permitted after obtaining authorization via email request. Available versions are listed below:\n\n\tåŸºç¡€æ¨¡å‹\tå¯¹è¯æ¨¡å‹\t4ä½é‡åŒ–å¯¹è¯æ¨¡å‹\n7B\tBaichuan2-7B-Base\tBaichuan2-7B-Chat\tBaichuan2-7B-Chat-4bits\n13B\tBaichuan2-13B-Base\tBaichuan2-13B-Chat\tBaichuan2-13B-Chat-4bits\næ€§èƒ½è¯„ä¼°/Benchmark Evaluation\n\næˆ‘ä»¬åœ¨[é€šç”¨é¢†åŸŸ]ã€[æ³•å¾‹ä¸åŒ»ç–—]ã€[æ•°å­¦ä¸ç¼–ç¨‹]ã€å¤šè¯­è¨€ç¿»è¯‘ç­‰å…­å¤§é¢†åŸŸçš„ä¸­è‹±æ–‡æƒå¨æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¯¦ç»†ç»“æœè¯·å‚é˜…GitHubã€‚\n\nThe models have been rigorously evaluated across six domains: General, Legal & Medical, Mathematics & Coding, and Multilingual Translation. For comprehensive results, visit GitHub.\n\n7Bæ¨¡å‹è¡¨ç°\n\tC-Eval\tMMLU\tCMMLU\té«˜è€ƒ\tAGIEval\tBBH\n\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t3æ¬¡é‡‡æ ·\nGPT-4\t68.40\t83.93\t70.33\t66.15\t63.27\t75.12\nGPT-3.5 Turbo\t51.10\t68.54\t54.06\t47.07\t46.13\t61.59\nLLaMA-7B\t27.10\t35.10\t26.75\t27.81\t28.17\t32.38\nLLaMA2-7B\t28.90\t45.73\t31.38\t25.97\t26.53\t39.16\nMPT-7B\t27.15\t27.93\t26.00\t26.54\t24.83\t35.20\nFalcon-7B\t24.23\t26.03\t25.66\t24.24\t24.10\t28.77\nChatGLM2-6B\t50.20\t45.90\t49.00\t49.44\t45.28\t31.65\nBaichuan-7B\t42.80\t42.30\t44.02\t36.34\t34.44\t32.48\nBaichuan2-7B-Base\t54.00\t54.16\t57.07\t47.47\t42.73\t41.56\n13Bæ¨¡å‹è¡¨ç°\n\tC-Eval\tMMLU\tCMMLU\té«˜è€ƒ\tAGIEval\tBBH\n\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t5æ¬¡é‡‡æ ·\t3æ¬¡é‡‡æ ·\nGPT-4\t68.40\t83.93\t70.33\t66.15\t63.27\t75.12\nGPT-3.5 Turbo\t51.10\t68.54\t54.06\t47.07\t46.13\t61.59\nLLaMA-13B\t28.50\t46.30\t31.15\t28.23\t28.22\t37.89\nLLaMA2-13B\t35.80\t55.09\t37.99\t30.83\t32.29\t46.98\nVicuna-13B\t32.80\t52.00\t36.28\t30.11\t31.55\t43.04\nChinese-Alpaca-Plus-13B\t38.80\t43.90\t33.43\t34.78\t35.46\t28.94\nXVERSE-13B\t53.70\t55.21\t58.44\t44.69\t42.54\t38.06\nBaichuan-13B-Base\t52.40\t51.60\t55.30\t49.69\t43.20\t43.01\nBaichuan2-13B-Base\t58.10\t59.17\t61.97\t54.33\t48.17\t48.78\nè®­ç»ƒè¿‡ç¨‹å­˜æ¡£/Training Dynamics\n\né™¤æœ€ç»ˆç‰ˆBaichuan2-7B-Baseæ¨¡å‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€æºäº†è®­ç»ƒè¿‡ç¨‹ä¸­11ä¸ªä¸­é—´æ£€æŸ¥ç‚¹ï¼ˆå¯¹åº”çº¦0.2è‡³2.4ä¸‡äº¿Tokensè®­ç»ƒé˜¶æ®µï¼‰ï¼Œä¾›ç ”ç©¶ç¤¾åŒºä½¿ç”¨ï¼ˆæ£€æŸ¥ç‚¹ä¸‹è½½ï¼‰ã€‚ä¸‹å›¾å±•ç¤ºäº†è¿™äº›æ£€æŸ¥ç‚¹åœ¨C-Evalã€MMLUã€CMMLUä¸‰å¤§åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ¼”è¿›ï¼š\n\nAlongside the final Baichuan2-7B-Base model, we release 11 intermediate checkpoints (covering ~0.2 to 2.4 trillion training tokens) for research purposes (Checkpoints). The graph illustrates their performance evolution on C-Eval, MMLU, and CMMLU benchmarks:\n\nä½¿ç”¨æ¡æ¬¾/Terms and Conditions\nå£°æ˜\n\næˆ‘ä»¬éƒ‘é‡å£°æ˜ï¼Œå›¢é˜ŸæœªåŸºäºç™¾å·å¤§æ¨¡å‹2å¼€å‘ä»»ä½•ç§»åŠ¨ç«¯ã€ç½‘é¡µç«¯æˆ–å…¶ä»–å¹³å°åº”ç”¨ã€‚æˆ‘ä»¬å¼ºçƒˆè¦æ±‚æ‰€æœ‰ä½¿ç”¨è€…ï¼šä¸å¾—åˆ©ç”¨æœ¬æ¨¡å‹ä»äº‹ä»»ä½•å±å®³å›½å®¶å®‰å…¨æˆ–è¿æ³•çš„æ´»åŠ¨ï¼›ä¸å¾—å°†æœ¬æ¨¡å‹ç”¨äºæœªç»å®‰å…¨å®¡æ ¸åŠå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬æœŸæœ›æ‰€æœ‰ä½¿ç”¨è€…èƒ½å…±åŒç»´æŠ¤æŠ€æœ¯å‘å±•çš„åˆè§„ç¯å¢ƒã€‚\n\nå°½ç®¡æˆ‘ä»¬å·²å°½æœ€å¤§åŠªåŠ›ç¡®ä¿è®­ç»ƒæ•°æ®çš„åˆè§„æ€§ï¼Œä½†ç”±äºæ¨¡å‹ä¸æ•°æ®çš„å¤æ‚æ€§ï¼Œä»å¯èƒ½å­˜åœ¨ä¸å¯é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¯¹äºå› ä½¿ç”¨ç™¾å·å¤§æ¨¡å‹2å¼€æºæ¨¡å‹å¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨ã€èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯ç”¨ã€æ»¥ç”¨åŠéæ³•ä¼ æ’­å¼•å‘çš„é£é™©ï¼‰ï¼Œæˆ‘ä»¬å‡ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n\nWe hereby clarify that no official applications (iOS/Android/web/etc.) have been developed based on Baichuan 2 models. We strictly prohibit: 1) Any activities compromising national/social security or violating laws; 2) Deploying the models in unregistered internet services without proper security reviews.\n\nWhile we've made exhaustive efforts to ensure training data compliance, unforeseen issues may persist due to model/data complexity. Therefore, we disclaim all liability for any consequences arising from using Baichuan 2 open-source models, including but not limited to data breaches, public opinion risks, or damages caused by model misuse/abuse/illegal distribution.\n\nåè®®\n\nä½¿ç”¨ Baichuan 2 æ¨¡å‹éœ€éµå®ˆ Apache 2.0 åè®®åŠã€ŠBaichuan 2 æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹ã€‚è¯¥æ¨¡å‹æ”¯æŒå•†ä¸šåº”ç”¨ï¼Œè‹¥æ‚¨è®¡åˆ’å°† Baichuan 2 æ¨¡å‹æˆ–å…¶è¡ç”Ÿä½œå“ç”¨äºå•†ä¸šç”¨é€”ï¼Œè¯·ç¡®ä¿æ‚¨çš„å®ä½“ç¬¦åˆä»¥ä¸‹æ¡ä»¶ï¼š\n\næ‚¨æˆ–å…³è”æ–¹çš„æœåŠ¡/äº§å“æ—¥å‡æ´»è·ƒç”¨æˆ·ï¼ˆDAUï¼‰æœªè¶…è¿‡100ä¸‡\næ‚¨åŠå…³è”æ–¹éè½¯ä»¶æœåŠ¡æä¾›å•†æˆ–äº‘æœåŠ¡æä¾›å•†\næ‚¨åŠå…³è”æ–¹ä¸ä¼šåœ¨æœªç»ç™¾å·æˆæƒçš„æƒ…å†µä¸‹ï¼Œå°†å•†ç”¨è®¸å¯äºŒæ¬¡æˆäºˆç¬¬ä¸‰æ–¹\n\næ»¡è¶³ä¸Šè¿°æ¡ä»¶åï¼Œè¯·å‘é€ã€ŠBaichuan 2 æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹è¦æ±‚çš„ç”³è¯·ææ–™è‡³é‚®ç®± opensource@baichuan-inc.comã€‚å®¡æ ¸é€šè¿‡åï¼Œç™¾å·å°†æˆäºˆæ‚¨éæ’ä»–æ€§ã€å…¨çƒèŒƒå›´ã€ä¸å¯è½¬è®©ã€ä¸å¯åˆ†è®¸å¯ä¸”å¯æ’¤é”€çš„å•†ç”¨æˆæƒã€‚\n\nå¿«é€Ÿå¼€å§‹\nå¾®è°ƒæŒ‡å—\næ•°æ®å‡†å¤‡\n\næˆ‘ä»¬æä¾›belle_chat_ramdonæ•°æ®é›†çš„é¢„å¤„ç†ä¸å¾®è°ƒç¤ºä¾‹ï¼Œæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š\n\nbelle_chat_ramdon_10k\n\nè¿è¡Œbelle_preprocess.pyè„šæœ¬ï¼Œå¯å°†å¸¦promptæ¨¡æ¿çš„åŸå§‹æ•°æ®è½¬æ¢ä¸ºmindrecordæ ¼å¼ï¼Œå®Œæˆæ•°æ®é¢„å¤„ç†å·¥ä½œã€‚\n\n# è„šæœ¬è·¯å¾„ï¼šexample/dataset/belle_preprocess.py\npython example/dataset/belle_preprocess.py \\\n--input_glob /{path}/belle_chat_ramdon_10k.json \\\n--output_file /{path}/belle_512.mindrecord \\\n--seq_length 512\n\n# å‚æ•°è¯´æ˜\ninput_glob: è¾“å…¥æ•°æ®é›†è·¯å¾„\nmodel_file: è¯è¡¨æ–‡ä»¶è·¯å¾„\noutput_file: è¾“å‡ºæ•°æ®é›†çš„è·¯å¾„å’Œåç§°\nseq_length: ç”Ÿæˆæ•°æ®é›†çš„åºåˆ—é•¿åº¦\n\nTraining\ncd example\nbash msrun.sh \"finetune.py --train_dataset /{path}/belle_512.mindrecord\"\n\nReasoning\nfrom mindspore import set_context\nfrom openmind import pipeline\n\nset_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\", model='MindSpore-Lab/baichuan2_7b_base', framework='ms', trust_remote_code=True)\npipeline_result = pipeline_task(\"<reserved_106>ä½ æ˜¯è°ï¼Ÿ<reserved_107>\", do_sample=False)\nprint(pipeline_result)\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"Other\"]"
  },
  {
    "url": "https://gitcode.com/openMind/dpn_ms",
    "project_name": "dpn_ms",
    "readme": "Original Text\nåŒè·¯å¾„ç½‘ç»œï¼ˆDPNï¼‰\n\nåŒè·¯å¾„ç½‘ç»œ\n\nç®€ä»‹\n\nå›¾ 1 å±•ç¤ºäº† ResNetã€DenseNet å’ŒåŒè·¯å¾„ç½‘ç»œï¼ˆDPNï¼‰çš„æ¨¡å‹æ¶æ„ã€‚DPN ç»“åˆäº† ResNet çš„ç‰¹å¾é‡ç”¨å’Œ DenseNet çš„æ–°ç‰¹å¾å¼•å…¥ï¼Œä»è€Œèƒ½å¤Ÿå…±äº«å¸¸è§ç‰¹å¾å¹¶ä¿æŒæ¢ç´¢æ–°ç‰¹å¾çš„çµæ´»æ€§ã€‚å› æ­¤ï¼Œä¸ ResNet å’Œ DenseNet ç›¸æ¯”ï¼ŒDPN åœ¨ ImageNet-1K æ•°æ®é›†ä¸Šå¯ä»¥å®ç°æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬æ›´ä½ã€‚[1]\n\nå›¾ 1. DPN æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K ä¸Šçš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æŠ¥å‘Šã€‚\n\næ¨¡å‹\tç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•° (M)\té…ç½®\tä¸‹è½½\ndpn92\tD910x8-G\t79.46\t94.49\t37.79\té…ç½®æ–‡ä»¶\tæƒé‡\ndpn98\tD910x8-G\t79.94\t94.57\t61.74\té…ç½®æ–‡ä»¶\tæƒé‡\ndpn107\tD910x8-G\t80.05\t94.74\t87.13\té…ç½®æ–‡ä»¶\tæƒé‡\ndpn131\tD910x8-G\t80.07\t94.72\t79.48\té…ç½®æ–‡ä»¶\tæƒé‡\næ³¨æ„äº‹é¡¹\nç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º {è®¾å¤‡}x{éƒ¨ä»¶}-{MS æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - pynative æ¨¡å¼ï¼Œä½¿ç”¨ ms å‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºåœ¨ 8 ç‰‡ Ascend 910 NPU ä¸Šä½¿ç”¨å›¾æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸Šçš„å‡†ç¡®åº¦æŠ¥å‘Šã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV çš„å®‰è£…è¯´æ˜ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…ç½®å¯ä»¥è½»æ¾é‡ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distrubted training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/dpn/dpn92_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nåŒæ ·åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\nå…³äºæ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨å¸Œæœ›åœ¨æ— éœ€åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/dpn/dpn92_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/dpn/dpn92_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²\n\nè¯·å‚è€ƒ MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] é™ˆæ¯…ï¼Œæé™ï¼Œè‚–åï¼Œç­‰. åŒè·¯å¾„ç½‘ç»œ[J]. ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ2017ï¼Œ30å·ã€‚",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/fastspeech2_ms",
    "project_name": "fastspeech2_ms",
    "readme": "Original Text\nFastSpeech2\n\nè¿™æ˜¯åŸºäºMindSporeå®ç°çš„å¾®è½¯æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»ŸFastSpeech 2ï¼šå¿«é€Ÿä¸”é«˜è´¨é‡çš„ç«¯åˆ°ç«¯æ–‡æœ¬è½¬è¯­éŸ³ã€‚\n\nFastSpeech 2æœ‰å‡ ä¸ªç‰ˆæœ¬ã€‚ æ­¤å®ç°ä¸ç‰ˆæœ¬1æ›´ä¸ºç›¸ä¼¼ï¼Œå®ƒä½¿ç”¨F0å€¼ä½œä¸ºéŸ³è°ƒç‰¹å¾ã€‚ å¦ä¸€æ–¹é¢ï¼Œåç»­ç‰ˆæœ¬ä½¿ç”¨è¿ç»­å°æ³¢å˜æ¢æå–çš„é¢‘è°±å›¾ä½œä¸ºéŸ³è°ƒç‰¹å¾ã€‚\n\né¢„è®­ç»ƒæ¨¡å‹\næ¨¡å‹\tæ•°æ®é›†\tæ£€æŸ¥ç‚¹\tæ€»æ‰¹é‡å¤§å°\tæ¢…å°”é¢‘è°±æ•°\tç¡¬ä»¶\tMindSporeç‰ˆæœ¬\nFastSpeech2ï¼ˆåŸºç¡€ç‰ˆï¼‰\tLJSpeech-1.1\t16ä¸‡ä¸ªæ­¥éª¤\t32\t128\t1 x Ascend\t1.9.0\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\næœ‰å…³å¦‚ä½•è®­ç»ƒå’Œæ¨æ–­æ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥é˜…MindAudio GitHub ä»“åº“ã€‚\n\nè®¸å¯è¯\n\nGNUé€šç”¨å…¬å…±è®¸å¯è¯v2.0",
    "tags": "[\"Text-to-Audio\", \"GNU General Public License v2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/mnasnet_ms",
    "project_name": "mnasnet_ms",
    "readme": "MnasNet\n\nMnasNet: Platform-Aware Neural Architecture Search for Mobile\n\nIntroduction\n\nDesigning convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, the authors propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, the authors propose a novel factorized hierarchical search space that encourages layer diversity throughout the network.[1]\n\nFigure 1. Architecture of MnasNet [1]\n\nResults\n\nOur reproduced model performance on ImageNet-1K is reported as follows.\n\nModel\tContext\tTop-1 (%)\tTop-5 (%)\tParams (M)\tRecipe\tDownload\nmnasnet_050\tD910x8-G\t68.07\t88.09\t2.14\tyaml\tweights\nmnasnet_075\tD910x8-G\t71.81\t90.53\t3.20\tyaml\tweights\nmnasnet_100\tD910x8-G\t74.28\t91.70\t4.42\tyaml\tweights\nmnasnet_130\tD910x8-G\t75.65\t92.64\t6.33\tyaml\tweights\nmnasnet_140\tD910x8-G\t76.01\t92.83\t7.16\tyaml\tweights\nNotes\nContext: Training context denoted as {device}x{pieces}-{MS mode}, where mindspore mode can be G - graph mode or F - pynative mode with ms function. For example, D910x8-G is for training on 8 pieces of Ascend 910 NPU using graph mode.\nTop-1 and Top-5: Accuracy reported on the validation set of ImageNet-1K.\nQuick Start\nPreparation\nInstallation\n\nPlease refer to the installation instruction in MindCV.\n\nDataset Preparation\n\nPlease download the ImageNet-1K dataset for model training and validation.\n\nTraining\nDistributed Training\n\nIt is easy to reproduce the reported results with the pre-defined training recipe. For distributed training on multiple Ascend 910 devices, please run\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/mnasnet/mnasnet_0.75_ascend.yaml --data_dir /path/to/imagenet\n\n\nIf the script is executed by the root user, the --allow-run-as-root parameter must be added to mpirun.\n\nSimilarly, you can train the model on multiple GPU devices with the above mpirun command.\n\nFor detailed illustration of all hyper-parameters, please refer to config.py.\n\nNote: As the global batch size (batch_size x num_devices) is an important hyper-parameter, it is recommended to keep the global batch size unchanged for reproduction or adjust the learning rate linearly to a new global batch size.\n\nStandalone Training\n\nIf you want to train or finetune the model on a smaller dataset without distributed training, please run:\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/mnasnet/mnasnet_0.75_ascend.yaml --data_dir /path/to/dataset --distribute False\n\nValidation\n\nTo validate the accuracy of the trained model, you can use validate.py and parse the checkpoint path with --ckpt_path.\n\npython validate.py -c configs/mnasnet/mnasnet_0.75_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\nDeployment\n\nPlease refer to the deployment tutorial in MindCV.\n\nReferences\n\n[1] Tan M, Chen B, Pang R, et al. Mnasnet: Platform-aware neural architecture search for mobile[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 2820-2828.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/mobilenetv3_ms",
    "project_name": "mobilenetv3_ms",
    "readme": "Original Text\nMobileNetV3\n\nå¯»æ‰¾MobileNetV3\n\nç®€ä»‹\n\nMobileNet v3äº2019å¹´å‘å¸ƒï¼Œè¯¥v3ç‰ˆæœ¬ç»“åˆäº†v1ç‰ˆæœ¬çš„æ·±åº¦å¯åˆ†ç¦»å·ç§¯ã€v2ç‰ˆæœ¬çš„å€’ç½®æ®‹å·®å’Œçº¿æ€§ç“¶é¢ˆä»¥åŠSEæ¨¡å—ï¼Œä½¿ç”¨NASï¼ˆç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼‰æ¥æœç´¢ç½‘ç»œçš„é…ç½®å’Œå‚æ•°ã€‚MobileNetV3é¦–å…ˆä½¿ç”¨MnasNetè¿›è¡Œç²—ç•¥çš„ç»“æ„æœç´¢ï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä»ä¸€ç»„ç¦»æ•£é€‰æ‹©ä¸­é€‰å‡ºæœ€ä¼˜é…ç½®ã€‚ä¹‹åï¼ŒMobileNetV3ä½¿ç”¨NetAdaptå¯¹æ¶æ„è¿›è¡Œå¾®è°ƒï¼Œè¿™ä½“ç°äº†NetAdaptè°ƒæ•´æœªè¢«å……åˆ†åˆ©ç”¨çš„æ¿€æ´»é€šé“çš„å°å¹…ä¸‹é™çš„è¡¥å……èƒ½åŠ›ã€‚\n\nmobilenet-v3æä¾›äº†ä¸¤ä¸ªç‰ˆæœ¬ï¼Œmobilenet-v3 largeå’Œmobilenet-v3 smallï¼Œä»¥æ»¡è¶³ä¸åŒèµ„æºéœ€æ±‚çš„æƒ…å†µã€‚è®ºæ–‡æåˆ°ï¼Œå¯¹äºImageNetåˆ†ç±»ä»»åŠ¡ï¼Œmobilenet-v3 smallç›¸è¾ƒäºmobilenet-v2å®ç°äº†çº¦3.2%çš„å‡†ç¡®åº¦æå‡å’Œ15%çš„æ—¶é—´å‡å°‘ï¼›mobilenet-v3 largeç›¸è¾ƒäºmobilenet-v2å®ç°äº†çº¦4.6%çš„å‡†ç¡®åº¦æå‡å’Œ5%çš„æ—¶é—´å‡å°‘ï¼ŒåŒæ—¶åœ¨COCOçš„åˆ†å‰²ç®—æ³•ä¸Šï¼Œmobilenet-v3 largeå®ç°äº†ä¸v2ç›¸åŒçš„å‡†ç¡®åº¦å¹¶æœ‰25%çš„é€Ÿåº¦æå‡ã€‚[1]\n\nå›¾1. MobileNetV3æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ImageNet-1Kä¸Šçš„æ¨¡å‹æ€§èƒ½å¤ç°å¦‚ä¸‹ã€‚\n\næ¨¡å‹\tè®­ç»ƒç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•°é‡ (M)\té…ç½®æ–‡ä»¶\tä¸‹è½½åœ°å€\nmobilenet_v3_small_100\tD910x8-G\t68.10\t87.86\t2.55\tyaml\tæƒé‡æ–‡ä»¶\nmobilenet_v3_large_100\tD910x8-G\t75.23\t92.31\t5.51\tyaml\tæƒé‡æ–‡ä»¶\nè¯´æ˜\nè®­ç»ƒç¯å¢ƒï¼šè®­ç»ƒä¸Šä¸‹æ–‡è¡¨ç¤ºä¸º{è®¾å¤‡}x{å—æ•°}-{MindSporeæ¨¡å¼}ï¼Œå…¶ä¸­MindSporeæ¨¡å¼å¯ä»¥æ˜¯G - å›¾æ¨¡å¼æˆ–F - pynativeæ¨¡å¼ä¸mså‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-Gè¡¨ç¤ºä½¿ç”¨å›¾æ¨¡å¼åœ¨8ç‰‡Ascend 910 NPUä¸Šè¿›è¡Œè®­ç»ƒã€‚\nTop-1å’ŒTop-5ï¼šåœ¨ImageNet-1KéªŒè¯é›†ä¸Šçš„å‡†ç¡®åº¦æŠ¥å‘Šã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒMindCVçš„å®‰è£…æŒ‡å—ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ImageNet-1Kæ•°æ®é›†ç”¨äºæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¾ˆå®¹æ˜“å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ªAscend 910è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/mobilenetv3/mobilenet_v3_small_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»å‘ mpirun å‘½ä»¤æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚é˜… config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶çº¿æ€§åœ°è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨æƒ³åœ¨ä¸éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒçš„å°æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/mobilenetv3/mobilenet_v3_small_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/mobilenetv3/mobilenet_v3_small_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²\n\nè¯·å‚è€ƒ MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Howard A, Sandler M, Chu G, ç­‰. å¯»æ‰¾ MobileNetV3[C]//IEEE/CVF å›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®è®ºæ–‡é›†. 2019: 1314-1324.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/svd_ms",
    "project_name": "svd_ms",
    "readme": "Original Text\nç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆVideoLDMï¼‰æ¨¡å‹å¡\n\nå¼•è¨€\n\nç¨³å®šè§†é¢‘æ‰©æ•£æ˜¯ä¸€ç§åŸºäºç¨³å®šæ‰©æ•£çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡åœ¨æ¶æ„ä¸­å¼•å…¥æ—¶é—´å±‚æ¥æ‰©å±•è§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼ˆå³VideoLDMï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ä½¿ç”¨äº†ä¸€ç§å¸¦æœ‰é™„åŠ æ—¶é—´å±‚çš„ä¿®æ”¹è§£ç å™¨æ¥å¯¹æŠ—é—ªçƒä¼ªå½±ã€‚\n\n\nä¸€ä¸ªå¸¦æœ‰é™„åŠ æ—¶é—´å±‚çš„å•ç‹¬ U-Net å—ç¤ºä¾‹ï¼ˆæ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è§ [2]ï¼‰\n\næ¨¡å‹æè¿°\nå¼€å‘æœºæ„ï¼š åŸºäºStability AIçš„å·¥ä½œï¼ŒMindSporeå®éªŒå®¤å¼€å‘\næ¡†æ¶ï¼š MindSpore\næ¨¡å‹ç±»å‹ï¼š åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹\nè®¸å¯è¯ï¼š CreativeML Open RAIL++-M è®¸å¯è¯\næ¨¡å‹æè¿°ï¼š è¿™æ˜¯ä¸€ä¸ªå¯ä»¥ç”¨äºåŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆå’Œä¿®æ”¹è§†é¢‘çš„æ¨¡å‹ã€‚\næ›´å¤šä¿¡æ¯èµ„æºï¼š æŸ¥çœ‹ä»¥ä¸‹GitHub ä»“åº“ã€‚\næ¨¡å‹ç»†èŠ‚\nSD åŸºç¡€ç‰ˆæœ¬\tSVD ç‰ˆæœ¬\tè®­ç»ƒç”¨äº\té…ç½®æ–‡ä»¶\tæ£€æŸ¥ç‚¹\nv2.0 & v2.1\tSVD\t14 å¸§ç”Ÿæˆ\tyaml\tä¸‹è½½ (9GB)\n\tSVD-XT\t25 å¸§ç”Ÿæˆ\tyaml\tä¸‹è½½ (9GB)\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\nå…³äºå¦‚ä½•è®­ç»ƒå’Œæ¨æ–­æ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥é˜… MindOne GitHub ä»“åº“ã€‚\n\nä½¿ç”¨åœºæ™¯\nç›´æ¥ä½¿ç”¨\n\nè¯¥æ¨¡å‹ä»…ä¾›ç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚å¯èƒ½çš„ç ”ç©¶é¢†åŸŸå’Œä»»åŠ¡åŒ…æ‹¬ï¼š\n\nè‰ºæœ¯ä½œå“çš„ç”Ÿæˆä»¥åŠåœ¨è®¾è®¡å’Œå…¶ä»–è‰ºæœ¯è¿‡ç¨‹ä¸­çš„åº”ç”¨ã€‚\næ•™è‚²æˆ–åˆ›æ„å·¥å…·çš„åº”ç”¨ã€‚\nç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶ã€‚\nå®‰å…¨éƒ¨ç½²å¯èƒ½ç”Ÿæˆæœ‰å®³å†…å®¹çš„æ¨¡å‹ã€‚\næ¢ç©¶å’Œç†è§£ç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§å’Œåè§ã€‚\n\nä»¥ä¸‹æè¿°äº†ç¦æ­¢ä½¿ç”¨çš„åœºæ™¯ã€‚\n\nè¶…å‡ºèŒƒå›´çš„ä½¿ç”¨\n\nè¯¥æ¨¡å‹å¹¶æœªè®­ç»ƒä¸ºäº‹å®æˆ–çœŸå®çš„äººç‰©æˆ–äº‹ä»¶è¡¨ç¤ºï¼Œå› æ­¤ä½¿ç”¨è¯¥æ¨¡å‹ç”Ÿæˆæ­¤ç±»å†…å®¹è¶…å‡ºäº†è¯¥æ¨¡å‹çš„èƒ½åŠ›èŒƒå›´ã€‚\n\nå±€é™æ€§å’Œåè§\nå±€é™æ€§\næ¨¡å‹æ— æ³•å®ç°å®Œç¾çš„ç…§ç‰‡çº§ç°å®ä¸»ä¹‰\næ¨¡å‹æ— æ³•æ¸²æŸ“å¯è¯†åˆ«æ–‡æœ¬\næ¨¡å‹åœ¨å¤„ç†æ›´å›°éš¾çš„ç»„åˆä»»åŠ¡æ—¶å­˜åœ¨å›°éš¾ï¼Œä¾‹å¦‚æ¸²æŸ“â€œä¸€ä¸ªçº¢è‰²ç«‹æ–¹ä½“åœ¨è“è‰²çƒä½“ä¸Šâ€çš„å›¾åƒ\nè„¸éƒ¨å’Œäººç‰©é€šå¸¸å¯èƒ½æ— æ³•æ­£ç¡®ç”Ÿæˆã€‚\næ¨¡å‹çš„è‡ªåŠ¨ç¼–ç éƒ¨åˆ†æ˜¯æŸå¤±æ€§çš„ã€‚\nåè§\n\nè™½ç„¶å›¾åƒç”Ÿæˆæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½åŠ å¼ºæˆ–åŠ å‰§ç¤¾ä¼šåè§ã€‚",
    "tags": "[\"Text-to-Video\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/telechat_7b_ms",
    "project_name": "telechat_7b_ms",
    "readme": "Original Text\næ˜Ÿè¾°è¯­ä¹‰å¤§æ¨¡å‹-TeleChat\n\nğŸ¤— Hugging Face â€¢ ğŸ” MindSpore â€¢ ğŸ¾ ç äº‘ï¸ â€¢ ğŸ’¬ å¾®ä¿¡\n\næŠ€æœ¯æŠ¥å‘Š\n\nä¿®æ”¹è¯´æ˜\nä¿®æ”¹äº†æ¨¡å‹æ¨ç†ç¤ºä¾‹ä»£ç \nå¢åŠ äº†å¿«é€Ÿå¼€å§‹ç« èŠ‚\næœ€æ–°åŠ¨æ€\n2024.3.20 å¼€æº12Bç‰ˆæœ¬chatæ¨¡å‹åŠé‡åŒ–ç‰ˆæœ¬\n2024.1.11 å¼€æº1Tä¸­æ–‡æ•°æ®é›†\n2024.1.10 å¼€æº7Bç‰ˆæœ¬chatæ¨¡å‹åŠå…¶é‡åŒ–ç‰ˆæœ¬\næ¨¡å‹ä»‹ç»\næ˜Ÿè¾°è¯­ä¹‰å¤§æ¨¡å‹-TeleChat\næ˜Ÿè¾°è¯­ä¹‰å¤§æ¨¡å‹TeleChatæ˜¯ç”±ä¸­ç”µä¿¡äººå·¥æ™ºèƒ½ç§‘æŠ€æœ‰é™å…¬å¸ç ”å‘è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå…¶ä¸­7Bæ¨¡å‹åŸºåº§é‡‡ç”¨1.5ä¸‡äº¿ Tokensä¸­è‹±æ–‡é«˜è´¨é‡è¯­æ–™è¿›è¡Œè®­ç»ƒï¼Œ12Bæ¨¡å‹åŸºåº§é‡‡ç”¨3ä¸‡äº¿ Tokensä¸­è‹±æ–‡é«˜è´¨é‡è¯­æ–™è¿›è¡Œè®­ç»ƒã€‚\næˆ‘ä»¬å¼€æºäº†å¯¹è¯æ¨¡å‹TeleChat-7B-botä¸TeleChat-12B-botï¼Œä»¥åŠå…¶huggingfaceæ ¼å¼çš„æƒé‡æ–‡ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€æºäº†7Bã€12Bæ¨¡å‹çš„int8å’Œint4é‡åŒ–ç‰ˆæœ¬ã€‚\nTeleChat-12B-botåœ¨æ¨¡å‹ç»“æ„ã€è®­ç»ƒæ•°æ®ã€è®­ç»ƒæ–¹æ³•ç­‰æ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼Œåœ¨é€šç”¨é—®ç­”å’ŒçŸ¥è¯†ç±»ã€ä»£ç ç±»ã€æ•°å­¦ç±»æ¦œå•ä¸Šç›¸æ¯”TeleChat-7B-botå‡æœ‰å¤§å¹…æå‡ã€‚åœ¨æ¨¡å‹ç»“æ„æ–¹é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨å°è§„æ¨¡çš„æ¨¡å‹å°è¯•å¤šç§æ¨¡å‹ç»“æ„çš„ç»„åˆï¼Œé€‰æ‹©æœ€ä¼˜ç»“æ„ã€‚ç›¸æ¯”TeleChat-7B-botæ¨¡å‹ï¼ŒTeleChat-12B-botæ¨¡å‹é‡‡ç”¨äº†è¯åµŒå…¥å±‚ä¸è¾“å‡ºå±‚è§£è€¦çš„ç»“æ„ï¼Œå°†è¯åµŒå…¥å±‚å’Œè¾“å‡ºlm headå±‚å‚æ•°åˆ†å¼€ï¼Œæœ‰åŠ©äºå¢å¼ºè®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚åœ¨è®­ç»ƒæ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬æ”¶é›†äº†è¦†ç›–ä¹¦ç±ã€ç™¾ç§‘ã€æ–°é—»ã€æ”¿åŠ¡ã€æ³•å¾‹ã€åŒ»è¯ã€ä¸“åˆ©ã€è®ºæ–‡ã€æ•°å­¦ã€ä»£ç ç­‰è¯¸å¤šæ–¹é¢çš„å¤§é‡ä¸­è‹±æ–‡æ•°æ®ï¼›é€šè¿‡ä¼˜åŒ–æ•°æ®æ¸…æ´—ç­–ç•¥å¤§å¹…æå‡æ•°æ®çš„æ–‡æœ¬å¹²å‡€åº¦ã€è§‚ç‚¹æ— åæ€§ã€å†…å®¹æœ‰æ•ˆæ€§ã€æ ¼å¼è§„èŒƒæ€§ã€‚åœ¨è®­ç»ƒæ–¹æ³•æ–¹é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨ç§‘å­¦æ•°æ®é…æ¯”å­¦ä¹ ä¸è¯¾ç¨‹å­¦ä¹ çš„æ–¹æ³•ï¼Œä½¿ç”¨å°å‚æ•°æ¨¡å‹åœ¨å¤šç§æ•°æ®é…æ¯”çš„æ•°æ®ä¸Šæ‹Ÿåˆï¼Œå¾—åˆ°å¯¹å„ä¸ªæ•°æ®é›†éš¾åº¦çš„å…ˆéªŒä¼°è®¡ï¼›è®­ç»ƒè¿‡ç¨‹ä¸­æ¯éš”ä¸€æ®µæ—¶é—´è‡ªåŠ¨åŒ–è¯„ä¼°å½“å‰æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„lossï¼Œä»¥åŠåœ¨è¯„æµ‹é›†ä¸Šçš„ç”Ÿæˆæ•ˆæœï¼ŒåŠ¨æ€æå‡è¾ƒéš¾å­¦ä¹ çš„æ•°æ®é›†æƒé‡ï¼Œä¿è¯æ¨¡å‹åœ¨å„ä¸ªæ•°æ®é›†ä¸Šéƒ½æœ‰è¾ƒä½³çš„æ‹Ÿåˆæ•ˆæœã€‚\næ¨¡å‹ç»“æ„\n\næˆ‘ä»¬é‡‡ç”¨æ ‡å‡†çš„ Decoder-only ç»“æ„è®¾è®¡äº† TeleChat æ¨¡å‹ï¼Œå¹¶åœ¨æ¨¡å‹ç»´åº¦åšäº†å¦‚ä¸‹çš„ä¸€äº›æ”¹è¿›ï¼š\n\nä½ç½®ç¼–ç ï¼šæˆ‘ä»¬ä½¿ç”¨ Rotary Embedding çš„ä½ç½®ç¼–ç æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ç›¸å¯¹ä½ç½®ä¿¡æ¯ä¾èµ–é›†æˆåˆ° self-attention ä¸­ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒå¥½çš„ä½ç½®å¤–æ¨æ€§ã€‚Rotary Embeddingè¿˜å¯ä»¥è¾ƒå¥½åœ°ä¸Flash-Attention v2 é…åˆä½¿ç”¨ï¼Œå°†æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦æå‡çº¦20%ã€‚\næ¿€æ´»å‡½æ•°ï¼šæˆ‘ä»¬ä½¿ç”¨ SwiGLU æ¿€æ´»å‡½æ•°æ¥æ›¿ä»£GELUæ¿€æ´»å‡½æ•°ï¼Œä¸ºäº†å‡å°‘è®¡ç®—é‡ï¼Œå°†ffn_hidden_sizeè®¾ç½®ä¸ºå°äºåŸå§‹SwiGLUä¸­çš„4å€éšè—å±‚å¤§å°ã€‚\nå±‚æ ‡å‡†åŒ–ï¼šåŸºäº RMSNorm çš„ Pre-Normalizationã€‚\nè¯åµŒå…¥å±‚ä¸è¾“å‡ºå±‚è§£è€¦ï¼šæˆ‘ä»¬å°†TeleChat-12B-botçš„è¯åµŒå…¥å±‚å’Œè¾“å‡ºlm headå±‚å‚æ•°åˆ†å¼€ï¼Œæœ‰åŠ©äºå¢å¼ºè®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚\n\tlayer_num\thidden_size\tffn_hidden_size\thead_num\ttie_word_embeddings\n7B\t30\t4096\t12288\t32\tæ˜¯\n12B\t38\t5120\t12288\t32\tå¦\n\næˆ‘ä»¬å¼€æºçš„TeleChatæ¨¡å‹ï¼š\n\næ”¯æŒdeepspeedå¾®è°ƒï¼Œå¼€æºäº†åŸºäºdeepspeedçš„è®­ç»ƒä»£ç ï¼Œæ”¯æŒZeroå¹¶è¡Œæ˜¾å­˜ä¼˜åŒ–ï¼ŒåŒæ—¶é›†æˆäº†FlashAttention2ã€‚\nå¤šè½®èƒ½åŠ›æ”¯æŒã€‚å¼€æºäº†å¤šè½®æ•°æ®æ„å»ºæ–¹å¼ï¼Œé’ˆå¯¹å¤šè½®æ¨¡å‹è®­ç»ƒé›†æˆäº†é’ˆå¯¹å¤šè½®çš„mask lossè®­ç»ƒæ–¹å¼ï¼Œæ›´å¥½åœ°èšç„¦å¤šè½®ç­”æ¡ˆï¼Œæå‡é—®ç­”æ•ˆæœã€‚\nå¤–æ¨èƒ½åŠ›æå‡ã€‚å¼€æºäº†8Kè®­ç»ƒç‰ˆæœ¬æ¨¡å‹ï¼Œé‡‡ç”¨NTK-awareå¤–æ¨å’Œattention scalingå¤–æ¨æ–¹å¼ï¼Œå¯ä»¥å¤–æ¨åˆ°96Kã€‚\nå…·å¤‡è¾ƒå¥½çš„é•¿æ–‡ç”Ÿæˆèƒ½åŠ›ã€‚åœ¨å·¥ä½œæ€»ç»“ã€å·¥ä½œè®¡åˆ’ã€PPTå¤§çº²ã€ç”³è®ºã€æ‹›æ ‡ä¹¦ã€é‚®ä»¶ã€æ–¹æ¡ˆã€å‘¨æŠ¥ã€JDå†™ä½œç­‰é•¿æ–‡å†™ä½œä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ã€‚\n\næœ¬æ¬¡å‘å¸ƒç‰ˆæœ¬å’Œä¸‹è½½é“¾æ¥è§ä¸‹è¡¨\n\næ¨¡å‹ç‰ˆæœ¬\tä¸‹è½½é“¾æ¥\n7B-FP16\tTeleChat-7B-FP16\n7B-int8\tTeleChat-7B-int8\n7B-int4\tTeleChat-7B-int4\n12B-FP16\tTeleChat-12B-FP16\n12B-int8\tTeleChat-12B-int8\n12B-int4\tTeleChat-12B-int4\næ•°æ®å¼€æº\næ•°æ®ä»‹ç»\n\nTeleChat-PTD æ˜¯ç”±ç”µä¿¡æ˜Ÿè¾°å¤§æ¨¡å‹TeleChaté¢„è®­ç»ƒè¯­æ–™ä¸­æŠ½å–å‡ºçš„çš„ç»¼åˆæ€§å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®é›†ã€‚æ•°æ®ä¸»è¦æ¥æºäºç½‘é¡µã€ä¹¦ç±ã€å®˜æ–¹åª’ä½“ç­‰ã€‚æˆ‘ä»¬ä½¿ç”¨è§„åˆ™+æ¨¡å‹çš„æ–¹å¼è¿›è¡Œäº†ç›¸å…³çš„è¿‡æ»¤ï¼Œå¹¶å¯¹æ•°æ®è¿›è¡Œäº†ç›¸ä¼¼æ€§å»é‡ï¼Œå°½å¯èƒ½åœ°æå–å‡ºé«˜è´¨é‡çš„æ•°æ®ã€‚\n\nTeleChat-PTD æ•°æ®é›†å¤§çº¦å…¬å¼€äº†2.7äº¿æ¡æ•°æ®ï¼Œæ•°æ®ç”±çº¯ä¸­æ–‡æ–‡æœ¬æ„æˆï¼ŒåŸå§‹å¤§å°çº¦1TBï¼Œå‹ç¼©å480Gï¼Œå…±189ä¸ªæ–‡ä»¶ã€‚æ•°æ®é›†ä¸­å·²ç»å»é™¤äº†å…¶ä»–å†—ä½™ä¿¡æ¯ã€‚\n\næ•°æ®ä¸‹è½½\n\nhuggingfaceä¸‹è½½åœ°å€ï¼šTeleChat-PTD\n\nå¤©ç¿¼äº‘ç›˜ä¸‹è½½åœ°å€ï¼šæ•°æ®ä¸‹è½½ï¼ˆè®¿é—®ç ï¼špkg8ï¼‰\n\næ•ˆæœè¯„æµ‹\n\nTeleChatæ¨¡å‹ç›¸æ¯”åŒè§„æ¨¡æ¨¡å‹åœ¨è¯„æµ‹æ•ˆæœæ–¹é¢ä¹Ÿæœ‰è¾ƒå¥½çš„è¡¨ç°ï¼Œæˆ‘ä»¬çš„è¯„æµ‹é›†æ¶µç›–äº†åŒ…æ‹¬MMLUã€C-Evalã€GAOKAOã€AGIEvalã€CMMLUã€GSM8Kã€MATHã€HumanEvalã€CHIDç­‰æ•°æ®é›†ï¼Œè¯„æµ‹èƒ½åŠ›åŒ…æ‹¬äº†è‡ªç„¶è¯­è¨€ç†è§£ã€çŸ¥è¯†ã€æ•°å­¦è®¡ç®—å’Œæ¨ç†ã€ä»£ç ç”Ÿæˆç­‰ã€‚\n\nè¯„æµ‹ç»“æœå¦‚ä¸‹\nModel\tMMLU\tC-Eval\tCMMLU\tAGIEval\tGAOKAO\tGSM8K\tMATH\tHumanEval\tCSL\tCHID\tEPRSTMT\tBBH\tHellaSwag\n\t5-shot\t5-shot\t5-shot\tzero-shot\tzero-shot\t4-shot\t4-shot\tzero-shot\tzero-shot\tzero-shot\tzero-shot\t3-shot\tzero-shot\nLLaMA2-7B-chat\t46.2\t31.9\t31.5\t28.5\t16.1\t26.3\t3.9\t12.2\t58.8\t44.1\t57.5\t35.6\t74.1\nLLaMA2-13B-chat\t54.6\t36.2\t38.7\t32.3\t18.6\t29.6\t5.0\t18.9\t61.2\t48.0\t59.4\t40.2\t78.2\nChatGLM2-6B-chat\t45.9\t52.6\t49.3\t39.0\t46.4\t28.8\t6.5\t11.0\t61.2\t57.9\t71.2\t32.7\t57.0\nChatGLM3-6B-chat\t51.9\t53.8\t54\t38.9\t49.3\t56.7\t18.7\t61\t65.6\t63.4\t85\t44.6\t62.7\nBaichuan2-7B-chat\t52.8\t55.6\t54.0\t35.3\t39.7\t32.8\t6\t13.4\t60\t75.2\t87.5\t35.8\t61.6\nBaichuan2-13B-chat\t57\t56.7\t58.4\t40\t51.4\t55.3\t8.6\t17.7\t63.1\t78.2\t87.5\t49.9\t66.9\nQwen-7B-chat\t56.6\t59.3\t59.5\t41.3\t63.3\t52.5\t10.3\t26.2\t63.1\t72.3\t88.8\t46.9\t59.9\nQwen-14B-chat\t66.4\t71.7\t70.0\t47.3\t76.5\t61.0\t26.8\t36.6\t55.6\t72.3\t91.2\t58.0\t65.2\nTeleChat-7B-chat\t60.5\t64.6\t64.3\t46.8\t59\t36.7\t10.3\t20.1\t66.8\t88.0\t87.5\t19.5\t36.7\nTeleChat-12B-chat\t73.3\t66.6\t74.2\t51.7\t53.1\t57.2\t16.0\t22.0\t60.6\t83.2\t86.3\t52.2\t71.5\n\nè¯´æ˜ï¼šCMMLUã€AGIEvalã€GAOKAOã€CSLã€CHIDã€EPRSTMTå‡åŸºäºOpenCompasså¹³å°æä¾›çš„è¯„æµ‹æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼Œè€Œå¯¹äºå¯¹æ¯”æ¨¡å‹ï¼Œæˆ‘ä»¬åŒæ—¶å‚è€ƒäº†å®˜æ–¹æ±‡æŠ¥ç»“æœå’ŒOpenCompassç»“æœã€‚æˆ‘ä»¬ä½¿ç”¨äº†è‡ªå·±çš„è¯„æµ‹è„šæœ¬è¯„æµ‹MMLUä¸CEVALæ¦œå•ï¼Œå…·ä½“æ–¹æ³•è§evaluation/æ–‡ä»¶å¤¹ã€‚\n\næ¨¡å‹æ¨ç†\n>>> import os\n>>> os.environ[\"OPENMIND_FRAMEWORK\"] = \"ms\"\n>>> from openmind import AutoModelForCausalLM, AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained('openmind/telechat_7b_ms', trust_remote_code=True)\n>>> model = AutoModelForCausalLM.from_pretrained('openmind/telechat_7b_ms', trust_remote_code=True)\n>>> question = \"<_user>{}<_bot>\".format(\"ç”ŸæŠ½ä¸è€æŠ½çš„åŒºåˆ«ï¼Ÿ\")\n>>> inputs = tokenizer(question)[\"input_ids\"]\n>>> outputs = model.generate(inputs, do_sample=True, top_k=3)\n>>> response = tokenizer.decode(outputs)\n>>> print(response[0])\n<_user>ç”ŸæŠ½ä¸è€æŠ½çš„åŒºåˆ«ï¼Ÿ<_bot>ç”ŸæŠ½ä¸è€æŠ½çš„åŒºåˆ«å¦‚ä¸‹ï¼š\n\n1.ç”ŸæŠ½æ˜¯ç”¨é»„è±†ã€æ·€ç²‰ç­‰åŸæ–™åŠ å·¥è€Œæˆï¼Œé¢œè‰²è¾ƒæ·±ï¼Œå£æ„Ÿè¾ƒå’¸ã€‚ç”ŸæŠ½å¸¸ä½œä¸ºè°ƒå‘³æ–™ä½¿ç”¨ã€‚\n\n2.è€æŠ½æ˜¯ç”¨é…±æ²¹ã€è€æŠ½ã€é£Ÿç›ç»è¿‡ä¸€å®šæ¯”ä¾‹çš„æ··åˆã€å‘é…µã€æè‰²è€Œæˆï¼Œé¢œè‰²è¾ƒæµ…ï¼Œå£æ„Ÿè¾ƒå’¸ã€‚è€æŠ½å¸¸ä½œä¸ºè°ƒå‘³æ–™ä½¿ç”¨ã€‚\n\næ€»ä½“æ¥è¯´ï¼Œç”ŸæŠ½ä¸è€æŠ½éƒ½æ˜¯å¸¸ç”¨çš„è°ƒå‘³å“ï¼Œç”ŸæŠ½é¢œè‰²æ·±ã€å’¸å‘³é‡ï¼Œé€‚åˆç”¨äºçƒ¹è°ƒèœå“ï¼›è€Œè€æŠ½é¢œè‰²æµ…ã€å’¸å‘³è½»ï¼Œé€‚åˆç”¨äºçƒ¹è°ƒæ±¤å“ã€‚åœ¨é€‰æ‹©è°ƒå‘³å“æ—¶ï¼Œåº”æ ¹æ®èœå“çš„å£å‘³å’Œé£å‘³éœ€æ±‚æ¥é€‰æ‹©åˆé€‚çš„è°ƒå‘³å“ã€‚<_end>\n\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡æ•°æ®é›†\n\nç¬¬ä¸€æ­¥ï¼šè·å–æ‰€éœ€æ•°æ®é›†\n\nè¯·ä»æ­¤å¤„ä¸‹è½½æ•°æ®é›†\n\næ•°æ®é›†çš„ç»“æ„æ ¼å¼å¦‚ä¸‹ï¼š\n\n# input_dataset examples:\n    {\"input\": \"ç”µä¿¡ä¸»å¡å’Œå‰¯å¡çš„åŒºåˆ«åœ¨å“ªé‡Œï¼Ÿ\", \"output\": \"ä¸»å¡å’Œå‰¯å¡çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œä¸»å¡åªèƒ½ä½¿ç”¨ä¸€å¼ æ‰‹æœºå·ç ã€‚<_end>\"}\n\n\næ­¥éª¤ 2ï¼šå°†æ•°æ®è½¬æ¢ä¸º MindRecord æ ¼å¼ã€‚\n\ncd example/dataset\n# éœ€è¦æå‰ä¸‹è½½transformers\npython telechat_preprocess.py \\\n--input_dataset_file /{path}/input_dataset.jsonl \\\n--max_length 2048 \\\n--output_path /{path}/input_dataset.mindrecord\n# å‚æ•°è¯´æ˜\ninput_dataset_file: é¢„è®­ç»ƒçš„æ•°æ®é›†\nvocab_file_path: è¯æ¨¡å‹æ–‡ä»¶è·¯å¾„\nmax_length: æ•°æ®é›†é•¿åº¦\noutput_path: ç”Ÿæˆæ•°æ®é›†çš„è·¯å¾„\n\nè®­ç»ƒç¯èŠ‚\ncd example\nbash msrun.sh \"finetune.py --train_dataset /{path}/input_dataset.mindrecord\"\n\næ¨ç†\n\nåœ¨é€»è¾‘å­¦ä¸­ï¼Œæ¨ç†æ˜¯ä¸€ç§æ€ç»´è¿‡ç¨‹ï¼Œé€šè¿‡å·²çŸ¥çš„åˆ¤æ–­æˆ–å‰æï¼Œå¾—å‡ºæ–°çš„åˆ¤æ–­æˆ–ç»“è®ºã€‚æ¨ç†é€šå¸¸åˆ†ä¸ºä¸¤å¤§ç±»ï¼šæ¼”ç»æ¨ç†å’Œå½’çº³æ¨ç†ã€‚\n\næ¼”ç»æ¨ç†ï¼šä»ä¸€èˆ¬åˆ°ç‰¹æ®Šçš„æ¨ç†æ–¹å¼ã€‚å®ƒä»æ™®éçš„åŸåˆ™å‡ºå‘ï¼Œæ¨å¯¼å‡ºç‰¹å®šæƒ…å†µä¸‹çš„ç»“è®ºã€‚å¦‚æœå‰ææ˜¯çœŸçš„ï¼Œé‚£ä¹ˆé€šè¿‡æ¼”ç»æ¨ç†å¾—åˆ°çš„ç»“è®ºä¹Ÿå¿…ç„¶æ˜¯çœŸçš„ã€‚\n\nå½’çº³æ¨ç†ï¼šä»ç‰¹æ®Šåˆ°ä¸€èˆ¬çš„æ¨ç†æ–¹å¼ã€‚å®ƒé€šè¿‡è§‚å¯Ÿä¸ªåˆ«ç°è±¡ï¼Œæ€»ç»“å‡ºä¸€èˆ¬æ€§çš„è§„å¾‹æˆ–åŸåˆ™ã€‚å½’çº³æ¨ç†å¹¶ä¸ä¿è¯ç»“è®ºçš„å¿…ç„¶æ€§ï¼Œä½†å¯ä»¥æä¾›ç»“è®ºçš„æ¦‚ç‡æ€§æ”¯æŒã€‚\n\næ¨ç†æ˜¯äººç±»è®¤è¯†ä¸–ç•Œã€è§£å†³é—®é¢˜çš„é‡è¦æ–¹æ³•ï¼Œæ˜¯ç§‘å­¦ç ”ç©¶ã€å“²å­¦æ¢è®¨å’Œæ³•å¾‹åˆ¤æ–­ç­‰é¢†åŸŸä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚\n\nimport os\nos.environ[\"OPENMIND_FRAMEWORK\"] = \"ms\"\n\nfrom mindspore import set_context\nfrom openmind import pipeline\n\nset_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\", model='MindSpore-Lab/telechat_7b', framework='ms', trust_remote_code=True)\npipeline_result = pipeline_task(\"<_user>ç”ŸæŠ½ä¸è€æŠ½çš„åŒºåˆ«ï¼Ÿ<_bot>\", do_sample=False)\nprint(pipeline_result)\n\n\nå£°æ˜ã€åè®®ä¸å‚è€ƒæ–‡çŒ®\nå£°æ˜\n\nåœ¨æ­¤ï¼Œæˆ‘ä»¬éƒ‘é‡å£°æ˜ï¼Œç¦æ­¢ä½¿ç”¨TeleChatæ¨¡å‹åŠå…¶è¡ç”Ÿæ¨¡å‹ä»äº‹ä»»ä½•å¯èƒ½å±å®³å›½å®¶å®‰å…¨ã€ç ´åç¤¾ä¼šç¨³å®šæˆ–è¿åæ³•å¾‹æ³•è§„çš„è¡Œä¸ºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¼ºçƒˆè¦æ±‚ç”¨æˆ·ä¸å¾—åœ¨æœªè¿›è¡Œå®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„æƒ…å†µä¸‹ï¼Œå°†TeleChatæ¨¡å‹åº”ç”¨äºäº’è”ç½‘æœåŠ¡ä¸­ã€‚æˆ‘ä»¬æœŸæœ›æ‰€æœ‰ç”¨æˆ·éƒ½èƒ½éµå®ˆè¿™äº›åŸåˆ™ï¼Œä¿éšœç§‘æŠ€è¿›æ­¥åœ¨åˆæ³•åˆè§„çš„æ¡†æ¶å†…è¿›è¡Œã€‚\n\næˆ‘ä»¬å·²å°½æœ€å¤§åŠªåŠ›ç¡®ä¿åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®ç¬¦åˆç›¸å…³è§„å®šã€‚å°½ç®¡æˆ‘ä»¬å·²ç»ä½œå‡ºäº†å·¨å¤§åŠªåŠ›ï¼Œä½†é‰´äºæ¨¡å‹å’Œæ•°æ®æœ¬èº«çš„å¤æ‚æ€§ï¼Œå¯èƒ½ä»å­˜åœ¨ä¸€äº›ä¸å¯é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¯¹äºå› ä½¿ç”¨TeleChatå¼€æºæ¨¡å‹è€Œå¯èƒ½å¼•èµ·çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œä»¥åŠå› æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“ä½¿ç”¨è€Œå¯¼è‡´çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n\nåè®®\n\nä½¿ç”¨TeleChatæ¨¡å‹éœ€éµå®ˆã€ŠTeleChatæ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹ã€‚TeleChatæ¨¡å‹å¯ç”¨äºå•†ä¸šç”¨é€”ã€‚å¦‚æ‚¨æ‰“ç®—å°†TeleChatæ¨¡å‹æˆ–å…¶è¡ç”Ÿäº§å“ç”¨äºå•†ä¸šç›®çš„ï¼Œè¯·é€šè¿‡é‚®ç®± tele_ai@chinatelecom.cn æäº¤ã€ŠTeleChatæ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹ä¸­è¦æ±‚çš„ç”³è¯·ææ–™ã€‚å®¡æ ¸é€šè¿‡åï¼Œæ‚¨å°†è·å¾—ä¸€ä¸ªéç‹¬å æ€§ã€å…¨çƒèŒƒå›´ã€ä¸å¯è½¬è®©ã€ä¸å¯å†æ¬¡æˆæƒä¸”å¯æ’¤é”€çš„å•†ä¸šç”¨é€”ç‰ˆæƒè®¸å¯ã€‚\n\nå¼•ç”¨\n\nè‹¥éœ€å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œï¼Œè¯·å‚è€ƒä»¥ä¸‹å‚è€ƒæ–‡çŒ®ï¼š\n\n@misc{wang2024telechat,\n      title={TeleChat Technical Report}, \n      author={Zihan Wang and Xinzhang Liu and Shixuan Liu and Yitong Yao and Yuyao Huang and Zhongjiang He and Xuelong Li and Yongxiang Li and Zhonghao Che and Zhaoxi Zhang and Yan Wang and Xin Wang and Luwen Pu and Huihan Xu and Ruiyu Fang and Yu Zhao and Jie Zhang and Xiaomeng Huang and Zhilong Lu and Jiaxin Peng and Wenjun Zheng and Shiquan Wang and Bingkai Yang and Xuewei he and Zhuoru Jiang and Qiyi Xie and Yanhan Zhang and Zhongqiu Li and Lingling Shi and Weiwei Fu and Yin Zhang and Zilu Huang and Sishi Xiong and Yuxiang Zhang and Chao Wang and Shuangyong Song},\n      year={2024},\n      eprint={2401.03804},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n",
    "tags": "[\"Text Generation\", \"TensorFlow\", \"Transformers\", \"MindSpore\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/MooYeh/t5_small",
    "project_name": "t5_small",
    "readme": "Original Text\nT5å°å‹æ¨¡å‹å¡\nä¿®æ”¹è¯´æ˜\n\nåœ¨åŸå§‹READMEåŸºç¡€ä¸Šå¢åŠ äº†CANNç‰ˆæœ¬ä¾èµ–è¯´æ˜ï¼Œå¹¶ä¿®æ”¹äº†ç¤ºä¾‹ä»£ç ã€‚\n\nç›®å½•\næ¨¡å‹è¯¦æƒ…\nç”¨é€”\nåå·®ã€é£é™©ä¸é™åˆ¶\nè®­ç»ƒè¯¦æƒ…\nè¯„ä¼°\nç¯å¢ƒå½±å“\nå¼•ç”¨\næ¨¡å‹å¡ä½œè€…\nå¿«é€Ÿå¼€å§‹æŒ‡å—\næ¨¡å‹è¯¦æƒ…\næ¨¡å‹æè¿°\n\næ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢Transformerï¼ˆT5ï¼‰çš„å¼€å‘è€…åœ¨åšå®¢ä¸­å†™é“ï¼š\n\né€šè¿‡T5ï¼Œæˆ‘ä»¬æå‡ºå°†æ‰€æœ‰NLPä»»åŠ¡é‡æ„ä¸ºç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼ï¼Œè¾“å…¥å’Œè¾“å‡ºå§‹ç»ˆæ˜¯æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œè¿™ä¸BERTç±»æ¨¡å‹åªèƒ½è¾“å‡ºç±»åˆ«æ ‡ç­¾æˆ–è¾“å…¥ç‰‡æ®µå½¢æˆå¯¹æ¯”ã€‚æˆ‘ä»¬çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ¡†æ¶å…è®¸æˆ‘ä»¬åœ¨ä»»ä½•NLPä»»åŠ¡ä¸Šä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œè¶…å‚æ•°ã€‚\n\nT5-Smallæ˜¯åŒ…å«6000ä¸‡å‚æ•°çš„æ£€æŸ¥ç‚¹ã€‚\n\nå¼€å‘è€…ï¼š Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liuã€‚å‚è§ç›¸å…³è®ºæ–‡å’ŒGitHubä»“åº“\næ¨¡å‹ç±»å‹ï¼š è¯­è¨€æ¨¡å‹\næ”¯æŒè¯­è¨€ï¼ˆNLPï¼‰ï¼š è‹±è¯­ã€æ³•è¯­ã€ç½—é©¬å°¼äºšè¯­ã€å¾·è¯­\nè®¸å¯è¯ï¼š Apache 2.0\næ›´å¤šä¿¡æ¯ï¼š\nç ”ç©¶è®ºæ–‡\nè°·æ­ŒT5åšå®¢æ–‡ç« \nGitHubä»“åº“\nç”¨é€”\nç›´æ¥ä½¿ç”¨ä¸ä¸‹æ¸¸åº”ç”¨\n\nå¼€å‘è€…åœ¨åšå®¢æ–‡ç« ä¸­æåˆ°ï¼š\n\næˆ‘ä»¬çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ¡†æ¶å…è®¸æˆ‘ä»¬åœ¨ä»»ä½•NLPä»»åŠ¡ä¸Šä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œè¶…å‚æ•°ï¼ŒåŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€æ–‡æ¡£æ‘˜è¦ã€é—®ç­”å’Œåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰ã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥é€šè¿‡è®­ç»ƒæ¨¡å‹é¢„æµ‹æ•°å­—çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼ï¼Œå°†å…¶åº”ç”¨äºå›å½’ä»»åŠ¡ã€‚\n\nè¯¦æƒ…è¯·å‚é˜…åšå®¢æ–‡ç« å’Œç ”ç©¶è®ºæ–‡ã€‚\n\nè¶…å‡ºé€‚ç”¨èŒƒå›´\n\néœ€è¦æ›´å¤šä¿¡æ¯ã€‚\n\nåå·®ã€é£é™©ä¸é™åˆ¶\n\néœ€è¦æ›´å¤šä¿¡æ¯ã€‚\n\nå»ºè®®\n\néœ€è¦æ›´å¤šä¿¡æ¯ã€‚\n\nè®­ç»ƒè¯¦æƒ…\nè®­ç»ƒæ•°æ®\n\næ¨¡å‹åœ¨Colossal Clean Crawled Corpus (C4)ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¯¥æ•°æ®é›†ä¸T5çš„ç ”ç©¶è®ºæ–‡åŒæ—¶å¼€å‘å’Œå‘å¸ƒã€‚\n\næ¨¡å‹åœ¨æ— ç›‘ç£ï¼ˆ1.ï¼‰å’Œæœ‰ç›‘ç£ä»»åŠ¡ï¼ˆ2.ï¼‰çš„å¤šä»»åŠ¡æ··åˆä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚å…·ä½“ä½¿ç”¨çš„æ•°æ®é›†å¦‚ä¸‹ï¼š\n\nç”¨äºæ— ç›‘ç£å»å™ªç›®æ ‡çš„æ•°æ®é›†ï¼š\nC4\nWiki-DPR\nç”¨äºæœ‰ç›‘ç£æ–‡æœ¬åˆ°æ–‡æœ¬è¯­è¨€å»ºæ¨¡ç›®æ ‡çš„æ•°æ®é›†\nå¥å­å¯æ¥å—æ€§åˆ¤æ–­\nCoLA Warstadt et al., 2018\næƒ…æ„Ÿåˆ†æ\nSST-2 Socher et al., 2013\nå¤è¿°/å¥å­ç›¸ä¼¼åº¦\nMRPC Dolan and Brockett, 2005\nSTS-B Cer et al., 2017\nQQP Iyer et al., 2017\nè‡ªç„¶è¯­è¨€æ¨ç†\nMNLI Williams et al., 2017\nQNLI Rajpurkar et al.,2016\nRTE Dagan et al., 2005\nCB De Marneff et al., 2019\nå¥å­è¡¥å…¨\nCOPA Roemmele et al., 2011\nè¯ä¹‰æ¶ˆæ­§\nWIC Pilehvar and Camacho-Collados, 2018\né—®ç­”\nMultiRC Khashabi et al., 2018\nReCoRD Zhang et al., 2018\nBoolQ Clark et al., 2019\nè®­ç»ƒè¿‡ç¨‹\n\nåœ¨æ‘˜è¦ä¸­ï¼Œå¼€å‘è€…å†™é“ï¼š\n\næœ¬æ–‡é€šè¿‡å¼•å…¥å°†æ¯ä¸ªè¯­è¨€é—®é¢˜è½¬æ¢ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ¢ç´¢äº†NLPè¿ç§»å­¦ä¹ æŠ€æœ¯çš„å…¨æ™¯ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç ”ç©¶æ¯”è¾ƒäº†æ•°åç§è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šçš„é¢„è®­ç»ƒç›®æ ‡ã€æ¶æ„ã€æœªæ ‡æ³¨æ•°æ®é›†ã€è¿ç§»æ–¹æ³•ç­‰å› ç´ ã€‚\n\nå¼•å…¥çš„T5æ¡†æ¶åŒ…å«ä¸€ä¸ªæ•´åˆäº†è®ºæ–‡ä¸­ç ”ç©¶æ–¹æ³•çš„è®­ç»ƒè¿‡ç¨‹ã€‚è¯¦æƒ…å‚è§ç ”ç©¶è®ºæ–‡ã€‚\n\nè¯„ä¼°\næµ‹è¯•æ•°æ®ã€å› ç´ ä¸æŒ‡æ ‡\n\nå¼€å‘è€…åœ¨24ä¸ªä»»åŠ¡ä¸Šè¯„ä¼°äº†æ¨¡å‹ï¼Œå®Œæ•´ç»†èŠ‚è§ç ”ç©¶è®ºæ–‡ã€‚\n\nç»“æœ\n\nT5-smallçš„å®Œæ•´ç»“æœè§ç ”ç©¶è®ºæ–‡è¡¨14ã€‚\n\nç¯å¢ƒå½±å“\n\nç¢³æ’æ”¾é‡å¯é€šè¿‡Lacoste et al. (2019)æå‡ºçš„æœºå™¨å­¦ä¹ å½±å“è®¡ç®—å™¨ä¼°ç®—ã€‚\n\nç¡¬ä»¶ç±»å‹ï¼š è°·æ­Œäº‘TPU Pods\nä½¿ç”¨æ—¶é•¿ï¼š éœ€è¦æ›´å¤šä¿¡æ¯\näº‘æœåŠ¡å•†ï¼š GCP\nè®¡ç®—åŒºåŸŸï¼š éœ€è¦æ›´å¤šä¿¡æ¯\nç¢³æ’æ”¾é‡ï¼š éœ€è¦æ›´å¤šä¿¡æ¯\nå¼•ç”¨\n\nBibTeX:\n\n@article{2020t5,\n  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {140},\n  pages   = {1-67},\n  url     = {http://jmlr.org/papers/v21/20-074.html}\n}\n\n\nAPAæ ¼å¼ï¼š\n\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). æ¢ç´¢è¿ç§»å­¦ä¹ çš„æé™ï¼šåŸºäºç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨çš„ç ”ç©¶ã€‚ ã€Šæœºå™¨å­¦ä¹ ç ”ç©¶æœŸåˆŠã€‹ï¼Œ21(140)ï¼Œ1-67.\næ¨¡å‹å¡ä½œè€…\n\næœ¬æ¨¡å‹å¡ç”±Hugging Faceå›¢é˜Ÿæ’°å†™ã€‚\n\nå¦‚ä½•å¼€å§‹ä½¿ç”¨è¯¥æ¨¡å‹\n\nä½¿ç”¨ä»¥ä¸‹ä»£ç å³å¯å¿«é€Ÿä¸Šæ‰‹è¯¥æ¨¡å‹ã€‚\n\nç‚¹å‡»å±•å¼€",
    "tags": "[\"Translation\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"ONNX\", \"Rust\", \"Safetensors\", \"JAX\", \"English\", \"Apache License 2.0\", \"c4\"]"
  },
  {
    "url": "https://gitcode.com/MooYeh/YOLOV9_for_PyTorch",
    "project_name": "YOLOV9_for_PyTorch",
    "readme": "YoloV9 for PyTorch\nç®€ä»‹\næ¨¡å‹ä»‹ç»\næ”¯æŒä»»åŠ¡åˆ—è¡¨\nä»£ç å®ç°\nè®­ç»ƒ\nå‡†å¤‡ç¯å¢ƒ\nå‡†å¤‡æ•°æ®é›†\nå¼€å§‹è®­ç»ƒ\nè®­ç»ƒç»“æœå±•ç¤º\nå…¬ç½‘åœ°å€è¯´æ˜\nå˜æ›´è¯´æ˜\nFAQ\nç®€ä»‹\næ¨¡å‹ä»‹ç»\n\nYOLOv9èåˆäº†æ·±åº¦å­¦ä¹ æŠ€æœ¯å’Œæ¶æ„è®¾è®¡çš„è¿›æ­¥ï¼Œä»¥åœ¨å¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸­å®ç°å“è¶Šçš„æ€§èƒ½ã€‚å°†å¯ç¼–ç¨‹æ¢¯åº¦ä¿¡æ¯ (PGI) æ¦‚å¿µä¸é€šç”¨ ELAN (GELAN)æ¶æ„ç›¸ç»“åˆè€Œå¼€å‘ï¼Œä»£è¡¨äº†å‡†ç¡®æ€§ã€é€Ÿåº¦å’Œæ•ˆç‡æ–¹é¢çš„é‡å¤§é£è·ƒã€‚\n\nä¸»è¦ç‰¹ç‚¹ï¼š\n\nå®æ—¶å¯¹è±¡æ£€æµ‹ï¼šYOLOv9 é€šè¿‡æä¾›å®æ—¶å¯¹è±¡æ£€æµ‹åŠŸèƒ½ä¿æŒäº† YOLO ç³»åˆ—çš„æ ‡å¿—æ€§åŠŸèƒ½ã€‚è¿™æ„å‘³ç€å®ƒå¯ä»¥å¿«é€Ÿå¤„ç†è¾“å…¥å›¾åƒæˆ–è§†é¢‘æµï¼Œå¹¶å‡†ç¡®æ£€æµ‹å…¶ä¸­çš„å¯¹è±¡ï¼Œè€Œä¸ä¼šå½±å“é€Ÿåº¦ã€‚\nPGIé›†æˆï¼šYOLOv9èåˆäº†å¯ç¼–ç¨‹æ¢¯åº¦ä¿¡æ¯ï¼ˆPGIï¼‰æ¦‚å¿µï¼Œæœ‰åŠ©äºé€šè¿‡è¾…åŠ©å¯é€†åˆ†æ”¯ç”Ÿæˆå¯é çš„æ¢¯åº¦ã€‚è¿™ç¡®ä¿æ·±åº¦ç‰¹å¾ä¿ç•™æ‰§è¡Œç›®æ ‡ä»»åŠ¡æ‰€éœ€çš„å…³é”®ç‰¹å¾ï¼Œè§£å†³æ·±åº¦ç¥ç»ç½‘ç»œå‰é¦ˆè¿‡ç¨‹ä¸­ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚\nGELANæ¶æ„ï¼šYOLOv9é‡‡ç”¨é€šç”¨ELANï¼ˆGELANï¼‰æ¶æ„ï¼Œæ—¨åœ¨ä¼˜åŒ–å‚æ•°ã€è®¡ç®—å¤æ‚åº¦ã€å‡†ç¡®æ€§å’Œæ¨ç†é€Ÿåº¦ã€‚é€šè¿‡å…è®¸ç”¨æˆ·ä¸ºä¸åŒçš„æ¨ç†è®¾å¤‡é€‰æ‹©åˆé€‚çš„è®¡ç®—æ¨¡å—ï¼ŒGELAN å¢å¼ºäº† YOLOv9 çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚\næ€§èƒ½æå‡ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒYOLOv9 åœ¨ MS COCO ç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚å®ƒåœ¨å‡†ç¡®æ€§ã€é€Ÿåº¦å’Œæ•´ä½“æ€§èƒ½æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„å®æ—¶ç‰©ä½“æ£€æµ‹å™¨ï¼Œä½¿å…¶æˆä¸ºéœ€è¦ç‰©ä½“æ£€æµ‹åŠŸèƒ½çš„å„ç§åº”ç”¨çš„æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆã€‚\nçµæ´»æ€§å’Œé€‚åº”æ€§ï¼šYOLOv9 æ—¨åœ¨é€‚åº”ä¸åŒçš„åœºæ™¯å’Œç”¨ä¾‹ã€‚å…¶æ¶æ„å¯ä»¥è½»æ¾é›†æˆåˆ°å„ç§ç³»ç»Ÿå’Œç¯å¢ƒä¸­ï¼Œä½¿å…¶é€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶è½¦è¾†ã€æœºå™¨äººç­‰ã€‚\næ”¯æŒä»»åŠ¡åˆ—è¡¨\n\næœ¬ä»“å·²æ”¯æŒä»¥ä¸‹æ¨¡å‹ä»»åŠ¡ç±»å‹ã€‚\n\næ¨¡å‹\tä»»åŠ¡ç±»å‹\tæ˜¯å¦æ”¯æŒ\nYOLOV9\tè®­ç»ƒ\tâœ…\nä»£ç å®ç°\n\nå‚è€ƒå®ç°ï¼š\n\nurl=https://github.com/WongKinYiu/yolov9.git\ncommit_id=5b1ea9a8b3f0ffe4fe0e203ec6232d788bb3fcff\n\n\né€‚é…æ˜‡è…¾ AI å¤„ç†å™¨çš„å®ç°ï¼š\n\nurl=https://gitee.com/ascend/ModelZoo-PyTorch.git\ncode_path=PyTorch/built-in/cv/detection/YOLOV9_for_PyTorch/\n\nè®­ç»ƒ\nå‡†å¤‡ç¯å¢ƒ\n\nå®‰è£…æ˜‡è…¾ç¯å¢ƒã€‚\n\nè¯·å‚è€ƒã€ŠPytorchæ¡†æ¶è®­ç»ƒç¯å¢ƒå‡†å¤‡ã€‹ã€‚\n\nå½“å‰æ¨¡å‹æ”¯æŒçš„ PyTorch ç‰ˆæœ¬å’Œå·²çŸ¥ä¸‰æ–¹åº“ä¾èµ–å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚\n\nè¡¨ 1 ç‰ˆæœ¬æ”¯æŒè¡¨\n\nTorch_Version\tä¸‰æ–¹åº“ä¾èµ–ç‰ˆæœ¬\nPyTorch 1.11\ttorchvision==0.12.0ï¼›torchvision_npu==0.12.0\nPyTorch 2.1\ttorchvision==0.16.0ï¼›torchvision_npu==0.16.0\n\nè¡¨ 2 æ˜‡è…¾è½¯ä»¶ç‰ˆæœ¬æ”¯æŒè¡¨\n\nè½¯ä»¶ç±»å‹\tæ”¯æŒç‰ˆæœ¬\nFrameworkPTAdapter\t6.0.RC1/åœ¨ç ”ç‰ˆæœ¬\nCANN\t8.0.RC1/åœ¨ç ”ç‰ˆæœ¬\næ˜‡è…¾NPUå›ºä»¶\t24.1.RC1/åœ¨ç ”ç‰ˆæœ¬\næ˜‡è…¾NPUé©±åŠ¨\t24.1.RC1/åœ¨ç ”ç‰ˆæœ¬\n\nå®‰è£…ä¾èµ–ã€‚\n\nåœ¨YOLOV9_for_PyTorchç›®å½•ä¸‹æ‰§è¡Œå‘½ä»¤ï¼Œå®‰è£…æ¨¡å‹å¯¹åº”PyTorchç‰ˆæœ¬éœ€è¦çš„ä¾èµ–ã€‚\n\npip install torchvision==0.12.0 # torch 1.11\n# pip install torchvision==0.16.0 # torch 2.1\ncd PyTorch/built-in/cv/detection/YOLOV9_for_PyTorch\npip install -r requirements.txt\n\n\nè¯´æ˜ï¼š åªéœ€æ‰§è¡Œä¸€æ¡å¯¹åº”çš„PyTorchç‰ˆæœ¬ä¾èµ–å®‰è£…å‘½ä»¤ã€‚\n\nå®‰è£…torchvision_npuã€‚\n\ncd ..\ngit clone -b v0.12.0-dev https://gitee.com/ascend/vision.git torchvision_npu # torch 1.11\n# git clone https://gitee.com/ascend/vision.git torchvision_npu # torch 2.1\n\ncd torchvision_npu\npip install -r requirement.txt\npython setup.py bdist_wheel\npip install dist/torchvision_npu*.whl\n\nå‡†å¤‡æ•°æ®é›†\n\nè·å–æ•°æ®é›†ã€‚\n\na. ä¸‹è½½ Arial.ttf æ–‡ä»¶ä¸Šä¼ è‡³æœåŠ¡å™¨ /root/.config/Ultralytics/Arial.ttfã€‚\n\nb. è”ç½‘æƒ…å†µä¸‹ï¼Œåœ¨YOLOV9_for_PyTorchç›®å½•æ‰§è¡Œbash scripts/get_coco.shï¼Œè‡ªåŠ¨ä¸‹è½½coco2017æ•°æ®é›†ï¼Œç„¶åç§»åŠ¨cocoè‡³YOLOV9_for_PyTorchåŒçº§åˆ«çš„datasetsç›®å½•ä¸‹ï¼Œæ•°æ®é›†ç›®å½•ç»“æ„å‚è€ƒå¦‚ä¸‹æ‰€ç¤ºã€‚\n\nc. æ— ç½‘ç»œæƒ…å†µä¸‹ï¼Œä¸‹è½½coco2017labels-segments.zipå¹¶è§£å‹è‡³datasetsç›®å½•ï¼Œä¸‹è½½val2017.zipã€test2017.zipã€train2017.zipå¹¶è§£å‹è‡³datasets/coco/imagesç›®å½•ã€‚\n\nâ”œâ”€â”€ datasets #æ•°æ®é›†ç›®å½•\n  â”œâ”€â”€ coco #cocoæ•°æ®é›†\n    â”œâ”€â”€ train2017.txt #è®­ç»ƒé›†å›¾ç‰‡åˆ—è¡¨\n    â”œâ”€â”€ val2017.txt #éªŒè¯é›†å›¾ç‰‡åˆ—è¡¨\n    â”œâ”€â”€ images #å›¾ç‰‡\n      â”œâ”€â”€ train2017 #è®­ç»ƒé›†å›¾ç‰‡ï¼Œ118287å¼ \n      â”œâ”€â”€ val2017 #éªŒè¯é›†å›¾ç‰‡ï¼Œ5000å¼ \n      â”œâ”€â”€ test2017 #æµ‹è¯•é›†å›¾ç‰‡ï¼Œ40670å¼ \nâ”œâ”€â”€ YOLOV9_for_PyTorch #æ ¹ç›®å½•\n  â”œâ”€â”€ scripts\n    â”œâ”€â”€ get_coco.sh #ä¸‹è½½æ•°æ®é›†è„šæœ¬\n\nå¼€å§‹è®­ç»ƒ\n\nè¿è¡Œè®­ç»ƒè„šæœ¬ã€‚\n\nè¯¥æ¨¡å‹æ”¯æŒå•æœºå•å¡è®­ç»ƒå’Œå•æœº8å¡è®­ç»ƒã€‚\n\nå•æœºå•å¡è®­ç»ƒ\n\nexport CPU_AFFINITY_CONF=1 # CPUç»‘æ ¸ï¼Œå¯é€‰\npython -m torch.distributed.launch --nproc_per_node 1 --master_port 9527 train_dual.py --workers 8 --device 0 --batch 32 --data data/coco.yaml --img 640 --cfg models/detect/yolov9-c.yaml --weights '' --name yolov9-c --hyp hyp.scratch-high.yaml --min-items 0 --epochs 100 --close-mosaic 15 --noplots\n\n\nå•æœº8å¡è®­ç»ƒ\n\nexport CPU_AFFINITY_CONF=1 # CPUç»‘æ ¸ï¼Œå¯é€‰\npython -m torch.distributed.launch --nproc_per_node 8 --master_port 9527 train_dual.py --workers 8 --device 0,1,2,3,4,5,6,7 --batch 256 --data data/coco.yaml --img 640 --cfg models/detect/yolov9-c.yaml --weights '' --name yolov9-c --hyp hyp.scratch-high.yaml --min-items 0 --epochs 100 --close-mosaic 15 --noplots\n\n\næ¨¡å‹è®­ç»ƒè„šæœ¬å‚æ•°è¯´æ˜å¦‚ä¸‹ã€‚\n\nå…¬å…±å‚æ•°ï¼š\n--nproc_per_node                 //ä½¿ç”¨çš„npuå¡æ•°é‡\n--device                         //è®­ç»ƒè®¾å¤‡å¡å·\n--batch                          //å•æ­¥è®­ç»ƒå›¾ç‰‡æ•°é‡\n--epochs                         //è®­ç»ƒæ¬¡æ•°\n--data                           //æ•°æ®é›†é…ç½®\n--cfg                            //è®­ç»ƒé…ç½®\n--name                           //è®­ç»ƒå\n\n\nè®­ç»ƒå®Œæˆåï¼Œæƒé‡æ–‡ä»¶ä¿å­˜åœ¨runs/trainï¼Œå¹¶è¾“å‡ºæ¨¡å‹è®­ç»ƒç²¾åº¦å’Œæ€§èƒ½ä¿¡æ¯ã€‚\n\nè®­ç»ƒç»“æœå±•ç¤º\n\nè¡¨ 3 è®­ç»ƒæ€§èƒ½\n\nNAME\tå¡æ•°\tTrain Times(1 epoch)\tTrain Times(1 epoch)\tTorch_Version\nç«å“V\t8p\t05:12\t00:33\t2.4\nAtlas 800T A2\t8p\t07:23\t01:44\t1.11\n\nè¡¨ 4 è®­ç»ƒç²¾åº¦\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.530\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.702\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.578\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.362\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.585\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.693\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.392\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.652\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.702\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.541\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.760\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.844\n\nå…¬ç½‘åœ°å€è¯´æ˜\n\nä»£ç æ¶‰åŠå…¬ç½‘åœ°å€å‚è€ƒ public_address_statement.md\n\nå˜æ›´è¯´æ˜\n\n2024.8.13ï¼šé¦–æ¬¡å‘å¸ƒã€‚\n\nFAQ",
    "tags": "[\"TensorFlow\", \"Transformers\", \"COMET\", \"â€¢ Running:4U8G\", \"4U8G\"]"
  },
  {
    "url": "https://gitcode.com/MooYeh/SDXL-Lightning",
    "project_name": "SDXL-Lightning",
    "readme": "Original Text\nSDXL-Lightning\n\nSDXL-Lightning æ˜¯ä¸€æ¬¾é€Ÿåº¦æå¿«çš„æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚å®ƒå¯ä»¥åœ¨å‡ æ­¥å†…ç”Ÿæˆé«˜è´¨é‡çš„ 1024 åƒç´ å›¾åƒã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ç ”ç©¶è®ºæ–‡ï¼šSDXL-Lightning: Progressive Adversarial Diffusion Distillationã€‚æˆ‘ä»¬ä½œä¸ºç ”ç©¶çš„ä¸€éƒ¨åˆ†å¼€æºäº†è¯¥æ¨¡å‹ã€‚\n\næˆ‘ä»¬çš„æ¨¡å‹æ˜¯ä» stabilityai/stable-diffusion-xl-base-1.0 ä¸­æå–å‡ºæ¥çš„ã€‚æ­¤å­˜å‚¨åº“åŒ…å« 1 æ­¥ã€2 æ­¥ã€4 æ­¥å’Œ 8 æ­¥æå–æ¨¡å‹çš„æ£€æŸ¥ç‚¹ã€‚æˆ‘ä»¬çš„ 2 æ­¥ã€4 æ­¥å’Œ 8 æ­¥æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ä»¤äººæƒŠå¹ã€‚æˆ‘ä»¬çš„ 1 æ­¥æ¨¡å‹æ›´å…·å®éªŒæ€§ã€‚\n\næˆ‘ä»¬æä¾›å®Œæ•´çš„ UNet å’Œ LoRA æ£€æŸ¥ç‚¹ã€‚å®Œæ•´çš„ UNet æ¨¡å‹è´¨é‡æœ€ä½³ï¼Œè€Œ LoRA æ¨¡å‹å¯ä»¥åº”ç”¨äºå…¶ä»–åŸºç¡€æ¨¡å‹ã€‚\n\nä¿®æ”¹\nå°†ç¤ºä¾‹ä¿®æ”¹ä¸º openMind å¹¶æ·»åŠ  NPU æ”¯æŒ\næ¼”ç¤º\nä½¿ç”¨æ‰€æœ‰é…ç½®ç”Ÿæˆï¼Œæœ€ä½³è´¨é‡ï¼šæ¼”ç¤º\næ£€æŸ¥ç‚¹\nsdxl_lightning_Nstep.safetensors: ä¸€ä½“åŒ–æ£€æŸ¥ç‚¹ï¼Œç”¨äº ComfyUIã€‚\nsdxl_lightning_Nstep_unet.safetensors: ä»… UNet æ£€æŸ¥ç‚¹ï¼Œç”¨äº Diffusersã€‚\nsdxl_lightning_Nstep_lora.safetensors: LoRA æ£€æŸ¥ç‚¹ï¼Œç”¨äº Diffusers å’Œ ComfyUIã€‚\nDiffusers ç”¨æ³•\n\nè¯·å§‹ç»ˆä½¿ç”¨ç›¸åº”çš„æ¨ç†æ­¥éª¤çš„æ­£ç¡®æ£€æŸ¥ç‚¹ã€‚\n\n2 æ­¥ã€4 æ­¥ã€8 æ­¥ UNet\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\nfrom safetensors.torch import load_file\nfrom openmind import is_torch_npu_available\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelse:\n    device = \"cpu\"\n\nbase = \"./PyTorch-NPU/stable-diffusion-xl-base-1_0\"\nckpt = \"./PyTorch-NPU/SDXL-Lightning/sdxl_lightning_4step_unet.safetensors\" # Use the correct ckpt for your step setting!\n\n# Load model.\nunet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(device, torch.float16)\nunet.load_state_dict(load_file(ckpt))\npipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n\n# Ensure sampler uses \"trailing\" timesteps.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n\nä¸¤æ­¥ã€å››æ­¥ã€å…«æ­¥ LoRA\n\nä»…å½“æ‚¨ä½¿ç”¨é SDXL åŸºç¡€æ¨¡å‹æ—¶æ‰ä½¿ç”¨ LoRAã€‚å¦åˆ™ï¼Œè¯·ä½¿ç”¨æˆ‘ä»¬çš„ UNet æ£€æŸ¥ç‚¹ä»¥è·å¾—æ›´é«˜çš„è´¨é‡ã€‚\n\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, EulerDiscreteScheduler\nfrom openmind import is_torch_npu_available\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelse:\n    device = \"cpu\"\n\nbase = \"./PyTorch-NPU/stable-diffusion-xl-base-1_0\"\nckpt = \"./PyTorch-NPU/SDXL-Lightning/sdxl_lightning_4step_lora.safetensors\" # Use the correct ckpt for your step setting!\n\n# Load model.\npipe = StableDiffusionXLPipeline.from_pretrained(base, torch_dtype=torch.float16, variant=\"fp16\").to(device)\npipe.load_lora_weights(ckpt)\npipe.fuse_lora()\n\n# Ensure sampler uses \"trailing\" timesteps.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n\nä¸€æ­¥ UNet\n\nä¸€æ­¥æ¨¡å‹åªæ˜¯å®éªŒæ€§çš„ï¼Œè´¨é‡ç¨³å®šæ€§è¾ƒå·®ã€‚å»ºè®®ä½¿ç”¨ä¸¤æ­¥æ¨¡å‹ä»¥è·å¾—æ›´é«˜çš„è´¨é‡ã€‚\n\nä¸€æ­¥æ¨¡å‹ä½¿ç”¨â€œé‡‡æ ·â€é¢„æµ‹è€Œä¸æ˜¯â€œepsilonâ€é¢„æµ‹ï¼è°ƒåº¦ç¨‹åºéœ€è¦é…ç½®æ­£ç¡®ã€‚\n\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\nfrom safetensors.torch import load_file\nfrom openmind import is_torch_npu_available\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelse:\n    device = \"cpu\"\n\nbase = \"./PyTorch-NPU/stable-diffusion-xl-base-1_0\"\nckpt = \"./PyTorch-NPU/SDXL-Lightning/sdxl_lightning_1step_unet_x0.safetensors\" # Use the correct ckpt for your step setting!\n \n# Load model.\nunet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(device, torch.float16)\nunet.load_state_dict(load_file(ckpt))\npipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n\n# Ensure sampler uses \"trailing\" timesteps and \"sample\" prediction type.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", prediction_type=\"sample\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=1, guidance_scale=0).images[0].save(\"output.png\")\n\nComfyUI ä½¿ç”¨æ–¹æ³•\n\nè¯·å§‹ç»ˆä¸ºç›¸åº”çš„æ¨ç†æ­¥éª¤ä½¿ç”¨æ­£ç¡®çš„æ£€æŸ¥ç‚¹ã€‚ è¯·ä½¿ç”¨æ¬§æ‹‰é‡‡æ ·å™¨å’Œ sgm_uniform è®¡åˆ’ç¨‹åºã€‚\n\n2 æ­¥ã€4 æ­¥ã€8 æ­¥å®Œæ•´ç‰ˆ\nå°†å®Œæ•´æ£€æŸ¥ç‚¹ (sdxl_lightning_Nstep.safetensors) ä¸‹è½½åˆ° /ComfyUI/models/checkpointsã€‚\nä¸‹è½½æˆ‘ä»¬çš„ ComfyUI å®Œæ•´å·¥ä½œæµç¨‹ã€‚\n\n2 æ­¥ã€4 æ­¥ã€8 æ­¥ LoRA\n\nä»…å½“æ‚¨ä½¿ç”¨é SDXL åŸºç¡€æ¨¡å‹æ—¶æ‰ä½¿ç”¨ LoRAã€‚å¦åˆ™ï¼Œè¯·ä½¿ç”¨æˆ‘ä»¬çš„å®Œæ•´æ£€æŸ¥ç‚¹ä»¥è·å¾—æ›´é«˜çš„è´¨é‡ã€‚\n\nå‡†å¤‡æ‚¨è‡ªå·±çš„åŸºç¡€æ¨¡å‹ã€‚\nå°† LoRA æ£€æŸ¥ç‚¹ (sdxl_lightning_Nstep_lora.safetensors) ä¸‹è½½åˆ° /ComfyUI/models/lorasã€‚\nä¸‹è½½æˆ‘ä»¬çš„ ComfyUI LoRA å·¥ä½œæµç¨‹ã€‚\n\n1 æ­¥\n\n1 æ­¥æ¨¡å‹ä»…å¤„äºå®éªŒé˜¶æ®µï¼Œè´¨é‡è¿œä¸å¦‚ç¨³å®šã€‚å»ºè®®ä½¿ç”¨ 2 æ­¥æ¨¡å‹è·å¾—æ›´é«˜çš„è´¨é‡ã€‚\n\nå°†æ‚¨çš„ ComfyUI æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬ã€‚\nå°†å®Œæ•´æ£€æŸ¥ç‚¹ (sdxl_lightning_1step_x0.safetensors) ä¸‹è½½åˆ° /ComfyUI/models/checkpointsã€‚\nä¸‹è½½æˆ‘ä»¬çš„ ComfyUI å®Œæ•´ 1 æ­¥å·¥ä½œæµç¨‹ã€‚\n\nå¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œ\n@misc{lin2024sdxllightning,\n      title={SDXL-Lightning: Progressive Adversarial Diffusion Distillation}, \n      author={Shanchuan Lin and Anran Wang and Xiao Yang},\n      year={2024},\n      eprint={2402.13929},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n\nå¥½çš„ï¼Œè¯·æä¾›æ‚¨éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ã€‚æˆ‘ä¼šå°½åŠ›å°†å…¶ç¿»è¯‘æˆé€šä¿—ã€ä¸“ä¸šã€ä¼˜é›…ä¸”æµç•…çš„ä¸­æ–‡ï¼Œå¹¶ä¿æŒåŸå§‹çš„ Markdown æ ¼å¼ã€‚",
    "tags": "[\"Diffusers\", \"Safetensors\", \"Open Rail++-M License\", \"text-to-image\", \"stable-diffusion\"]"
  },
  {
    "url": "https://gitcode.com/MooYeh/telechat_7b_ms",
    "project_name": "telechat_7b_ms",
    "readme": "æ˜Ÿè¾°è¯­ä¹‰å¤§æ¨¡å‹-TeleChat\n\nğŸ¤— Hugging Face â€¢ ğŸ” MindSpore â€¢ ğŸ¾ giteeï¸ â€¢ ğŸ’¬ WeChat\n\nTech Report\n\nä¿®æ”¹è¯´æ˜\nä¿®æ”¹äº†æ¨¡å‹æ¨ç†ç¤ºä¾‹ä»£ç \nå¢åŠ äº†å¿«é€Ÿå¼€å§‹ç« èŠ‚\næœ€æ–°åŠ¨æ€\n2024.3.20 å¼€æº12Bç‰ˆæœ¬chatæ¨¡å‹åŠé‡åŒ–ç‰ˆæœ¬\n2024.1.11 å¼€æº1Tä¸­æ–‡æ•°æ®é›†\n2024.1.10 å¼€æº7Bç‰ˆæœ¬chatæ¨¡å‹åŠå…¶é‡åŒ–ç‰ˆæœ¬\næ¨¡å‹ä»‹ç»\næ˜Ÿè¾°è¯­ä¹‰å¤§æ¨¡å‹-TeleChat\næ˜Ÿè¾°è¯­ä¹‰å¤§æ¨¡å‹TeleChatæ˜¯ç”±ä¸­ç”µä¿¡äººå·¥æ™ºèƒ½ç§‘æŠ€æœ‰é™å…¬å¸ç ”å‘è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå…¶ä¸­7Bæ¨¡å‹åŸºåº§é‡‡ç”¨1.5ä¸‡äº¿ Tokensä¸­è‹±æ–‡é«˜è´¨é‡è¯­æ–™è¿›è¡Œè®­ç»ƒï¼Œ12Bæ¨¡å‹åŸºåº§é‡‡ç”¨3ä¸‡äº¿ Tokensä¸­è‹±æ–‡é«˜è´¨é‡è¯­æ–™è¿›è¡Œè®­ç»ƒã€‚\næˆ‘ä»¬å¼€æºäº†å¯¹è¯æ¨¡å‹TeleChat-7B-botä¸TeleChat-12B-botï¼Œä»¥åŠå…¶huggingfaceæ ¼å¼çš„æƒé‡æ–‡ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€æºäº†7Bã€12Bæ¨¡å‹çš„int8å’Œint4é‡åŒ–ç‰ˆæœ¬ã€‚\nTeleChat-12B-botåœ¨æ¨¡å‹ç»“æ„ã€è®­ç»ƒæ•°æ®ã€è®­ç»ƒæ–¹æ³•ç­‰æ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼Œåœ¨é€šç”¨é—®ç­”å’ŒçŸ¥è¯†ç±»ã€ä»£ç ç±»ã€æ•°å­¦ç±»æ¦œå•ä¸Šç›¸æ¯”TeleChat-7B-botå‡æœ‰å¤§å¹…æå‡ã€‚åœ¨æ¨¡å‹ç»“æ„æ–¹é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨å°è§„æ¨¡çš„æ¨¡å‹å°è¯•å¤šç§æ¨¡å‹ç»“æ„çš„ç»„åˆï¼Œé€‰æ‹©æœ€ä¼˜ç»“æ„ã€‚ç›¸æ¯”TeleChat-7B-botæ¨¡å‹ï¼ŒTeleChat-12B-botæ¨¡å‹é‡‡ç”¨äº†è¯åµŒå…¥å±‚ä¸è¾“å‡ºå±‚è§£è€¦çš„ç»“æ„ï¼Œå°†è¯åµŒå…¥å±‚å’Œè¾“å‡ºlm headå±‚å‚æ•°åˆ†å¼€ï¼Œæœ‰åŠ©äºå¢å¼ºè®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚åœ¨è®­ç»ƒæ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬æ”¶é›†äº†è¦†ç›–ä¹¦ç±ã€ç™¾ç§‘ã€æ–°é—»ã€æ”¿åŠ¡ã€æ³•å¾‹ã€åŒ»è¯ã€ä¸“åˆ©ã€è®ºæ–‡ã€æ•°å­¦ã€ä»£ç ç­‰è¯¸å¤šæ–¹é¢çš„å¤§é‡ä¸­è‹±æ–‡æ•°æ®ï¼›é€šè¿‡ä¼˜åŒ–æ•°æ®æ¸…æ´—ç­–ç•¥å¤§å¹…æå‡æ•°æ®çš„æ–‡æœ¬å¹²å‡€åº¦ã€è§‚ç‚¹æ— åæ€§ã€å†…å®¹æœ‰æ•ˆæ€§ã€æ ¼å¼è§„èŒƒæ€§ã€‚åœ¨è®­ç»ƒæ–¹æ³•æ–¹é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨ç§‘å­¦æ•°æ®é…æ¯”å­¦ä¹ ä¸è¯¾ç¨‹å­¦ä¹ çš„æ–¹æ³•ï¼Œä½¿ç”¨å°å‚æ•°æ¨¡å‹åœ¨å¤šç§æ•°æ®é…æ¯”çš„æ•°æ®ä¸Šæ‹Ÿåˆï¼Œå¾—åˆ°å¯¹å„ä¸ªæ•°æ®é›†éš¾åº¦çš„å…ˆéªŒä¼°è®¡ï¼›è®­ç»ƒè¿‡ç¨‹ä¸­æ¯éš”ä¸€æ®µæ—¶é—´è‡ªåŠ¨åŒ–è¯„ä¼°å½“å‰æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„lossï¼Œä»¥åŠåœ¨è¯„æµ‹é›†ä¸Šçš„ç”Ÿæˆæ•ˆæœï¼ŒåŠ¨æ€æå‡è¾ƒéš¾å­¦ä¹ çš„æ•°æ®é›†æƒé‡ï¼Œä¿è¯æ¨¡å‹åœ¨å„ä¸ªæ•°æ®é›†ä¸Šéƒ½æœ‰è¾ƒä½³çš„æ‹Ÿåˆæ•ˆæœã€‚\næ¨¡å‹ç»“æ„\n\næˆ‘ä»¬é‡‡ç”¨æ ‡å‡†çš„ Decoder-only ç»“æ„è®¾è®¡äº† TeleChat æ¨¡å‹ï¼Œå¹¶åœ¨æ¨¡å‹ç»´åº¦åšäº†å¦‚ä¸‹çš„ä¸€äº›æ”¹è¿›ï¼š\n\nä½ç½®ç¼–ç ï¼šæˆ‘ä»¬ä½¿ç”¨ Rotary Embedding çš„ä½ç½®ç¼–ç æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ç›¸å¯¹ä½ç½®ä¿¡æ¯ä¾èµ–é›†æˆåˆ° self-attention ä¸­ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒå¥½çš„ä½ç½®å¤–æ¨æ€§ã€‚Rotary Embeddingè¿˜å¯ä»¥è¾ƒå¥½åœ°ä¸Flash-Attention v2 é…åˆä½¿ç”¨ï¼Œå°†æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦æå‡çº¦20%ã€‚\næ¿€æ´»å‡½æ•°ï¼šæˆ‘ä»¬ä½¿ç”¨ SwiGLU æ¿€æ´»å‡½æ•°æ¥æ›¿ä»£GELUæ¿€æ´»å‡½æ•° , ä¸ºäº†å‡å°‘è®¡ç®—é‡ï¼Œå°†ffn_hidden_sizeè®¾ç½®ä¸ºå°äºåŸå§‹SwiGLUä¸­çš„4å€éšè—å±‚å¤§å°ã€‚\nå±‚æ ‡å‡†åŒ–: åŸºäº RMSNorm çš„ Pre-Normalizationã€‚\nè¯åµŒå…¥å±‚ä¸è¾“å‡ºå±‚è§£è€¦ï¼šæˆ‘ä»¬å°†TeleChat-12B-botçš„è¯åµŒå…¥å±‚å’Œè¾“å‡ºlm headå±‚å‚æ•°åˆ†å¼€ï¼Œæœ‰åŠ©äºå¢å¼ºè®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚\n\tlayer_num\thidden_size\tffn_hidden_size\thead_num\ttie_word_embeddings\n7B\t30\t4096\t12288\t32\tæ˜¯\n12B\t38\t5120\t12288\t32\tå¦\n\næˆ‘ä»¬å¼€æºçš„TeleChatæ¨¡å‹ï¼š\n\næ”¯æŒdeepspeedå¾®è°ƒï¼Œå¼€æºäº†åŸºäºdeepspeedçš„è®­ç»ƒä»£ç ï¼Œæ”¯æŒZeroå¹¶è¡Œæ˜¾å­˜ä¼˜åŒ–ï¼ŒåŒæ—¶é›†æˆäº†FlashAttention2\nå¤šè½®èƒ½åŠ›æ”¯æŒã€‚å¼€æºäº†å¤šè½®æ•°æ®æ„å»ºæ–¹å¼ï¼Œé’ˆå¯¹å¤šè½®æ¨¡å‹è®­ç»ƒé›†æˆäº†é’ˆå¯¹å¤šè½®çš„mask lossè®­ç»ƒæ–¹å¼ï¼Œæ›´å¥½çš„èšç„¦å¤šè½®ç­”æ¡ˆï¼Œæå‡é—®ç­”æ•ˆæœã€‚\nå¤–æ¨èƒ½åŠ›æå‡ã€‚å¼€æºäº†8Kè®­ç»ƒç‰ˆæœ¬æ¨¡å‹ï¼Œé‡‡ç”¨NTK-awareå¤–æ¨å’Œattention scalingå¤–æ¨æ–¹å¼ï¼Œå¯ä»¥å¤–æ¨åˆ°96Kã€‚\nå…·å¤‡è¾ƒå¥½çš„é•¿æ–‡ç”Ÿæˆèƒ½åŠ›ã€‚åœ¨å·¥ä½œæ€»ç»“ã€å·¥ä½œè®¡åˆ’ã€PPTå¤§çº²ã€ç”³è®ºã€æ‹›æ ‡ä¹¦ã€é‚®ä»¶ã€æ–¹æ¡ˆã€å‘¨æŠ¥ã€JDå†™ä½œç­‰é•¿æ–‡å†™ä½œä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ã€‚\n\næœ¬æ¬¡å‘å¸ƒç‰ˆæœ¬å’Œä¸‹è½½é“¾æ¥è§ä¸‹è¡¨\n\næ¨¡å‹ç‰ˆæœ¬\tä¸‹è½½é“¾æ¥\n7B-FP16\tTeleChat-7B-FP16\n7B-int8\tTeleChat-7B-int8\n7B-int4\tTeleChat-7B-int4\n12B-FP16\tTeleChat-12B-FP16\n12B-int8\tTeleChat-12B-int8\n12B-int4\tTeleChat-12B-int4\næ•°æ®å¼€æº\næ•°æ®ä»‹ç»\n\nTeleChat-PTD æ˜¯ç”±ç”µä¿¡æ˜Ÿè¾°å¤§æ¨¡å‹TeleChaté¢„è®­ç»ƒè¯­æ–™ä¸­æŠ½å–å‡ºçš„çš„ç»¼åˆæ€§å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®é›†ã€‚æ•°æ®ä¸»è¦æ¥æºäºç½‘é¡µã€ä¹¦ç±ã€å®˜æ–¹åª’ä½“ç­‰ã€‚ æˆ‘ä»¬ä½¿ç”¨è§„åˆ™+æ¨¡å‹çš„æ–¹å¼è¿›è¡Œäº†ç›¸å…³çš„è¿‡æ»¤ï¼Œå¹¶å¯¹æ•°æ®è¿›è¡Œäº†ç›¸ä¼¼æ€§å»é‡ï¼Œå°½å¯èƒ½åœ°æå–å‡ºé«˜è´¨é‡åœ°æ•°æ®ã€‚\n\nTeleChat-PTD æ•°æ®é›†å¤§çº¦å…¬å¼€äº†2.7äº¿æ¡æ•°æ®ï¼Œæ•°æ®ç”±çº¯ä¸­æ–‡æ–‡æœ¬æ„æˆæ„æˆï¼ŒåŸå§‹å¤§å°çº¦1TB,å‹ç¼©å480Gï¼Œå…±189ä¸ªæ–‡ä»¶ã€‚æ•°æ®é›†ä¸­å·²ç»å»é™¤äº†å…¶å®ƒå†—ä½™ä¿¡æ¯ã€‚\n\næ•°æ®ä¸‹è½½\n\nhuggingfaceä¸‹è½½åœ°å€ï¼šTeleChat-PTD\n\nå¤©ç¿¼äº‘ç›˜ä¸‹è½½åœ°å€ï¼šæ•°æ®ä¸‹è½½ï¼ˆè®¿é—®ç ï¼špkg8ï¼‰\n\næ•ˆæœè¯„æµ‹\n\nTeleChatæ¨¡å‹ç›¸æ¯”åŒè§„æ¨¡æ¨¡å‹åœ¨è¯„æµ‹æ•ˆæœæ–¹é¢ä¹Ÿæœ‰è¾ƒå¥½çš„è¡¨ç°ï¼Œæˆ‘ä»¬çš„è¯„æµ‹é›†æ¶µç›–äº†åŒ…æ‹¬MMLUã€C-Evalã€GAOKAOã€AGIEvalã€CMMLUã€ GSM8Kã€MATHã€HumanEvalã€CHIDç­‰æ•°æ®é›†ï¼Œè¯„æµ‹èƒ½åŠ›åŒ…æ‹¬äº†è‡ªç„¶è¯­è¨€ç†è§£ã€çŸ¥è¯†ã€æ•°å­¦è®¡ç®—å’Œæ¨ç†ã€ä»£ç ç”Ÿæˆç­‰\n\nè¯„æµ‹ç»“æœå¦‚ä¸‹\nModel\tMMLU\tC-Eval\tCMMLU\tAGIEval\tGAOKAO\tGSM8K\tMATH\tHumanEval\tCSL\tCHID\tEPRSTMT\tBBH\tHellaSwag\n\t5-shot\t5-shot\t5-shot\tzero-shot\tzero-shot\t4-shot\t4-shot\tzero-shot\tzero-shot\tzero-shot\tzero-shot\t3-shot\tzero-shot\nLLaMA2-7B-chat\t46.2\t31.9\t31.5\t28.5\t16.1\t26.3\t3.9\t12.2\t58.8\t44.1\t57.5\t35.6\t74.1\nLLaMA2-13B-chat\t54.6\t36.2\t38.7\t32.3\t18.6\t29.6\t5.0\t18.9\t61.2\t48.0\t59.4\t40.2\t78.2\nChatGLM2-6B-chat\t45.9\t52.6\t49.3\t39.0\t46.4\t28.8\t6.5\t11.0\t61.2\t57.9\t71.2\t32.7\t57.0\nChatGLM3-6B-chat\t51.9\t53.8\t54\t38.9\t49.3\t56.7\t18.7\t61\t65.6\t63.4\t85\t44.6\t62.7\nBaichuan2-7B-chat\t52.8\t55.6\t54.0\t35.3\t39.7\t32.8\t6\t13.4\t60\t75.2\t87.5\t35.8\t61.6\nBaichuan2-13B-chat\t57\t56.7\t58.4\t40\t51.4\t55.3\t8.6\t17.7\t63.1\t78.2\t87.5\t49.9\t66.9\nQwen-7B-chat\t56.6\t59.3\t59.5\t41.3\t63.3\t52.5\t10.3\t26.2\t63.1\t72.3\t88.8\t46.9\t59.9\nQwen-14B-chat\t66.4\t71.7\t70.0\t47.3\t76.5\t61.0\t26.8\t36.6\t55.6\t72.3\t91.2\t58.0\t65.2\nTeleChat-7B-chat\t60.5\t64.6\t64.3\t46.8\t59\t36.7\t10.3\t20.1\t66.8\t88.0\t87.5\t19.5\t36.7\nTeleChat-12B-chat\t73.3\t66.6\t74.2\t51.7\t53.1\t57.2\t16.0\t22.0\t60.6\t83.2\t86.3\t52.2\t71.5\n\nè¯´æ˜ï¼šCMMLUã€AGIEvalã€GAOKAOã€CSLã€CHIDã€EPRSTMTå‡åŸºäºOpenCompasså¹³å°æä¾›çš„è¯„æµ‹æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼Œè€Œå¯¹äºå¯¹æ¯”æ¨¡å‹ï¼Œæˆ‘ä»¬åŒæ—¶å‚è€ƒäº†å®˜æ–¹æ±‡æŠ¥ç»“æœå’ŒOpenCompassç»“æœã€‚æˆ‘ä»¬ä½¿ç”¨äº†è‡ªå·±çš„è¯„æµ‹è„šæœ¬è¯„æµ‹MMLUä¸CEVALæ¦œå•ï¼Œå…·ä½“æ–¹æ³•è§evaluation/æ–‡ä»¶å¤¹ã€‚\n\næ¨¡å‹æ¨ç†\n>>> import os\n>>> os.environ[\"OPENMIND_FRAMEWORK\"] = \"ms\"\n>>> from openmind import AutoModelForCausalLM, AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained('openmind/telechat_7b_ms', trust_remote_code=True)\n>>> model = AutoModelForCausalLM.from_pretrained('openmind/telechat_7b_ms', trust_remote_code=True)\n>>> question = \"<_user>{}<_bot>\".format(\"ç”ŸæŠ½ä¸è€æŠ½çš„åŒºåˆ«ï¼Ÿ\")\n>>> inputs = tokenizer(question)[\"input_ids\"]\n>>> outputs = model.generate(inputs, do_sample=True, top_k=3)\n>>> response = tokenizer.decode(outputs)\n>>> print(response[0])\n<_user>ç”ŸæŠ½ä¸è€æŠ½çš„åŒºåˆ«ï¼Ÿ<_bot>ç”ŸæŠ½ä¸è€æŠ½çš„åŒºåˆ«å¦‚ä¸‹ï¼š\n\n1.ç”ŸæŠ½æ˜¯ç”¨é»„è±†ã€æ·€ç²‰ç­‰åŸæ–™åŠ å·¥è€Œæˆï¼Œé¢œè‰²è¾ƒæ·±ï¼Œå£æ„Ÿè¾ƒå’¸ã€‚ç”ŸæŠ½å¸¸ä½œä¸ºè°ƒå‘³æ–™ä½¿ç”¨ã€‚\n\n2.è€æŠ½æ˜¯ç”¨é…±æ²¹ã€è€æŠ½ã€é£Ÿç›ç»è¿‡ä¸€å®šæ¯”ä¾‹çš„æ··åˆã€å‘é…µã€æè‰²è€Œæˆï¼Œé¢œè‰²è¾ƒæµ…ï¼Œå£æ„Ÿè¾ƒå’¸ã€‚è€æŠ½å¸¸ä½œä¸ºè°ƒå‘³æ–™ä½¿ç”¨ã€‚\n\næ€»ä½“æ¥è¯´ï¼Œç”ŸæŠ½ä¸è€æŠ½éƒ½æ˜¯å¸¸ç”¨çš„è°ƒå‘³å“ï¼Œç”ŸæŠ½é¢œè‰²æ·±ã€å’¸å‘³é‡ï¼Œé€‚åˆç”¨äºçƒ¹è°ƒèœå“ï¼›è€Œè€æŠ½é¢œè‰²æµ…ã€å’¸å‘³è½»ï¼Œé€‚åˆç”¨äºçƒ¹è°ƒæ±¤å“ã€‚åœ¨é€‰æ‹©è°ƒå‘³å“æ—¶ï¼Œåº”æ ¹æ®èœå“çš„å£å‘³å’Œé£å‘³éœ€æ±‚æ¥é€‰æ‹©åˆé€‚çš„è°ƒå‘³å“ã€‚<_end>\n\nå¿«é€Ÿå¼€å§‹\næ•°æ®é›†å‡†å¤‡\n\nstep 1. è·å–æ•°æ®é›†\n\næ•°æ®é›†\n\næ•°æ®é›†çš„æ ¼å¼ï¼š\n\n# input_dataset examples:\n    {\"input\": \"ç”µä¿¡ä¸»å¡å’Œå‰¯å¡çš„åŒºåˆ«åœ¨å“ªé‡Œï¼Ÿ\", \"output\": \"ä¸»å¡å’Œå‰¯å¡çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œä¸»å¡åªèƒ½ä½¿ç”¨ä¸€å¼ æ‰‹æœºå·ç ã€‚<_end>\"}\n\n\nstep 2. å¤„ç†æ•°æ®æˆmindrecordæ ¼å¼\n\ncd example/dataset\n# éœ€è¦æå‰ä¸‹è½½transformers\npython telechat_preprocess.py \\\n--input_dataset_file /{path}/input_dataset.jsonl \\\n--max_length 2048 \\\n--output_path /{path}/input_dataset.mindrecord\n# å‚æ•°è¯´æ˜\ninput_dataset_file: é¢„è®­ç»ƒçš„æ•°æ®é›†\nvocab_file_path: è¯æ¨¡å‹æ–‡ä»¶è·¯å¾„\nmax_length: æ•°æ®é›†é•¿åº¦\noutput_path: ç”Ÿæˆæ•°æ®é›†çš„è·¯å¾„\n\nè®­ç»ƒ\ncd example\nbash msrun.sh \"finetune.py --train_dataset /{path}/input_dataset.mindrecord\"\n\næ¨ç†\ncd example\npython inference.py\n\nå£°æ˜ã€åè®®ã€å¼•ç”¨\nå£°æ˜\n\næˆ‘ä»¬åœ¨æ­¤å£°æ˜ï¼Œä¸è¦ä½¿ç”¨TeleChatæ¨¡å‹åŠå…¶è¡ç”Ÿæ¨¡å‹è¿›è¡Œä»»ä½•å±å®³å›½å®¶ç¤¾ä¼šå®‰å…¨æˆ–è¿æ³•çš„æ´»åŠ¨ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿè¦æ±‚ä½¿ç”¨è€…ä¸è¦å°†TeleChatæ¨¡å‹ç”¨äºæ²¡æœ‰å®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰ä½¿ç”¨è€…éµå®ˆä¸Šè¿°åŸåˆ™ï¼Œç¡®ä¿ç§‘æŠ€å‘å±•åœ¨åˆæ³•åˆè§„çš„ç¯å¢ƒä¸‹è¿›è¡Œã€‚\n\næˆ‘ä»¬å·²ç»å°½æˆ‘ä»¬æ‰€èƒ½ï¼Œæ¥ç¡®ä¿æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®çš„åˆè§„æ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡æˆ‘ä»¬å·²ç»åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å’Œæ•°æ®çš„å¤æ‚æ€§ï¼Œä»æœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ— æ³•é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚æœç”±äºä½¿ç”¨TeleChatå¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n\nåè®®\n\nç¤¾åŒºä½¿ç”¨ TeleChat æ¨¡å‹éœ€è¦éµå¾ªã€ŠTeleChatæ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹ã€‚TeleChatæ¨¡å‹æ”¯æŒå•†ä¸šç”¨é€”ï¼Œå¦‚æœæ‚¨è®¡åˆ’å°† TeleChat æ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨äºå•†ä¸šç›®çš„ï¼Œæ‚¨éœ€è¦é€šè¿‡ä»¥ä¸‹è”ç³»é‚®ç®± tele_ai@chinatelecom.cnï¼Œæäº¤ã€ŠTeleChatæ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹è¦æ±‚çš„ç”³è¯·ææ–™ã€‚å®¡æ ¸é€šè¿‡åï¼Œå°†ç‰¹æ­¤æˆäºˆæ‚¨ä¸€ä¸ªéæ’ä»–æ€§ã€å…¨çƒæ€§ã€ä¸å¯è½¬è®©ã€ä¸å¯å†è®¸å¯ã€å¯æ’¤é”€çš„å•†ç”¨ç‰ˆæƒè®¸å¯ã€‚\n\nå¼•ç”¨\n\nå¦‚éœ€å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œï¼Œè¯·ä½¿ç”¨å¦‚ä¸‹ reference:\n\n@misc{wang2024telechat,\n      title={TeleChat Technical Report}, \n      author={Zihan Wang and Xinzhang Liu and Shixuan Liu and Yitong Yao and Yuyao Huang and Zhongjiang He and Xuelong Li and Yongxiang Li and Zhonghao Che and Zhaoxi Zhang and Yan Wang and Xin Wang and Luwen Pu and Huihan Xu and Ruiyu Fang and Yu Zhao and Jie Zhang and Xiaomeng Huang and Zhilong Lu and Jiaxin Peng and Wenjun Zheng and Shiquan Wang and Bingkai Yang and Xuewei he and Zhuoru Jiang and Qiyi Xie and Yanhan Zhang and Zhongqiu Li and Lingling Shi and Weiwei Fu and Yin Zhang and Zilu Huang and Sishi Xiong and Yuxiang Zhang and Chao Wang and Shuangyong Song},\n      year={2024},\n      eprint={2401.03804},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n",
    "tags": "[\"Text Generation\", \"TensorFlow\", \"Transformers\", \"MindSpore\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/albert_xlarge_v2",
    "project_name": "albert_xlarge_v2",
    "readme": "Original Text\nä¿®æ”¹å†…å®¹\nä¿®æ”¹ç¤ºä¾‹å¹¶æ·»åŠ  npu æ”¯æŒï¼›\nä¿®æ”¹â€œé™åˆ¶ä¸åå·®â€å’Œâ€œé¢„æœŸç”¨é€”ä¸é™åˆ¶â€éƒ¨åˆ†ã€‚\nALBERT XLarge v2\n\nè¿™æ˜¯ä¸€æ¬¾åŸºäºè‹±è¯­è¯­è¨€çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨äº†é®è”½è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰çš„ç›®æ ‡å‡½æ•°ã€‚è¯¥æ¨¡å‹åœ¨ è¿™ç¯‡è®ºæ–‡ä¸­æå‡ºï¼Œå¹¶é¦–æ¬¡åœ¨ è¿™ä¸ªä»£ç åº“ä¸­å‘å¸ƒã€‚ä¸æ‰€æœ‰ALBERTæ¨¡å‹ä¸€æ ·ï¼Œè¯¥æ¨¡å‹ä¸åŒºåˆ†å¤§å°å†™ï¼šå®ƒå¯¹å¾…englishå’ŒEnglishæ²¡æœ‰å·®å¼‚ã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒALBERTçš„å›¢é˜Ÿå¹¶æœªä¸ºæ­¤æ¨¡å‹ç¼–å†™æ¨¡å‹å¡ï¼Œå› æ­¤æ­¤æ¨¡å‹å¡ç”± Hugging Face å›¢é˜Ÿæ’°å†™ã€‚\n\næ¨¡å‹æè¿°\n\nALBERT æ˜¯ä¸€ç§åœ¨å¤§é‡è‹±æ–‡æ•°æ®é›†ä¸Šä»¥è‡ªç›‘ç£æ–¹å¼é¢„è®­ç»ƒçš„è½¬æ¢å™¨æ¨¡å‹ã€‚è¿™æ„å‘³ç€å®ƒæ˜¯ä»…å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œé¢„è®­ç»ƒï¼Œè€Œæ²¡æœ‰é€šè¿‡ä»»ä½•äººå·¥æ ‡æ³¨çš„æ–¹å¼ï¼ˆè¿™ä¹Ÿæ˜¯å®ƒå¯ä»¥ä½¿ç”¨å¤§é‡å…¬å¼€å¯ç”¨æ•°æ®çš„åŸå› ï¼‰å¹¶é€šè¿‡è‡ªåŠ¨æµç¨‹ä»è¿™äº›æ–‡æœ¬ç”Ÿæˆè¾“å…¥å’Œæ ‡ç­¾ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒæ˜¯é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒçš„ï¼š\n\né®è”½è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ï¼šæ¨¡å‹æ¥æ”¶ä¸€ä¸ªå¥å­ï¼Œéšæœºé®è”½è¾“å…¥ä¸­çš„15%å•è¯ï¼Œç„¶åè®©æ•´ä¸ªé®è”½å¥å­é€šè¿‡æ¨¡å‹ï¼Œå¹¶é¢„æµ‹é®è”½çš„å•è¯ã€‚è¿™ä¸ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ä¸åŒï¼Œåè€…é€šå¸¸ä¸€æ¬¡çœ‹åˆ°ä¸€è¯ï¼Œæˆ–è€…ä¸åƒGPTè¿™æ ·çš„è‡ªå›å½’æ¨¡å‹å†…éƒ¨é®è”½æœªæ¥æ ‡è®°ä¸åŒã€‚å®ƒå…è®¸æ¨¡å‹å­¦ä¹ å¥å­åŒå‘è¡¨ç¤ºã€‚\nå¥å­é¡ºåºé¢„æµ‹ï¼ˆSOPï¼‰ï¼šALBERT ä½¿ç”¨åŸºäºé¢„æµ‹ä¸¤ä¸ªè¿ç»­æ–‡æœ¬ç‰‡æ®µé¡ºåºçš„é¢„è®­ç»ƒæŸå¤±ã€‚\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å­¦ä¹ äº†è‹±è¯­è¯­è¨€çš„å†…éƒ¨è¡¨ç¤ºï¼Œç„¶åå¯ä»¥ç”¨äºæå–å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾ï¼šä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªæ ‡è®°çš„å¥å­æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨ALBERTæ¨¡å‹äº§ç”Ÿçš„ç‰¹å¾ä½œä¸ºè¾“å…¥æ¥è®­ç»ƒä¸€ä¸ªæ ‡å‡†åˆ†ç±»å™¨ã€‚\n\nALBERT çš„ç‰¹åˆ«ä¹‹å¤„åœ¨äºå®ƒåœ¨ Transformer ä¸­å…±äº«å…¶å±‚ã€‚å› æ­¤ï¼Œæ‰€æœ‰å±‚éƒ½æœ‰ç›¸åŒçš„æƒé‡ã€‚ä½¿ç”¨é‡å¤å±‚å¯ä»¥å‡å°‘å†…å­˜å ç”¨ï¼Œä½†è®¡ç®—æˆæœ¬ä¸å…·æœ‰ç›¸åŒæ•°é‡çš„éšè—å±‚çš„BERTç±»æ¶æ„ç›¸ä¼¼ï¼Œå› ä¸ºå®ƒéœ€è¦éå†ç›¸åŒæ•°é‡çš„ï¼ˆé‡å¤çš„ï¼‰å±‚ã€‚\n\nè¿™æ˜¯xlargeæ¨¡å‹çš„ç¬¬äºŒä¸ªç‰ˆæœ¬ã€‚ç‰ˆæœ¬2ä¸ç‰ˆæœ¬1ä¸åŒï¼Œå› ä¸ºæœ‰ä¸åŒçš„ä¸¢å¼ƒç‡ã€é¢å¤–çš„è®­ç»ƒæ•°æ®å’Œæ›´é•¿çš„è®­ç»ƒæ—¶é—´ã€‚å®ƒåœ¨å‡ ä¹æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šéƒ½æœ‰æ›´å¥½çš„ç»“æœã€‚\n\næ­¤æ¨¡å‹å…·æœ‰ä»¥ä¸‹é…ç½®ï¼š\n\n24ä¸ªé‡å¤å±‚\n128ç»´åµŒå…¥\n2048ç»´éšè—å±‚\n16ä¸ªæ³¨æ„åŠ›å¤´\n5800ä¸‡ä¸ªå‚æ•°\né¢„æœŸç”¨é€”ä¸é™åˆ¶\n\næ³¨æ„ï¼Œæ­¤æ¨¡å‹ä¸»è¦æ—¨åœ¨å¯¹æ•´ä¸ªå¥å­ï¼ˆå¯èƒ½æ˜¯é®è”½çš„ï¼‰è¿›è¡Œå¾®è°ƒä»¥åšå‡ºå†³ç­–çš„ä»»åŠ¡ä¸Šä½¿ç”¨ï¼Œå¦‚åºåˆ—åˆ†ç±»ã€æ ‡è®°åˆ†ç±»æˆ–é—®é¢˜å›ç­”ã€‚å¯¹äºåƒæ–‡æœ¬ç”Ÿæˆè¿™æ ·çš„ä»»åŠ¡ï¼Œæ‚¨åº”è¯¥è€ƒè™‘ä½¿ç”¨GPT2ç­‰æ¨¡å‹ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\næ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤æ¨¡å‹ä¸é®è”½è¯­è¨€æ¨¡å‹çš„ç®¡é“ï¼š\n\nimport torch\nfrom openmind import pipeline, is_torch_npu_available\n\nunmasker = pipeline('fill-mask', device_map=\"npu:0\", model='PyTorch-NPU/albert_xlarge_v2')\nunmasker(\"Hello I'm a [MASK] model.\")\n\nå±€é™æ€§ä¸åè§\n\næ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡æˆ–ä¸‹ä¸€å¥é¢„æµ‹ï¼Œä½†å®ƒä¸»è¦ç›®çš„æ˜¯åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n\nå³ä½¿ç”¨äºæ­¤æ¨¡å‹çš„è®­ç»ƒæ•°æ®å¯ä»¥è¢«è®¤ä¸ºæ˜¯ç›¸å¯¹ä¸­ç«‹çš„ï¼Œè¯¥æ¨¡å‹ä¹Ÿå¯èƒ½äº§ç”Ÿæœ‰åè§çš„é¢„æµ‹ã€‚è¿™ç§åè§è¿˜ä¼šå½±å“æ‰€æœ‰åŸºäºæ­¤æ¨¡å‹çš„å¾®è°ƒç‰ˆæœ¬ã€‚\n\nè®­ç»ƒæ•°æ®\n\nALBERT æ¨¡å‹åœ¨ BookCorpus æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…æ‹¬ 11,038 æœ¬æœªå‘è¡¨ä¹¦ç±ä»¥åŠ è‹±è¯­ç»´åŸºç™¾ç§‘ï¼ˆä¸åŒ…æ‹¬åˆ—è¡¨ã€è¡¨æ ¼å’Œæ ‡é¢˜ï¼‰ã€‚\n\nè®­ç»ƒè¿‡ç¨‹\né¢„å¤„ç†\n\næ–‡æœ¬ä¼šè¢«è½¬æ¢ä¸ºå°å†™ï¼Œå¹¶ä½¿ç”¨ SentencePiece å’Œ 30,000 çš„è¯æ±‡é‡è¿›è¡Œåˆ†è¯ã€‚æ¨¡å‹çš„è¾“å…¥å½¢å¼å¦‚ä¸‹ï¼š\n\n[CLS] Sentence A [SEP] Sentence B [SEP]\n\nè®­ç»ƒ\n\nALBERT ç®—æ³•çš„è®­ç»ƒæµç¨‹éµå¾ª BERT çš„è®¾ç½®ã€‚\n\nå¯¹äºæ¯ä¸ªå¥å­çš„æ©ç è¿‡ç¨‹ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n15% çš„æ ‡è®°è¢«æ©ç ã€‚\nåœ¨ 80% çš„æƒ…å†µä¸‹ï¼Œæ©ç çš„æ ‡è®°è¢«æ›¿æ¢ä¸º `\n@article{DBLP:journals/corr/abs-1909-11942,\n  author    = {Zhenzhong Lan and\n               Mingda Chen and\n               Sebastian Goodman and\n               Kevin Gimpel and\n               Piyush Sharma and\n               Radu Soricut},\n  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language\n               Representations},\n  journal   = {CoRR},\n  volume    = {abs/1909.11942},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1909.11942},\n  archivePrefix = {arXiv},\n  eprint    = {1909.11942},\n  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n\nè¯·æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬å’Œç›¸åº”çš„ Markdown æ ¼å¼ï¼Œæˆ‘å°†ä¼šæŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"English\", \"Apache License 2.0\", \"bookcorpus\", \"wikipedia\"]"
  },
  {
    "url": "https://gitcode.com/openMind/bert_base_cased",
    "project_name": "bert_base_cased",
    "readme": "Original Text\n\nè¯­è¨€: è‹±è¯­\næ ‡ç­¾:\n\nexbert\nè®¸å¯è¯: Apache-2.0\næ•°æ®é›†:\nBookCorpus\nWikipedia\nä»»åŠ¡æ ‡ç­¾: å¡«å……æ©ç \næ¡†æ¶:\nPyTorch\nPTA: å¾…ç¡®è®¤å•†ç”¨å‘å¸ƒç‰ˆæœ¬\nCANN: å¾…ç¡®è®¤å•†ç”¨å‘å¸ƒç‰ˆæœ¬\nBERTåŸºç¡€æ¨¡å‹ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰\n\næœ¬æ¨¡å‹é‡‡ç”¨æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ç›®æ ‡å¯¹è‹±è¯­è¿›è¡Œé¢„è®­ç»ƒï¼Œé¦–æ¬¡å‘è¡¨äºè¿™ç¯‡è®ºæ–‡ï¼Œå¹¶åœ¨æ­¤ä»£ç åº“ä¸­å¼€æºã€‚è¯¥æ¨¡å‹åŒºåˆ†å¤§å°å†™ï¼šä¾‹å¦‚\"english\"å’Œ\"English\"ä¼šè¢«è§†ä¸ºä¸åŒè¯æ±‡ã€‚\n\nå…è´£å£°æ˜ï¼šBERTå‘å¸ƒå›¢é˜Ÿæœªæä¾›æœ¬æ¨¡å‹çš„è¯´æ˜æ–‡æ¡£ï¼Œå½“å‰æ–‡æ¡£ç”±Hugging Faceå›¢é˜Ÿæ’°å†™ã€‚\n\næ¨¡å‹æè¿°\n\nBERTæ˜¯åŸºäºTransformeræ¶æ„çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡è‡ªç›‘ç£æ–¹å¼åœ¨æµ·é‡è‹±æ–‡è¯­æ–™ä¸Šè®­ç»ƒè€Œæˆã€‚è¿™æ„å‘³ç€å®ƒä»…é€šè¿‡åŸå§‹æ–‡æœ¬è¿›è¡Œé¢„è®­ç»ƒï¼Œæ— éœ€äººå·¥æ ‡æ³¨ï¼ˆå› æ­¤å¯åˆ©ç”¨å¤§é‡å…¬å¼€æ•°æ®ï¼‰ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ä»æ–‡æœ¬ç”Ÿæˆè¾“å…¥å’Œæ ‡ç­¾ã€‚å…·ä½“è€Œè¨€ï¼Œå…¶é¢„è®­ç»ƒåŒ…å«ä¸¤å¤§ç›®æ ‡ï¼š\n\næ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ï¼šæ¨¡å‹éšæœºé®è”½è¾“å…¥å¥å­ä¸­15%çš„è¯æ±‡ï¼Œé€šè¿‡å¤„ç†æ•´ä¸ªè¢«é®è”½çš„å¥å­æ¥é¢„æµ‹ç¼ºå¤±è¯æ±‡ã€‚ä¸ä¼ ç»Ÿå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰é€è¯å¤„ç†æˆ–GPTç±»è‡ªå›å½’æ¨¡å‹ï¼ˆå†…éƒ¨é®è”½æœªæ¥è¯å…ƒï¼‰ä¸åŒï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹èƒ½å­¦ä¹ å¥å­çš„åŒå‘è¡¨å¾ã€‚\nä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNSPï¼‰ï¼šé¢„è®­ç»ƒæ—¶å°†ä¸¤ä¸ªé®è”½åçš„å¥å­æ‹¼æ¥ä½œä¸ºè¾“å…¥ï¼Œæ¨¡å‹éœ€åˆ¤æ–­è¿™ä¸¤ä¸ªå¥å­åœ¨åŸæ–‡ä¸­æ˜¯å¦ç›¸é‚»ã€‚\n\né€šè¿‡è¿™ç§è®¾è®¡ï¼Œæ¨¡å‹å¯å­¦ä¹ è‹±è¯­çš„å†…åœ¨è¡¨å¾ï¼Œè¿›è€Œæå–é€‚ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„ç‰¹å¾ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå·²æ ‡æ³¨çš„å¥å­æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨BERTç”Ÿæˆçš„ç‰¹å¾ä½œä¸ºè¾“å…¥æ¥è®­ç»ƒæ ‡å‡†åˆ†ç±»å™¨ã€‚\n\nåº”ç”¨åœºæ™¯ä¸é™åˆ¶\n\næ‚¨å¯ç›´æ¥å°†åŸå§‹æ¨¡å‹ç”¨äºæ©ç è¯­è¨€å»ºæ¨¡æˆ–ä¸‹ä¸€å¥é¢„æµ‹ï¼Œä½†å…¶ä¸»è¦ç”¨é€”æ˜¯åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n\néœ€æ³¨æ„ï¼Œæœ¬æ¨¡å‹ä¸»è¦é€‚ç”¨äºéœ€è¦åŸºäºå®Œæ•´å¥å­ï¼ˆå¯èƒ½å«æ©ç ï¼‰è¿›è¡Œå†³ç­–çš„ä»»åŠ¡ï¼Œå¦‚åºåˆ—åˆ†ç±»ã€è¯å…ƒåˆ†ç±»æˆ–é—®ç­”ç³»ç»Ÿã€‚è‹¥éœ€æ–‡æœ¬ç”ŸæˆåŠŸèƒ½ï¼Œå»ºè®®è€ƒè™‘GPT2ç­‰æ¨¡å‹ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\næ‚¨å¯é€šè¿‡ä»¥ä¸‹æµæ°´çº¿ç›´æ¥ä½¿ç”¨æœ¬æ¨¡å‹è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡ï¼š\n\n>>> from openmind import pipeline\n>>> unmasker = pipeline('fill-mask', model='PyTorch-NPU/bert_base_cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] Hello I'm a fashion model. [SEP]\",\n  'score': 0.09019174426794052,\n  'token': 4633,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] Hello I'm a new model. [SEP]\",\n  'score': 0.06349995732307434,\n  'token': 1207,\n  'token_str': 'new'},\n {'sequence': \"[CLS] Hello I'm a male model. [SEP]\",\n  'score': 0.06228214129805565,\n  'token': 2581,\n  'token_str': 'male'},\n {'sequence': \"[CLS] Hello I'm a professional model. [SEP]\",\n  'score': 0.0441727414727211,\n  'token': 1848,\n  'token_str': 'professional'},\n {'sequence': \"[CLS] Hello I'm a super model. [SEP]\",\n  'score': 0.03326151892542839,\n  'token': 7688,\n  'token_str': 'super'}]\n\n\nä»¥ä¸‹æ˜¯ä½¿ç”¨è¯¥æ¨¡å‹åœ¨ PyTorch ä¸­è·å–ç»™å®šæ–‡æœ¬ç‰¹å¾çš„æ–¹æ³•ï¼š\n\nimport openmind\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('PyTorch-NPU/bert_base_cased')\nmodel = BertModel.from_pretrained(\"PyTorch-NPU/bert_base_cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n\nå±€é™æ€§ä¸åå·®\n\nå°½ç®¡ç”¨äºè®­ç»ƒè¯¥æ¨¡å‹çš„æ•°æ®å¯ä»¥è¢«è§†ä¸ºç›¸å¯¹ä¸­ç«‹ï¼Œä½†è¯¥æ¨¡å‹ä»å¯èƒ½äº§ç”Ÿå¸¦æœ‰åå·®çš„é¢„æµ‹ï¼š\n\n>>> from openmind import pipeline\n>>> unmasker = pipeline('fill-mask', model='PyTorch-NPU/bert_base_cased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] The man worked as a lawyer. [SEP]',\n  'score': 0.04804691672325134,\n  'token': 4545,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] The man worked as a waiter. [SEP]',\n  'score': 0.037494491785764694,\n  'token': 17989,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] The man worked as a cop. [SEP]',\n  'score': 0.035512614995241165,\n  'token': 9947,\n  'token_str': 'cop'},\n {'sequence': '[CLS] The man worked as a detective. [SEP]',\n  'score': 0.031271643936634064,\n  'token': 9140,\n  'token_str': 'detective'},\n {'sequence': '[CLS] The man worked as a doctor. [SEP]',\n  'score': 0.027423162013292313,\n  'token': 3995,\n  'token_str': 'doctor'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] The woman worked as a nurse. [SEP]',\n  'score': 0.16927455365657806,\n  'token': 7439,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] The woman worked as a waitress. [SEP]',\n  'score': 0.1501094549894333,\n  'token': 15098,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] The woman worked as a maid. [SEP]',\n  'score': 0.05600163713097572,\n  'token': 13487,\n  'token_str': 'maid'},\n {'sequence': '[CLS] The woman worked as a housekeeper. [SEP]',\n  'score': 0.04838843643665314,\n  'token': 26458,\n  'token_str': 'housekeeper'},\n {'sequence': '[CLS] The woman worked as a cook. [SEP]',\n  'score': 0.029980547726154327,\n  'token': 9834,\n  'token_str': 'cook'}]\n\n\nè¿™ç§åå·®åŒæ ·ä¼šå½±å“è¯¥æ¨¡å‹çš„æ‰€æœ‰å¾®è°ƒç‰ˆæœ¬ã€‚\n\nè®­ç»ƒæ•°æ®\n\nBERTæ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®æ¥æºäºä¸¤ä¸ªéƒ¨åˆ†ï¼šBookCorpusï¼ˆåŒ…å«11,038æœ¬æœªå‡ºç‰ˆä¹¦ç±çš„è¯­æ–™åº“ï¼‰ä»¥åŠè‹±æ–‡ç»´åŸºç™¾ç§‘ï¼ˆæ’é™¤äº†åˆ—è¡¨ã€è¡¨æ ¼å’Œæ ‡é¢˜éƒ¨åˆ†ï¼‰ã€‚\n\nè®­ç»ƒæµç¨‹\né¢„å¤„ç†\n\næ–‡æœ¬é€šè¿‡WordPieceæ–¹æ³•è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œè¯æ±‡è¡¨è§„æ¨¡ä¸º30,000ã€‚æ¨¡å‹è¾“å…¥æ ¼å¼å¦‚ä¸‹ï¼š\n\n[CLS] Sentence A [SEP] Sentence B [SEP]\n\n\nåœ¨50%çš„æ¦‚ç‡ä¸‹ï¼Œå¥å­Aå’Œå¥å­Bå¯¹åº”åŸå§‹è¯­æ–™åº“ä¸­ä¸¤ä¸ªè¿ç»­çš„å¥å­ï¼›å…¶ä½™æƒ…å†µä¸‹ï¼Œåˆ™éšæœºé€‰å–è¯­æ–™åº“ä¸­çš„å¦ä¸€ä¸ªå¥å­ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œæ‰€è°“çš„\"å¥å­\"é€šå¸¸æ˜¯æŒ‡ä¸€æ®µè¿ç»­çš„æ–‡æœ¬ï¼Œå…¶é•¿åº¦å¾€å¾€è¶…è¿‡å•ä¸ªå¥å­ã€‚å”¯ä¸€çš„é™åˆ¶æ˜¯è¿™ä¸¤ä¸ª\"å¥å­\"åˆå¹¶åçš„æ€»é•¿åº¦ä¸å¾—è¶…è¿‡512ä¸ªæ ‡è®°ã€‚\n\næ¯ä¸ªå¥å­çš„æ©ç å¤„ç†ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n15%çš„æ ‡è®°ä¼šè¢«æ©ç \nå…¶ä¸­80%çš„æƒ…å†µï¼Œè¢«æ©ç çš„æ ‡è®°ä¼šè¢«æ›¿æ¢ä¸º[MASK]\n10%çš„æƒ…å†µï¼Œè¢«æ©ç çš„æ ‡è®°ä¼šè¢«æ›¿æ¢ä¸ºéšæœºé€‰å–çš„ä¸åŒæ ‡è®°\nå‰©ä½™10%çš„æƒ…å†µï¼Œè¢«æ©ç çš„æ ‡è®°ä¿æŒåŸæ ·ä¸å˜\né¢„è®­ç»ƒè¿‡ç¨‹\n\nè¯¥æ¨¡å‹åœ¨4ä¸ªäº‘TPUï¼ˆå…±16ä¸ªTPUèŠ¯ç‰‡ï¼‰ç»„æˆçš„Podé…ç½®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…±æ‰§è¡Œ100ä¸‡æ­¥è®­ç»ƒï¼Œæ‰¹æ¬¡å¤§å°ä¸º256ã€‚åºåˆ—é•¿åº¦åœ¨90%çš„è®­ç»ƒæ­¥æ•°ä¸­é™åˆ¶ä¸º128ä¸ªæ ‡è®°ï¼Œå‰©ä½™10%åˆ™å…è®¸è¾¾åˆ°512ä¸ªæ ‡è®°ã€‚ä¼˜åŒ–å™¨é‡‡ç”¨Adamï¼Œå­¦ä¹ ç‡ä¸º1e-4ï¼Œ\\(\\beta_{1} = 0.9\\)ï¼Œ\\(\\beta_{2} = 0.999\\)ï¼Œæƒé‡è¡°å‡ä¸º0.01ï¼Œå‰10,000æ­¥è¿›è¡Œå­¦ä¹ ç‡é¢„çƒ­ï¼Œä¹‹åé‡‡ç”¨çº¿æ€§è¡°å‡ã€‚\n\nè¯„ä¼°ç»“æœ\n\nåœ¨ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒåï¼Œè¯¥æ¨¡å‹å–å¾—å¦‚ä¸‹è¡¨ç°ï¼š\n\nGLUEæµ‹è¯•ç»“æœï¼š\n\nä»»åŠ¡\tMNLI-(m/mm)\tQQP\tQNLI\tSST-2\tCoLA\tSTS-B\tMRPC\tRTE\tå¹³å‡åˆ†\n\t84.6/83.4\t71.2\t90.5\t93.5\t52.1\t85.8\t88.9\t66.4\t79.6\nBibTeXæ¡ç›®åŠå¼•ç”¨ä¿¡æ¯\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Safetensors\", \"JAX\", \"English\", \"Apache License 2.0\", \"bookcorpus\", \"wikipedia\", \"exbert\"]"
  },
  {
    "url": "https://gitcode.com/openMind/bloom_1b1",
    "project_name": "bloom_1b1",
    "readme": "BLOOM LM\nBigScience Large Open-science Open-access Multilingual Language Model\nModel Card\n\nVersion 1.0 / 26.May.2022\n\nTable of Contents\nModel Details\nUses\nTraining Data\nRisks and Limitations\nEvaluation\nRecommendations\nGlossary and Calculations\nMore Information\nModel Card Authors\nModification\n\nAdded the example code and modify link path\n\nModel Details\nQickstart\n\nWe show an example of interaction with bloom_1b1 in the following code:\n\nimport torch\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bloom_1b1\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"PyTorch-NPU/bloom_1b1\", trust_remote_code=True, device_map=\"auto\")\n\ninput = \"Give three tips for staying healthy.\"\nprompt = (\"Below is an instrunction that describes a task. \"\n              \"Write a response that appropriately completes the requests\\n\\n\"\n              f\"### Instruction:\\n{input}\\n\\n### Response:\")\ninputs = tokenizer(prompt, return_tensors=\"pt\")\ninputs = inputs.to(model.device)\n\npred = model.generate(**inputs, max_new_tokens=512, repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\nBasics\n\nThis section provides information for anyone who wants to know about the model.\n\nClick to expand\nTechnical Specifications\n\nThis section provides information for people who work on model development.\n\nClick to expand\nEnvironmental Impact\nClick to expand\n\nÂ \n\nUses\n\nThis section addresses questions around how the model is intended to be used, discusses the foreseeable users of the model (including those affected by the model), and describes uses that are considered out of scope or misuse of the model. It provides information for anyone considering using the model or who is affected by the model.\n\nClick to expand\n\nÂ \n\nTraining Data\n\nThis section provides a high-level overview of the training data. It is relevant for anyone who wants to know the basics of what the model is learning.\n\nClick to expand\n\nÂ \n\nRisks and Limitations\n\nThis section identifies foreseeable harms and misunderstandings.\n\nClick to expand\n\nÂ \n\nEvaluation\n\nThis section describes the evaluation protocols and provides the results.\n\nClick to expand\n\nÂ \n\nRecommendations\n\nThis section provides information on warnings and potential mitigations.\n\nClick to expand\n\nÂ \n\nGlossary and Calculations\n\nThis section defines common terms and how metrics are calculated.\n\nClick to expand\n\nÂ \n\nMore Information\nClick to expand\n\nÂ \n\nModel Card Authors\n\nOrdered roughly chronologically and by amount of time spent.\n\nMargaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos MuÃ±oz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana IliÄ‡, GÃ©rard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"ONNX\", \"Safetensors\", \"JAX\", \"English\", \"Chinese\", \"Other\", \"bloom\"]"
  },
  {
    "url": "https://gitcode.com/openMind/byt5_base",
    "project_name": "byt5_base",
    "readme": "",
    "tags": "[\"PyTorch\", \"Transformers\", \"English\", \"Apache License 2.0\", \"mc4\"]"
  },
  {
    "url": "https://gitcode.com/openMind/byt5_small",
    "project_name": "byt5_small",
    "readme": "",
    "tags": "[\"PyTorch\", \"Transformers\", \"English\", \"Apache License 2.0\", \"mc4\"]"
  },
  {
    "url": "https://gitcode.com/openMind/chatglm2_6b",
    "project_name": "chatglm2_6b",
    "readme": "Original Text\nChatGLM2-6B\n\nğŸ’» Github ä»“åº“ â€¢ ğŸ¦ Twitter â€¢ ğŸ“ƒ [GLM@ACL 22] [GitHub] â€¢ ğŸ“ƒ [GLM-130B@ICLR 23] [GitHub]\n\n\nğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ Slack å’Œ å¾®ä¿¡\n\nğŸ“è®¿é—®æ›´å¤§è§„æ¨¡çš„ ChatGLM æ¨¡å‹ï¼Œè¯·å‰å¾€ chatglm.cn\n\næ›´æ–°è¯´æ˜ (Updates)\n\nä¼˜åŒ–ç¤ºä¾‹ä»£ç ï¼Œæ–°å¢ NPU æ”¯æŒï¼›\n\nè°ƒæ•´ä¾èµ–é¡¹é…ç½®ï¼›\n\nUpdated examples with NPU support;\n\nModified dependencies;\n\nç®€ä»‹\n\nChatGLM2-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ ChatGLM-6B çš„å‡çº§ç‰ˆæœ¬ï¼Œåœ¨ç»§æ‰¿åˆä»£æ¨¡å‹æµç•…å¯¹è¯ä½“éªŒå’Œä½éƒ¨ç½²é—¨æ§›ç­‰ä¼˜åŠ¿çš„åŒæ—¶ï¼ŒChatGLM2-6B å¸¦æ¥äº†ä»¥ä¸‹æ˜¾è‘—æå‡ï¼š\n\næ€§èƒ½é£è·ƒï¼šåŸºäº ChatGLM åˆä»£æ¨¡å‹çš„ç ”å‘ç§¯ç´¯ï¼Œæˆ‘ä»¬å…¨é¢ä¼˜åŒ–äº† ChatGLM2-6B çš„åŸºç¡€æ¶æ„ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ GLM çš„æ··åˆç›®æ ‡å‡½æ•°ï¼Œç»è¿‡ 1.4T ä¸­è‹±åŒè¯­è¯­æ–™é¢„è®­ç»ƒå’Œäººç±»åå¥½å¯¹é½è®­ç»ƒã€‚è¯„ä¼°æ•°æ®æ˜¾ç¤ºï¼Œç›¸è¾ƒåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6B åœ¨ MMLUï¼ˆ+23%ï¼‰ã€CEvalï¼ˆ+33%ï¼‰ã€GSM8Kï¼ˆ+571%ï¼‰ã€BBHï¼ˆ+60%ï¼‰ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—æå‡ï¼Œåœ¨åŒè§„æ¨¡å¼€æºæ¨¡å‹ä¸­å±•ç°å¼ºåŠ²ç«äº‰åŠ›ã€‚\nè¶…é•¿ä¸Šä¸‹æ–‡ï¼šé€šè¿‡é›†æˆ FlashAttention æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ä»åˆä»£çš„ 2K æ‰©å±•è‡³ 32Kï¼Œå¹¶åœ¨å¯¹è¯å¾®è°ƒé˜¶æ®µé‡‡ç”¨ 8K ä¸Šä¸‹æ–‡è®­ç»ƒï¼Œæ”¯æŒæ›´æ·±å…¥çš„å¤šè½®äº¤æµã€‚å½“å‰ç‰ˆæœ¬å¯¹å•è½®è¶…é•¿æ–‡æœ¬çš„ç†è§£ä»æœ‰æå‡ç©ºé—´ï¼Œåç»­ç‰ˆæœ¬å°†é‡ç‚¹ä¼˜åŒ–è¿™ä¸€èƒ½åŠ›ã€‚\né«˜æ•ˆæ¨ç†ï¼šå¾—ç›Šäº Multi-Query Attention æŠ€æœ¯ï¼ŒChatGLM2-6B å®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½æ˜¾å­˜å ç”¨ï¼šå®˜æ–¹å®ç°ä¸‹æ¨ç†é€Ÿåº¦æå‡ 42%ï¼ŒINT4 é‡åŒ–æ¨¡å¼ä¸‹ 6G æ˜¾å­˜æ”¯æŒçš„å¯¹è¯é•¿åº¦ä» 1K å»¶ä¼¸è‡³ 8Kã€‚\nå¼€æ”¾æˆæƒï¼šChatGLM2-6B æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œæäº¤é—®å·ç™»è®°åå¯å…è´¹å•†ç”¨ã€‚\n\nChatGLM2-6B is the enhanced iteration of the open-source bilingual (Chinese-English) dialogue model ChatGLM-6B. While preserving the smooth conversational experience and low deployment barrier of its predecessor, ChatGLM2-6B introduces groundbreaking improvements:\n\nPerformance Leap: Building upon the development insights from the first-generation model, we comprehensively upgraded ChatGLM2-6B's architecture. Employing GLM's hybrid objective function and trained on 1.4T bilingual tokens with human preference alignment, evaluation results demonstrate remarkable gains over the original model: +23% on MMLU, +33% on CEval, +571% on GSM8K, and +60% on BBH, establishing it as a top contender among open-source models of comparable scale.\nExtended Context: Leveraging FlashAttention, we expanded the context window from 2K to 32K, with 8K context length fine-tuning for dialogues, enabling more in-depth multi-turn conversations. While current capabilities in processing ultra-long single documents remain limited, subsequent versions will prioritize this enhancement.\nOptimized Inference: Through Multi-Query Attention, ChatGLM2-6B achieves 42% faster inference speed and reduced GPU memory consumption in official implementations. Under INT4 quantization, 6G GPU memory now supports 8K context dialogues (vs. 1K previously).\nLiberal Licensing: ChatGLM2-6B weights are fully open for academic research, with free commercial use permitted upon completing the registration form.\nç³»ç»Ÿä¾èµ–\npip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate, openmind\n\nCode Invocation\n\nYou can invoke the ChatGLM-6B model to generate dialogues with the following code:\n\nfrom openmind import is_torch_npu_available, AutoTokenizer, AutoModel\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/chatglm2_6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"PyTorch-NPU/chatglm2_6b\", trust_remote_code=True, device_map=device).half()\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\nprint(response)\nresponse, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\nprint(response)\n\n\nFor more detailed instructions, including how to run command-line and web-based demos, as well as utilizing model quantization to save GPU memory, please visit our Github Repo.\n\nChange Log\nv1.0\nLicense\n\nThe code in this repository is open-sourced under the Apache-2.0 license. The use of ChatGLM2-6B model weights is subject to the Model License.\n\nCitation\n\nIf you find our work helpful, please consider citing the following papers. The paper for ChatGLM2-6B will be released soonâ€”stay tuned!\n\n@article{zeng2022glm,\n  title={Glm-130b: An open bilingual pre-trained model},\n  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},\n  journal={arXiv preprint arXiv:2210.02414},\n  year={2022}\n}\n\n@inproceedings{du2022glm,\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={320--335},\n  year={2022}\n}\n",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"English\", \"Chinese\", \"Apache License 2.0\", \"thudm\", \"glm\", \"chatglm\"]"
  },
  {
    "url": "https://gitcode.com/openMind/beit_base_patch16",
    "project_name": "beit_base_patch16",
    "readme": "Original Text\nBEiT (åŸºç¡€æ¨¡å‹ï¼Œå¾®è°ƒè‡ªImageNet-1k)\n\nBEiTæ¨¡å‹æ˜¯åœ¨ImageNet-21kï¼ˆ1400ä¸‡å¼ å›¾åƒï¼Œ21841ä¸ªç±»åˆ«ï¼‰ä¸Šè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒçš„ï¼Œåˆ†è¾¨ç‡ä¸º224x224ï¼Œå¹¶åœ¨ImageNet 2012ï¼ˆ100ä¸‡å¼ å›¾åƒï¼Œ1000ä¸ªç±»åˆ«ï¼‰ä¸Šè¿›è¡Œå¾®è°ƒï¼Œåˆ†è¾¨ç‡åŒæ ·ä¸º224x224ã€‚è¯¥æ¨¡å‹åœ¨è®ºæ–‡BEIT: BERT Pre-Training of Image Transformersä¸­ç”±åŒ…æ±‰å ¡ã€ææ ‹å’Œé­å¯Œæå‡ºï¼Œå¹¶é¦–æ¬¡åœ¨è¿™ä¸ªä»“åº“ä¸­å‘å¸ƒã€‚\n\nä¿®æ”¹\n\nåœ¨åŸå§‹READMEä¸Šæ·»åŠ äº†CANNç‰ˆæœ¬ä¾èµ–è¯´æ˜ï¼Œå¹¶ä¿®æ”¹äº†ç¤ºä¾‹ä»£ç éƒ¨åˆ†ã€‚\n\næ¨¡å‹æè¿°\n\nBEiTæ¨¡å‹æ˜¯ä¸€ç§è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œæ˜¯ä¸€ç§è½¬æ¢å™¨ç¼–ç æ¨¡å‹ï¼ˆç±»ä¼¼äºBERTï¼‰ã€‚ä¸åŸå§‹ViTæ¨¡å‹ä¸åŒï¼ŒBEiTæ¨¡å‹åœ¨å¤§é‡å›¾åƒçš„è‡ªç›‘ç£æ–¹å¼ä¸‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå³ImageNet-21kï¼Œåˆ†è¾¨ç‡ä¸º224x224åƒç´ ã€‚æ¨¡å‹çš„é¢„è®­ç»ƒç›®æ ‡æ˜¯æ ¹æ®OpenAIçš„DALL-Eçš„VQ-VAEç¼–ç å™¨ä¸­çš„é®è”½å—é¢„æµ‹è§†è§‰æ ‡è®°ã€‚ æ¥ä¸‹æ¥ï¼Œæ¨¡å‹ä»¥ç›‘ç£æ–¹å¼åœ¨ImageNetï¼ˆä¹Ÿç§°ä¸ºILSVRC2012ï¼‰ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¯¥æ•°æ®é›†åŒ…æ‹¬100ä¸‡å¼ å›¾åƒå’Œ1000ä¸ªç±»åˆ«ï¼Œåˆ†è¾¨ç‡ä¹Ÿæ˜¯224x224ã€‚\n\nå›¾åƒè¢«å‘ˆç°ä¸ºå›ºå®šå¤§å°å—ï¼ˆåˆ†è¾¨ç‡16x16ï¼‰çš„åºåˆ—ï¼Œè¿™äº›å—è¢«çº¿æ€§åµŒå…¥ã€‚ä¸åŸå§‹ViTæ¨¡å‹ä¸åŒï¼ŒBEiTæ¨¡å‹ä½¿ç”¨ç›¸å¯¹ä½ç½®åµŒå…¥ï¼ˆç±»ä¼¼äºT5ï¼‰è€Œä¸æ˜¯ç»å¯¹ä½ç½®åµŒå…¥ï¼Œå¹¶é€šè¿‡å‡å€¼æ± åŒ–å—çš„æœ€ç»ˆéšè—çŠ¶æ€æ¥è¿›è¡Œå›¾åƒåˆ†ç±»ï¼Œè€Œä¸æ˜¯åœ¨[CLS]ä»¤ç‰Œçš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šæ”¾ç½®ä¸€ä¸ªçº¿æ€§å±‚ã€‚\n\né€šè¿‡é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒå­¦ä¹ å›¾åƒçš„å†…éƒ¨è¡¨ç¤ºï¼Œç„¶åå¯ç”¨äºæå–å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾ï¼šä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªæ ‡è®°å›¾åƒçš„æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥é€šè¿‡åœ¨é¢„è®­ç»ƒç¼–ç å™¨ä¸Šæ”¾ç½®ä¸€ä¸ªçº¿æ€§å±‚æ¥è®­ç»ƒä¸€ä¸ªæ ‡å‡†åˆ†ç±»å™¨ã€‚é€šå¸¸åœ¨[CLS]ä»¤ç‰Œä¸Šæ”¾ç½®ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå› ä¸ºæ­¤ä»¤ç‰Œçš„æœ€åéšè—çŠ¶æ€å¯ä»¥è¢«è§†ä¸ºæ•´ä¸ªå›¾åƒçš„è¡¨ç¤ºã€‚æˆ–è€…ï¼Œä¹Ÿå¯ä»¥å‡å€¼æ± åŒ–å—çš„æœ€ç»ˆéšè—çŠ¶æ€ï¼Œå¹¶åœ¨å…¶ä¸Šæ”¾ç½®ä¸€ä¸ªçº¿æ€§å±‚ã€‚\n\nä½¿ç”¨ç›®çš„ä¸é™åˆ¶\n\næ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\nä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨æ­¤æ¨¡å‹å°†COCO 2017æ•°æ®é›†ä¸­çš„å›¾åƒåˆ†ç±»åˆ°ImageNetçš„1000ä¸ªç±»åˆ«ä¹‹ä¸€ï¼š\n\nimport torch\nfrom openmind import is_torch_npu_available\nfrom transformers import BeitImageProcessor, BeitForImageClassification\nfrom PIL import Image\nimport requests\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelse:\n    device = \"cpu\"\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = BeitImageProcessor.from_pretrained(\"PyTorch-NPU/beit_base_patch16_224\")\nmodel = BeitForImageClassification.from_pretrained(\"PyTorch-NPU/beit_base_patch16_224\", device_map=device)\ninputs = processor(images=image, return_tensors=\"pt\")\ninputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(device)\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n\n\nå½“å‰ï¼Œç‰¹å¾æå–å™¨å’Œæ¨¡å‹å‡æ”¯æŒ PyTorchã€‚\n\nè®­ç»ƒæ•°æ®\n\nBEiT æ¨¡å‹åœ¨åŒ…å« 1400ä¸‡å¼ å›¾åƒå’Œ 21k ç±»åˆ«çš„ ImageNet-21k æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶åœ¨åŒ…å« 100ä¸‡å¼ å›¾åƒå’Œ 1k ç±»åˆ«çš„ ImageNet æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚\n\nè®­ç»ƒæµç¨‹\né¢„å¤„ç†\n\nè®­ç»ƒ/éªŒè¯æœŸé—´å›¾åƒé¢„å¤„ç†çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨ è¿™é‡Œ æ‰¾åˆ°ã€‚\n\nå›¾åƒè¢«è°ƒæ•´/ç¼©æ”¾åˆ°ç›¸åŒçš„åˆ†è¾¨ç‡ï¼ˆ224x224ï¼‰ï¼Œå¹¶åœ¨ RGB é€šé“ä¸Šä½¿ç”¨å‡å€¼ï¼ˆ0.5, 0.5, 0.5ï¼‰å’Œæ ‡å‡†å·®ï¼ˆ0.5, 0.5, 0.5ï¼‰è¿›è¡Œå½’ä¸€åŒ–ã€‚\n\né¢„è®­ç»ƒ\n\nå…³äºæ‰€æœ‰ä¸é¢„è®­ç»ƒç›¸å…³çš„è¶…å‚æ•°ï¼Œæˆ‘ä»¬å‚è€ƒäº† åŸè®ºæ–‡ çš„ç¬¬ 15 é¡µã€‚\n\nè¯„ä¼°ç»“æœ\n\nå…³äºåœ¨å¤šä¸ªå›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœï¼Œæˆ‘ä»¬å‚è€ƒäº†åŸè®ºæ–‡ä¸­çš„è¡¨ 1 å’Œè¡¨ 2ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºå¾®è°ƒï¼Œæœ€ä½³ç»“æœæ˜¯åœ¨æ›´é«˜åˆ†è¾¨ç‡ï¼ˆ384x384ï¼‰ä¸‹è·å¾—çš„ã€‚å½“ç„¶ï¼Œå¢åŠ æ¨¡å‹å¤§å°å°†å¯¼è‡´æ›´å¥½çš„æ€§èƒ½ã€‚\n\nBibTeXæ¡ç›®ä¸å¼•ç”¨ä¿¡æ¯\n  author    = {Hangbo Bao and\n               Li Dong and\n               Furu Wei},\n  title     = {BEiT: {BERT} Pre-Training of Image Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2106.08254},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2106.08254},\n  archivePrefix = {arXiv},\n  eprint    = {2106.08254},\n  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-08254.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n\nè¯·æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬å’ŒMarkdownæ ¼å¼ï¼Œæˆ‘å°†ä¼šæŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚\n\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n\n\nå½“ç„¶ï¼Œæˆ‘ä¼šå°½åŠ›æ»¡è¶³æ‚¨çš„è¦æ±‚ã€‚ä¸è¿‡ï¼Œæ‚¨ä¼¼ä¹å¿˜è®°æä¾›éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬å†…å®¹ã€‚è¯·æä¾›åŸå§‹æ–‡æœ¬ï¼Œæˆ‘å°†ä¼šä¸ºæ‚¨ç¿»è¯‘ã€‚",
    "tags": "[\"Image Classification\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"NeMo\", \"ELM\", \"JAX\", \"English\", \"Apache License 2.0\", \"imagenet\", \"imagenet-21k\", \"vision\"]"
  },
  {
    "url": "https://gitcode.com/openMind/bit_50",
    "project_name": "bit_50",
    "readme": "Original Text\nå¤§è¿ç§»ï¼ˆBiTï¼‰\n\nBiTæ¨¡å‹åœ¨å¤§è¿ç§»ï¼ˆBiTï¼‰ï¼šé€šç”¨è§†è§‰è¡¨å¾å­¦ä¹ ä¸€æ–‡ä¸­ç”±Alexander Kolesnikovã€Lucas Beyerã€Xiaohua Zhaiã€Joan Puigcerverã€Jessica Yungã€Sylvain Gellyå’ŒNeil Houlsbyæå‡ºã€‚BiTä¸ºæ‰©å±•ResNetç±»æ¶æ„ï¼ˆå…·ä½“ä¸ºResNetv2ï¼‰é¢„è®­ç»ƒæä¾›äº†ä¸€ç§ç®€æ´çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è¿ç§»å­¦ä¹ çš„æ•ˆæœã€‚\n\nå…è´£å£°æ˜ï¼šResNetæ¨¡å‹çš„å‘å¸ƒå›¢é˜Ÿå¹¶æœªä¸ºæ­¤æ¨¡å‹ç¼–å†™æ¨¡å‹å¡ç‰‡ï¼Œå› æ­¤æœ¬æ¨¡å‹å¡ç‰‡ç”±Hugging Faceå›¢é˜Ÿç¼–å†™ã€‚\n\nä¿®æ”¹\nä¿®æ”¹ç¤ºä¾‹ä»£ç ä»¥æ”¯æŒopenMindå’Œæ·»åŠ npuæ”¯æŒ\næ¨¡å‹æè¿°\n\nä»¥ä¸‹æ˜¯è®ºæ–‡æ‘˜è¦çš„å†…å®¹ï¼š\n\nè¿ç§»é¢„è®­ç»ƒè¡¨å¾å¯æé«˜åœ¨è§†è§‰ä»»åŠ¡è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæ—¶çš„æ ·æœ¬æ•ˆç‡ï¼Œå¹¶ç®€åŒ–è¶…å‚æ•°çš„è°ƒæ•´ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†åœ¨å¤§å‹ç›‘ç£æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒçš„èŒƒä¾‹ã€‚æˆ‘ä»¬æ‰©å±•äº†é¢„è®­ç»ƒçš„è§„æ¨¡ï¼Œå¹¶æå‡ºäº†æˆ‘ä»¬ç§°ä¹‹ä¸ºå¤§è¿ç§»ï¼ˆBiTï¼‰çš„ç®€æ´æ–¹æ³•ã€‚é€šè¿‡ç»“åˆå‡ ä¸ªç²¾å¿ƒé€‰æ‹©çš„ç»„ä»¶ï¼Œå¹¶é‡‡ç”¨ç®€å•çš„å¯å‘å¼è¿ç§»ç­–ç•¥ï¼Œæˆ‘ä»¬åœ¨è¶…è¿‡20ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ã€‚BiTåœ¨ä»æ¯ç±»1ä¸ªæ ·æœ¬åˆ°æ€»å…±1Mä¸ªæ ·æœ¬çš„å¹¿æ³›æ•°æ®èŒƒå›´å†…è¡¨ç°è‰¯å¥½ã€‚BiTåœ¨ILSVRC-2012ä¸Šè¾¾åˆ°äº†87.5%çš„top-1å‡†ç¡®ç‡ï¼Œåœ¨CIFAR-10ä¸Šè¾¾åˆ°99.4%ï¼Œåœ¨19ä¸ªä»»åŠ¡çš„è§†è§‰ä»»åŠ¡é€‚åº”åŸºå‡†ï¼ˆVTABï¼‰ä¸Šè¾¾åˆ°76.3%ã€‚åœ¨å°æ•°æ®é›†ä¸Šï¼ŒBiTåœ¨ILSVRC-2012ä¸­æ¯ç±»10ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹è¾¾åˆ°76.8%ï¼Œåœ¨CIFAR-10ä¸­æ¯ç±»10ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹è¾¾åˆ°97.0%ã€‚æˆ‘ä»¬å¯¹å¯¼è‡´é«˜è¿ç§»æ€§èƒ½çš„ä¸»è¦ç»„ä»¶è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚\n\nä½¿ç”¨ç›®çš„ä¸é™åˆ¶\n\næ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\nä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨æ­¤æ¨¡å‹å°†COCO 2017æ•°æ®é›†ä¸­çš„å›¾åƒåˆ†ç±»åˆ°ImageNetçš„1,000ä¸ªç±»åˆ«ä¸­çš„ä¸€ä¸ªï¼š\n\nimport torch\nfrom datasets import load_dataset\nfrom openmind import is_torch_npu_available\nfrom transformers import BitImageProcessor, BitForImageClassification\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelse:\n    device = \"cpu\"\n\nmodel_path= \"PyTorch-NPU/bit_50\"\ndataset = load_dataset(\"./cats_image\")\nimage = dataset[\"train\"][\"image\"][0]\n\nfeature_extractor = BitImageProcessor.from_pretrained(model_path)\nmodel = BitForImageClassification.from_pretrained(model_path).to(device)\ninputs = feature_extractor(image, return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_label = logits.argmax(-1).item()\nprint(f'>>>result={model.config.id2label[predicted_label]}')\n\nBibTeXæ¡ç›®ä¸å¼•ç”¨ä¿¡æ¯\n@misc{https://doi.org/10.48550/arxiv.1912.11370,\n  doi = {10.48550/ARXIV.1912.11370},\n  \n  url = {https://arxiv.org/abs/1912.11370},\n  \n  author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Big Transfer (BiT): General Visual Representation Learning},\n  \n  publisher = {arXiv},\n  \n  year = {2019},\n  \n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n\n\n\nå½“ç„¶å¯ä»¥ã€‚è¯·æ‚¨æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬å’Œç›¸åº”çš„ Markdown æ ¼å¼ï¼Œæˆ‘å°†ä¸ºæ‚¨ç¿»è¯‘æˆä¸­æ–‡ã€‚",
    "tags": "[\"Image Classification\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"NeMo\", \"ELM\", \"Apache License 2.0\", \"imagenet-1k\", \"vision\"]"
  },
  {
    "url": "https://gitcode.com/openMind/distilbert_base_uncased_finetuned_sst_2_english",
    "project_name": "distilbert_base_uncased_finetuned_sst_2_english",
    "readme": "Original Text\nDistilBERTåŸºç¡€æœªåŒºåˆ†å¤§å°å†™ç‰ˆæœ¬å¾®è°ƒSST-2\nä¿®æ”¹\n\nä¿®æ”¹ç¤ºä¾‹ä»£ç éƒ¨åˆ†ã€‚\n\nç›®å½•\næ¨¡å‹è¯¦æƒ…\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\nåº”ç”¨åœºæ™¯\né£é™©ã€é™åˆ¶å’Œåè§\nè®­ç»ƒ\næ¨¡å‹è¯¦æƒ…\n\næ¨¡å‹æè¿°ï¼š è¯¥æ¨¡å‹æ˜¯DistilBERT-base-uncasedçš„å¾®è°ƒæ£€æŸ¥ç‚¹ï¼Œé’ˆå¯¹SST-2è¿›è¡Œå¾®è°ƒã€‚ è¯¥æ¨¡å‹åœ¨å¼€å‘é›†ä¸Šè¾¾åˆ°äº†91.3%çš„å‡†ç¡®ç‡ï¼ˆä½œä¸ºå¯¹æ¯”ï¼ŒBert bert-base-uncasedç‰ˆæœ¬è¾¾åˆ°äº†92.7%çš„å‡†ç¡®ç‡ï¼‰ã€‚\n\næ¨¡å‹ç±»å‹ï¼š æ–‡æœ¬åˆ†ç±»\nè¯­è¨€ï¼š è‹±è¯­\nè®¸å¯è¯ï¼š Apache-2.0\nçˆ¶æ¨¡å‹ï¼š å¦‚éœ€äº†è§£æ›´å¤šå…³äºDistilBERTçš„è¯¦æƒ…ï¼Œæˆ‘ä»¬é¼“åŠ±ç”¨æˆ·æŸ¥çœ‹è¿™ä¸ªæ¨¡å‹å¡ç‰‡ã€‚\næ›´å¤šä¿¡æ¯èµ„æºï¼š\nDistilBERTè®ºæ–‡\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\næƒ…æ„Ÿåˆ†æçš„ç¤ºä¾‹ï¼š\n\nfrom openmind import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\", model=\"PyTorch-NPU/distilbert_base_uncased_finetuned_sst_2_english\")\ntext = \"Wish you have a nice day.\"\nprint(classifier(text))\n\nç”¨é€”\nç›´æ¥ä½¿ç”¨\n\nè¯¥æ¨¡å‹å¯ç”¨äºä¸»é¢˜åˆ†ç±»ã€‚æ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡æˆ–ä¸‹ä¸€å¥é¢„æµ‹ï¼Œä½†å…¶ä¸»è¦ç”¨é€”æ˜¯é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚è¯·æŸ¥é˜…æ¨¡å‹åº“ä»¥å¯»æ‰¾åœ¨æ‚¨æ„Ÿå…´è¶£çš„ä»»åŠ¡ä¸Šå¾®è°ƒåçš„ç‰ˆæœ¬ã€‚\n\nè¯¯ç”¨å’Œè¶Šç•Œä½¿ç”¨\n\nä¸åº”ä½¿ç”¨è¯¥æ¨¡å‹æ•…æ„ä¸ºäººä»¬åˆ›é€ æ•Œå¯¹æˆ–ç–è¿œçš„ç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æœªç»è¿‡è®­ç»ƒä»¥æˆä¸ºäº‹å®æ€§æˆ–å¯¹äººç‰©æˆ–äº‹ä»¶çš„çœŸå®è¡¨ç°ï¼Œå› æ­¤ä½¿ç”¨è¯¥æ¨¡å‹ç”Ÿæˆæ­¤ç±»å†…å®¹è¶…å‡ºäº†æ¨¡å‹èƒ½åŠ›çš„èŒƒå›´ã€‚\n\né£é™©ã€å±€é™æ€§å’Œåè§\n\nåŸºäºä¸€äº›å®éªŒï¼Œæˆ‘ä»¬å‘ç°è¯¥æ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿé’ˆå¯¹ä»£è¡¨æ€§ä¸è¶³ç¾¤ä½“çš„åè§é¢„æµ‹ã€‚\n\nä¾‹å¦‚ï¼Œå¯¹äºç±»ä¼¼äºâ€œè¿™éƒ¨ç”µå½±æ‹æ‘„äº[COUNTRY]â€çš„å¥å­ï¼Œè¿™ä¸ªäºŒåˆ†ç±»æ¨¡å‹ä¼šæ ¹æ®å›½å®¶çš„ä¸åŒç»™å‡ºæˆªç„¶ä¸åŒçš„æ­£é¢æ ‡ç­¾æ¦‚ç‡ï¼ˆå¦‚æœå›½å®¶æ˜¯æ³•å›½ï¼Œæ¦‚ç‡ä¸º0.89ï¼Œè€Œå¦‚æœæ˜¯é˜¿å¯Œæ±—ï¼Œæ¦‚ç‡åˆ™ä¸º0.08ï¼‰ï¼Œè€Œè¾“å…¥ä¸­æ²¡æœ‰ä»»ä½•è¿¹è±¡è¡¨æ˜è¿™ç§å¼ºçƒˆçš„è¯­ä¹‰è½¬æ¢ã€‚åœ¨è¿™ä¸ªColabä¸­ï¼ŒAurÃ©lien GÃ©ronåˆ¶ä½œäº†ä¸€å¼ æœ‰è¶£çš„åœ°å›¾ï¼Œç»˜åˆ¶äº†æ¯ä¸ªå›½å®¶çš„è¿™äº›æ¦‚ç‡ã€‚\n\næˆ‘ä»¬å¼ºçƒˆå»ºè®®ç”¨æˆ·å½»åº•æ¢ç©¶è¿™äº›æ–¹é¢åœ¨å…¶ç”¨ä¾‹ä¸­ï¼Œä»¥è¯„ä¼°è¯¥æ¨¡å‹çš„é£é™©ã€‚æˆ‘ä»¬å»ºè®®ä»ä»¥ä¸‹åè§è¯„ä¼°æ•°æ®é›†å¼€å§‹ç ”ç©¶ï¼šWinoBiasã€WinoGenderã€Stereosetã€‚\n\nè®­ç»ƒ\nè®­ç»ƒæ•°æ®\n\nä½œè€…ä½¿ç”¨äº†ä»¥ä¸‹æ–¯å¦ç¦æƒ…æ„Ÿæ ‘åº“ï¼ˆsst2ï¼‰è¯­æ–™åº“æ¥è®­ç»ƒæ¨¡å‹ã€‚\n\nè®­ç»ƒè¿‡ç¨‹\nå¾®è°ƒè¶…å‚æ•°\nlearning_rate = 1e-5\nbatch_size = 32\nwarmup = 600\nmax_seq_length = 128\nnum_train_epochs = 3.0",
    "tags": "[\"Text Classification\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"ONNX\", \"Rust\", \"Safetensors\", \"English\", \"Apache License 2.0\", \"exbert\"]"
  },
  {
    "url": "https://gitcode.com/openMind/baichuan2_7b_base",
    "project_name": "baichuan2_7b_base",
    "readme": "Original Text\nç™¾å·æ™ºèƒ½å¤§æ¨¡å‹ 2\nğŸ¦‰GitHub | ğŸ’¬å¾®ä¿¡\nç™¾å·APIç°å·²æ”¯æŒæœç´¢å¢å¼ºä¸192Kè¶…é•¿ä¸Šä¸‹æ–‡çª—å£ï¼Œæ–°å¢çŸ¥è¯†åº“æ£€ç´¢åŠŸèƒ½ï¼Œé™æ—¶å…è´¹ä½“éªŒï¼\nğŸš€ ç™¾å·å¤§æ¨¡å‹åœ¨çº¿å¯¹è¯å¹³å° ç°å·²å…¨é¢å¼€æ”¾å…¬æµ‹ ğŸ‰\nä¿®æ”¹è¯´æ˜ï¼ˆModificationï¼‰\n\nå¯¹åŸå§‹READMEä¸­çš„ç¤ºä¾‹ä»£ç éƒ¨åˆ†è¿›è¡Œäº†ä¼˜åŒ–è°ƒæ•´ã€‚ Optimized the example code section in the original README.\n\nç›®å½•/Table of Contents\nğŸ“– æ¨¡å‹æ¦‚è§ˆ/Introduction\nâš™ï¸ å¿«é€Ÿä¸Šæ‰‹/Quick Start\nğŸ“Š æ€§èƒ½è¯„ä¼°/Benchmark Evaluation\nğŸ“œ ä½¿ç”¨æ¡æ¬¾/Terms and Conditions\næ¨¡å‹æ¦‚è§ˆ/Introduction\n\nç™¾å·æ™ºèƒ½å¤§æ¨¡å‹ 2 æ˜¯ç”±ç™¾å·æ™ºèƒ½ç ”å‘çš„æ–°ä¸€ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº 2.6 ä¸‡äº¿ tokens çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®æ‰“é€ ï¼Œåœ¨ä¸­è‹±æ–‡æƒå¨åŸºå‡†æµ‹è¯•ä¸­å‡åˆ›ä¸‹åŒè§„æ¨¡æ¨¡å‹çš„æœ€ä½³è¡¨ç°ã€‚æœ¬æ¬¡å‘å¸ƒçš„ç‰ˆæœ¬åŒ…å« 7B å’Œ 13B çš„åŸºç¡€ç‰ˆä¸å¯¹è¯ç‰ˆï¼Œå¹¶ç‰¹åˆ«æä¾›å¯¹è¯ç‰ˆçš„ 4bit é‡åŒ–ç‰ˆæœ¬ã€‚æ‰€æœ‰ç‰ˆæœ¬ä¸ä»…å‘å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¼€å‘è€…åªéœ€é€šè¿‡é‚®ä»¶ç”³è¯·è·å–å®˜æ–¹å•†ç”¨æˆæƒåï¼Œå³å¯å…è´¹ç”¨äºå•†ä¸šåœºæ™¯ã€‚ Baichuan 2 is the next-generation open-source large language model developed by Baichuan Intelligence inc.. Trained on a premium corpus of 2.6 trillion tokens, it has set new benchmarks in authoritative Chinese and English evaluations for models of comparable scale. This release includes 7B and 13B variants of both base and chat-optimized models, with an additional 4bit quantized version of the chat model. All versions are fully accessible for academic research, and commercial use is available at no cost upon obtaining official licensing via email request.\n\nå¿«é€Ÿä¸Šæ‰‹/Quick Start\n\nç™¾å·æ™ºèƒ½å¤§æ¨¡å‹ 2 ç³»åˆ—ä¸ºæå‡æ¨ç†æ•ˆç‡ï¼Œé‡‡ç”¨äº† PyTorch 2.0 æ–°å¢çš„ F.scaled_dot_product_attention åŠ é€ŸæŠ€æœ¯ï¼Œå› æ­¤éœ€è¦è¿è¡Œåœ¨ PyTorch 2.0 åŠä»¥ä¸Šç‰ˆæœ¬ç¯å¢ƒã€‚\n\nTo maximize inference speed, the Baichuan 2 model series leverages PyTorch 2.0's cutting-edge F.scaled_dot_product_attention feature, requiring a PyTorch 2.0+ runtime environment.\n\nimport torch\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/baichuan2_7b_base\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"PyTorch-NPU/baichuan2_7b_base\", device_map=\"npu:0\", trust_remote_code=True)\ninputs = tokenizer('ç™»é¹³é›€æ¥¼->ç‹ä¹‹æ¶£\\nå¤œé›¨å¯„åŒ—->', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\nåŸºå‡†æµ‹è¯•ç»“æœ/Benchmark Evaluation\n\næˆ‘ä»¬åœ¨é€šç”¨é¢†åŸŸã€æ³•å¾‹ã€åŒ»ç–—ã€æ•°å­¦ã€ç¼–ç¨‹å’Œå¤šè¯­è¨€ç¿»è¯‘å…­å¤§é¢†åŸŸçš„ä¸­è‹±æ–‡æƒå¨æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ›´å¤šè¯¦ç»†æµ‹è¯•æ•°æ®è¯·å‚é˜…GitHubã€‚\n\nWe have conducted comprehensive evaluations of the model on authoritative Chinese-English datasets across six major domains: General, Legal, Medical, Mathematics, Code, and Multilingual Translation. For more detailed evaluation metrics, please visit GitHub.\n\n7Bæ¨¡å‹è¡¨ç°\n\tC-Eval\tMMLU\tCMMLU\té«˜è€ƒ\tAGIEval\tBBH\n\t5æ¬¡æµ‹è¯•\t5æ¬¡æµ‹è¯•\t5æ¬¡æµ‹è¯•\t5æ¬¡æµ‹è¯•\t5æ¬¡æµ‹è¯•\t3æ¬¡æµ‹è¯•\nGPT-4\t68.40\t83.93\t70.33\t66.15\t63.27\t75.12\nGPT-3.5 Turbo\t51.10\t68.54\t54.06\t47.07\t46.13\t61.59\nLLaMA-7B\t27.10\t35.10\t26.75\t27.81\t28.17\t32.38\nLLaMA2-7B\t28.90\t45.73\t31.38\t25.97\t26.53\t39.16\nMPT-7B\t27.15\t27.93\t26.00\t26.54\t24.83\t35.20\nFalcon-7B\t24.23\t26.03\t25.66\t24.24\t24.10\t28.77\nChatGLM2-6B\t50.20\t45.90\t49.00\t49.44\t45.28\t31.65\nBaichuan-7B\t42.80\t42.30\t44.02\t36.34\t34.44\t32.48\nBaichuan2-7B-Base\t54.00\t54.16\t57.07\t47.47\t42.73\t41.56\n13Bæ¨¡å‹è¡¨ç°\n\tC-Eval\tMMLU\tCMMLU\té«˜è€ƒ\tAGIEval\tBBH\n\t5æ¬¡æµ‹è¯•\t5æ¬¡æµ‹è¯•\t5æ¬¡æµ‹è¯•\t5æ¬¡æµ‹è¯•\t5æ¬¡æµ‹è¯•\t3æ¬¡æµ‹è¯•\nGPT-4\t68.40\t83.93\t70.33\t66.15\t63.27\t75.12\nGPT-3.5 Turbo\t51.10\t68.54\t54.06\t47.07\t46.13\t61.59\nLLaMA-13B\t28.50\t46.30\t31.15\t28.23\t28.22\t37.89\nLLaMA2-13B\t35.80\t55.09\t37.99\t30.83\t32.29\t46.98\nVicuna-13B\t32.80\t52.00\t36.28\t30.11\t31.55\t43.04\nChinese-Alpaca-Plus-13B\t38.80\t43.90\t33.43\t34.78\t35.46\t28.94\nXVERSE-13B\t53.70\t55.21\t58.44\t44.69\t42.54\t38.06\nBaichuan-13B-Base\t52.40\t51.60\t55.30\t49.69\t43.20\t43.01\nBaichuan2-13B-Base\t58.10\t59.17\t61.97\t54.33\t48.17\t48.78\nä½¿ç”¨æ¡æ¬¾/Terms and Conditions\nå…è´£å£°æ˜\n\næˆ‘ä»¬éƒ‘é‡å£°æ˜ï¼Œå¼€å‘å›¢é˜ŸæœªåŸºäºBaichuan 2æ¨¡å‹å¼€å‘ä»»ä½•ç§»åŠ¨ç«¯ã€ç½‘é¡µç«¯æˆ–å…¶ä»–å¹³å°çš„åº”ç”¨ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‰€æœ‰ä½¿ç”¨è€…ä¸å¾—å°†Baichuan 2æ¨¡å‹ç”¨äºå±å®³å›½å®¶å®‰å…¨æˆ–è¿åæ³•å¾‹æ³•è§„çš„æ´»åŠ¨ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¦æ±‚ç”¨æˆ·ä¸å¾—å°†è¯¥æ¨¡å‹ç”¨äºæœªç»å®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚å¸Œæœ›æ‰€æœ‰ç”¨æˆ·éƒ½èƒ½éµå®ˆè¿™ä¸€åŸåˆ™ï¼Œç¡®ä¿æŠ€æœ¯å‘å±•åœ¨è§„èŒƒåˆæ³•çš„ç¯å¢ƒä¸‹è¿›è¡Œã€‚\n\næˆ‘ä»¬å·²ç«­å°½å…¨åŠ›ç¡®ä¿æ¨¡å‹è®­ç»ƒæ•°æ®çš„åˆè§„æ€§ã€‚ç„¶è€Œï¼Œé‰´äºæ¨¡å‹ä¸æ•°æ®çš„å¤æ‚æ€§ï¼Œä»å¯èƒ½å­˜åœ¨ä¸å¯é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¯¹äºå› ä½¿ç”¨Baichuan 2å¼€æºæ¨¡å‹å¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é£é™©ã€èˆ†æƒ…é£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­åŠä¸å½“åˆ©ç”¨å¼•å‘çš„å„ç±»é£é™©ï¼‰ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n\nWe solemnly declare that our development team has not created any applications based on the Baichuan 2 model for mobile, web, or other platforms. We strongly advise all users against employing the Baichuan 2 model for activities that jeopardize national security or violate laws and regulations. Additionally, we prohibit the use of this model for internet services that have not undergone security reviews and filings. We expect all users to adhere to this principle to ensure technological advancement proceeds within a regulated and lawful framework.\n\nWe have made every effort to ensure the compliance of the data used in model training. However, given the complexity of models and data, unforeseeable issues may still arise. Therefore, we shall not be held liable for any problems resulting from the use of the Baichuan 2 open-source model (including but not limited to data security risks, public opinion risks, or any risks stemming from the model being misled, abused, disseminated, or improperly utilized).\n\nè®¸å¯åè®®\n\nç¤¾åŒºä½¿ç”¨Baichuan 2æ¨¡å‹éœ€éµå®ˆApache 2.0åŠã€ŠBaichuan 2æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹ã€‚Baichuan 2æ¨¡å‹æ”¯æŒå•†ä¸šç”¨é€”ï¼Œå¦‚è®¡åˆ’å°†æ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨äºå•†ä¸šç›®çš„ï¼Œè¯·ç¡®ä¿ç¬¦åˆä»¥ä¸‹æ¡ä»¶ï¼š\n\næ‚¨æˆ–å…³è”æ–¹çš„æœåŠ¡/äº§å“æ—¥å‡æ´»è·ƒç”¨æˆ·(DAU)ä½äº100ä¸‡\næ‚¨æˆ–å…³è”æ–¹éè½¯ä»¶æœåŠ¡æä¾›å•†æˆ–äº‘æœåŠ¡æä¾›å•†\næ‚¨æˆ–å…³è”æ–¹ä¸å¾—æœªç»ç™¾å·è®¸å¯å°†å•†ç”¨æˆæƒäºŒæ¬¡æˆäºˆç¬¬ä¸‰æ–¹\n\næ»¡è¶³æ¡ä»¶åï¼Œè¯·é€šè¿‡opensource@baichuan-inc.comæäº¤åè®®è¦æ±‚çš„ç”³è¯·ææ–™ã€‚ç»å®¡æ ¸é€šè¿‡åï¼Œç™¾å·å°†æˆäºˆéæ’ä»–æ€§ã€å…¨çƒæ€§ã€ä¸å¯è½¬è®©ã€ä¸å¯åˆ†è®¸å¯ä¸”å¯æ’¤é”€çš„å•†ä¸šä½¿ç”¨è®¸å¯ã€‚\n\nCommunity use of the Baichuan 2 model requires compliance with Apache 2.0 and the Baichuan 2 Model Community License Agreement. The model supports commercial use. For commercial deployment of the model or its derivatives, please ensure compliance with the following conditions:\n\nDaily Active Users (DAU) of your or affiliated services/products are below 1 million\nYou or affiliates are not software service providers or cloud service providers\nYou or affiliates shall not sublicense commercial rights to third parties without Baichuan's authorization\n\nUpon meeting these conditions, please submit required application materials to opensource@baichuan-inc.com. After approval, Baichuan will grant a non-exclusive, worldwide, non-transferable, non-sublicensable, and revocable commercial use license.",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"English\", \"Chinese\", \"Other\"]"
  },
  {
    "url": "https://gitcode.com/openMind/blip-image-captioning-large",
    "project_name": "blip-image-captioning-large",
    "readme": "READMENeed a different language? Check our translations.View Translated VersionOriginal TextTranslationOriginal Text",
    "tags": "[\"Image-to-Text\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Safetensors\", \"BSD 3-Clause New or Revised\", \"image-captioning\"]"
  },
  {
    "url": "https://gitcode.com/openMind/bloom_3b",
    "project_name": "bloom_3b",
    "readme": "BLOOM LM\nBigScience Large Open-science Open-access Multilingual Language Model\nModel Card\n\nVersion 1.0 / 26.May.2022\n\nTable of Contents\nModel Details\nUses\nTraining Data\nRisks and Limitations\nEvaluation\nRecommendations\nGlossary and Calculations\nMore Information\nModel Card Authors\nModification\n\nAdded the example code and modify link path\n\nModel Details\nQickstart\n\nWe show an example of interaction with bloom_3b in the following code:\n\nimport torch\nfrom openmind import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bloom_3b\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"PyTorch-NPU/bloom_3b\", trust_remote_code=True, device_map=\"auto\")\n\ninput = \"Give three tips for staying healthy.\"\nprompt = (\"Below is an instrunction that describes a task. \"\n              \"Write a response that appropriately completes the requests\\n\\n\"\n              f\"### Instruction:\\n{input}\\n\\n### Response:\")\ninputs = tokenizer(prompt, return_tensors=\"pt\")\ninputs = inputs.to(model.device)\n\npred = model.generate(**inputs, max_new_tokens=512, repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\nBasics\n\nThis section provides information for anyone who wants to know about the model.\n\nClick to expand\nTechnical Specifications\n\nThis section provides information for people who work on model development.\n\nClick to expand\nEnvironmental Impact\nClick to expand\n\nÂ \n\nUses\n\nThis section addresses questions around how the model is intended to be used, discusses the foreseeable users of the model (including those affected by the model), and describes uses that are considered out of scope or misuse of the model. It provides information for anyone considering using the model or who is affected by the model.\n\nClick to expand\n\nÂ \n\nTraining Data\n\nThis section provides a high-level overview of the training data. It is relevant for anyone who wants to know the basics of what the model is learning.\n\nClick to expand\n\nÂ \n\nRisks and Limitations\n\nThis section identifies foreseeable harms and misunderstandings.\n\nClick to expand\n\nÂ \n\nEvaluation\n\nThis section describes the evaluation protocols and provides the results.\n\nClick to expand\n\nÂ \n\nRecommendations\n\nThis section provides information on warnings and potential mitigations.\n\nClick to expand\n\nÂ \n\nGlossary and Calculations\n\nThis section defines common terms and how metrics are calculated.\n\nClick to expand\n\nÂ \n\nMore Information\nClick to expand\n\nÂ \n\nModel Card Authors\n\nOrdered roughly chronologically and by amount of time spent.\n\nMargaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos MuÃ±oz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana IliÄ‡, GÃ©rard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",
    "tags": "[\"Text Generation\", \"PyTorch\", \"Transformers\", \"Safetensors\", \"English\", \"Chinese\", \"Other\", \"bloom\"]"
  },
  {
    "url": "https://gitcode.com/openMind/deberta_base",
    "project_name": "deberta_base",
    "readme": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\nDeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.\n\nPlease check the official repository for more details and updates.\n\nModification\n\nAdd example with how to use DeBERTa in openmind.\n\nFine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and MNLI tasks.\n\nModel\tSQuAD 1.1\tSQuAD 2.0\tMNLI-m\nRoBERTa-base\t91.5/84.6\t83.7/80.5\t87.6\nXLNet-Large\t-/-\t-/80.2\t86.8\nDeBERTa-base\t93.1/87.2\t86.2/83.1\t88.8\nHow to use DeBERTa in openmind\nimport torch\nfrom openmind import AutoTokenizer, is_torch_npu_available\nfrom transformers import DebertaForMaskedLM\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\nmodel_path= \"PyTorch-NPU/deberta_base\"\n\n#æ¨ç†\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = DebertaForMaskedLM.from_pretrained(model_path).to(device)\ninputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\nmask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n\npredicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\nprint(\">>>\", tokenizer.decode(predicted_token_id))\n\nCitation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Rust\", \"English\", \"MIT\", \"deberta-v1\"]"
  },
  {
    "url": "https://gitcode.com/openMind/deberta_v2_xlarge",
    "project_name": "deberta_v2_xlarge",
    "readme": "Original Text\nDeBERTaï¼šåŸºäºè§£è€¦æ³¨æ„åŠ›ä¸å¢å¼ºæ©ç è§£ç çš„BERTæ”¹è¿›æ¨¡å‹\n\nDeBERTaé€šè¿‡è§£è€¦æ³¨æ„åŠ›æœºåˆ¶å’Œå¢å¼ºå‹æ©ç è§£ç å™¨æ”¹è¿›äº†BERTä¸RoBERTaæ¨¡å‹ã€‚åœ¨80GBè®­ç»ƒæ•°æ®ä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ•°è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šè¶…è¶Šäº†BERTå’ŒRoBERTaçš„è¡¨ç°ã€‚\n\næ›´å¤šç»†èŠ‚ä¸æ›´æ–°è¯·å‚é˜…å®˜æ–¹ä»£ç åº“ã€‚\n\næ­¤ä¸ºDeBERTa V2 xlargeæ¨¡å‹ï¼ŒåŒ…å«24å±‚ç½‘ç»œç»“æ„ï¼Œéšå±‚ç»´åº¦1536ï¼Œå‚æ•°é‡è¾¾9äº¿ï¼Œè®­ç»ƒæ•°æ®è§„æ¨¡ä¸º160GBåŸå§‹æ–‡æœ¬ã€‚\n\næ”¹è¿›è¯´æ˜\nè°ƒæ•´pipeline_tagä¸æ¡†æ¶æ ‡è¯†\næ–°å¢NPUç¡¬ä»¶æ”¯æŒ\nè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡å¾®è°ƒç»“æœ\n\nä¸‹è¡¨å±•ç¤ºäº†æ¨¡å‹åœ¨SQuAD 1.1/2.0åŠGLUEåŸºå‡†ä»»åŠ¡ä¸­çš„å¼€å‘é›†è¡¨ç°ï¼š\n\næ¨¡å‹\tSQuAD 1.1\tSQuAD 2.0\tMNLI-m/mm\tSST-2\tQNLI\tCoLA\tRTE\tMRPC\tQQP\tSTS-B\n\tF1/EM\tF1/EM\tå‡†ç¡®ç‡\tå‡†ç¡®ç‡\tå‡†ç¡®ç‡\tMCC\tå‡†ç¡®ç‡\tå‡†ç¡®ç‡/F1\tå‡†ç¡®ç‡/F1\tçš®å°”é€Š/æ–¯çš®å°”æ›¼\nBERT-Large\t90.9/84.1\t81.8/79.0\t86.6/-\t93.2\t92.3\t60.6\t70.4\t88.0/-\t91.3/-\t90.0/-\nRoBERTa-Large\t94.6/88.9\t89.4/86.5\t90.2/-\t96.4\t93.9\t68.0\t86.6\t90.9/-\t92.2/-\t92.4/-\nXLNet-Large\t95.1/89.7\t90.6/87.9\t90.8/-\t97.0\t94.9\t69.0\t85.9\t90.8/-\t92.3/-\t92.5/-\nDeBERTa-Large\t95.5/90.1\t90.7/88.0\t91.3/91.1\t96.5\t95.3\t69.5\t91.0\t92.6/94.6\t92.3/-\t92.8/92.5\nDeBERTa-XLarge\t-/-\t-/-\t91.5/91.2\t97.0\t-\t-\t93.1\t92.1/94.3\t-\t92.9/92.7\nDeBERTa-V2-XLarge\t95.8/90.8\t91.4/88.9\t91.7/91.6\t97.5\t95.8\t71.1\t93.9\t92.0/94.2\t92.3/89.8\t92.9/92.9\nDeBERTa-V2-XXLarge\t96.1/91.4\t92.2/89.7\t91.7/91.9\t97.2\t96.0\t72.0\t93.5\t93.1/94.9\t92.7/90.3\t93.2/93.1\næ¨ç†åº”ç”¨\nimport torch\nfrom openmind import pipeline, is_torch_npu_available\nfrom openmind_hub import snapshot_download\n\n\nmodel_path = snapshot_download(\"PyTorch-NPU/deberta_v2_xlarge\", revision=\"main\", resume_download=True,\n                                ignore_patterns=[\"*.h5\", \"*.ot\", \"*.msgpack\"])\ndevice = \"npu:0\"\n# æ¨ç†\nunmasker = pipeline('fill-mask', model=model_path, device=device)\nprint(unmasker(\"Hello I'm a [MASK] model.\"))\n\nå¼•ç”¨å£°æ˜\n\nå¦‚æœæ‚¨è®¤ä¸º DeBERTa å¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¸®åŠ©ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼š\n\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n",
    "tags": "[\"Fill-Mask\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"English\", \"MIT\"]"
  },
  {
    "url": "https://gitcode.com/openMind/resnet_50",
    "project_name": "resnet_50",
    "readme": "ResNet-50 v1.5\n\nResNet model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al.\n\nDisclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.\n\nModification\n\nAdded the CANN version dependency description to the original README and modified the example code.\n\nModel description\n\nResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.\n\nThis is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to Nvidia.\n\nHow to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\nfrom openmind import AutoImageProcessor\nfrom transformers import ResNetForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\") # Datasets need to be downloaded manually.\nimage = dataset[\"test\"][\"image\"][0]\n\nprocessor = AutoImageProcessor.from_pretrained(\"PyTorch-NPU/resnet_50\")\nmodel = ResNetForImageClassification.from_pretrained(\"PyTorch-NPU/resnet_50\", device_map=\"npu:0\")\n\ninputs = processor(image, return_tensors=\"pt\").to(\"npu:0\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])\n\nBibTeX entry and citation info\n@inproceedings{he2016deep,\n  title={Deep residual learning for image recognition},\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages={770--778},\n  year={2016}\n}\n",
    "tags": "[\"Image Classification\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"NeMo\", \"ELM\", \"English\", \"Apache License 2.0\", \"imagenet-1k\", \"vision\"]"
  },
  {
    "url": "https://gitcode.com/openMind/tapas_base_finetuned_wtq",
    "project_name": "tapas_base_finetuned_wtq",
    "readme": "Original Text\nä¿®æ”¹\n\nåœ¨åŸå§‹ README ä¸­æ·»åŠ äº† CANN ç‰ˆæœ¬ä¾èµ–æ€§æè¿°ï¼Œå¹¶ä¿®æ”¹äº†ç¤ºä¾‹ä»£ç ã€‚\n\nåŸºäº WikiTable é—®é¢˜ï¼ˆWTQï¼‰çš„ TAPAS åŸºç¡€æ¨¡å‹å¾®è°ƒ\n\næ­¤æ¨¡å‹æœ‰ä¸¤ä¸ªç‰ˆæœ¬å¯ä¾›ä½¿ç”¨ã€‚é»˜è®¤ç‰ˆæœ¬å¯¹åº”äº åŸå§‹ GitHub ä»“åº“ ä¸­çš„ tapas_wtq_wikisql_sqa_inter_masklm_base_reset æ£€æŸ¥ç‚¹ã€‚ æ­¤æ¨¡å‹åœ¨ MLM ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶è¿›è¡Œä½œè€…ç§°ä¸ºä¸­é—´é¢„è®­ç»ƒçš„é¢å¤–æ­¥éª¤ï¼Œç„¶åä¾æ¬¡åœ¨ SQAã€WikiSQL å’Œæœ€ç»ˆ WTQ ä¸Šè¿›è¡Œå¾®è°ƒã€‚å®ƒä½¿ç”¨ç›¸å¯¹ä½ç½®åµŒå…¥ï¼ˆå³åœ¨è¡¨æ ¼çš„æ¯ä¸ªå•å…ƒæ ¼ä¸­é‡ç½®ä½ç½®ç´¢å¼•ï¼‰ã€‚\n\nå¦ä¸€ä¸ªï¼ˆéé»˜è®¤ï¼‰å¯ç”¨ç‰ˆæœ¬æ˜¯ï¼š\n\nno_resetï¼Œå¯¹åº”äº tapas_wtq_wikisql_sqa_inter_masklm_baseï¼ˆä¸­é—´é¢„è®­ç»ƒï¼Œç»å¯¹ä½ç½®åµŒå…¥ï¼‰ã€‚\næ¨¡å‹æè¿°\n\nTAPAS æ˜¯ä¸€ç§ç±»ä¼¼äº BERT çš„è½¬æ¢å™¨æ¨¡å‹ï¼Œåœ¨æ¥è‡ªç»´åŸºçš„å¤§é‡è‹±æ–‡æ•°æ®é›†ä¸Šä»¥è‡ªç›‘ç£æ–¹å¼è¿›è¡Œäº†é¢„è®­ç»ƒã€‚ è¿™æ„å‘³ç€å®ƒä»…åœ¨æœªç»ä»»ä½•äººå·¥æ ‡æ³¨çš„åŸå§‹è¡¨æ ¼å’Œå…³è”æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ˆè¿™ä¹Ÿæ˜¯å®ƒèƒ½ä½¿ç”¨å¤§é‡å…¬å¼€æ•°æ®çš„åŸå› ï¼‰ï¼Œé€šè¿‡è‡ªåŠ¨æµç¨‹ä»è¿™äº›æ–‡æœ¬ç”Ÿæˆè¾“å…¥å’Œæ ‡ç­¾ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒæ˜¯ç”¨ä»¥ä¸‹ä¸¤ä¸ªç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒçš„ï¼š\n\né®è”½è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ï¼šæ¨¡å‹æ¥æ”¶ä¸€ä¸ªï¼ˆå±•å¹³çš„ï¼‰è¡¨æ ¼å’Œå…³è”ä¸Šä¸‹æ–‡ï¼Œéšæœºé®è”½è¾“å…¥ä¸­çš„ 15% çš„å•è¯ï¼Œç„¶åå°†æ•´ä¸ªï¼ˆéƒ¨åˆ†é®è”½ï¼‰åºåˆ—é€šè¿‡æ¨¡å‹ã€‚æ¨¡å‹éšåéœ€è¦é¢„æµ‹é®è”½çš„å•è¯ã€‚è¿™ä¸ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰é€šå¸¸é€ä¸ªæŸ¥çœ‹å•è¯ï¼Œæˆ–è€…åƒ GPT è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹å†…éƒ¨é®è”½æœªæ¥æ ‡è®°çš„æ–¹å¼ä¸åŒã€‚å®ƒä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ è¡¨æ ¼åŠå…¶å…³è”æ–‡æœ¬çš„åŒå‘è¡¨ç¤ºã€‚\nä¸­é—´é¢„è®­ç»ƒï¼šä¸ºäº†é¼“åŠ±åœ¨è¡¨æ ¼ä¸Šè¿›è¡Œæ•°å€¼æ¨ç†ï¼Œä½œè€…é€šè¿‡åˆ›å»ºæ•°ç™¾ä¸‡ä¸ªå¥æ³•ç”Ÿæˆçš„è®­ç»ƒç¤ºä¾‹çš„å¹³è¡¡æ•°æ®é›†ï¼Œå¯¹æ¨¡å‹è¿›è¡Œäº†é¢å¤–çš„é¢„è®­ç»ƒã€‚åœ¨æ­¤ï¼Œæ¨¡å‹å¿…é¡»é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ä¸€ä¸ªå¥å­æ˜¯å¦è¢«è¡¨æ ¼å†…å®¹æ‰€æ”¯æŒæˆ–åé©³ã€‚è®­ç»ƒç¤ºä¾‹åŸºäºåˆæˆå£°æ˜ä»¥åŠåäº‹å®å£°æ˜åˆ›å»ºã€‚\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å­¦ä¹ äº†è¡¨æ ¼å’Œå…³è”æ–‡æœ¬ä¸­ä½¿ç”¨çš„è‹±è¯­è¯­è¨€çš„å†…éƒ¨è¡¨ç¤ºï¼Œè¿™å¯ä»¥ç”¨æ¥æå–å¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚å›ç­”å…³äºè¡¨æ ¼çš„é—®é¢˜æˆ–ç¡®å®šä¸€ä¸ªå¥å­æ˜¯å¦è¢«è¡¨æ ¼å†…å®¹æ‰€è•´å«æˆ–åé©³ï¼‰æœ‰ç”¨çš„ç‰¹å¾ã€‚å¾®è°ƒæ˜¯é€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¹‹ä¸Šæ·»åŠ ä¸€ä¸ªå•å…ƒæ ¼é€‰æ‹©å¤´å’Œèšåˆå¤´æ¥å®Œæˆçš„ï¼Œç„¶ååœ¨è¿™äº›éšæœºåˆå§‹åŒ–çš„åˆ†ç±»å¤´ä¸åŸºç¡€æ¨¡å‹ä¹‹é—´å…±åŒè®­ç»ƒï¼Œä¾æ¬¡åœ¨ SQaã€WikiSQL å’Œæœ€ç»ˆ WTQ ä¸Šè¿›è¡Œã€‚\n\né¢„å®šç”¨é€”åŠé™åˆ¶\n\næ‚¨å¯ä»¥ä½¿ç”¨æ­¤æ¨¡å‹å›ç­”ä¸è¡¨æ ¼ç›¸å…³çš„é—®é¢˜ã€‚\n\nè®­ç»ƒæµç¨‹\né¢„å¤„ç†\n\næ–‡æœ¬è¢«è½¬æ¢ä¸ºå°å†™ï¼Œå¹¶ä½¿ç”¨ WordPiece å’Œè¯æ±‡é‡å¤§å°ä¸º 30,000 è¿›è¡Œåˆ†è¯ã€‚æ¨¡å‹çš„è¾“å…¥å½¢å¼å¦‚ä¸‹ï¼š\n\n[CLS] Question [SEP] Flattened table [SEP]\n\n\nä½œè€…ä»¬é¦–å…ˆåˆ©ç”¨è‡ªåŠ¨è½¬æ¢è„šæœ¬æ¥å°†WTQæ•°æ®é›†è½¬æ¢ä¸ºSQAçš„æ ¼å¼ã€‚\n\nå¾®è°ƒ\n\næ¨¡å‹åœ¨32ä¸ªCloud TPU v3æ ¸å¿ƒä¸Šè¿›è¡Œäº†ä¸ºæœŸ50,000æ­¥çš„å¾®è°ƒï¼Œåºåˆ—é•¿åº¦æœ€å¤§ä¸º512ï¼Œæ‰¹é‡å¤§å°ä¸º512ã€‚ åœ¨è¿™ç§é…ç½®ä¸‹ï¼Œå¾®è°ƒå¤§çº¦éœ€è¦10å°æ—¶ã€‚ä½¿ç”¨çš„ä¼˜åŒ–å™¨æ˜¯Adamï¼Œå­¦ä¹ ç‡ä¸º1.93581e-5ï¼Œé¢„çƒ­æ¯”ä¾‹ä¸º0.128960ã€‚æ·»åŠ äº†å½’çº³åç½®ï¼Œä½¿å¾—æ¨¡å‹ä»…é€‰æ‹©åŒä¸€åˆ—çš„å•å…ƒæ ¼ã€‚è¿™é€šè¿‡TapasConfigçš„select_one_columnå‚æ•°ä½“ç°ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è§è®ºæ–‡ï¼ˆè¡¨æ ¼11å’Œ12ï¼‰ã€‚\n\næ¨ç†\nimport torch\nimport pandas as pd\n\nfrom openmind_hub import snapshot_download\nfrom openmind import is_torch_npu_available, pipeline\n\n\nmodel_path = snapshot_download(\"PyTorch-NPU/tapas_base_finetuned_wtq\", revision=\"main\", resume_download=True,\n                                ignore_patterns=[\"*.h5\", \"*.ot\", \"*.msgpack\"])\n\n# prepare table + question\ndata = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\ntable = pd.DataFrame.from_dict(data)\nquestion = \"how many movies does Leonardo Di Caprio have?\"\n\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=model_path, device=\"npu\")\n\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n\nBibTeXæ¡ç›®ä¸å¼•ç”¨ä¿¡æ¯\n@misc{herzig2020tapas,\n      title={TAPAS: Weakly Supervised Table Parsing via Pre-training}, \n      author={Jonathan Herzig and PaweÅ‚ Krzysztof Nowak and Thomas MÃ¼ller and Francesco Piccinno and Julian Martin Eisenschlos},\n      year={2020},\n      eprint={2004.02349},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n\nå½“ç„¶å¯ä»¥ï¼Œä¸è¿‡è¯·æ‚¨æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬ï¼Œæˆ‘æ‰èƒ½è¿›è¡Œç¿»è¯‘å·¥ä½œã€‚è°¢è°¢ï¼\n\n@misc{eisenschlos2020understanding,\n      title={Understanding tables with intermediate pre-training}, \n      author={Julian Martin Eisenschlos and Syrine Krichene and Thomas MÃ¼ller},\n      year={2020},\n      eprint={2010.00571},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n\nå½“ç„¶ï¼Œæˆ‘ä¼šæŒ‰ç…§æ‚¨çš„è¦æ±‚è¿›è¡Œç¿»è¯‘ã€‚è¯·æ‚¨æä¾›éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ã€‚\n\n@article{DBLP:journals/corr/PasupatL15,\n  author    = {Panupong Pasupat and\n               Percy Liang},\n  title     = {Compositional Semantic Parsing on Semi-Structured Tables},\n  journal   = {CoRR},\n  volume    = {abs/1508.00305},\n  year      = {2015},\n  url       = {http://arxiv.org/abs/1508.00305},\n  archivePrefix = {arXiv},\n  eprint    = {1508.00305},\n  timestamp = {Mon, 13 Aug 2018 16:47:37 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/PasupatL15.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n\nè¯·æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬ä»¥åŠæ‚¨å¸Œæœ›è¾¾åˆ°çš„ç¿»è¯‘é£æ ¼çš„å…·ä½“è¦æ±‚ï¼Œè¿™æ ·æˆ‘æ‰èƒ½æ ¹æ®æ‚¨çš„éœ€æ±‚æä¾›ç¬¦åˆè¦æ±‚çš„ç¿»è¯‘å†…å®¹ã€‚",
    "tags": "[\"Table Question Answering\", \"PyTorch\", \"Transformers\", \"English\", \"Apache License 2.0\", \"wikitablequestions\", \"tapas\"]"
  },
  {
    "url": "https://gitcode.com/openMind/roberta_base_squad2",
    "project_name": "roberta_base_squad2",
    "readme": "Original Text\nä¿®æ”¹\nä¿®æ”¹ç¤ºä¾‹å¹¶æ·»åŠ  NPU æ”¯æŒï¼›\nä¿®æ”¹é™åˆ¶å’Œåè§ä»¥åŠé¢„æœŸç”¨é€”å’Œé™åˆ¶éƒ¨åˆ†ã€‚\nroberta-base ç”¨äº QA\n\nè¿™æ˜¯åŸºäº roberta-base çš„æ¨¡å‹ï¼Œç»è¿‡ SQuAD2.0 æ•°æ®é›†çš„å¾®è°ƒã€‚å®ƒç»è¿‡è®­ç»ƒï¼Œèƒ½å¤Ÿå¤„ç†é—®é¢˜-ç­”æ¡ˆå¯¹ï¼ŒåŒ…æ‹¬æ— æ³•å›ç­”çš„é—®é¢˜ï¼Œç”¨äºé—®é¢˜å›ç­”ä»»åŠ¡ã€‚\n\næ¦‚è¿°\n\nè¯­è¨€æ¨¡å‹ï¼š roberta-base\nè¯­è¨€ï¼š è‹±è¯­\nä¸‹æ¸¸ä»»åŠ¡ï¼š æå–å¼ QA\nè®­ç»ƒæ•°æ®ï¼š SQuAD 2.0\nè¯„ä¼°æ•°æ®ï¼š SQuAD 2.0\nä»£ç ï¼š å‚è§ åœ¨ Haystack ä¸Šçš„ç¤ºä¾‹ QA ç®¡é“\nåŸºç¡€è®¾æ–½ï¼š 4x Tesla v100\n\nè¶…å‚æ•°\nbatch_size = 96\nn_epochs = 2\nbase_LM_model = \"roberta-base\"\nmax_seq_len = 386\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n\nä½¿ç”¨ç²¾ç®€æ¨¡å‹æ›¿ä»£\n\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†è¯¥æ¨¡å‹çš„ç²¾ç®€ç‰ˆï¼Œåä¸º deepset/tinyroberta-squad2ã€‚ç²¾ç®€æ¨¡å‹çš„é¢„æµ‹è´¨é‡ä¸åŸºç¡€æ¨¡å‹ç›¸å½“ï¼Œä½†è¿è¡Œé€Ÿåº¦æ˜¯åŸºç¡€æ¨¡å‹çš„ä¸¤å€ã€‚\n\nä½¿ç”¨æ–¹æ³•\nåœ¨ Haystack ä¸­\n\nHaystack æ˜¯ deepset å¼€å‘çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¡†æ¶ã€‚æ‚¨å¯ä»¥åœ¨ Haystack ç®¡é“ä¸­ä½¿ç”¨æ­¤æ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡ï¼ˆæ¶µç›–ä¼—å¤šæ–‡æ¡£ï¼‰çš„é—®ç­”å¤„ç†ã€‚åœ¨ Haystack ä¸­åŠ è½½æ¨¡å‹å¦‚ä¸‹ï¼š\n\nreader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\")\n# or \nreader = TransformersReader(model_name_or_path=\"deepset/roberta-base-squad2\",tokenizer=\"deepset/roberta-base-squad2\")\n\n\nè¦æŸ¥çœ‹ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ roberta-base-squad2 è¿›è¡Œé—®ç­”ï¼ˆQuestion Answeringï¼‰ï¼Œè¯·å‚è€ƒ Haystack æ–‡æ¡£ä¸­çš„æ•™ç¨‹ã€‚\n\nåœ¨ OpenMind ä¸­\nimport torch\nfrom openmind import pipeline\n\nmodel_name = \"PyTorch-NPU/roberta_base_squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, device_map=\"auto\", tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\næ€§èƒ½è¡¨ç°\n\nåœ¨ SQuAD 2.0 å¼€å‘é›†ä¸Šè¯„ä¼°ï¼Œä½¿ç”¨å®˜æ–¹è¯„ä¼°è„šæœ¬ã€‚\n\n\"exact\": 79.87029394424324,\n\"f1\": 82.91251169582613,\n\n\"total\": 11873,\n\"HasAns_exact\": 77.93522267206478,\n\"HasAns_f1\": 84.02838248389763,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 81.79983179142137,\n\"NoAns_f1\": 81.79983179142137,\n\"NoAns_total\": 5945\n\nä½œè€…\n\nBranden Chan: branden.chan@deepset.ai\nTimo MÃ¶ller: timo.moeller@deepset.ai\nMalte Pietsch: malte.pietsch@deepset.ai\nTanay Soni: tanay.soni@deepset.ai\n\nå…³äºæˆ‘ä»¬\n\ndeepset æ˜¯å¼€æºè‡ªç„¶è¯­è¨€å¤„ç†æ¡†æ¶ Haystack çš„èƒŒåå…¬å¸ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å¸®åŠ©æ‚¨æ„å»ºä½¿ç”¨é—®ç­”ã€æ‘˜è¦ã€æ’åºç­‰åŠŸèƒ½çš„ç”Ÿäº§å°±ç»ªçš„è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿã€‚\n\næˆ‘ä»¬çš„ä¸€äº›å…¶ä»–å·¥ä½œï¼š\n\nè’¸é¦ç‰ˆ roberta-base-squad2ï¼ˆåˆå \"tinyroberta-squad2\"ï¼‰\nGerman BERTï¼ˆåˆå \"bert-base-german-cased\"ï¼‰\nGermanQuAD å’Œ GermanDPR æ•°æ®é›†åŠæ¨¡å‹ï¼ˆåˆå \"gelectra-base-germanquad\", \"gbert-base-germandpr\"ï¼‰\nè”ç³»æˆ‘ä»¬å¹¶åŠ å…¥ Haystack ç¤¾åŒº\n\næƒ³è¦äº†è§£æ›´å¤šå…³äº Haystack çš„ä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„ GitHub ä»“åº“å’Œ æ–‡æ¡£ã€‚\n\næˆ‘ä»¬è¿˜æ‹¥æœ‰ä¸€ä¸ª é¢å‘æ‰€æœ‰äººçš„ Discord ç¤¾åŒºï¼\n\nTwitter | LinkedIn | Discord | GitHub è®¨è®º | ç½‘ç«™\n\né¡ºä¾¿ä¸€æï¼šæˆ‘ä»¬æ­£åœ¨æ‹›è˜ï¼",
    "tags": "[\"Question Answering\", \"PyTorch\", \"Transformers\", \"Safetensors\", \"English\", \"Creative Commons Attribution 4.0\", \"squad_v2\"]"
  },
  {
    "url": "https://gitcode.com/openMind/SDXL-Lightning",
    "project_name": "SDXL-Lightning",
    "readme": "Original Text\nSDXL-é—ªç”µ\n\nSDXL-é—ªç”µæ˜¯ä¸€æ¬¾æé€Ÿæ–‡ç”Ÿå›¾ç”Ÿæˆæ¨¡å‹ï¼Œä»…éœ€å‡ æ­¥å³å¯ç”Ÿæˆé«˜è´¨é‡çš„1024åƒç´ å›¾åƒã€‚æ›´å¤šæŠ€æœ¯ç»†èŠ‚è¯·å‚é˜…æˆ‘ä»¬çš„ç ”ç©¶è®ºæ–‡ï¼šSDXL-é—ªç”µï¼šæ¸è¿›å¼å¯¹æŠ—æ‰©æ•£è’¸é¦ã€‚ä½œä¸ºç ”ç©¶æˆæœçš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¼€æºäº†è¯¥æ¨¡å‹ã€‚\n\næœ¬æ¨¡å‹åŸºäºstabilityai/stable-diffusion-xl-base-1.0è’¸é¦è€Œæ¥ã€‚æ­¤ä»“åº“åŒ…å«1æ­¥ã€2æ­¥ã€4æ­¥å’Œ8æ­¥è’¸é¦çš„æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚å…¶ä¸­2æ­¥ã€4æ­¥å’Œ8æ­¥æ¨¡å‹çš„ç”Ÿæˆè´¨é‡å°¤ä¸ºå‡ºè‰²ï¼Œ1æ­¥æ¨¡å‹åˆ™æ›´å…·å®éªŒæ€§è´¨ã€‚\n\næˆ‘ä»¬æä¾›å®Œæ•´UNetå’ŒLoRAä¸¤ç§æ£€æŸ¥ç‚¹æ ¼å¼ã€‚å®Œæ•´UNetæ¨¡å‹å“è´¨æœ€ä½³ï¼ŒLoRAæ¨¡å‹å¯é€‚é…å…¶ä»–åŸºç¡€æ¨¡å‹ã€‚\n\né‡è¦æ›´æ–°\nç¤ºä¾‹é€‚é…æ˜‡è…¾å¹³å°å¹¶æ–°å¢NPUæ”¯æŒ\nåœ¨çº¿ä½“éªŒ\nå…¨é…ç½®ç”Ÿæˆï¼ˆæœ€ä½³æ•ˆæœï¼‰ï¼šæ¼”ç¤ºå¹³å°\næ¨¡å‹æ–‡ä»¶\nsdxl_lightning_Næ­¥.safetensorsï¼šå…¨åŠŸèƒ½æ£€æŸ¥ç‚¹ï¼ˆé€‚ç”¨äºComfyUIï¼‰\nsdxl_lightning_Næ­¥_unet.safetensorsï¼šçº¯UNetæ£€æŸ¥ç‚¹ï¼ˆé€‚ç”¨äºDiffusersï¼‰\nsdxl_lightning_Næ­¥_lora.safetensorsï¼šLoRAæ£€æŸ¥ç‚¹ï¼ˆé€‚é…Diffuserså’ŒComfyUIï¼‰\nDiffusersä½¿ç”¨æŒ‡å—\n\nè¯·ç¡®ä¿ä½¿ç”¨ä¸æ¨ç†æ­¥æ•°åŒ¹é…çš„å¯¹åº”æ£€æŸ¥ç‚¹ã€‚\n\n2æ­¥/4æ­¥/8æ­¥UNetæ¨¡å‹\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\nfrom safetensors.torch import load_file\nfrom openmind import is_torch_npu_available\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelse:\n    device = \"cpu\"\n\nbase = \"./PyTorch-NPU/stable-diffusion-xl-base-1_0\"\nckpt = \"./PyTorch-NPU/SDXL-Lightning/sdxl_lightning_4step_unet.safetensors\" # Use the correct ckpt for your step setting!\n\n# Load model.\nunet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(device, torch.float16)\nunet.load_state_dict(load_file(ckpt))\npipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n\n# Ensure sampler uses \"trailing\" timesteps.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n\n2æ­¥ã€4æ­¥ã€8æ­¥LoRA\n\nä»…åœ¨ä½¿ç”¨éSDXLåŸºç¡€æ¨¡å‹æ—¶é‡‡ç”¨LoRAæŠ€æœ¯ã€‚è‹¥è¿½æ±‚æ›´ä¼˜ç”»è´¨ï¼Œè¯·é€‰ç”¨æˆ‘ä»¬çš„UNeté¢„è®­ç»ƒæ¨¡å‹ã€‚\n\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, EulerDiscreteScheduler\nfrom openmind import is_torch_npu_available\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelse:\n    device = \"cpu\"\n\nbase = \"./PyTorch-NPU/stable-diffusion-xl-base-1_0\"\nckpt = \"./PyTorch-NPU/SDXL-Lightning/sdxl_lightning_4step_lora.safetensors\" # Use the correct ckpt for your step setting!\n\n# Load model.\npipe = StableDiffusionXLPipeline.from_pretrained(base, torch_dtype=torch.float16, variant=\"fp16\").to(device)\npipe.load_lora_weights(ckpt)\npipe.fuse_lora()\n\n# Ensure sampler uses \"trailing\" timesteps.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n\nä¸€æ­¥å¼UNet\n\nä¸€æ­¥å¼æ¨¡å‹ä»…ä¸ºå®éªŒæ€§è´¨ï¼Œå…¶ç”Ÿæˆè´¨é‡ç¨³å®šæ€§è¾ƒä½ã€‚å»ºè®®ä½¿ç”¨ä¸¤æ­¥å¼æ¨¡å‹ä»¥è·å¾—æ›´ä¼˜æ•ˆæœã€‚\n\næ³¨æ„ï¼šä¸€æ­¥å¼æ¨¡å‹é‡‡ç”¨\"æ ·æœ¬\"é¢„æµ‹è€Œé\"epsilon\"é¢„æµ‹ï¼éœ€æ­£ç¡®é…ç½®è°ƒåº¦å™¨å‚æ•°ã€‚\n\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\nfrom safetensors.torch import load_file\nfrom openmind import is_torch_npu_available\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelse:\n    device = \"cpu\"\n\nbase = \"./PyTorch-NPU/stable-diffusion-xl-base-1_0\"\nckpt = \"./PyTorch-NPU/SDXL-Lightning/sdxl_lightning_1step_unet_x0.safetensors\" # Use the correct ckpt for your step setting!\n \n# Load model.\nunet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(device, torch.float16)\nunet.load_state_dict(load_file(ckpt))\npipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n\n# Ensure sampler uses \"trailing\" timesteps and \"sample\" prediction type.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", prediction_type=\"sample\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=1, guidance_scale=0).images[0].save(\"output.png\")\n\nComfyUI ä½¿ç”¨æŒ‡å—\n\nè¯·å§‹ç»ˆä¸ºå¯¹åº”çš„æ¨ç†æ­¥éª¤ä½¿ç”¨æ­£ç¡®çš„æ£€æŸ¥ç‚¹ã€‚\nå»ºè®®é‡‡ç”¨ Euler é‡‡æ ·å™¨é…åˆ sgm_uniform è°ƒåº¦å™¨ã€‚\n\n2æ­¥/4æ­¥/8æ­¥å®Œæ•´æ¨¡å‹\nå°†å®Œæ•´æ£€æŸ¥ç‚¹ (sdxl_lightning_Nstep.safetensors) ä¸‹è½½è‡³ /ComfyUI/models/checkpoints ç›®å½•\nä¸‹è½½æˆ‘ä»¬çš„ ComfyUI å®Œæ•´å·¥ä½œæµ\n\n2æ­¥/4æ­¥/8æ­¥ LoRA æ¨¡å‹\n\nä»…åœ¨ä½¿ç”¨é SDXL åŸºç¡€æ¨¡å‹æ—¶é‡‡ç”¨ LoRA æ–¹æ¡ˆï¼Œå¦åˆ™æ¨èä½¿ç”¨å®Œæ•´æ£€æŸ¥ç‚¹ä»¥è·å¾—æ›´ä½³æ•ˆæœã€‚\n\nå‡†å¤‡æ‚¨çš„åŸºç¡€æ¨¡å‹\nå°† LoRA æ£€æŸ¥ç‚¹ (sdxl_lightning_Nstep_lora.safetensors) ä¸‹è½½è‡³ /ComfyUI/models/loras ç›®å½•\nä¸‹è½½æˆ‘ä»¬çš„ ComfyUI LoRA å·¥ä½œæµ\n\n1æ­¥æ¨¡å‹\n\n1æ­¥æ¨¡å‹ç›®å‰ä»…ä¸ºå®éªŒæ€§è´¨ï¼Œç”Ÿæˆè´¨é‡æ³¢åŠ¨è¾ƒå¤§ã€‚å»ºè®®ä¼˜å…ˆé€‰ç”¨2æ­¥æ¨¡å‹ä»¥è·å¾—ç¨³å®šæ•ˆæœã€‚\n\nå°† ComfyUI æ›´æ–°è‡³æœ€æ–°ç‰ˆæœ¬\nå°†å®Œæ•´æ£€æŸ¥ç‚¹ (sdxl_lightning_1step_x0.safetensors) ä¸‹è½½è‡³ /ComfyUI/models/checkpoints ç›®å½•\nä¸‹è½½æˆ‘ä»¬çš„ ComfyUI 1æ­¥å®Œæ•´å·¥ä½œæµ\n\nå¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œ\n@misc{lin2024sdxllightning,\n      title={SDXL-Lightning: Progressive Adversarial Diffusion Distillation}, \n      author={Shanchuan Lin and Anran Wang and Xiao Yang},\n      year={2024},\n      eprint={2402.13929},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n",
    "tags": "[\"Diffusers\", \"Safetensors\", \"Open Rail++-M License\", \"stable-diffusion\", \"text-to-image\"]"
  },
  {
    "url": "https://gitcode.com/openMind/t5_base",
    "project_name": "t5_base",
    "readme": "Original Text\nT5åŸºç¡€ç‰ˆæ¨¡å‹å¡ç‰‡\nä¿®æ”¹è¯´æ˜\n\nåœ¨åŸå§‹READMEä¸­å¢åŠ äº†CANNç‰ˆæœ¬ä¾èµ–æè¿°ï¼Œå¹¶ä¿®æ”¹äº†ç¤ºä¾‹ä»£ç ã€‚\n\nç›®å½•\næ¨¡å‹è¯¦æƒ…\nç”¨é€”\nåå·®ã€é£é™©ä¸é™åˆ¶\nè®­ç»ƒè¯¦æƒ…\nè¯„ä¼°\nç¯å¢ƒå½±å“\nå¼•ç”¨\næ¨¡å‹å¡ç‰‡ä½œè€…\nå¿«é€Ÿå¼€å§‹ä½¿ç”¨æ¨¡å‹\næ¨¡å‹è¯¦æƒ…\næ¨¡å‹æè¿°\n\næ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢Transformerï¼ˆT5ï¼‰çš„å¼€å‘è€…åœ¨åšå®¢ä¸­å†™é“ï¼š\n\né€šè¿‡T5ï¼Œæˆ‘ä»¬æå‡ºå°†æ‰€æœ‰NLPä»»åŠ¡é‡æ„ä¸ºç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼ï¼Œè¾“å…¥å’Œè¾“å‡ºå§‹ç»ˆæ˜¯æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œè¿™ä¸BERTç±»æ¨¡å‹åªèƒ½è¾“å‡ºç±»åˆ«æ ‡ç­¾æˆ–è¾“å…¥ç‰‡æ®µå½¢æˆå¯¹æ¯”ã€‚æˆ‘ä»¬çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ¡†æ¶å…è®¸æˆ‘ä»¬åœ¨ä»»ä½•NLPä»»åŠ¡ä¸Šä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œè¶…å‚æ•°ã€‚\n\nT5-Baseæ˜¯åŒ…å«2.2äº¿å‚æ•°çš„æ£€æŸ¥ç‚¹ã€‚\n\nå¼€å‘è€…: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liuã€‚å‚è§ç›¸å…³è®ºæ–‡å’ŒGitHubä»“åº“\næ¨¡å‹ç±»å‹: è¯­è¨€æ¨¡å‹\næ”¯æŒè¯­è¨€: è‹±è¯­ã€æ³•è¯­ã€ç½—é©¬å°¼äºšè¯­ã€å¾·è¯­\nè®¸å¯åè®®: Apache 2.0\næ›´å¤šä¿¡æ¯:\nç ”ç©¶è®ºæ–‡\nGoogle T5åšå®¢æ–‡ç« \nGitHubä»“åº“\nç”¨é€”\nç›´æ¥ä½¿ç”¨ä¸ä¸‹æ¸¸åº”ç”¨\n\nå¼€å‘è€…åœ¨åšå®¢ä¸­è¡¨ç¤ºï¼š\n\næˆ‘ä»¬çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ¡†æ¶å…è®¸æˆ‘ä»¬åœ¨ä»»ä½•NLPä»»åŠ¡ä¸Šä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œè¶…å‚æ•°ï¼ŒåŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€æ–‡æ¡£æ‘˜è¦ã€é—®ç­”å’Œåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰ã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥é€šè¿‡è®­ç»ƒæ¨¡å‹é¢„æµ‹æ•°å­—çš„å­—ç¬¦ä¸²è¡¨ç¤ºè€Œéæ•°å­—æœ¬èº«ï¼Œå°†å…¶åº”ç”¨äºå›å½’ä»»åŠ¡ã€‚\n\nè¯¦æƒ…å‚è§åšå®¢æ–‡ç« å’Œç ”ç©¶è®ºæ–‡ã€‚\n\nä¸é€‚ç”¨åœºæ™¯\n\néœ€è¦æ›´å¤šä¿¡æ¯ã€‚\n\nåå·®ã€é£é™©ä¸é™åˆ¶\n\néœ€è¦æ›´å¤šä¿¡æ¯ã€‚\n\nå»ºè®®\n\néœ€è¦æ›´å¤šä¿¡æ¯ã€‚\n\nè®­ç»ƒè¯¦æƒ…\nè®­ç»ƒæ•°æ®\n\næ¨¡å‹é€šè¿‡æ— ç›‘ç£(1.)ä¸æœ‰ç›‘ç£ä»»åŠ¡(2.)çš„å¤šä»»åŠ¡æ··åˆè¿›è¡Œé¢„è®­ç»ƒã€‚å…·ä½“ä½¿ç”¨ä»¥ä¸‹æ•°æ®é›†ï¼š\n\næ— ç›‘ç£å»å™ªç›®æ ‡æ•°æ®é›†:\nC4\nWiki-DPR\næœ‰ç›‘ç£æ–‡æœ¬åˆ°æ–‡æœ¬è¯­è¨€å»ºæ¨¡ç›®æ ‡æ•°æ®é›†\nå¥å­å¯æ¥å—æ€§åˆ¤æ–­\nCoLA Warstadtç­‰, 2018\næƒ…æ„Ÿåˆ†æ\nSST-2 Socherç­‰, 2013\nå¤è¿°/å¥å­ç›¸ä¼¼åº¦\nMRPC Dolanå’ŒBrockett, 2005\nSTS-B Cerç­‰, 2017\nQQP Iyerç­‰, 2017\nè‡ªç„¶è¯­è¨€æ¨ç†\nMNLI Williamsç­‰, 2017\nQNLI Rajpurkarç­‰,2016\nRTE Daganç­‰, 2005\nCB De Marneffç­‰, 2019\nå¥å­è¡¥å…¨\nCOPA Roemmeleç­‰, 2011\nè¯ä¹‰æ¶ˆæ­§\nWIC Pilehvarå’ŒCamacho-Collados, 2018\né—®ç­”ç³»ç»Ÿ\nMultiRC Khashabiç­‰, 2018\nReCoRD Zhangç­‰, 2018\nBoolQ Clarkç­‰, 2019\nè®­ç»ƒè¿‡ç¨‹\n\nå¼€å‘è€…åœ¨æ‘˜è¦ä¸­å†™é“ï¼š\n\næœ¬æ–‡é€šè¿‡å¼•å…¥å°†æ¯ä¸ªè¯­è¨€é—®é¢˜è½¬æ¢ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ¢ç´¢äº†NLPè¿ç§»å­¦ä¹ æŠ€æœ¯çš„å…¨æ™¯ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç ”ç©¶æ¯”è¾ƒäº†æ•°åé¡¹è¯­è¨€ç†è§£ä»»åŠ¡ä¸­çš„é¢„è®­ç»ƒç›®æ ‡ã€æ¶æ„ã€æ— æ ‡ç­¾æ•°æ®é›†ã€è¿ç§»æ–¹æ³•ç­‰å› ç´ ã€‚\n\næ‰€æå‡ºçš„T5æ¡†æ¶æ•´åˆäº†è®ºæ–‡ä¸­ç ”ç©¶çš„å„ç§æ–¹æ³•ã€‚è¯¦è§ç ”ç©¶è®ºæ–‡ã€‚\n\nè¯„ä¼°\næµ‹è¯•æ•°æ®ã€å› ç´ ä¸æŒ‡æ ‡\n\nå¼€å‘è€…åœ¨24ä¸ªä»»åŠ¡ä¸Šè¯„ä¼°äº†æ¨¡å‹ï¼Œå®Œæ•´ç»†èŠ‚å‚è§ç ”ç©¶è®ºæ–‡ã€‚\n\nç»“æœ\n\nT5-Baseçš„å®Œæ•´ç»“æœè§ç ”ç©¶è®ºæ–‡è¡¨14ã€‚\n\nç¯å¢ƒå½±å“\n\nç¢³æ’æ”¾é‡å¯ä½¿ç”¨Lacosteç­‰(2019)æå‡ºçš„æœºå™¨å­¦ä¹ å½±å“è®¡ç®—å™¨ä¼°ç®—ã€‚\n\nç¡¬ä»¶ç±»å‹: Google Cloud TPU Pods\nä½¿ç”¨æ—¶é•¿: éœ€è¦æ›´å¤šä¿¡æ¯\näº‘æœåŠ¡å•†: GCP\nè®¡ç®—åŒºåŸŸ: éœ€è¦æ›´å¤šä¿¡æ¯\nç¢³æ’æ”¾é‡: éœ€è¦æ›´å¤šä¿¡æ¯\nå¼•ç”¨\n\nBibTeX:\n\n@article{2020t5,\n  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {140},\n  pages   = {1-67},\n  url     = {http://jmlr.org/papers/v21/20-074.html}\n}\n\n\nç¾å›½å¿ƒç†å­¦ä¼šæ ¼å¼ï¼ˆAPAï¼‰:\n\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). æ¢ç´¢è¿ç§»å­¦ä¹ çš„æé™ï¼šåŸºäºç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨çš„ç ”ç©¶ã€‚ ã€Šæœºå™¨å­¦ä¹ ç ”ç©¶æœŸåˆŠã€‹ï¼Œ21(140), 1-67.\næ¨¡å‹å¡ä½œè€…\n\næœ¬æ¨¡å‹å¡ç”±Hugging Faceå›¢é˜Ÿæ’°å†™ã€‚\n\nå¦‚ä½•å¼€å§‹ä½¿ç”¨è¯¥æ¨¡å‹\n\né€šè¿‡ä»¥ä¸‹ä»£ç å¿«é€Ÿå…¥é—¨è¯¥æ¨¡å‹çš„ä½¿ç”¨ã€‚\n\nç‚¹å‡»å±•å¼€",
    "tags": "[\"Translation\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Rust\", \"Safetensors\", \"JAX\", \"English\", \"Apache License 2.0\", \"c4\", \"summarization\"]"
  },
  {
    "url": "https://gitcode.com/openMind/siglip_so400m_patch14_384",
    "project_name": "siglip_so400m_patch14_384",
    "readme": "Original Text\nSigLIPï¼ˆå½¢çŠ¶ä¼˜åŒ–æ¨¡å‹ï¼‰\n\nåœ¨384x384åˆ†è¾¨ç‡ä¸‹åŸºäºWebLié¢„è®­ç»ƒçš„SigLIPæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨Zhaiç­‰äººå‘è¡¨çš„è®ºæ–‡Sigmoid Loss for Language Image Pre-Trainingä¸­é¦–æ¬¡è¢«æå‡ºï¼Œå¹¶åœ¨è¿™ä¸ªä»“åº“ä¸­é¦–æ¬¡å‘å¸ƒã€‚\n\nè¯¥æ¨¡å‹é‡‡ç”¨SoViT-400mæ¶æ„ï¼Œè¿™æ˜¯åœ¨Alabdulmohsinç­‰äººå‘è¡¨çš„Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Designä¸­æå‡ºçš„å½¢çŠ¶ä¼˜åŒ–ç‰ˆæœ¬ã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒSigLIPæ¨¡å‹çš„å›¢é˜Ÿæœªç¼–å†™æ¨¡å‹å¡ç‰‡ï¼Œå› æ­¤æ­¤æ¨¡å‹å¡ç‰‡ç”±Hugging Faceå›¢é˜Ÿæ’°å†™ã€‚\n\nä¿®æ”¹\n\nä¿®æ”¹ç¤ºä¾‹ä»£ç éƒ¨åˆ†ã€‚\n\næ¨¡å‹æè¿°\n\nSigLIPæ˜¯CLIPï¼Œä¸€ç§å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ‹¥æœ‰æ›´ä¼˜çš„æŸå¤±å‡½æ•°ã€‚SigmoidæŸå¤±ä»…ä½œç”¨äºå›¾åƒ-æ–‡æœ¬å¯¹ï¼Œä¸éœ€è¦å¯¹æˆå¯¹ç›¸ä¼¼åº¦è¿›è¡Œå½’ä¸€åŒ–çš„å…¨å±€è§†å›¾ã€‚è¿™å…è®¸è¿›ä¸€æ­¥å¢å¤§æ‰¹é‡å¤§å°ï¼ŒåŒæ—¶åœ¨è¾ƒå°çš„æ‰¹é‡å¤§å°ä¸‹è¡¨ç°æ›´ä½³ã€‚\n\næ¨¡å‹çš„TLDRç‰ˆæœ¬ï¼Œç”±ä¸€ä½ä½œè€…æä¾›ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°è¿™é‡Œã€‚\n\né¢„æœŸç”¨é€”åŠé™åˆ¶\n\næ‚¨å¯ä»¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œå›¾åƒ-æ–‡æœ¬æ£€ç´¢ç­‰ä»»åŠ¡ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\nä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨æ­¤æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬å›¾åƒåˆ†ç±»çš„æ–¹æ³•ï¼š\n\nfrom PIL import Image\nimport requests\nfrom openmind import AutoProcessor, AutoModel\nimport torch\n\nmodel = AutoModel.from_pretrained(\"PyTorch-NPU/siglip_so400m_patch14_384\").to(\"npu:0\")\nprocessor = AutoProcessor.from_pretrained(\"PyTorch-NPU/siglip_so400m_patch14_384\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ntexts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\ninputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(\"npu:0\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nlogits_per_image = outputs.logits_per_image\nprobs = torch.sigmoid(logits_per_image) # these are the probabilities\nprint(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n\n\næˆ–è€…ï¼Œå¯ä»¥åˆ©ç”¨ç®¡é“ API æ¥ç®€åŒ–ç”¨æˆ·çš„å¤æ‚æ€§å¤„ç†ï¼š\n\nfrom openmind import pipeline\nfrom PIL import Image\nimport requests\n\n# load pipe\nimage_classifier = pipeline(task=\"zero-shot-image-classification\", model=\"PyTorch-NPU/siglip_so400m_patch14_384\", device=\"npu:0\")\n\n# load image\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# inference\noutputs = image_classifier(image, candidate_labels=[\"2 cats\", \"a plane\", \"a remote\"])\noutputs = [{\"score\": round(output[\"score\"], 4), \"label\": output[\"label\"] } for output in outputs]\nprint(outputs)\n\nè®­ç»ƒæµç¨‹\nè®­ç»ƒæ•°æ®\n\nSigLIP åœ¨ WebLI æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒ (Chen et al., 2023)ã€‚\n\né¢„å¤„ç†\n\nå›¾åƒè°ƒæ•´/ç¼©æ”¾åˆ°ç»Ÿä¸€çš„åˆ†è¾¨ç‡ï¼ˆ384x384ï¼‰ï¼Œå¹¶åœ¨ RGB é€šé“ä¸Šè¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¹³å‡å€¼è®¾ä¸ºï¼ˆ0.5, 0.5, 0.5ï¼‰ï¼Œæ ‡å‡†å·®è®¾ä¸ºï¼ˆ0.5, 0.5, 0.5ï¼‰ã€‚\n\næ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦ï¼ˆ64ä¸ªæ ‡è®°ï¼‰ã€‚\n\nè®¡ç®—\n\næ¨¡å‹åœ¨16ä¸ªTPU-v4èŠ¯ç‰‡ä¸Šè®­ç»ƒäº†ä¸‰å¤©ã€‚\n\nè¯„ä¼°ç»“æœ\n\nä»¥ä¸‹æ˜¯ SigLIP ä¸ CLIP å¯¹æ¯”çš„è¯„ä¼°ç»“æœï¼ˆæ‘˜è‡ªè®ºæ–‡ï¼‰ã€‚\n\nBibTeX æ¡ç›®ä¸å¼•ç”¨ä¿¡æ¯\n@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training}, \n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n\nè¯·æä¾›éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬ï¼Œæˆ‘ä¼šæŒ‰ç…§æ‚¨çš„è¦æ±‚å°†å…¶ç¿»è¯‘æˆä¸­æ–‡ï¼Œå¹¶ä¿æŒåŸå§‹çš„ Markdown æ ¼å¼ã€‚",
    "tags": "[\"Zero-Shot Image Classification\", \"Transformers\", \"Safetensors\", \"English\", \"Apache License 2.0\", \"vision\"]"
  },
  {
    "url": "https://gitcode.com/openMind/stable_diffusion_v1_5",
    "project_name": "stable_diffusion_v1_5",
    "readme": "Original Text\nç¨³å®šæ‰©æ•£ v1-5 æ¨¡å‹å¡\n\nç¨³å®šæ‰©æ•£æ˜¯ä¸€ç§æ½œåœ¨æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»ä½•æ–‡æœ¬è¾“å…¥ç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚\n\nStable-Diffusion-v1-5 æ£€æŸ¥ç‚¹ä»¥ Stable-Diffusion-v1-2 çš„æƒé‡åˆå§‹åŒ–ï¼Œå¹¶åœ¨512x512åˆ†è¾¨ç‡ä¸‹å¯¹\"laion-aesthetics v2 5+\"è¿›è¡Œ595kæ­¥çš„å¾®è°ƒï¼ŒåŒæ—¶å°†æ–‡æœ¬æ¡ä»¶è®¾ç½®é™ä½10%ï¼Œä»¥æ”¹å–„æ— éœ€åˆ†ç±»å™¨çš„å¼•å¯¼é‡‡æ ·ã€‚\n\næ‚¨å¯ä»¥ä½¿ç”¨ğŸ§¨Diffusersåº“å’ŒRunwayML GitHubä»“åº“æ¥ä½¿ç”¨æ­¤æ¨¡å‹ã€‚\n\nä¿®æ”¹\nä¿®æ”¹é“¾æ¥ä»¥æ‰“å¼€Mindï¼Œå¹¶æ·»åŠ npuæ”¯æŒç¤ºä¾‹\næ‰©æ•£å™¨\nfrom diffusers import StableDiffusionPipeline\nimport torch\nfrom openmind import is_torch_npu_available\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelse:\n    device = \"cpu\"\n\nmodel_id = \"./stable_diffusion_v1_5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\ngenerator = torch.Generator(device=device).manual_seed(1234)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt, generator=generator).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n\n\nä¸ºäº†è·å–æ›´è¯¦ç»†çš„æŒ‡å¯¼ã€ç”¨ä¾‹å’Œ JAX ä¸­çš„ç¤ºä¾‹ï¼Œè¯·éµå¾ªæ­¤å¤„çš„æŒ‡ç¤ºã€‚\n\nåŸå§‹ GitHub ä»“åº“\n\nä¸‹è½½æƒé‡æ–‡ä»¶\n\nv1-5-pruned-emaonly.ckpt - 4.27GBï¼Œä»…åŒ…å« ema æƒé‡ã€‚å ç”¨è¾ƒå°‘çš„ VRAM - é€‚ç”¨äºæ¨ç†\nv1-5-pruned.ckpt - 7.7GBï¼ŒåŒ…å« ema å’Œé ema æƒé‡ã€‚å ç”¨è¾ƒå¤šçš„ VRAM - é€‚ç”¨äºå¾®è°ƒ\n\néµå¾ªæ­¤å¤„çš„æŒ‡ç¤ºã€‚\n\næ¨¡å‹è¯¦æƒ…\n\nå¼€å‘è€…ï¼š Robin Rombach, Patrick Esser\n\næ¨¡å‹ç±»å‹ï¼š åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹\n\nè¯­è¨€ï¼š è‹±è¯­\n\nè®¸å¯è¯ï¼š CreativeML OpenRAIL M è®¸å¯è¯æ˜¯ä¸€ç§Open RAIL M è®¸å¯è¯ï¼Œç”±BigScienceå’ŒRAIL åˆ›è®®åœ¨è´Ÿè´£ä»»äººå·¥æ™ºèƒ½è®¸å¯é¢†åŸŸåˆä½œæˆæœçš„æ”¹ç¼–ç‰ˆã€‚è¯·å‚è§å…³äº BLOOM Open RAIL è®¸å¯è¯çš„è®ºæ–‡ï¼Œæˆ‘ä»¬çš„è®¸å¯è¯åŸºäºæ­¤ã€‚\n\næ¨¡å‹æè¿°ï¼š è¿™æ˜¯ä¸€ä¸ªå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå’Œä¿®æ”¹å›¾åƒçš„æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ç§æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨äº†å›ºå®šçš„é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ï¼ˆCLIP ViT-L/14ï¼‰ï¼Œå¦‚Imagen è®ºæ–‡ä¸­æ‰€è¿°ã€‚\n\næ›´å¤šä¿¡æ¯èµ„æºï¼š GitHub ä»“åº“ï¼Œè®ºæ–‡ã€‚\n\nå¼•ç”¨æ–¹å¼ï¼š\n\n@InProceedings{Rombach_2022_CVPR,\n    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2022},\n    pages     = {10684-10695}\n}\n\nç”¨é€”\nç›´æ¥ä½¿ç”¨\n\nè¯¥æ¨¡å‹ä»…ç”¨äºç ”ç©¶ç›®çš„ã€‚å¯èƒ½çš„ç ”ç©¶é¢†åŸŸå’Œä»»åŠ¡åŒ…æ‹¬\n\nå®‰å…¨éƒ¨ç½²å…·æœ‰ç”Ÿæˆæœ‰å®³å†…å®¹æ½œåŠ›çš„æ¨¡å‹ã€‚\næ¢ç´¢å’Œç†è§£ç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§å’Œåè§ã€‚\nç”Ÿæˆè‰ºæœ¯å“å’Œåœ¨è®¾è®¡å’Œå…¶ä»–è‰ºæœ¯è¿‡ç¨‹ä¸­çš„åº”ç”¨ã€‚\næ•™è‚²æˆ–åˆ›æ„å·¥å…·ä¸­çš„åº”ç”¨ã€‚\nç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶ã€‚\n\nä»¥ä¸‹æè¿°äº†ç¦æ­¢çš„ä½¿ç”¨æ–¹å¼ã€‚\n\nè¯¯ç”¨ã€æ¶æ„ä½¿ç”¨å’Œè¶…å‡ºèŒƒå›´çš„ä½¿ç”¨\n\næ³¨æ„ï¼šæœ¬èŠ‚å†…å®¹æºè‡ªDALLE-MINI æ¨¡å‹å¡ï¼Œä½†åŒæ ·é€‚ç”¨äº Stable Diffusion v1ã€‚\n\næ¨¡å‹ä¸åº”ç”¨äºæ•…æ„åˆ›å»ºæˆ–ä¼ æ’­å¯¹äººä»¬äº§ç”Ÿæ•Œæ„æˆ–ç–è¿œç¯å¢ƒçš„å›¾åƒã€‚è¿™åŒ…æ‹¬ç”Ÿæˆäººä»¬é¢„è§åˆ°ä¼šæ„Ÿåˆ°ä¸å®‰ã€å›°æ‰°æˆ–å†’çŠ¯çš„å›¾åƒï¼›æˆ–ä¼ æ’­å†å²ä¸Šæˆ–å½“å‰çš„åˆ»æ¿å°è±¡çš„å†…å®¹ã€‚\n\nè¶…å‡ºèŒƒå›´çš„ä½¿ç”¨\n\næ¨¡å‹æœªç»è¿‡è®­ç»ƒä»¥ç”Ÿæˆäº‹å®æˆ–çœŸå®çš„äººç‰©æˆ–äº‹ä»¶è¡¨ç¤ºï¼Œå› æ­¤ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ­¤ç±»å†…å®¹è¶…å‡ºäº†æ¨¡å‹çš„èƒ½åŠ›èŒƒå›´ã€‚\n\nè¯¯ç”¨å’Œæ¶æ„ä½¿ç”¨\n\nä½¿ç”¨æ¨¡å‹ç”Ÿæˆå¯¹ä¸ªäººæ®‹å¿çš„å†…å®¹å±äºæ»¥ç”¨ã€‚è¿™åŒ…æ‹¬ä½†ä¸é™äºï¼š\n\nç”Ÿæˆè´¬ä½ã€å»äººæ€§åŒ–æˆ–ä»¥å…¶ä»–æ–¹å¼å¯¹ä¸ªäººæˆ–å…¶ç¯å¢ƒã€æ–‡åŒ–ã€å®—æ•™ç­‰æœ‰å®³çš„è¡¨ç¤ºã€‚\næ•…æ„ä¿ƒè¿›æˆ–ä¼ æ’­æ­§è§†æ€§å†…å®¹æˆ–æœ‰å®³åˆ»æ¿å°è±¡ã€‚\nåœ¨æœªç»åŒæ„çš„æƒ…å†µä¸‹æ¨¡ä»¿ä¸ªäººã€‚\næœªå–å¾—å¯èƒ½çœ‹åˆ°è¯¥å†…å®¹çš„äººçš„åŒæ„çš„è‰²æƒ…å†…å®¹ã€‚\né”™è¯¯å’Œè¯¯å¯¼æ€§ä¿¡æ¯\nä»¤äººæ¯›éª¨æ‚šç„¶çš„æš´åŠ›å’Œè¡€è…¥å†…å®¹çš„è¡¨ç¤º\nè¿åä½¿ç”¨æ¡æ¬¾å…±äº«å—ç‰ˆæƒæˆ–è®¸å¯ä¿æŠ¤çš„èµ„æ–™ã€‚\nåˆ†äº«å—ç‰ˆæƒæˆ–è®¸å¯ä¿æŠ¤çš„èµ„æ–™çš„ä¿®æ”¹ç‰ˆæœ¬ï¼Œè¿åå…¶ä½¿ç”¨æ¡æ¬¾ã€‚\nå±€é™æ€§å’Œåè§\nå±€é™æ€§\næ¨¡å‹æ— æ³•è¾¾åˆ°å®Œç¾çš„ç…§ç‰‡çº§çœŸå®æ„Ÿ\næ¨¡å‹æ— æ³•å‘ˆç°å¯è¯»æ–‡æœ¬\næ¨¡å‹åœ¨æ¶‰åŠç»„åˆæ€§çš„æ›´å›°éš¾ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚æ¸²æŸ“ä¸â€œä¸€ä¸ªçº¢è‰²ç«‹æ–¹ä½“æ”¾åœ¨è“è‰²çƒä¸Šâ€ç›¸å¯¹åº”çš„å›¾åƒ\näººè„¸å’Œä¸€èˆ¬çš„äººæ¥è¯´å¯èƒ½æ— æ³•æ­£ç¡®ç”Ÿæˆã€‚\næ¨¡å‹ä¸»è¦ä½¿ç”¨è‹±è¯­æ ‡é¢˜è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤åœ¨å…¶ä»–è¯­è¨€ä¸­å¯èƒ½æ— æ³•å¾ˆå¥½åœ°å·¥ä½œã€‚\næ¨¡å‹çš„è‡ªåŠ¨ç¼–ç éƒ¨åˆ†æ˜¯æœ‰æŸçš„\næ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®é›†LAION-5Bä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­åŒ…å«æˆäººå†…å®¹ï¼Œæ²¡æœ‰é¢å¤–çš„å®‰å…¨æœºåˆ¶å’Œè€ƒè™‘ï¼Œä¸é€‚åˆäº§å“ä½¿ç”¨ã€‚\nåœ¨æ•°æ®é›†ä¸­æ²¡æœ‰ä½¿ç”¨é¢å¤–çš„æªæ–½å»é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸€äº›åœ¨è®­ç»ƒæ•°æ®ä¸­é‡å¤çš„å›¾åƒæœ‰ä¸€å®šçš„è®°å¿†ç¨‹åº¦ã€‚å¯ä»¥åœ¨https://rom1504.github.io/clip-retrieval/ä¸Šæœç´¢è®­ç»ƒæ•°æ®ï¼Œä»¥å¸®åŠ©æ£€æµ‹è®°å¿†å›¾åƒã€‚\nåè§\n\nå°½ç®¡å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½å¼ºåŒ–æˆ–åŠ å‰§ç¤¾ä¼šåè§ã€‚ Stable Diffusion v1 åœ¨LAION-2B(en)çš„å­é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†ä¸»è¦é™äºè‹±æ–‡æè¿°çš„å›¾åƒã€‚ æ¥è‡ªä½¿ç”¨å…¶ä»–è¯­è¨€çš„æ–‡åŒ–å’Œç¤¾åŒºçš„æ–‡å­—å’Œå›¾åƒå¾ˆå¯èƒ½æ²¡æœ‰å¾—åˆ°å……åˆ†çš„è€ƒè™‘ã€‚è¿™å½±å“äº†æ¨¡å‹çš„æ€»ä½“è¾“å‡ºï¼Œå› ä¸ºç™½äººå’Œè¥¿æ–¹æ–‡åŒ–é€šå¸¸è¢«è§†ä¸ºé»˜è®¤è®¾ç½®ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ä½¿ç”¨éè‹±è¯­æç¤ºç”Ÿæˆå†…å®¹çš„èƒ½åŠ›æ¯”ä½¿ç”¨è‹±è¯­æç¤ºæ—¶è¦å·®å¾—å¤šã€‚\n\nå®‰å…¨æ¨¡å—\n\nè¯¥æ¨¡å‹æ‰“ç®—ä¸ Diffusers ä¸­çš„å®‰å…¨æ£€æŸ¥å™¨ä¸€èµ·ä½¿ç”¨ã€‚ å®‰å…¨æ£€æŸ¥å™¨é€šè¿‡æ£€æŸ¥æ¨¡å‹è¾“å‡ºä¸å·²çŸ¥ç¡¬ç¼–ç çš„ NSFW æ¦‚å¿µæ˜¯å¦åŒ¹é…æ¥å·¥ä½œã€‚ è¿™äº›æ¦‚å¿µæ•…æ„éšè—ï¼Œä»¥å‡å°‘åå‘å·¥ç¨‹è¿‡æ»¤å™¨çš„å¯èƒ½æ€§ã€‚ å…·ä½“è€Œè¨€ï¼Œæ£€æŸ¥å™¨ä¼šåœ¨å›¾åƒç”Ÿæˆåå°†æœ‰å®³æ¦‚å¿µåœ¨ CLIPTextModel çš„åµŒå…¥ç©ºé—´ä¸­çš„ç±»æ¦‚ç‡è¿›è¡Œæ¯”è¾ƒã€‚å°†æ¦‚å¿µä¸ç”Ÿæˆçš„å›¾åƒä¸€èµ·ä¼ é€’åˆ°æ¨¡å‹ä¸­ï¼Œå¹¶ä¸æ¯ä¸ª NSFW æ¦‚å¿µçš„æ‰‹å·¥ç¨‹åº¦æƒé‡è¿›è¡Œæ¯”è¾ƒã€‚\n\nè®­ç»ƒ\n\nè®­ç»ƒæ•°æ® æ¨¡å‹å¼€å‘è€…ä½¿ç”¨äº†ä»¥ä¸‹æ•°æ®é›†æ¥è®­ç»ƒæ¨¡å‹ï¼š\n\nLAION-2B (en) åŠå…¶å­é›†ï¼ˆè§ä¸‹ä¸€èŠ‚ï¼‰\n\nè®­ç»ƒè¿‡ç¨‹ Stable Diffusion v1-5 æ˜¯ä¸€ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå®ƒç»“åˆäº†è‡ªåŠ¨ç¼–ç å™¨å’Œåœ¨è‡ªåŠ¨ç¼–ç å™¨æ½œåœ¨ç©ºé—´ä¸­è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ\n\nå›¾åƒé€šè¿‡ç¼–ç å™¨è¿›è¡Œç¼–ç ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºæ½œåœ¨çš„è¡¨ç¤ºã€‚è‡ªåŠ¨ç¼–ç å™¨ä½¿ç”¨ç›¸å¯¹é™é‡‡æ ·å› å­ 8ï¼Œå°†å½¢çŠ¶ä¸º H x W x 3 çš„å›¾åƒæ˜ å°„åˆ°å½¢çŠ¶ä¸º H/f x W/f x 4 çš„æ½œåœ¨è¡¨ç¤ºã€‚\næ–‡æœ¬æç¤ºé€šè¿‡ ViT-L/14 æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œç¼–ç ã€‚\næ–‡æœ¬ç¼–ç å™¨çš„éæ± åŒ–è¾“å‡ºé€šè¿‡äº¤å‰æ³¨æ„åŠ›é¦ˆé€åˆ°æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ UNet ä¸»å¹²ã€‚\næŸå¤±æ˜¯ä¸€ä¸ªé‡æ„ç›®æ ‡ï¼Œåœ¨æ·»åŠ åˆ°æ½œåœ¨çš„å™ªå£°å’Œ UNet é¢„æµ‹ä¹‹é—´è¿›è¡Œæ¯”è¾ƒã€‚\n\nç›®å‰æä¾›äº†å…­ä¸ª Stable Diffusion æ£€æŸ¥ç‚¹ï¼Œä»¥ä¸‹æ˜¯å®ƒä»¬çš„è®­ç»ƒè¿‡ç¨‹ã€‚\n\nstable-diffusion-v1-1ï¼šåœ¨ laion2B-en çš„ 256x256 åˆ†è¾¨ç‡ä¸Šè®­ç»ƒäº† 237,000 æ­¥ã€‚åœ¨ laion-high-resolutionï¼ˆæ¥è‡ª LAION-5B çš„ 170M ä¸ªåˆ†è¾¨ç‡ >= 1024x1024 çš„ä¾‹å­ï¼‰ä¸Šä»¥ 512x512 åˆ†è¾¨ç‡è®­ç»ƒäº† 194,000 æ­¥ã€‚\n\nstable-diffusion-v1-2ï¼šä» stable-diffusion-v1-1 ä¸­æ¢å¤ã€‚åœ¨ \"laion-improved-aesthetics\"ï¼ˆlaion2B-en çš„ä¸€ä¸ªå­é›†ï¼Œè¿‡æ»¤ä¸ºåŸå§‹å¤§å° >= 512x512ã€ç¾å­¦è¯„åˆ† > 5.0 ä¸”æ°´å°æ¦‚ç‡ < 0.5 çš„å›¾åƒï¼‰ä¸Šä»¥ 512x512 åˆ†è¾¨ç‡è®­ç»ƒäº† 515,000 æ­¥ã€‚\n\nstable-diffusion-v1-3ï¼šä» stable-diffusion-v1-2 ä¸­æ¢å¤ - åœ¨ \"laion-improved-aesthetics\" ä¸Šä»¥ 512x512 åˆ†è¾¨ç‡è®­ç»ƒäº† 195,000 æ­¥ï¼Œå¹¶é™ä½äº† 10% çš„æ–‡æœ¬æ¡ä»¶ï¼Œä»¥æé«˜æ— åˆ†ç±»å™¨æŒ‡å¯¼é‡‡æ ·ã€‚\n\nstable-diffusion-v1-4ï¼šä» stable-diffusion-v1-2 ä¸­æ¢å¤ - åœ¨ \"laion-aesthetics v2 5+\" ä¸Šä»¥ 512x512 åˆ†è¾¨ç‡è®­ç»ƒäº† 225,000 æ­¥ï¼Œå¹¶é™ä½äº† 10% çš„æ–‡æœ¬æ¡ä»¶ï¼Œä»¥æé«˜æ— åˆ†ç±»å™¨æŒ‡å¯¼é‡‡æ ·ã€‚\n\nstable-diffusion-v1-5ï¼šä» stable-diffusion-v1-2 ä¸­æ¢å¤ - åœ¨ \"laion-aesthetics v2 5+\" ä¸Šä»¥ 512x512 åˆ†è¾¨ç‡è®­ç»ƒäº† 595,000 æ­¥ï¼Œå¹¶é™ä½äº† 10% çš„æ–‡æœ¬æ¡ä»¶ï¼Œä»¥æé«˜æ— åˆ†ç±»å™¨æŒ‡å¯¼é‡‡æ ·ã€‚\n\nstable-diffusion-inpaintingï¼šä» stable-diffusion-v1-5 ä¸­æ¢å¤ - ç„¶åï¼Œåœ¨ â€œlaion-aesthetics v2 5+â€ ä¸Šä»¥ 512x512 åˆ†è¾¨ç‡è¿›è¡Œ 440,000 æ­¥çš„ä¿®å¤è®­ç»ƒå’Œ 10% çš„æ–‡æœ¬æ¡ä»¶é™ä½ã€‚å¯¹äºä¿®å¤ï¼ŒUNet æœ‰ 5 ä¸ªé¢å¤–çš„è¾“å…¥é€šé“ï¼ˆ4 ä¸ªç”¨äºç¼–ç çš„é®ç½©å›¾åƒï¼Œ1 ä¸ªç”¨äºé®ç½©æœ¬èº«ï¼‰ï¼Œè¿™äº›æƒé‡åœ¨æ¢å¤éä¿®å¤æ£€æŸ¥ç‚¹ååˆå§‹åŒ–ä¸ºé›¶ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆåˆæˆé®ç½©ï¼Œå¹¶åœ¨ 25% çš„æƒ…å†µä¸‹é®ç›–ä¸€åˆ‡ã€‚\n\nç¡¬ä»¶ï¼š 32 x 8 x A100 GPUs\n\nä¼˜åŒ–å™¨ï¼š AdamW\n\næ¢¯åº¦ç´¯åŠ ï¼š 2\n\næ‰¹æ¬¡ï¼š 32 x 8 x 2 x 4 = 2048\n\nå­¦ä¹ ç‡ï¼š åœ¨å‰ 10,000 æ­¥é¢„çƒ­åˆ° 0.0001ï¼Œç„¶åä¿æŒä¸å˜\n\nè¯„ä¼°ç»“æœ\n\né‡‡ç”¨ä¸åŒæ— åˆ†ç±»å™¨æŒ‡å¯¼ç³»æ•°ï¼ˆ1.5ã€2.0ã€3.0ã€4.0ã€5.0ã€6.0ã€7.0ã€8.0ï¼‰å’Œ50æ­¥PNDM/PLMSé‡‡æ ·æ­¥éª¤çš„è¯„ä¼°æ˜¾ç¤ºäº†æ£€æŸ¥ç‚¹çš„ç›¸å¯¹æ”¹è¿›ï¼š\n\nåœ¨512x512åˆ†è¾¨ç‡ä¸‹ï¼Œä½¿ç”¨50æ­¥PLMSé‡‡æ ·å’ŒCOCO2017éªŒè¯é›†ä¸­çš„10000ä¸ªéšæœºæç¤ºè¿›è¡Œè¯„ä¼°ã€‚æœªé’ˆå¯¹FIDåˆ†æ•°è¿›è¡Œä¼˜åŒ–ã€‚\n\nç¯å¢ƒå½±å“\n\nStable Diffusion v1 é¢„ä¼°æ’æ”¾é‡ åŸºäºè¿™äº›ä¿¡æ¯ï¼Œæˆ‘ä»¬ä½¿ç”¨Lacosteç­‰äºº(2019)æå‡ºçš„æœºå™¨å­¦ä¹ å½±å“è®¡ç®—å™¨é¢„ä¼°ä»¥ä¸‹CO2æ’æ”¾é‡ã€‚æˆ‘ä»¬åˆ©ç”¨ç¡¬ä»¶ç±»å‹ã€è¿è¡Œæ—¶é—´ã€äº‘æœåŠ¡æä¾›å•†å’Œè®¡ç®—åŒºåŸŸæ¥ä¼°è®¡ç¢³å½±å“ã€‚\n\nç¡¬ä»¶ç±»å‹ï¼š A100 PCIe 40GB\nä½¿ç”¨æ—¶é•¿ï¼š 150000å°æ—¶\näº‘æœåŠ¡æä¾›å•†ï¼š AWS\nè®¡ç®—åŒºåŸŸï¼š ç¾å›½ä¸œéƒ¨\nç¢³æ’æ”¾é‡ï¼ˆåŸºäºç”µç½‘ä½ç½®çš„ç”µè€— x æ—¶é—´ x ç¢³æ’æ”¾ï¼‰ï¼š 11250åƒå…‹CO2å½“é‡\nå¼•ç”¨\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n\n\næœ¬æ¨¡å‹å¡ç‰‡ç”±Robin Rombachä¸Patrick Esseræ’°å†™ï¼ŒåŸºäºDALL-E Miniæ¨¡å‹å¡ç‰‡åˆ¶ä½œã€‚",
    "tags": "[\"PyTorch\", \"TensorFlow\", \"Transformers\", \"Diffusers\", \"Safetensors\", \"CreativeML OpenRAIL-M\", \"text-to-image\", \"stable-diffusion\", \"stable-diffusion-diffusers\"]"
  },
  {
    "url": "https://gitcode.com/openMind/xglm_564m",
    "project_name": "xglm_564m",
    "readme": "XGLM-564M\n\nXGLM-564M is a multilingual autoregressive language model (with 564 million parameters) trained on a balanced corpus of a diverse set of 30 languages totaling 500 billion sub-tokens. It was introduced in the paper Few-shot Learning with Multilingual Language Models by Xi Victoria Lin*, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li* (*Equal Contribution). The original implementation was released in this repository.\n\nModification\n\nModify examples in README.md and add npu support.\n\nTraining Data Statistics\n\nThe training data statistics of XGLM-564M is shown in the table below.\n\nISO-639-1\tfamily\tname\t# tokens\tratio\tratio w/ lowRes upsampling\nen\tIndo-European\tEnglish\t803526736124\t0.489906\t0.3259\nru\tIndo-European\tRussian\t147791898098\t0.0901079\t0.0602\nzh\tSino-Tibetan\tChinese\t132770494630\t0.0809494\t0.0483\nde\tIndo-European\tGerman\t89223707856\t0.0543992\t0.0363\nes\tIndo-European\tSpanish\t87303083105\t0.0532282\t0.0353\nfr\tIndo-European\tFrench\t77419639775\t0.0472023\t0.0313\nja\tJaponic\tJapanese\t66054364513\t0.040273\t0.0269\nit\tIndo-European\tItalian\t41930465338\t0.0255648\t0.0171\npt\tIndo-European\tPortuguese\t36586032444\t0.0223063\t0.0297\nel\tIndo-European\tGreek (modern)\t28762166159\t0.0175361\t0.0233\nko\tKoreanic\tKorean\t20002244535\t0.0121953\t0.0811\nfi\tUralic\tFinnish\t16804309722\t0.0102455\t0.0681\nid\tAustronesian\tIndonesian\t15423541953\t0.00940365\t0.0125\ntr\tTurkic\tTurkish\t12413166065\t0.00756824\t0.0101\nar\tAfro-Asiatic\tArabic\t12248607345\t0.00746791\t0.0099\nvi\tAustroasiatic\tVietnamese\t11199121869\t0.00682804\t0.0091\nth\tTaiâ€“Kadai\tThai\t10842172807\t0.00661041\t0.044\nbg\tIndo-European\tBulgarian\t9703797869\t0.00591635\t0.0393\nca\tIndo-European\tCatalan\t7075834775\t0.0043141\t0.0287\nhi\tIndo-European\tHindi\t3448390110\t0.00210246\t0.014\net\tUralic\tEstonian\t3286873851\t0.00200399\t0.0133\nbn\tIndo-European\tBengali, Bangla\t1627447450\t0.000992245\t0.0066\nta\tDravidian\tTamil\t1476973397\t0.000900502\t0.006\nur\tIndo-European\tUrdu\t1351891969\t0.000824241\t0.0055\nsw\tNigerâ€“Congo\tSwahili\t907516139\t0.000553307\t0.0037\nte\tDravidian\tTelugu\t689316485\t0.000420272\t0.0028\neu\tLanguage isolate\tBasque\t105304423\t6.42035e-05\t0.0043\nmy\tSino-Tibetan\tBurmese\t101358331\t6.17976e-05\t0.003\nht\tCreole\tHaitian, Haitian Creole\t86584697\t5.27902e-05\t0.0035\nqu\tQuechuan\tQuechua\t3236108\t1.97304e-06\t0.0001\nModel card\n\nFor intended usage of the model, please refer to the model card released by the XGLM-564M development team.\n\nExample (COPA)\n\nThe following snippet shows how to evaluate our models (GPT-3 style, zero-shot) on the Choice of Plausible Alternatives (COPA) task, using examples in English, Chinese and Hindi.\n\nimport torch\nimport torch.nn.functional as F\nfrom openmind import is_torch_npu_available, AutoTokenizer\nfrom transformers import XGLMForCausalLM\n\n\nif is_torch_npu_available():\n    device = \"npu:0\"\nelif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\nmodel_name_or_path = 'PyTorch-NPU/xglm_564m'\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\nmodel = XGLMForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, device_map=device)\n\ndata_samples = {\n    'en': [\n        {\n            \"premise\": \"I wanted to conserve energy.\",\n            \"choice1\": \"I swept the floor in the unoccupied room.\",\n            \"choice2\": \"I shut off the light in the unoccupied room.\",\n            \"question\": \"effect\",\n            \"label\": \"1\"\n        },\n        {\n            \"premise\": \"The flame on the candle went out.\",\n            \"choice1\": \"I blew on the wick.\",\n            \"choice2\": \"I put a match to the wick.\",\n            \"question\": \"cause\",\n            \"label\": \"0\"\n        }\n    ],\n    'zh': [\n        {\n            \"premise\": \"æˆ‘æƒ³èŠ‚çº¦èƒ½æºã€‚\",\n            \"choice1\": \"æˆ‘åœ¨ç©ºç€çš„æˆ¿é—´é‡Œæ‰«äº†åœ°æ¿ã€‚\",\n            \"choice2\": \"æˆ‘æŠŠç©ºæˆ¿é—´é‡Œçš„ç¯å…³äº†ã€‚\",\n            \"question\": \"effect\",\n            \"label\": \"1\"\n        },\n        {\n            \"premise\": \"èœ¡çƒ›ä¸Šçš„ç«ç„°ç†„ç­äº†ã€‚\",\n            \"choice1\": \"æˆ‘å¹ç­äº†ç¯èŠ¯ã€‚\",\n            \"choice2\": \"æˆ‘æŠŠä¸€æ ¹ç«æŸ´æ”¾åœ¨ç¯èŠ¯ä¸Šã€‚\",\n            \"question\": \"cause\",\n            \"label\": \"0\"\n        }\n    ],\n    'hi': [\n        {\n            \"premise\": \"M te vle konsÃ¨ve enÃ¨ji.\",\n            \"choice1\": \"Mwen te fin baleye chanm lib la.\",\n            \"choice2\": \"Mwen te femen limyÃ¨ nan chanm lib la.\",\n            \"question\": \"effect\",\n            \"label\": \"1\"\n        },\n        {\n            \"premise\": \"Flam bouji a te etenn.\",\n            \"choice1\": \"Mwen te soufle bouji a.\",\n            \"choice2\": \"Mwen te limen mÃ¨ch bouji a.\",\n            \"question\": \"cause\",\n            \"label\": \"0\"\n        }\n    ]\n}\n\n\ndef get_logprobs(prompt, device):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    input_ids, output_ids = inputs[\"input_ids\"], inputs[\"input_ids\"][:, 1:]\n    outputs = model(**inputs, labels=input_ids)\n    logits = outputs.logits\n    logprobs = torch.gather(F.log_softmax(logits, dim=2), 2,\n                            output_ids.unsqueeze(2))\n    return logprobs\n\n\ndef COPA_eval(prompt, alternative1, alternative2, device):\n    lprob1 = get_logprobs(prompt + \"\\n\" + alternative1, device).sum()\n    lprob2 = get_logprobs(prompt + \"\\n\" + alternative2, device).sum()\n    return 0 if lprob1 > lprob2 else 1\n\nfor lang in data_samples:\n    for idx, example in enumerate(data_samples[lang]):\n        predict = COPA_eval(example[\"premise\"], example[\"choice1\"],\n                            example[\"choice2\"], device)\n        print(f'{lang}-{idx}', predict, example['label'])\n\n\n\n",
    "tags": "[\"Text Generation\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"JAX\", \"31 languages\", \"MIT\"]"
  },
  {
    "url": "https://gitcode.com/openMind/xlnet_base_cased",
    "project_name": "xlnet_base_cased",
    "readme": "Original Text\nä¿®æ”¹è¯´æ˜\næ›´æ–°äº† README.md ä¸­çš„ç¤ºä¾‹ä»£ç ï¼Œå¹¶å¢åŠ äº† NPU æ”¯æŒï¼›\nä¿®æ”¹äº† README.md ä¸­é¢„æœŸç”¨é€”ä¸é™åˆ¶ç« èŠ‚çš„å†…å®¹ã€‚\nXLNetï¼ˆåŸºç¡€è§„æ¨¡æ¨¡å‹ï¼‰\n\nåŸºäºè‹±æ–‡è¯­æ–™é¢„è®­ç»ƒçš„ XLNet æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç”± Yang ç­‰äººåœ¨è®ºæ–‡ã€ŠXLNet: å¹¿ä¹‰è‡ªå›å½’é¢„è®­ç»ƒè¯­è¨€ç†è§£ã€‹ä¸­æå‡ºï¼Œå¹¶é¦–æ¬¡å‘å¸ƒäºæ­¤ä»£ç åº“ã€‚\n\nå…è´£å£°æ˜ï¼šXLNet å‘å¸ƒå›¢é˜Ÿæœªä¸ºæ­¤æ¨¡å‹ç¼–å†™è¯´æ˜å¡ç‰‡ï¼Œæœ¬å¡ç‰‡ç”± Hugging Face å›¢é˜Ÿæ’°å†™ã€‚\n\næ¨¡å‹æè¿°\n\nXLNet æ˜¯ä¸€ç§åŸºäºæ–°å‹å¹¿ä¹‰æ’åˆ—è¯­è¨€å»ºæ¨¡ç›®æ ‡çš„éç›‘ç£å¼è¯­è¨€è¡¨å¾å­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒXLNet é‡‡ç”¨ Transformer-XL ä½œä¸ºä¸»å¹²æ¨¡å‹ï¼Œåœ¨æ¶‰åŠé•¿æ–‡æœ¬çš„è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼ŒXLNet åœ¨é—®ç­”ã€è‡ªç„¶è¯­è¨€æ¨ç†ã€æƒ…æ„Ÿåˆ†æå’Œæ–‡æ¡£æ’åºç­‰å¤šä¸ªä¸‹æ¸¸è¯­è¨€ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ•ˆæœã€‚\n\né¢„æœŸç”¨é€”ä¸é™åˆ¶\n\nè¯·æ³¨æ„ï¼Œè¯¥æ¨¡å‹ä¸»è¦é€‚ç”¨äºéœ€è¦å¯¹æ•´å¥ï¼ˆå¯èƒ½åŒ…å«æ©ç ï¼‰è¿›è¡Œå†³ç­–çš„ä»»åŠ¡å¾®è°ƒï¼Œä¾‹å¦‚åºåˆ—åˆ†ç±»ã€æ ‡è®°åˆ†ç±»æˆ–é—®ç­”ã€‚å¯¹äºæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ï¼Œå»ºè®®ä½¿ç”¨ç±»ä¼¼ GPT2 çš„æ¨¡å‹ã€‚\n\nä½¿ç”¨ç¤ºä¾‹\n\nä»¥ä¸‹æ˜¯ä½¿ç”¨ PyTorch è·å–ç»™å®šæ–‡æœ¬ç‰¹å¾çš„è°ƒç”¨æ–¹æ³•ï¼š\n\nfrom openmind import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained('PyTorch-NPU/xlnet_base_cased')\nmodel = AutoModel.from_pretrained('PyTorch-NPU/xlnet_base_cased').to(\"npu:0\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to(\"npu:0\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n\nBibTeX æ¡ç›®ä¸å¼•ç”¨ä¿¡æ¯\n@article{DBLP:journals/corr/abs-1906-08237,\n  author    = {Zhilin Yang and\n               Zihang Dai and\n               Yiming Yang and\n               Jaime G. Carbonell and\n               Ruslan Salakhutdinov and\n               Quoc V. Le},\n  title     = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1906.08237},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1906.08237},\n  eprinttype = {arXiv},\n  eprint    = {1906.08237},\n  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-08237.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n",
    "tags": "[\"Feature Extraction\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"Rust\", \"English\", \"MIT\", \"bookcorpus\", \"wikipedia\"]"
  },
  {
    "url": "https://gitcode.com/openMind/baichuan2_13b_base_ms",
    "project_name": "baichuan2_13b_base_ms",
    "readme": "Baichuan 2\nğŸ¦‰GitHub | ğŸ’¬WeChat\nç™¾å·APIæ”¯æŒæœç´¢å¢å¼ºå’Œ192Ké•¿çª—å£ï¼Œæ–°å¢ç™¾å·æœç´¢å¢å¼ºçŸ¥è¯†åº“ã€é™æ—¶å…è´¹ï¼\nğŸš€ ç™¾å·å¤§æ¨¡å‹åœ¨çº¿å¯¹è¯å¹³å° å·²æ­£å¼å‘å…¬ä¼—å¼€æ”¾ ğŸ‰\nä¿®æ”¹è¯´æ˜/Modification\n\nä¿®æ”¹äº†å¿«é€Ÿå¼€å§‹çš„ç¤ºä¾‹ä»£ç ã€‚/ Modify the example code section.\n\nç›®å½•/Table of Contents\nğŸ“– æ¨¡å‹ä»‹ç»/Introduction\nâš™ï¸ å¿«é€Ÿå¼€å§‹/Quick Start\nğŸ“Š Benchmarkè¯„ä¼°/Benchmark Evaluation\nğŸ“œ å£°æ˜ä¸åè®®/Terms and Conditions\næ¨¡å‹ä»‹ç»/Introduction\n\nBaichuan 2 æ˜¯ç™¾å·æ™ºèƒ½æ¨å‡ºçš„æ–°ä¸€ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨ 2.6 ä¸‡äº¿ Tokens çš„é«˜è´¨é‡è¯­æ–™è®­ç»ƒï¼Œåœ¨æƒå¨çš„ä¸­æ–‡å’Œè‹±æ–‡ benchmark ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚æœ¬æ¬¡å‘å¸ƒåŒ…å«æœ‰ 7Bã€13B çš„ Base å’Œ Chat ç‰ˆæœ¬ï¼Œå¹¶æä¾›äº† Chat ç‰ˆæœ¬çš„ 4bits é‡åŒ–ï¼Œæ‰€æœ‰ç‰ˆæœ¬ä¸ä»…å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¼€å‘è€…ä¹Ÿä»…éœ€é‚®ä»¶ç”³è¯·å¹¶è·å¾—å®˜æ–¹å•†ç”¨è®¸å¯åï¼Œå³å¯ä»¥å…è´¹å•†ç”¨ã€‚å…·ä½“å‘å¸ƒç‰ˆæœ¬å’Œä¸‹è½½è§ä¸‹è¡¨ï¼š\n\nBaichuan 2 is the new generation of large-scale open-source language models launched by Baichuan Intelligence inc.. It is trained on a high-quality corpus with 2.6 trillion tokens and has achieved the best performance in authoritative Chinese and English benchmarks of the same size. This release includes 7B and 13B versions for both Base and Chat models, along with a 4bits quantized version for the Chat model. All versions are fully open to academic research, and developers can also use them for free in commercial applications after obtaining an official commercial license through email request. The specific release versions and download links are listed in the table below:\n\n\tBase Model\tChat Model\t4bits Quantized Chat Model\n7B\tBaichuan2-7B-Base\tBaichuan2-7B-Chat\tBaichuan2-7B-Chat-4bits\n13B\tBaichuan2-13B-Base\tBaichuan2-13B-Chat\tBaichuan2-13B-Chat-4bits\nå¿«é€Ÿå¼€å§‹/Quick Start\næ¨ç†\nimport os\nos.environ[\"OPENMIND_FRAMEWORK\"] = \"ms\"\n\nfrom mindspore import set_context\nfrom openmind import pipeline\n\nset_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\", model='MindSpore-Lab/baichuan2_13b_base', framework='ms', trust_remote_code=True)\npipeline_result = pipeline_task(\"ä½ æ˜¯è°ï¼Ÿ\", do_sample=False)\nprint(pipeline_result)\n\nBenchmark ç»“æœ/Benchmark Evaluation\n\næˆ‘ä»¬åœ¨é€šç”¨ã€æ³•å¾‹ã€åŒ»ç–—ã€æ•°å­¦ã€ä»£ç å’Œå¤šè¯­è¨€ç¿»è¯‘å…­ä¸ªé¢†åŸŸçš„ä¸­è‹±æ–‡æƒå¨æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼Œæ›´å¤šè¯¦ç»†æµ‹è¯„ç»“æœå¯æŸ¥çœ‹GitHubã€‚\n\nWe have extensively tested the model on authoritative Chinese-English datasets across six domains: General, Legal, Medical, Mathematics, Code, and Multilingual Translation. For more detailed evaluation results, please refer to GitHub.\n\n7B Model Results\n\tC-Eval\tMMLU\tCMMLU\tGaokao\tAGIEval\tBBH\n\t5-shot\t5-shot\t5-shot\t5-shot\t5-shot\t3-shot\nGPT-4\t68.40\t83.93\t70.33\t66.15\t63.27\t75.12\nGPT-3.5 Turbo\t51.10\t68.54\t54.06\t47.07\t46.13\t61.59\nLLaMA-7B\t27.10\t35.10\t26.75\t27.81\t28.17\t32.38\nLLaMA2-7B\t28.90\t45.73\t31.38\t25.97\t26.53\t39.16\nMPT-7B\t27.15\t27.93\t26.00\t26.54\t24.83\t35.20\nFalcon-7B\t24.23\t26.03\t25.66\t24.24\t24.10\t28.77\nChatGLM2-6B\t50.20\t45.90\t49.00\t49.44\t45.28\t31.65\nBaichuan-7B\t42.80\t42.30\t44.02\t36.34\t34.44\t32.48\nBaichuan2-7B-Base\t54.00\t54.16\t57.07\t47.47\t42.73\t41.56\n13B Model Results\n\tC-Eval\tMMLU\tCMMLU\tGaokao\tAGIEval\tBBH\n\t5-shot\t5-shot\t5-shot\t5-shot\t5-shot\t3-shot\nGPT-4\t68.40\t83.93\t70.33\t66.15\t63.27\t75.12\nGPT-3.5 Turbo\t51.10\t68.54\t54.06\t47.07\t46.13\t61.59\nLLaMA-13B\t28.50\t46.30\t31.15\t28.23\t28.22\t37.89\nLLaMA2-13B\t35.80\t55.09\t37.99\t30.83\t32.29\t46.98\nVicuna-13B\t32.80\t52.00\t36.28\t30.11\t31.55\t43.04\nChinese-Alpaca-Plus-13B\t38.80\t43.90\t33.43\t34.78\t35.46\t28.94\nXVERSE-13B\t53.70\t55.21\t58.44\t44.69\t42.54\t38.06\nBaichuan-13B-Base\t52.40\t51.60\t55.30\t49.69\t43.20\t43.01\nBaichuan2-13B-Base\t58.10\t59.17\t61.97\t54.33\t48.17\t48.78\nè®­ç»ƒè¿‡ç¨‹æ¨¡å‹/Training Dynamics\n\né™¤äº†è®­ç»ƒäº† 2.6 ä¸‡äº¿ Tokens çš„ Baichuan2-7B-Baseæ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†åœ¨æ­¤ä¹‹å‰çš„å¦å¤– 11 ä¸ªä¸­é—´è¿‡ç¨‹çš„æ¨¡å‹ï¼ˆåˆ†åˆ«å¯¹åº”è®­ç»ƒäº†çº¦ 0.2 ~ 2.4 ä¸‡äº¿ Tokensï¼‰ä¾›ç¤¾åŒºç ”ç©¶ä½¿ç”¨ ï¼ˆè®­ç»ƒè¿‡ç¨‹checkpointä¸‹è½½ï¼‰ã€‚ä¸‹å›¾ç»™å‡ºäº†è¿™äº› checkpoints åœ¨ C-Evalã€MMLUã€CMMLU ä¸‰ä¸ª benchmark ä¸Šçš„æ•ˆæœå˜åŒ–ï¼š\n\nIn addition to the Baichuan2-7B-Base model trained on 2.6 trillion tokens, we also offer 11 additional intermediate-stage models for community research, corresponding to training on approximately 0.2 to 2.4 trillion tokens each (Intermediate Checkpoints Download). The graph below shows the performance changes of these checkpoints on three benchmarks: C-Eval, MMLU, and CMMLU.\n\nå£°æ˜ä¸åè®®/Terms and Conditions\nå£°æ˜\n\næˆ‘ä»¬åœ¨æ­¤å£°æ˜ï¼Œæˆ‘ä»¬çš„å¼€å‘å›¢é˜Ÿå¹¶æœªåŸºäº Baichuan 2 æ¨¡å‹å¼€å‘ä»»ä½•åº”ç”¨ï¼Œæ— è®ºæ˜¯åœ¨ iOSã€Androidã€ç½‘é¡µæˆ–ä»»ä½•å…¶ä»–å¹³å°ã€‚æˆ‘ä»¬å¼ºçƒˆå‘¼åæ‰€æœ‰ä½¿ç”¨è€…ï¼Œä¸è¦åˆ©ç”¨ Baichuan 2 æ¨¡å‹è¿›è¡Œä»»ä½•å±å®³å›½å®¶ç¤¾ä¼šå®‰å…¨æˆ–è¿æ³•çš„æ´»åŠ¨ã€‚å¦å¤–ï¼Œæˆ‘ä»¬ä¹Ÿè¦æ±‚ä½¿ç”¨è€…ä¸è¦å°† Baichuan 2 æ¨¡å‹ç”¨äºæœªç»é€‚å½“å®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰çš„ä½¿ç”¨è€…éƒ½èƒ½éµå®ˆè¿™ä¸ªåŸåˆ™ï¼Œç¡®ä¿ç§‘æŠ€çš„å‘å±•èƒ½åœ¨è§„èŒƒå’Œåˆæ³•çš„ç¯å¢ƒä¸‹è¿›è¡Œã€‚\n\næˆ‘ä»¬å·²ç»å°½æˆ‘ä»¬æ‰€èƒ½ï¼Œæ¥ç¡®ä¿æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®çš„åˆè§„æ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡æˆ‘ä»¬å·²ç»åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å’Œæ•°æ®çš„å¤æ‚æ€§ï¼Œä»æœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ— æ³•é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚æœç”±äºä½¿ç”¨ Baichuan 2 å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n\nWe hereby declare that our team has not developed any applications based on Baichuan 2 models, not on iOS, Android, the web, or any other platform. We strongly call on all users not to use Baichuan 2 models for any activities that harm national / social security or violate the law. Also, we ask users not to use Baichuan 2 models for Internet services that have not undergone appropriate security reviews and filings. We hope that all users can abide by this principle and ensure that the development of technology proceeds in a regulated and legal environment.\n\nWe have done our best to ensure the compliance of the data used in the model training process. However, despite our considerable efforts, there may still be some unforeseeable issues due to the complexity of the model and data. Therefore, if any problems arise due to the use of Baichuan 2 open-source models, including but not limited to data security issues, public opinion risks, or any risks and problems brought about by the model being misled, abused, spread or improperly exploited, we will not assume any responsibility.\n\nåè®®\n\nç¤¾åŒºä½¿ç”¨ Baichuan 2 æ¨¡å‹éœ€è¦éµå¾ª Apache 2.0 å’Œã€ŠBaichuan 2 æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹ã€‚Baichuan 2 æ¨¡å‹æ”¯æŒå•†ä¸šç”¨é€”ï¼Œå¦‚æœæ‚¨è®¡åˆ’å°† Baichuan 2 æ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨äºå•†ä¸šç›®çš„ï¼Œè¯·æ‚¨ç¡®è®¤æ‚¨çš„ä¸»ä½“ç¬¦åˆä»¥ä¸‹æƒ…å†µï¼š\n\næ‚¨æˆ–æ‚¨çš„å…³è”æ–¹çš„æœåŠ¡æˆ–äº§å“çš„æ—¥å‡ç”¨æˆ·æ´»è·ƒé‡ï¼ˆDAUï¼‰ä½äº100ä¸‡ã€‚\næ‚¨æˆ–æ‚¨çš„å…³è”æ–¹ä¸æ˜¯è½¯ä»¶æœåŠ¡æä¾›å•†ã€äº‘æœåŠ¡æä¾›å•†ã€‚\næ‚¨æˆ–æ‚¨çš„å…³è”æ–¹ä¸å­˜åœ¨å°†æˆäºˆæ‚¨çš„å•†ç”¨è®¸å¯ï¼Œæœªç»ç™¾å·è®¸å¯äºŒæ¬¡æˆæƒç»™å…¶ä»–ç¬¬ä¸‰æ–¹çš„å¯èƒ½ã€‚\n\nåœ¨ç¬¦åˆä»¥ä¸Šæ¡ä»¶çš„å‰æä¸‹ï¼Œæ‚¨éœ€è¦é€šè¿‡ä»¥ä¸‹è”ç³»é‚®ç®± opensource@baichuan-inc.com ï¼Œæäº¤ã€ŠBaichuan 2 æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹è¦æ±‚çš„ç”³è¯·ææ–™ã€‚å®¡æ ¸é€šè¿‡åï¼Œç™¾å·å°†ç‰¹æ­¤æˆäºˆæ‚¨ä¸€ä¸ªéæ’ä»–æ€§ã€å…¨çƒæ€§ã€ä¸å¯è½¬è®©ã€ä¸å¯å†è®¸å¯ã€å¯æ’¤é”€çš„å•†ç”¨ç‰ˆæƒè®¸å¯ã€‚\n\nThe community usage of Baichuan 2 model requires adherence to Apache 2.0 and Community License for Baichuan2 Model. The Baichuan 2 model supports commercial use. If you plan to use the Baichuan 2 model or its derivatives for commercial purposes, please ensure that your entity meets the following conditions:\n\nThe Daily Active Users (DAU) of your or your affiliate's service or product is less than 1 million.\nNeither you nor your affiliates are software service providers or cloud service providers.\nThere is no possibility for you or your affiliates to grant the commercial license given to you, to reauthorize it to other third parties without Baichuan's permission.\n\nUpon meeting the above conditions, you need to submit the application materials required by the Baichuan 2 Model Community License Agreement via the following contact email: opensource@baichuan-inc.com. Once approved, Baichuan will hereby grant you a non-exclusive, global, non-transferable, non-sublicensable, revocable commercial copyright license.",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"English\", \"Chinese\", \"Other\"]"
  },
  {
    "url": "https://gitcode.com/openMind/convnext_ms",
    "project_name": "convnext_ms",
    "readme": "Original Text\nConvNeXt\n\né¢å‘2020å¹´ä»£çš„å·ç§¯ç½‘ç»œ\n\nç®€ä»‹\n\nåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä½œè€…é‡æ–°å®¡è§†äº†è®¾è®¡ç©ºé—´ï¼Œå¹¶æµ‹è¯•äº†çº¯å·ç§¯ç½‘ç»œèƒ½å¤Ÿè¾¾åˆ°çš„æé™ã€‚ä½œè€…é€æ¸å°†æ ‡å‡†çš„ResNetâ€œç°ä»£åŒ–â€ä¸ºè§†è§‰Transformerçš„è®¾è®¡ï¼Œå¹¶åœ¨è¿‡ç¨‹ä¸­å‘ç°äº†å‡ ç§å¯¹æ€§èƒ½å·®å¼‚èµ·åˆ°å…³é”®ä½œç”¨çš„éƒ¨åˆ†ã€‚è¿™ä¸€æ¢ç´¢çš„æˆæœæ˜¯ä¸€ç§åä¸ºConvNeXtçš„çº¯å·ç§¯ç½‘ç»œæ¨¡å‹å®¶æ—ã€‚ConvNeXtså®Œå…¨ç”±æ ‡å‡†çš„å·ç§¯ç½‘ç»œæ¨¡å—æ„æˆï¼Œå…¶å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ä¸Transformerç›¸åª²ç¾ï¼Œå®ç°äº†87.8%çš„ImageNet top-1å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†æ ‡å‡†å·ç§¯ç½‘ç»œçš„ç®€æ´æ€§å’Œæ•ˆç‡ã€‚[1]\n\nå›¾1. ConvNeXtæ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ImageNet-1Kä¸Šçš„æ¨¡å‹æ€§èƒ½å¤ç°ç»“æœå¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•°é‡ (M)\té…ç½®æ–‡ä»¶[yaml]\tä¸‹è½½[weights]\nconvnext_tiny\tD910x64-G\t81.91\t95.79\t28.59\tyaml\tweights\nconvnext_small\tD910x64-G\t83.40\t96.36\t50.22\tyaml\tweights\nconvnext_base\tD910x64-G\t83.32\t96.24\t88.59\tyaml\tweights\nå¤‡æ³¨\nç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º{è®¾å¤‡}x{å—æ•°}-{MindSporeæ¨¡å¼}ï¼Œå…¶ä¸­MindSporeæ¨¡å¼å¯ä»¥æ˜¯G - å›¾æ¨¡å¼æˆ–F - pynativeæ¨¡å¼ï¼Œå¹¶å¸¦æœ‰mså‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-Gè¡¨ç¤ºä½¿ç”¨å›¾æ¨¡å¼åœ¨8ç‰‡Ascend 910 NPUä¸Šè¿›è¡Œè®­ç»ƒã€‚\nTop-1å’ŒTop-5ï¼šåœ¨ImageNet-1KéªŒè¯é›†ä¸Šçš„å‡†ç¡®æ€§æŠ¥å‘Šã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒMindCVä¸­çš„å®‰è£…è¯´æ˜ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ImageNet-1Kæ•°æ®é›†ç”¨äºæ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ªAscend 910è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/convnext/convnext_tiny_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­åŠ å…¥ --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨é‡ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨æƒ³åœ¨ä¸éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/convnext/convnext_tiny_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/convnext/convnext_tiny_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²æŒ‡å—\n\nè¯·å‚è€ƒ MindCV æä¾›çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] åˆ˜æ³½ï¼Œæ¯›åï¼Œå´æ˜Œä¹‰ç­‰ã€‚é¢å‘2020å¹´ä»£çš„å·ç§¯ç¥ç»ç½‘ç»œ[C]//ã€ŠIEEE/CVFè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ã€‹ã€‚2022å¹´ï¼š11976-11986é¡µã€‚",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/glm2_6b_ms",
    "project_name": "glm2_6b_ms",
    "readme": "ChatGLM2-6B\n\nğŸ’» Github Repo â€¢ ğŸ¦ Twitter â€¢ ğŸ“ƒ [GLM@ACL 22] [GitHub] â€¢ ğŸ“ƒ [GLM-130B@ICLR 23] [GitHub]\n\n\nğŸ‘‹ Join our Slack and WeChat\n\nğŸ“Experience the larger-scale ChatGLM model at chatglm.cn\n\nä¿®æ”¹è¯´æ˜\nä¿®æ”¹ä»£ç è°ƒç”¨ç« èŠ‚ä»£ç \nå¢åŠ \"å¿«é€Ÿä½¿ç”¨\"ç« èŠ‚\nä»‹ç»\n\nChatGLM2-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ ChatGLM-6B çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM2-6B å¼•å…¥äº†å¦‚ä¸‹æ–°ç‰¹æ€§ï¼š\n\næ›´å¼ºå¤§çš„æ€§èƒ½ï¼šåŸºäº ChatGLM åˆä»£æ¨¡å‹çš„å¼€å‘ç»éªŒï¼Œæˆ‘ä»¬å…¨é¢å‡çº§äº† ChatGLM2-6B çš„åŸºåº§æ¨¡å‹ã€‚ChatGLM2-6B ä½¿ç”¨äº† GLM çš„æ··åˆç›®æ ‡å‡½æ•°ï¼Œç»è¿‡äº† 1.4T ä¸­è‹±æ ‡è¯†ç¬¦çš„é¢„è®­ç»ƒä¸äººç±»åå¥½å¯¹é½è®­ç»ƒï¼Œè¯„æµ‹ç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6B åœ¨ MMLUï¼ˆ+23%ï¼‰ã€CEvalï¼ˆ+33%ï¼‰ã€GSM8Kï¼ˆ+571%ï¼‰ ã€BBHï¼ˆ+60%ï¼‰ç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½å–å¾—äº†å¤§å¹…åº¦çš„æå‡ï¼Œåœ¨åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ã€‚\næ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼šåŸºäº FlashAttention æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†åŸºåº§æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆContext Lengthï¼‰ç”± ChatGLM-6B çš„ 2K æ‰©å±•åˆ°äº† 32Kï¼Œå¹¶åœ¨å¯¹è¯é˜¶æ®µä½¿ç”¨ 8K çš„ä¸Šä¸‹æ–‡é•¿åº¦è®­ç»ƒï¼Œå…è®¸æ›´å¤šè½®æ¬¡çš„å¯¹è¯ã€‚ä½†å½“å‰ç‰ˆæœ¬çš„ ChatGLM2-6B å¯¹å•è½®è¶…é•¿æ–‡æ¡£çš„ç†è§£èƒ½åŠ›æœ‰é™ï¼Œæˆ‘ä»¬ä¼šåœ¨åç»­è¿­ä»£å‡çº§ä¸­ç€é‡è¿›è¡Œä¼˜åŒ–ã€‚\næ›´é«˜æ•ˆçš„æ¨ç†ï¼šåŸºäº Multi-Query Attention æŠ€æœ¯ï¼ŒChatGLM2-6B æœ‰æ›´é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ï¼šåœ¨å®˜æ–¹çš„æ¨¡å‹å®ç°ä¸‹ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åˆä»£æå‡äº† 42%ï¼ŒINT4 é‡åŒ–ä¸‹ï¼Œ6G æ˜¾å­˜æ”¯æŒçš„å¯¹è¯é•¿åº¦ç”± 1K æå‡åˆ°äº† 8Kã€‚\næ›´å¼€æ”¾çš„åè®®ï¼šChatGLM2-6B æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œåœ¨å¡«å†™é—®å·è¿›è¡Œç™»è®°åäº¦å…è®¸å…è´¹å•†ä¸šä½¿ç”¨ã€‚\n\nChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:\n\nStronger Performance: Based on the development experience of the first-generation ChatGLM model, we have fully upgraded the base model of ChatGLM2-6B. ChatGLM2-6B uses the hybrid objective function of GLM, and has undergone pre-training with 1.4T bilingual tokens and human preference alignment training. The evaluation results show that, compared to the first-generation model, ChatGLM2-6B has achieved substantial improvements in performance on datasets like MMLU (+23%), CEval (+33%), GSM8K (+571%), BBH (+60%), showing strong competitiveness among models of the same size.\nLonger Context: Based on FlashAttention technique, we have extended the context length of the base model from 2K in ChatGLM-6B to 32K, and trained with a context length of 8K during the dialogue alignment, allowing for more rounds of dialogue. However, the current version of ChatGLM2-6B has limited understanding of single-round ultra-long documents, which we will focus on optimizing in future iterations.\nMore Efficient Inference: Based on Multi-Query Attention technique, ChatGLM2-6B has more efficient inference speed and lower GPU memory usage: under the official implementation, the inference speed has increased by 42% compared to the first generation; under INT4 quantization, the dialogue length supported by 6G GPU memory has increased from 1K to 8K.\nMore Open License: ChatGLM2-6B weights are completely open for academic research, and free commercial use is also allowed after completing the questionnaire.\nä»£ç è°ƒç”¨\n\nå¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM-6B æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š\n\n>>> from openmind import AutoTokenizer, AutoModelForCausalLM\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openmind/glm2_6b_ms\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"openmind/glm2_6b_ms\")\n>>> queries = [\"ä½ å¥½\", \"è¯·ä»‹ç»ä¸€ä¸‹æ­å·\"]\n>>> history = []\n>>> for query in queries:\n>>>     prompt = tokenizer.build_prompt(query, history=history) \n>>>     input_id = tokenizer.encode(prompt)\n>>>     output = model.generate([input_id], do_sample=False)\n>>>     response = tokenizer.decode(output)\n>>>     print(response)\n>>>     history += [(query, response)]\nresponse1:ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\nresponse2:æ­å·æ˜¯ä¸­å›½æµ™æ±Ÿçœçœä¼šï¼Œä½äºæµ™æ±Ÿçœä¸œå—éƒ¨ï¼Œåœ°å¤„æµ™æ±ŸçœåŒ—éƒ¨ï¼Œä¸œä¸´ä¸œæµ·ï¼Œå—æ¥ç¦å»ºçœï¼ŒåŒ—ä¸æ±Ÿè‹çœæ¯—é‚»ï¼Œæ˜¯ä¸­å›½è‘—åçš„æ—…æ¸¸åŸå¸‚ä¹‹ä¸€ã€‚\\n\\næ­å·æœ‰ç€æ‚ ä¹…çš„å†å²å’Œæ–‡åŒ–ï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€ï¼Œè‡ªä¸œæ™‹ä»¥æ¥ï¼Œæ­å·ä¸€ç›´æ˜¯æ”¿æ²»ã€ç»æµã€æ–‡åŒ–å’Œäº¤é€šä¸­å¿ƒã€‚å®‹ä»£ï¼Œæ­å·æˆä¸ºç¹åçš„å•†ä¸šåŸå¸‚ï¼Œè¢«èª‰ä¸ºâ€œä¸œå—ç¬¬ä¸€é‡é•‡â€ã€‚æ˜æ¸…æ—¶æœŸï¼Œæ­å·æˆä¸ºå…¨å›½è‘—åçš„â€œäººé—´å¤©å ‚â€ï¼Œå¸å¼•äº†å¤§é‡å•†äººã€æ–‡äººã€å®˜å‘˜å‰æ¥è§‚å…‰ã€äº¤æµã€‚\\n\\nå¦‚ä»Šï¼Œæ­å·å·²æˆä¸ºä¸­å›½çš„è‘—åæ—…æ¸¸åŸå¸‚ä¹‹ä¸€ï¼Œè¢«èª‰ä¸ºâ€œä¸­å›½æœ€å…·é­…åŠ›çš„åŸå¸‚â€ä¹‹ä¸€ã€‚æ­å·æ‹¥æœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿï¼Œå¦‚è¥¿æ¹–ã€çµéšå¯ºã€åƒå²›æ¹–ã€å®‹åŸç­‰ï¼Œå¸å¼•äº†å¤§é‡å›½å†…å¤–æ¸¸å®¢å‰æ¥è§‚å…‰æ—…æ¸¸ã€‚æ­¤å¤–ï¼Œæ­å·è¿˜æœ‰ç€å‘è¾¾çš„ç»æµå’Œä¼˜ç¾çš„è‡ªç„¶ç¯å¢ƒï¼Œè¢«èª‰ä¸ºâ€œå±±æ°´ç”²å¤©ä¸‹â€ã€‚\n\n\nå…³äºæ›´å¤šçš„ä½¿ç”¨è¯´æ˜ï¼ŒåŒ…æ‹¬å¦‚ä½•è¿è¡Œå‘½ä»¤è¡Œå’Œç½‘é¡µç‰ˆæœ¬çš„ DEMOï¼Œä»¥åŠä½¿ç”¨æ¨¡å‹é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ Github Repoã€‚\n\nFor more instructions, including how to run CLI and web demos, and model quantization, please refer to our Github Repo.\n\nå¿«é€Ÿä½¿ç”¨\næ•°æ®é›†å‡†å¤‡\n\nä»¥ ADGEN (å¹¿å‘Šç”Ÿæˆ) æ•°æ®é›†ä¸ºä¾‹ï¼ŒADGEN æ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚\n\n{\"content\": \"ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³\", \"summary\": \"è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚\"}\n\n\nä» Google Drive æˆ–è€… Tsinghua Cloud ä¸‹è½½å¤„ç†å¥½çš„ ADGEN æ•°æ®é›†ï¼Œç›®å½•ç»“æ„ä¸º\n\nAdvertiseGen\n  â”œâ”€â”€ train.json\n  â””â”€â”€ dev.json\n\nå¾®è°ƒ\ncd example\nbash msrun.sh \"finetune.py --train_dataset /{path}/AdvertiseGen/train.json\"\n\næ¨ç†\ncd example\npython inference.py\n\nChange Log\nv1.0\nåè®®\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºï¼ŒChatGLM2-6B æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª Model Licenseã€‚\n\nå¼•ç”¨\n\nå¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡ï¼ŒChatGLM2-6B çš„è®ºæ–‡ä¼šåœ¨è¿‘æœŸå…¬å¸ƒï¼Œæ•¬è¯·æœŸå¾…ï½\n\n@article{zeng2022glm,\n  title={Glm-130b: An open bilingual pre-trained model},\n  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},\n  journal={arXiv preprint arXiv:2210.02414},\n  year={2022}\n}\n\n@inproceedings{du2022glm,\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={320--335},\n  year={2022}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"English\", \"Chinese\", \"Apache License 2.0\", \"chatglm\", \"thudm\", \"glm\"]"
  },
  {
    "url": "https://gitcode.com/openMind/glm3_6b_ms",
    "project_name": "glm3_6b_ms",
    "readme": "ChatGLM3-6B\n\nğŸ’» Github Repo â€¢ ğŸ¦ Twitter â€¢ ğŸ“ƒ [GLM@ACL 22] [GitHub] â€¢ ğŸ“ƒ [GLM-130B@ICLR 23] [GitHub]\n\n\nğŸ‘‹ Join our Slack and WeChat\n\nğŸ“Experience the larger-scale ChatGLM model at chatglm.cn\n\nä¿®æ”¹è¯´æ˜\n\nä¿®æ”¹äº†ä»£ç è°ƒç”¨ä¸ºopenmindè°ƒç”¨\n\nä»‹ç»\n\nChatGLM3-6B æ˜¯ ChatGLM ç³»åˆ—æœ€æ–°ä¸€ä»£çš„å¼€æºæ¨¡å‹ï¼Œåœ¨ä¿ç•™äº†å‰ä¸¤ä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›ä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¸Šï¼ŒChatGLM3-6B å¼•å…¥äº†å¦‚ä¸‹ç‰¹æ€§ï¼š\n\næ›´å¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼š ChatGLM3-6B çš„åŸºç¡€æ¨¡å‹ ChatGLM3-6B-Base é‡‡ç”¨äº†æ›´å¤šæ ·çš„è®­ç»ƒæ•°æ®ã€æ›´å……åˆ†çš„è®­ç»ƒæ­¥æ•°å’Œæ›´åˆç†çš„è®­ç»ƒç­–ç•¥ã€‚åœ¨è¯­ä¹‰ã€æ•°å­¦ã€æ¨ç†ã€ä»£ç ã€çŸ¥è¯†ç­‰ä¸åŒè§’åº¦çš„æ•°æ®é›†ä¸Šæµ‹è¯„æ˜¾ç¤ºï¼ŒChatGLM3-6B-Base å…·æœ‰åœ¨ 10B ä»¥ä¸‹çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­æœ€å¼ºçš„æ€§èƒ½ã€‚\næ›´å®Œæ•´çš„åŠŸèƒ½æ”¯æŒï¼š ChatGLM3-6B é‡‡ç”¨äº†å…¨æ–°è®¾è®¡çš„ Prompt æ ¼å¼ã€‚\næ›´å…¨é¢çš„å¼€æºåºåˆ—ï¼š é™¤äº†å¯¹è¯æ¨¡å‹ ChatGLM3-6B å¤–ï¼Œè¿˜å¼€æºäº†åŸºç¡€æ¨¡å‹ ChatGLM-6B-Baseã€é•¿æ–‡æœ¬å¯¹è¯æ¨¡å‹ ChatGLM3-6B-32Kã€‚ä»¥ä¸Šæ‰€æœ‰æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œåœ¨å¡«å†™é—®å·è¿›è¡Œç™»è®°åäº¦å…è®¸å…è´¹å•†ä¸šä½¿ç”¨ã€‚\nä»£ç è°ƒç”¨\n\nå¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM3-6B æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š\n\nfrom mindspore import set_context\nfrom openmind import pipeline\n\nset_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\", model=\"MindSpore-Lab/glm3_6b\", framework=\"ms\")\npipeline_result = pipeline_task(\"ä½ å¥½\", do_sample=False)\n\n\nå…³äºæ›´å¤šçš„ä½¿ç”¨è¯´æ˜ï¼ŒåŒ…æ‹¬å¦‚ä½•è¿è¡Œå‘½ä»¤è¡Œå’Œç½‘é¡µç‰ˆæœ¬çš„ DEMOï¼Œä»¥åŠä½¿ç”¨æ¨¡å‹é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ Github Repoã€‚\n\nFor more instructions, including how to run CLI and web demos, and model quantization, please refer to our Github Repo.\n\nå¿«é€Ÿä½¿ç”¨\næ•°æ®é›†å‡†å¤‡\n\nä»¥ ADGEN (å¹¿å‘Šç”Ÿæˆ) æ•°æ®é›†ä¸ºä¾‹ï¼ŒADGEN æ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚\n\n{\"content\": \"ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³\", \"summary\": \"è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚\"}\n\n\nä» Google Drive æˆ–è€… Tsinghua Cloud ä¸‹è½½å¤„ç†å¥½çš„ ADGEN æ•°æ®é›†ï¼Œç›®å½•ç»“æ„ä¸º\n\nAdvertiseGen\n  â”œâ”€â”€ train.json\n  â””â”€â”€ dev.json\n\nå¾®è°ƒ\ncd example\nbash msrun.sh \"finetune.py --train_dataset /{path}/AdvertiseGen/train.json\"\n\næ¨ç†\nfrom mindspore import set_context\nfrom openmind import pipeline\n\nset_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\", model=\"MindSpore-Lab/glm3_6b\", framework=\"ms\")\npipeline_result = pipeline_task(\"ä½ å¥½\", do_sample=False)\nprint(pipeline_result)\n\nåè®®\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºï¼ŒChatGLM3-6B æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª Model Licenseã€‚\n\nå¼•ç”¨\n\nå¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡ã€‚\n\n@article{zeng2022glm,\n  title={Glm-130b: An open bilingual pre-trained model},\n  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},\n  journal={arXiv preprint arXiv:2210.02414},\n  year={2022}\n}\n\n@inproceedings{du2022glm,\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={320--335},\n  year={2022}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"English\", \"Chinese\", \"Apache License 2.0\", \"glm\", \"chatglm\", \"thudm\"]"
  },
  {
    "url": "https://gitcode.com/openMind/googlenet_ms",
    "project_name": "googlenet_ms",
    "readme": "Original Text\nGoogLeNet\n\nGoogLeNet: æ·±å…¥ç ”ç©¶å·ç§¯\n\nç®€ä»‹\n\nGoogLeNet æ˜¯ç”± Christian Szegedy åœ¨ 2014 å¹´æå‡ºçš„ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ ç»“æ„ã€‚åœ¨æ­¤ä¹‹å‰ï¼ŒAlexNetã€VGG ç­‰ç»“æ„é€šè¿‡å¢åŠ ç½‘ç»œæ·±åº¦ï¼ˆå±‚æ•°ï¼‰å®ç°äº†æ›´å¥½çš„è®­ç»ƒæ•ˆæœï¼Œä½†å±‚æ•°çš„å¢åŠ ä¼šå¸¦æ¥è®¸å¤šè´Ÿé¢å½±å“ï¼Œä¾‹å¦‚è¿‡æ‹Ÿåˆã€æ¢¯åº¦æ¶ˆå¤±ã€æ¢¯åº¦çˆ†ç‚¸ç­‰ã€‚Inception çš„æå‡ºä»å¦ä¸€ä¸ªè§’åº¦æ”¹å–„äº†è®­ç»ƒç»“æœï¼šå®ƒèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºï¼Œåœ¨ç›¸åŒè®¡ç®—é‡ä¸‹æå–æ›´å¤šç‰¹å¾ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆæœã€‚[1]\n\nå›¾ 1. GoogLeNet çš„æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K æ•°æ®é›†ä¸Šå¤ç°çš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tè®­ç»ƒç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•°é‡ (M)\té…ç½®æ–‡ä»¶\tä¸‹è½½\ngooglenet\tD910x8-G\t72.68\t90.89\t6.99\tyaml\tæƒé‡æ–‡ä»¶\nè¯´æ˜\nè®­ç»ƒç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º {è®¾å¤‡}x{åˆ†ç‰‡}-{MindSpore æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - PyNative æ¨¡å¼ï¼Œå¸¦æœ‰ ms å‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºåœ¨ 8 ç‰‡ Ascend 910 NPU ä¸Šä½¿ç”¨å›¾æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®ç‡ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV çš„å®‰è£…æŒ‡å—ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†ä»¥ä¾›æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/googlenet/googlenet_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nåŒæ ·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹æ¬¡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹æ¬¡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹æ¬¡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨æƒ³åœ¨ä¸éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/googlenet/googlenet_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/googlenet/googlenet_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²æŒ‡å—\n\nè¯·å‚è€ƒ MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Szegedy C, Liu W, Jia Y, ç­‰äºº. æ·±åº¦å·ç§¯ç½‘ç»œç ”ç©¶[C]//IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†. 2015: 1-9.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/mobilenetv1_ms",
    "project_name": "mobilenetv1_ms",
    "readme": "Original Text\nMobileNetV1\n\nMobileNetsï¼šé¢å‘ç§»åŠ¨è§†è§‰åº”ç”¨çš„è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œ\n\nç®€ä»‹\n\nä¸ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œç›¸æ¯”ï¼ŒMobileNetV1 åœ¨ç•¥å¾®é™ä½å‡†ç¡®ç‡ï¼ˆç›¸æ¯” VGG16ï¼Œå‡†ç¡®ç‡é™ä½ 0.9%ï¼‰çš„å‰æä¸‹ï¼Œå¤§å¹…å‡å°‘äº†å‚æ•°é‡å’Œè®¡ç®—é‡ï¼ˆæ¨¡å‹å‚æ•°ä»…ä¸º VGG çš„ 1/32ï¼‰ã€‚è¯¥æ¨¡å‹åŸºäºä¸€ç§ç²¾ç®€æ¶æ„ï¼Œä½¿ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯æ„å»ºè½»é‡çº§æ·±åº¦ç¥ç»ç½‘ç»œã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸¤ä¸ªç®€å•çš„å…¨å±€è¶…å‚æ•°ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¹³è¡¡å»¶è¿Ÿå’Œå‡†ç¡®æ€§ã€‚[1]\n\nå›¾ 1. MobileNetV1 æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K æ•°æ®é›†ä¸Šå¤ç°çš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹ã€‚\n\næ¨¡å‹\tè®­ç»ƒç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•°é‡ (M)\té…ç½®æ–‡ä»¶\tä¸‹è½½åœ°å€\nmobilenet_v1_025\tD910x8-G\t53.87\t77.66\t0.47\tyaml\tæƒé‡æ–‡ä»¶\nmobilenet_v1_050\tD910x8-G\t65.94\t86.51\t1.34\tyaml\tæƒé‡æ–‡ä»¶\nmobilenet_v1_075\tD910x8-G\t70.44\t89.49\t2.60\tyaml\tæƒé‡æ–‡ä»¶\nmobilenet_v1_100\tD910x8-G\t72.95\t91.01\t4.25\tyaml\tæƒé‡æ–‡ä»¶\nè¯´æ˜\nè®­ç»ƒç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º {è®¾å¤‡}x{ç‰‡æ•°}-{MindSpore æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - pynative æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨ ms å‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºä½¿ç”¨å›¾æ¨¡å¼åœ¨ 8 ç‰‡ Ascend 910 NPU ä¸Šè¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV çš„å®‰è£…æŒ‡å—ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ImageNet-1K æ•°æ®é›†ä»¥ä¾›æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚åœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/mobilenetv1/mobilenet_v1_0.25_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»å‘ mpirun å‘½ä»¤æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\nå…³äºæ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨é‡ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨æƒ³åœ¨ä¸éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/mobilenetv1/mobilenet_v1_0.25_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯æ¨¡å‹å‡†ç¡®æ€§\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/mobilenetv1/mobilenet_v1_0.25_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²æŒ‡å—\n\nè¯·å‚é˜… MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Howard A G, Zhu M, Chen B, ç­‰äºº. Mobilenets:é¢å‘ç§»åŠ¨è§†è§‰åº”ç”¨çš„çš„é«˜æ•ˆå·ç§¯ç¥ç»ç½‘ç»œ[J]. arXiv é¢„å°æœ¬ arXiv:1704.04861, 2017.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/mobilenetv2_ms",
    "project_name": "mobilenetv2_ms",
    "readme": "Original Text\nMobileNetV2\n\nMobileNetV2: å€’ç½®æ®‹å·®ä¸çº¿æ€§ç“¶é¢ˆ\n\nç®€ä»‹\n\nè¯¥æ¨¡å‹æ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä¸“ä¸ºç§»åŠ¨å’Œèµ„æºå—é™ç¯å¢ƒè®¾è®¡ã€‚è¯¥ç½‘ç»œæ¨åŠ¨äº†ç§»åŠ¨å®šåˆ¶è®¡ç®—æœºè§†è§‰æ¨¡å‹é¢†åŸŸçš„æœ€æ–°æŠ€æœ¯ï¼Œåœ¨ä¿æŒç›¸åŒå‡†ç¡®åº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„æ“ä½œé‡å’Œå†…å­˜ã€‚\n\næ¨¡å‹çš„ä¸»è¦åˆ›æ–°ç‚¹æ˜¯æå‡ºäº†ä¸€ç§æ–°çš„å±‚æ¨¡å—ï¼šå€’ç½®æ®‹å·®ä¸çº¿æ€§ç“¶é¢ˆã€‚è¯¥æ¨¡å—çš„è¾“å…¥æ˜¯ä¸€ä¸ªä½ç»´åº¦çš„å‹ç¼©è¡¨ç¤ºï¼Œé¦–å…ˆå°†å…¶æ‰©å±•åˆ°é«˜ç»´åº¦ï¼Œç„¶åä½¿ç”¨è½»é‡çº§æ·±åº¦å·ç§¯è¿›è¡Œè¿‡æ»¤ã€‚éšåä½¿ç”¨çº¿æ€§å·ç§¯å°†ç‰¹å¾æŠ•å½±å›ä½ç»´è¡¨ç¤º[1]\n\nå›¾ 1. MobileNetV2 æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K æ•°æ®é›†ä¸Šå¤ç°çš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tè®­ç»ƒç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•° (M)\té…ç½®æ–‡ä»¶\tä¸‹è½½åœ°å€\nmobilenet_v2_075\tD910x8-G\t69.98\t89.32\t2.66\tyaml\tæƒé‡æ–‡ä»¶\nmobilenet_v2_100\tD910x8-G\t72.27\t90.72\t3.54\tyaml\tæƒé‡æ–‡ä»¶\nmobilenet_v2_140\tD910x8-G\t75.56\t92.56\t6.15\tyaml\tæƒé‡æ–‡ä»¶\nè¯´æ˜\nè®­ç»ƒç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º {è®¾å¤‡}x{ç‰‡æ•°}-{MindSpore æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - PyNative æ¨¡å¼ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºä½¿ç”¨å›¾æ¨¡å¼åœ¨ 8 ç‰‡ Ascend 910 NPU ä¸Šè¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®åº¦ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV ä¸­çš„å®‰è£…æŒ‡å—ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†ç”¨äºæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹å¯ä»¥è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/mobilenetv2/mobilenet_v2_0.75_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”±æ ¹ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨å¸Œæœ›åœ¨æ— éœ€åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å°æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/mobilenetv2/mobilenet_v2_0.75_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/mobilenetv2/mobilenet_v2_0.75_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²æŒ‡å—\n\nè¯·å‚è€ƒ MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Sandler M, Howard A, Zhu M, ç­‰äºº. Mobilenetv2: é€†æ®‹å·®å’Œçº¿æ€§ç“¶é¢ˆ[C]//IEEE è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†. 2018: 4510-4520.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/opensora-hpcai-1_0_ms",
    "project_name": "opensora-hpcai-1_0_ms",
    "readme": "Original Text\nOpen Sora hpcai 1.0 æ¨¡å‹å¡ç‰‡\n16x256x720\t16x640x360\n\t\né›ªèŠ±åœ¨å†¬å¤œæ˜Ÿç©ºä¸‹é£˜è½ï¼Œè¦†ç›–ç€å¤šåº§æˆ¿å±‹å’Œæ ‘æœ¨ã€‚å‘ˆç°åœ£è¯èŠ‚åº†ç¥å’Œæ¬¢åº†çš„æ¦‚å¿µã€‚\té›ªèŠ±åœ¨å†¬å¤œæ˜Ÿç©ºä¸‹é£˜è½ï¼Œè¦†ç›–ç€å¤šåº§æˆ¿å±‹å’Œæ ‘æœ¨ã€‚å‘ˆç°åœ£è¯èŠ‚åº†ç¥å’Œæ¬¢åº†çš„æ¦‚å¿µã€‚\n\t\nå±±ä¸Š Milky Way çš„å»¶æ—¶æ‘„å½±\tå±±ä¸Š Milky Way çš„å»¶æ—¶æ‘„å½±\n\t\nåœ¨å¼€é˜”çš„ä¹¡æ‘æ™¯è§‚ä¸­ï¼Œä¸€æ£µæ ‘ä¸Šæ–¹å‡èµ·çš„å¤ªé˜³çš„å»¶æ—¶æ‘„å½±ï¼Œè“å¤©çš„äº‘æœµä¸å…‰çº¿ç¾ä¸½åœ°äº¤ç»‡ã€‚\tåœ¨æµ·åº•ï¼Œä¸€åªå¤§å‹çš„æ©™è‰²ç« é±¼æ­£åœ¨ä¼‘æ¯ï¼Œä¸æ²™çŸ³åœ°å½¢èä¸ºä¸€ä½“ã€‚å®ƒçš„è§¦æ‰‹å‘å››å‘¨ä¼¸å±•ï¼Œçœ¼ç›ç´§é—­ã€‚ç« é±¼æ²¡æœ‰æ„è¯†åˆ°ä¸€åªèƒèŸ¹ä»ä¸€å—çŸ³å¤´åé¢æ‚„æ‚„é è¿‘ï¼ŒèŸ¹é’³æŠ¬èµ·ï¼Œå‡†å¤‡æ”»å‡»ã€‚èƒèŸ¹æ˜¯æ£•è‰²å¸¦åˆºçš„ï¼Œæœ‰ç€é•¿è…¿å’Œè§¦é¡»ã€‚é•œå¤´é‡‡ç”¨å¹¿è§’æ‹æ‘„ï¼Œå±•ç¤ºäº†æµ·æ´‹çš„å¹¿é˜”å’Œæ·±é‚ƒã€‚æµ·æ°´æ¸…æ¾ˆè”šè“ï¼Œé˜³å…‰é€è¿‡æ°´é¢ã€‚ç”»é¢æ¸…æ™°é”åˆ©ï¼ŒåŠ¨æ€èŒƒå›´é«˜ã€‚ç« é±¼å’ŒèƒèŸ¹éƒ½åœ¨ç„¦ç‚¹ä¸­ï¼Œè€ŒèƒŒæ™¯ç•¥å¾®æ¨¡ç³Šï¼Œè¥é€ å‡ºæ™¯æ·±æ•ˆæœã€‚\nå¼•è¨€\n\næœ¬æ–‡æä¾›äº†MindSporeå¯¹OpenSoraçš„é«˜æ•ˆå®ç°ï¼ŒOpenSoraæ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨ä¿ƒè¿›å†…å®¹åˆ›ä½œé¢†åŸŸçš„åˆ›æ–°ã€åˆ›é€ æ€§å’ŒåŒ…å®¹æ€§ã€‚\n\næ­¤ä»£ç åº“ä¸­çš„æ£€æŸ¥ç‚¹åŸºäºHPC-AI Techå‘å¸ƒçš„æ¨¡å‹æ„å»ºã€‚æˆ‘ä»¬å¯¹ä»–ä»¬å“è¶Šçš„å·¥ä½œå’Œå¯¹å¼€æºç¤¾åŒºçš„æ…·æ…¨è´¡çŒ®è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ã€‚\n\næ¨¡å‹æè¿°\nå¼€å‘è€…ï¼š MindSporeå®éªŒå®¤åŸºäºHPC-AI Techçš„å·¥ä½œ\næ¡†æ¶ï¼š MindSpore\næ¨¡å‹ç±»å‹ï¼š åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹\nè®¸å¯ï¼š Apache-2.0\næ¨¡å‹æè¿°ï¼š è¿™æ˜¯ä¸€ä¸ªå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå’Œä¿®æ”¹è§†é¢‘çš„æ¨¡å‹ã€‚\næ›´å¤šä¿¡æ¯èµ„æºï¼š è¯·è®¿é—®GitHubä»£ç åº“ã€‚\næ¨¡å‹æƒé‡\n\næˆ‘ä»¬ç›®å‰æä¾›ä»¥ä¸‹æ£€æŸ¥ç‚¹ï¼š\n\nOpenSora-v1-HQ-16x256x256.ckpt\nOpenSora-v1-HQ-16x512x512.ckpt\n\nè¿™äº›æ¨¡å‹æƒé‡éƒ¨åˆ†åˆå§‹åŒ–è‡ªPixArt-Î±ã€‚å‚æ•°æ•°é‡ä¸º724Mã€‚å…³äºè®­ç»ƒçš„æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨HPC-AI Techçš„æŠ¥å‘Šä¸­æ‰¾åˆ°ã€‚\n\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\nå…³äºå¦‚ä½•è®­ç»ƒå’Œæ¨æ–­æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥é˜…MindOne GitHubä»£ç åº“ã€‚\n\nä½¿ç”¨åœºæ™¯\nç›´æ¥ä½¿ç”¨\n\nè¯¥æ¨¡å‹ä»…ä¾›ç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚å¯èƒ½çš„ç ”ç©¶é¢†åŸŸå’Œä»»åŠ¡åŒ…æ‹¬ï¼š\n\nç”Ÿæˆè‰ºæœ¯å“ï¼Œç”¨äºè®¾è®¡å’Œå…¶ä»–è‰ºæœ¯è¿‡ç¨‹ã€‚\nç”¨äºæ•™è‚²æˆ–åˆ›æ„å·¥å…·çš„åº”ç”¨ã€‚\nå¯¹ç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶ã€‚\nå®‰å…¨éƒ¨ç½²æœ‰ç”Ÿæˆæœ‰å®³å†…å®¹æ½œåŠ›çš„æ¨¡å‹ã€‚\næ¢ç©¶å’Œç†è§£ç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§å’Œåè§ã€‚\n\nä»¥ä¸‹æè¿°äº†ç¦æ­¢çš„ä½¿ç”¨åœºæ™¯ã€‚\n\nä¸é€‚ç”¨èŒƒå›´\n\nè¯¥æ¨¡å‹æœªç»è¿‡è®­ç»ƒä»¥æˆä¸ºäº‹å®æˆ–çœŸå®äººç‰©æˆ–äº‹ä»¶çš„è¡¨ç¤ºï¼Œå› æ­¤ä½¿ç”¨è¯¥æ¨¡å‹ç”Ÿæˆæ­¤ç±»å†…å®¹è¶…å‡ºäº†æ¨¡å‹èƒ½åŠ›çš„èŒƒå›´ã€‚\n\nå±€é™æ€§å’Œåè§\nå±€é™æ€§\næ¨¡å‹æ— æ³•å®ç°å®Œç¾çš„ç…§ç‰‡çº§çœŸå®æ„Ÿ\næ¨¡å‹æ— æ³•æ¸²æŸ“å¯è¯†åˆ«çš„æ–‡æœ¬\næ¨¡å‹çš„è‡ªåŠ¨ç¼–ç éƒ¨åˆ†æ˜¯æŸå¤±æ€§çš„ã€‚\nåè§\n\nå°½ç®¡å›¾åƒç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½åŠ å¼ºæˆ–åŠ å‰§ç¤¾ä¼šåè§ã€‚",
    "tags": "[\"Text Generation\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/stable-diffusion-xl-base-1_0_ms",
    "project_name": "stable-diffusion-xl-base-1_0_ms",
    "readme": "Original Text\nStable Diffusion XL æ¨¡å‹å¡ç‰‡\n\næ³¨ï¼šä»¥ä¸‹æ ·å›¾ç”±SDXL-1.0-Baseåœ¨æ˜‡è…¾910å¹³å°ï¼ˆåœ¨çº¿æ¨ç†ï¼‰ä¸Šé‡‡æ ·40æ­¥ç”Ÿæˆã€‚\n\nå›¾1ï¼š\"è¨å°”ç“¦å¤šÂ·è¾¾åˆ©å……æ»¡æ´»åŠ›çš„è‚–åƒç”»ï¼ŒåŠå¼ è„¸å‘ˆç°æœºæ¢°æ„é€ \"\nå›¾2ï¼š\"ç”±ä½“ç´ æ„æˆçš„å¡çš®å·´æ‹‰ååœ¨ç”°é‡ä¸­\"\nå›¾3ï¼š\"å¯çˆ±å°å±±ç¾Šï¼Œè™šå¹»å¼•æ“æ¸²æŸ“ï¼Œæ¸©é¦¨å®¤å†…å…‰çº¿ï¼Œè‰ºæœ¯ç«™é£æ ¼ï¼Œç²¾ç»†æ•°å­—ç»˜ç”»ï¼Œç”µå½±æ„Ÿï¼Œå…«åº¦æ¸²æŸ“\"\nå›¾4ï¼š\"è¢‹é¼ è‚–åƒç…§ï¼Œç©¿ç€æ©™è‰²è¿å¸½è¡«å’Œè“è‰²å¤ªé˜³é•œï¼Œç«™åœ¨æ‚‰å°¼æ­Œå‰§é™¢å‰çš„è‰åœ°ä¸Šï¼Œèƒ¸å‰ä¸¾ç€å†™æœ‰'SDXL'çš„æ ‡ç‰Œ\"\n\n\n\n\næ¨¡å‹ä»‹ç»\n\næœ¬ç›®å½•åŒ…å«åŸºäºæ˜‡æ€æ¡†æ¶å®ç°çš„Stable Diffusion XL (SDXL)æ¨¡å‹ï¼Œå…¶è®¾è®¡å‚è€ƒäº†Stability-AIçš„å®˜æ–¹å®ç°ã€‚\n\næ¨¡å‹è¯´æ˜\nå¼€å‘å›¢é˜Ÿï¼š åŸºäºStability AIå·¥ä½œæˆæœçš„æ˜‡æ€å®éªŒå®¤\næ¡†æ¶ï¼š æ˜‡æ€MindSpore\næ¨¡å‹ç±»å‹ï¼š åŸºäºæ‰©æ•£è¿‡ç¨‹çš„æ–‡ç”Ÿå›¾ç”Ÿæˆæ¨¡å‹\nè®¸å¯åè®®ï¼š CreativeML Open RAIL++-M è®¸å¯è¯\næ¨¡å‹æè¿°ï¼š æœ¬æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå¹¶ä¿®æ”¹å›¾åƒï¼Œæ˜¯åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹æ¶æ„ï¼Œé‡‡ç”¨åŒæ–‡æœ¬ç¼–ç å™¨ï¼ˆOpenCLIP-ViT/Gï¼‰çš„ç”Ÿæˆç³»ç»Ÿã€‚\nå»¶ä¼¸é˜…è¯»ï¼š è¯¦è§GitHubä»£ç åº“ã€‚\nä½¿ç”¨åœºæ™¯\nç›´æ¥åº”ç”¨\n\næœ¬æ¨¡å‹ä»…é™ç ”ç©¶ç”¨é€”ï¼Œé€‚ç”¨åœºæ™¯åŒ…æ‹¬ï¼š\n\nè‰ºæœ¯åˆ›ä½œä¸è®¾è®¡è¾…åŠ©\næ•™è‚²å·¥å…·åŠåˆ›æ„å¼€å‘\nç”Ÿæˆæ¨¡å‹ç›¸å…³ç ”ç©¶\nå†…å®¹å®‰å…¨éƒ¨ç½²ç ”ç©¶\nç”Ÿæˆæ¨¡å‹å±€é™æ€§åŠåå·®åˆ†æ\n\nä»¥ä¸‹ä¸ºç¦æ­¢ä½¿ç”¨åœºæ™¯è¯´æ˜ã€‚\n\néé€‚ç”¨åœºæ™¯\n\næœ¬æ¨¡å‹æœªé’ˆå¯¹äººç‰©æˆ–äº‹ä»¶çš„çœŸå®å†ç°è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤ç”Ÿæˆæ­¤ç±»å†…å®¹ä¸å±äºæ¨¡å‹è®¾è®¡ç›®æ ‡ã€‚\n\nå±€é™æ€§ä¸åå·®\næŠ€æœ¯å±€é™\nå›¾åƒç”Ÿæˆå°šæœªè¾¾åˆ°å®Œç¾çœŸå®æ„Ÿ\næ— æ³•ç”Ÿæˆå¯è¾¨è¯†æ–‡å­—\nå¯¹å¤åˆå‹æŒ‡ä»¤ï¼ˆå¦‚\"è“è‰²çƒä½“ä¸Šçš„çº¢è‰²ç«‹æ–¹ä½“\"ï¼‰å¤„ç†èƒ½åŠ›æœ‰é™\näººç‰©é¢éƒ¨ç”Ÿæˆå¯èƒ½å­˜åœ¨ç¼ºé™·\nè‡ªç¼–ç æ¨¡å—å­˜åœ¨ä¿¡æ¯æŸè€—\nç¤¾ä¼šåå·®\n\nå°½ç®¡å›¾åƒç”Ÿæˆèƒ½åŠ›å“è¶Šï¼Œè¯¥æ¨¡å‹å¯èƒ½å¼ºåŒ–æˆ–æ”¾å¤§ç¤¾ä¼šå›ºæœ‰åè§ã€‚",
    "tags": "[\"Text-to-Image\", \"Apache License 2.0\", \"stable-diffusion\"]"
  },
  {
    "url": "https://gitcode.com/openMind/yolov4_ms",
    "project_name": "yolov4_ms",
    "readme": "Original Text\nYOLOv4\n\nYOLOv4: é¢å‘ç›®æ ‡æ£€æµ‹çš„é€Ÿåº¦ä¸ç²¾åº¦æœ€ä¼˜åŒ–\n\nç®€ä»‹\n\nå­˜åœ¨å¤§é‡ç‰¹æ€§è¢«å£°ç§°èƒ½å¤Ÿæå‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å‡†ç¡®æ€§ã€‚å¯¹äºè¿™äº›ç‰¹æ€§çš„ç»„åˆåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®é™…æµ‹è¯•ä»¥åŠç†è®ºç»“æœçš„éªŒè¯æ˜¯å¿…è¦çš„ã€‚æŸäº›ç‰¹æ€§ä»…é€‚ç”¨äºç‰¹å®šæ¨¡å‹ã€ç‰¹å®šé—®é¢˜æˆ–ä»…é™äºå°è§„æ¨¡æ•°æ®é›†ï¼›è€Œæœ‰äº›ç‰¹æ€§ï¼Œå¦‚æ‰¹é‡å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥ï¼Œé€‚ç”¨äºå¤§å¤šæ•°æ¨¡å‹ã€ä»»åŠ¡å’Œæ•°æ®é›†ã€‚æˆ‘ä»¬å‡è®¾è¿™äº›é€šç”¨ç‰¹æ€§åŒ…æ‹¬åŠ æƒæ®‹å·®è¿æ¥ï¼ˆWRCï¼‰ã€è·¨é˜¶æ®µéƒ¨åˆ†è¿æ¥ï¼ˆCSPï¼‰ã€è·¨å°æ‰¹é‡å½’ä¸€åŒ–ï¼ˆCmBNï¼‰ã€è‡ªå¯¹æŠ—è®­ç»ƒï¼ˆSATï¼‰å’ŒMishæ¿€æ´»ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä»¥ä¸‹æ–°ç‰¹æ€§ï¼šWRCã€CSPã€CmBNã€SATã€Mishæ¿€æ´»ã€Mosaicæ•°æ®å¢å¼ºã€DropBlockæ­£åˆ™åŒ–å’ŒCIoUæŸå¤±ï¼Œå¹¶å°†å®ƒä»¬ä¸­çš„ä¸€äº›ç»„åˆä»¥å®ç°æœ€å…ˆè¿›çš„ç»“æœï¼šåœ¨Tesla V100ä¸Šä»¥65 FPSçš„å®æ—¶é€Ÿåº¦è¾¾åˆ°MS COCOæ•°æ®é›†çš„43.5% APï¼ˆ65.7% AP50ï¼‰ã€‚\n\nè¯„ä¼°ç»“æœ\nåç§°\tå°ºå¯¸\tè®­ç»ƒç¯å¢ƒ\tå›¾åƒå¤§å°\tæ•°æ®é›†\tæ¡†å¹³å‡ç²¾åº¦ï¼ˆ%ï¼‰\tå‚æ•°é‡\tFLOPs\té…ç½®æ–‡ä»¶\tä¸‹è½½é“¾æ¥\nYOLOv4\tCSPDarknet53\tD910x8-G\t608\tMS COCO 2017\t45.4\t27.6M\t52G\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nYOLOv4\tCSPDarknet53(silu)\tD910x8-G\t608\tMS COCO 2017\t45.8\t27.6M\t52G\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\n\n\nå¤‡æ³¨\nè®­ç»ƒç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º{è®¾å¤‡}x{å—æ•°}-{MindSporeæ¨¡å¼}ï¼Œå…¶ä¸­MindSporeæ¨¡å¼å¯ä»¥æ˜¯G - å›¾æ¨¡å¼æˆ–F - ä½¿ç”¨mså‡½æ•°çš„pynativeæ¨¡å¼ã€‚ä¾‹å¦‚ï¼ŒD910x8-Gè¡¨ç¤ºä½¿ç”¨8å—Ascend 910 NPUè¿›è¡Œå›¾æ¨¡å¼è®­ç»ƒã€‚\næ¡†å¹³å‡ç²¾åº¦ï¼šåœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®åº¦æŠ¥å‘Šã€‚\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\næœ‰å…³å¦‚ä½•è®­ç»ƒå’Œæ¨æ–­æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…MindYOLO GitHub ä»“åº“ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Alexey Bochkovskiy, Chien-Yao Wang å’Œ Ali Farhadi. YOLOv4: é¢å‘ç›®æ ‡æ£€æµ‹çš„é€Ÿåº¦ä¸ç²¾åº¦æœ€ä¼˜åŒ–. arXiv é¢„å°æœ¬ arXiv:2004.10934, 2020.",
    "tags": "[\"Object Detection\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/yolox_ms",
    "project_name": "yolox_ms",
    "readme": "Original Text\nYOLOX\nç®€ä»‹\n\nYOLOX æ˜¯ä¸€æ¬¾æ–°å‹é«˜æ€§èƒ½æ£€æµ‹å™¨ï¼Œå¯¹ YOLO ç³»åˆ—è¿›è¡Œäº†ç»éªŒæ€§çš„æ”¹è¿›ã€‚æˆ‘ä»¬å°† YOLO æ£€æµ‹å™¨è½¬æ¢ä¸ºæ— é”šæ¡†æ–¹å¼ï¼Œå¹¶é‡‡ç”¨äº†å…¶ä»–å…ˆè¿›æ£€æµ‹æŠ€æœ¯ï¼Œä¾‹å¦‚è§£è€¦å¤´å’Œé¢†å…ˆçš„æ ‡ç­¾åˆ†é…ç­–ç•¥ SimOTAï¼Œä»¥è¾¾åˆ°å„ç§è§„æ¨¡æ¨¡å‹çš„æœ€å…ˆè¿›ç»“æœï¼šå¯¹äºä»…æœ‰ 0.91M å‚æ•°å’Œ 1.08G FLOPs çš„ YOLO-Nanoï¼Œæˆ‘ä»¬åœ¨ COCO æ•°æ®é›†ä¸Šå–å¾—äº† 25.3% çš„ APï¼Œè¶…è¿‡ NanoDet 1.8% çš„ APï¼›å¯¹äºå·¥ä¸šç•Œæœ€å¹¿æ³›ä½¿ç”¨çš„æ£€æµ‹å™¨ä¹‹ä¸€ YOLOv3ï¼Œæˆ‘ä»¬å°†å…¶æå‡è‡³ COCO æ•°æ®é›†ä¸Šçš„ 47.3% APï¼Œè¶…è¿‡å½“å‰æœ€ä½³å®è·µ 3.0% çš„ APï¼›å¯¹äºå…·æœ‰ä¸ YOLOv4-CSPã€YOLOv5-L ç›¸ä¼¼å‚æ•°é‡çš„ YOLOX-Lï¼Œæˆ‘ä»¬åœ¨ Tesla V100 ä¸Šçš„é€Ÿåº¦è¾¾åˆ° 68.9 FPS æ—¶ï¼Œåœ¨ COCO æ•°æ®é›†ä¸Šå–å¾—äº† 50.0% çš„ APï¼Œè¶…è¿‡ YOLOv5-L 1.8% çš„ APã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨å•ä¸ª YOLOX-L æ¨¡å‹çš„ CVPR 2021 è‡ªåŠ¨é©¾é©¶ç ”è®¨ä¼šæµæ„ŸçŸ¥æŒ‘æˆ˜ä¸­è£è·ç¬¬ä¸€åã€‚\n\nè¯„ä¼°ç»“æœ\nåç§°\tè§„æ¨¡\tè®­ç»ƒç¯å¢ƒ\tå›¾åƒå¤§å°\tæ•°æ®é›†\tæ¡† mAP (%)\tå‚æ•°é‡\tFLOPs\té…ç½®æ–‡ä»¶\tä¸‹è½½é“¾æ¥\nYOLOX\tN\tD910x8-G\t416\tMS COCO 2017\t24.1\t0.9M\t1.1G\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nYOLOX\tTiny\tD910x8-G\t416\tMS COCO 2017\t33.3\t5.1M\t6.5G\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nYOLOX\tS\tD910x8-G\t640\tMS COCO 2017\t40.7\t9.0M\t26.8G\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nYOLOX\tM\tD910x8-G\t640\tMS COCO 2017\t46.7\t25.3M\t73.8G\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nYOLOX\tL\tD910x8-G\t640\tMS COCO 2017\t49.2\t54.2M\t155.6G\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nYOLOX\tX\tD910x8-G\t640\tMS COCO 2017\t51.6\t99.1M\t281.9G\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nYOLOX\tDarknet53\tD910x8-G\t640\tMS COCO 2017\t47.7\t63.7M\t185.3G\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\n\n\næ³¨æ„äº‹é¡¹\nè®­ç»ƒç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º {è®¾å¤‡}x{åˆ†ç‰‡}-{MindSpore æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - pynative æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨ ms å‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºåœ¨ 8 ç‰‡ Ascend 910 NPU ä¸Šä½¿ç”¨å›¾æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚\næ¡† mAPï¼šéªŒè¯é›†ä¸Šçš„å‡†ç¡®åº¦æŠ¥å‘Šã€‚\næˆ‘ä»¬å‚è€ƒå®˜æ–¹çš„ YOLOX æ¥å¤ç°ç»“æœã€‚\nå¦‚ä½•å¼€å§‹ä½¿ç”¨æ¨¡å‹\n\næœ‰å…³å¦‚ä½•è®­ç»ƒå’Œæ¨æ–­æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ MindYOLO GitHub ä»“åº“ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Zheng Ge. YOLOX: Exceeding YOLO Series in 2021. https://arxiv.org/abs/2107.08430, 2021ã€‚",
    "tags": "[\"Object Detection\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/vit_base_patch16_224",
    "project_name": "vit_base_patch16_224",
    "readme": "Original Text\nä¿®æ”¹è¯´æ˜\næ›´æ–°README.mdä¸­çš„ç¤ºä¾‹å¹¶æ·»åŠ npuæ”¯æŒï¼›\nä¿®æ”¹README.mdä¸­ç”¨é€”ä¸é™åˆ¶ç« èŠ‚ã€‚\nVision Transformerï¼ˆåŸºç¡€è§„æ¨¡æ¨¡å‹ï¼‰\n\nè¯¥Vision Transformerï¼ˆViTï¼‰æ¨¡å‹å…ˆåœ¨ImageNet-21kæ•°æ®é›†ï¼ˆ1400ä¸‡å¼ å›¾åƒï¼Œ21,843ä¸ªç±»åˆ«ï¼‰ä¸Šä»¥224x224åˆ†è¾¨ç‡è¿›è¡Œé¢„è®­ç»ƒï¼Œéšååœ¨ImageNet 2012æ•°æ®é›†ï¼ˆ100ä¸‡å¼ å›¾åƒï¼Œ1,000ä¸ªç±»åˆ«ï¼‰ä¸Šä»¥ç›¸åŒåˆ†è¾¨ç‡å¾®è°ƒã€‚è¯¥æ¨¡å‹ç”±Dosovitskiyç­‰äººåœ¨è®ºæ–‡An Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleä¸­æå‡ºï¼Œå¹¶é¦–æ¬¡å‘å¸ƒäºæ­¤ä»£ç åº“ã€‚ä¸è¿‡ï¼Œæ¨¡å‹æƒé‡æ˜¯ç”±Ross Wightmanä»timmä»£ç åº“è½¬æ¢è€Œæ¥ï¼Œä»–å·²å°†æƒé‡ä»JAXæ ¼å¼è½¬æ¢ä¸ºPyTorchæ ¼å¼ã€‚åœ¨æ­¤å‘ä»–è‡´è°¢ã€‚\n\nå…è´£å£°æ˜ï¼šå‘å¸ƒViTçš„å›¢é˜Ÿæœªä¸ºæ­¤æ¨¡å‹ç¼–å†™è¯´æ˜å¡ç‰‡ï¼Œæ•…æœ¬è¯´æ˜å¡ç‰‡ç”±Hugging Faceå›¢é˜Ÿæ’°å†™ã€‚\n\næ¨¡å‹æè¿°\n\nVision Transformerï¼ˆViTï¼‰æ˜¯ä¸€ç§ç±»ä¼¼BERTçš„Transformerç¼–ç å™¨æ¨¡å‹ï¼Œé‡‡ç”¨ç›‘ç£å­¦ä¹ æ–¹å¼åœ¨ImageNet-21kå¤§è§„æ¨¡å›¾åƒé›†ï¼ˆ224x224åƒç´ åˆ†è¾¨ç‡ï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚éšåï¼Œè¯¥æ¨¡å‹åœ¨ImageNetï¼ˆåˆç§°ILSVRC2012ï¼‰æ•°æ®é›†ä¸Šå¾®è°ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«100ä¸‡å¼ å›¾åƒå’Œ1,000ä¸ªç±»åˆ«ï¼ŒåŒæ ·é‡‡ç”¨224x224åˆ†è¾¨ç‡ã€‚\n\nå›¾åƒè¢«å¤„ç†ä¸ºå›ºå®šå°ºå¯¸ï¼ˆ16x16ï¼‰çš„å›¾å—åºåˆ—å¹¶è¿›è¡Œçº¿æ€§åµŒå…¥ã€‚åºåˆ—å‰ç«¯ä¼šæ·»åŠ ä¸€ä¸ª[CLS]æ ‡è®°ç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚åœ¨å°†åºåˆ—è¾“å…¥Transformerç¼–ç å™¨å„å±‚ä¹‹å‰ï¼Œè¿˜ä¼šæ·»åŠ ç»å¯¹ä½ç½®åµŒå…¥ã€‚\n\né€šè¿‡é¢„è®­ç»ƒï¼Œæ¨¡å‹å­¦ä¹ åˆ°å›¾åƒçš„å†…åœ¨è¡¨ç¤ºï¼Œå¯ç”¨äºæå–ä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„ç‰¹å¾ï¼šä¾‹å¦‚ï¼Œè‹¥æ‚¨æœ‰ä¸€ä¸ªå¸¦æ ‡æ³¨çš„å›¾åƒæ•°æ®é›†ï¼Œå¯ä»¥åœ¨é¢„è®­ç»ƒç¼–ç å™¨é¡¶éƒ¨æ·»åŠ çº¿æ€§å±‚æ¥è®­ç»ƒæ ‡å‡†åˆ†ç±»å™¨ã€‚é€šå¸¸ä¼šåœ¨[CLS]æ ‡è®°é¡¶éƒ¨æ·»åŠ çº¿æ€§å±‚ï¼Œå› ä¸ºè¯¥æ ‡è®°çš„æœ€åéšè—çŠ¶æ€å¯è§†ä¸ºæ•´å¼ å›¾åƒçš„è¡¨ç¤ºã€‚\n\nç”¨é€”ä¸é™åˆ¶\n\næ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨è¯¥åŸå§‹æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\nä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨è¯¥æ¨¡å‹å°†COCO 2017æ•°æ®é›†ä¸­çš„å›¾åƒåˆ†ç±»ä¸ºImageNetçš„1,000ä¸ªç±»åˆ«ä¹‹ä¸€ï¼š\n\nfrom openmind_hub import snapshot_download\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel_path = snapshot_download(\"PyTorch-NPU/vit_base_patch16_224\", revision=\"main\", resume_download=True, ignore_patterns=[\"*.h5\", \"*.ot\", \"*.msgpack\"])\nprocessor = ViTImageProcessor.from_pretrained(model_path)\nmodel = ViTForImageClassification.from_pretrained(model_path)\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n\n\nå¦‚éœ€æ›´å¤šä»£ç ç¤ºä¾‹ï¼Œè¯·å‚é˜…æ–‡æ¡£ã€‚\n\nè®­ç»ƒæ•°æ®\n\nViTæ¨¡å‹åœ¨ImageNet-21kä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«1400ä¸‡å¼ å›¾åƒå’Œ21kä¸ªç±»åˆ«ï¼Œå¹¶åœ¨ImageNetä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«100ä¸‡å¼ å›¾åƒå’Œ1kä¸ªç±»åˆ«ã€‚\n\nè®­ç»ƒæµç¨‹\né¢„å¤„ç†\n\nè®­ç»ƒ/éªŒè¯æœŸé—´å›¾åƒé¢„å¤„ç†çš„å…·ä½“ç»†èŠ‚å¯åœ¨æ­¤å¤„æ‰¾åˆ°ã€‚\n\nå›¾åƒè¢«è°ƒæ•´å¤§å°/ç¼©æ”¾è‡³ç›¸åŒåˆ†è¾¨ç‡ï¼ˆ224x224ï¼‰ï¼Œå¹¶åœ¨RGBé€šé“ä¸Šä»¥å‡å€¼ï¼ˆ0.5, 0.5, 0.5ï¼‰å’Œæ ‡å‡†å·®ï¼ˆ0.5, 0.5, 0.5ï¼‰è¿›è¡Œå½’ä¸€åŒ–ã€‚\n\né¢„è®­ç»ƒ\n\næ¨¡å‹åœ¨TPUv3ç¡¬ä»¶ï¼ˆ8æ ¸ï¼‰ä¸Šè®­ç»ƒã€‚æ‰€æœ‰æ¨¡å‹å˜ä½“å‡ä»¥4096çš„æ‰¹é‡å¤§å°å’Œ10kæ­¥çš„å­¦ä¹ ç‡é¢„çƒ­è¿›è¡Œè®­ç»ƒã€‚å¯¹äºImageNetï¼Œä½œè€…å‘ç°é¢å¤–åº”ç”¨å…¨å±€èŒƒæ•°ä¸º1çš„æ¢¯åº¦è£å‰ªæœ‰ç›Šã€‚è®­ç»ƒåˆ†è¾¨ç‡ä¸º224ã€‚\n\nè¯„ä¼°ç»“æœ\n\nå…³äºå¤šä¸ªå›¾åƒåˆ†ç±»åŸºå‡†çš„è¯„ä¼°ç»“æœï¼Œè¯·å‚é˜…åŸå§‹è®ºæ–‡çš„è¡¨2å’Œè¡¨5ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¾®è°ƒæ—¶ï¼Œä½¿ç”¨æ›´é«˜åˆ†è¾¨ç‡ï¼ˆ384x384ï¼‰å¯è·å¾—æœ€ä½³ç»“æœã€‚å½“ç„¶ï¼Œå¢å¤§æ¨¡å‹å°ºå¯¸ä¼šå¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚\n\nBibTeXæ¡ç›®åŠå¼•ç”¨ä¿¡æ¯\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n",
    "tags": "[\"Image Classification\", \"PyTorch\", \"TensorFlow\", \"Transformers\", \"NeMo\", \"ELM\", \"Apache License 2.0\", \"imagenet-1k\", \"imagenet-21k\", \"vision\"]"
  },
  {
    "url": "https://gitcode.com/openMind/efficientnet_ms",
    "project_name": "efficientnet_ms",
    "readme": "Original Text\nEfficientNet\n\nEfficientNetï¼šé‡æ–°æ€è€ƒå·ç§¯ç¥ç»ç½‘ç»œçš„æ¨¡å‹ç¼©æ”¾\n\nç®€ä»‹\n\nå›¾ 1 å±•ç¤ºäº†ä»ä¸‰ä¸ªç»´åº¦â€”â€”å®½åº¦ã€æ·±åº¦ã€åˆ†è¾¨ç‡å’Œå¤åˆæ‰©å¼ æ¨¡å‹çš„æ–¹æ³•ã€‚ä»…å¢åŠ æ¨¡å‹å¤§å°å°†å¯¼è‡´æ¨¡å‹æ€§èƒ½æ¬¡ä¼˜ã€‚ç„¶è€Œï¼Œå¦‚æœå°†è¿™ä¸‰ç§æ–¹æ³•ä¸€èµ·åº”ç”¨äºæ¨¡å‹ï¼Œæ›´æœ‰å¯èƒ½å®ç°æœ€ä¼˜è§£ã€‚é€šè¿‡ä½¿ç”¨ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼Œå¯ä»¥æ‰¾åˆ°å®½åº¦ç¼©æ”¾ã€æ·±åº¦ç¼©æ”¾å’Œåˆ†è¾¨ç‡ç¼©æ”¾çš„æœ€ä½³é…ç½®ã€‚EfficientNet åœ¨ ImageNet-1K æ•°æ®é›†ä¸Šå®ç°äº†æ¯”ä¹‹å‰æ–¹æ³•æ›´å¥½çš„æ¨¡å‹æ€§èƒ½ã€‚1\n\nå›¾ 1. Efficientent æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K ä¸Šçš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•° (M)\té…ç½®\tä¸‹è½½åœ°å€\nefficientnet_b0\tD910x64-G\t76.89\t93.16\t5.33\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\nefficientnet_b1\tD910x64-G\t78.95\t94.34\t7.86\té…ç½®æ–‡ä»¶\tæƒé‡æ–‡ä»¶\næ³¨æ„\nç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º {è®¾å¤‡}x{å—}-{MS æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - ä½¿ç”¨ ms å‡½æ•°çš„ pynative æ¨¡å¼ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºä½¿ç”¨å›¾æ¨¡å¼åœ¨ 8 å— Ascend 910 NPU ä¸Šè¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV çš„å®‰è£…è¯´æ˜ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†ä»¥ä¾›æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 64 python train.py --config configs/efficientnet/efficientnet_b0_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚é˜… config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…æ ¹æ®æ–°çš„å…¨å±€æ‰¹é‡å¤§å°çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨æƒ³åœ¨ä¸è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹åœ¨å°æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/efficientnet/efficientnet_b0_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/efficientnet/efficientnet_b0_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²\n\nè¯·å‚è€ƒ MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Tan M, Le Q. Efficientnet: Rethinking model scaling for convolutional neural networks[C]//å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ã€‚PMLR, 2019: 6105-6114.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/inceptionv4_ms",
    "project_name": "inceptionv4_ms",
    "readme": "Original Text\nInceptionV4\n\nInceptionV4: Inception-v4, Inception-ResNet åŠæ®‹å·®è¿æ¥å¯¹å­¦ä¹ çš„å½±å“\n\nç®€ä»‹\n\nInceptionV4 ç ”ç©¶äº† Inception æ¨¡å—ç»“åˆæ®‹å·®è¿æ¥æ˜¯å¦èƒ½å¤Ÿå¾—åˆ°ä¼˜åŒ–ã€‚ç ”ç©¶å‘ç°ï¼ŒResNet çš„ç»“æ„å¯ä»¥æå¤§åœ°åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶ä¸”æ€§èƒ½ä¹Ÿæœ‰æ‰€æå‡ã€‚å¾—åˆ°äº†ä¸€ä¸ª Inception-ResNet v2 ç½‘ç»œï¼Œå¹¶ä¸”è¿˜è®¾è®¡äº†ä¸€ä¸ªæ›´æ·±å…¥ä¸”ä¼˜åŒ–çš„ Inception v4 æ¨¡å‹ï¼Œå…¶æ€§èƒ½å¯ä»¥ä¸ Inception-ResNet v2 ç›¸åª²ç¾ã€‚[1]\n\nå›¾ 1. InceptionV4 æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K ä¸Šçš„æ¨¡å‹æ€§èƒ½å¤ç°ç»“æœå¦‚ä¸‹ã€‚\n\næ¨¡å‹\tè®­ç»ƒç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•°é‡ (M)\té…ç½®æ–‡ä»¶\tä¸‹è½½åœ°å€\ninception_v4\tD910x8-G\t80.88\t95.34\t42.74\tyaml\tæƒé‡æ–‡ä»¶\næ³¨æ„\nè®­ç»ƒç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒçš„è¡¨ç¤ºæ–¹æ³•ä¸º {è®¾å¤‡}x{éƒ¨ä»¶}-{MindSpore æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - PyNative æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨ ms å‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºä½¿ç”¨å›¾æ¨¡å¼åœ¨ 8 ç‰‡ Ascend 910 NPU ä¸Šè¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ ImageNet-1K éªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®åº¦ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV çš„å®‰è£…æŒ‡å—ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†ç”¨äºæ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹è½»æ¾å¤ç°æŠ¥å‘Šçš„ç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/inceptionv4/inception_v4_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨æƒ³åœ¨ä¸éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å°æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/inceptionv4/inception_v4_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/inceptionv4/inception_v4_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²æŒ‡å—\n\nè¯·å‚è€ƒ MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Szegedy C, Ioffe S, Vanhoucke V, ç­‰äºº. Inception-v4, inception-resnet ä»¥åŠæ®‹å·®è¿æ¥å¯¹å­¦ä¹ çš„å½±å“[C]//ç¬¬ä¸‰åä¸€å±Šç¾å›½äººå·¥æ™ºèƒ½åä¼šå¹´ä¼šã€‚2017å¹´ã€‚",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/llama_7b_ms",
    "project_name": "llama_7b_ms",
    "readme": "OpenLLaMA: An Open Reproduction of LLaMA\nModification\n\nModify the example code section.\n\nOverview\n\nIn this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.\n\nQuick Start\nFinetune\n\nThe preprocessing scripts for alpaca datasets are currently available for tasks involving full-parameter fine-tuning and lora fine-tuning.\n\nadd prompts for dataset\n\npython example/dataset/alpaca_converter.py \\\n--data_path /{path}/alpaca_data.json \\\n--output_path /{path}/alpaca-data-conversation.json\n\n\ngenerate mindrecord dataset\n\npython llama_preprocess.py \\\n--input_glob /{path}/alpaca-data-conversation.json \\\n--seq_length 2048 \\\n--output_file /{path}/alpaca-fastchat2048.mindrecord\n\n\ntrain\n\ncd example\nbash msrun.sh \"finetune.py --train_dataset /{path}/alpaca-fastchat2048.mindrecord\"\n\nInference\nimport mindspore as ms\nfrom openmind import pipeline\n\nms.set_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\",\n                         model='MindSpore-Lab/llama_7b',\n                         framework='ms',\n                         model_kwargs={\"use_past\":True},\n                         trust_remote_code=True)\n\ntext = \"I love Beijing, because\"\n\npipeline_result = pipeline_task(text, do_sample=False)\nprint(pipeline_result)\n\nDataset and Training\n\nWe train our models on the RedPajama dataset released by Together, which is a reproduction of the LLaMA training dataset containing over 1.2 trillion tokens. We follow the exactly same preprocessing steps and training hyperparameters as the original LLaMA paper, including model architecture, context length, training steps, learning rate schedule, and optimizer. The only difference between our setting and the original one is the dataset used: OpenLLaMA employs the RedPajama dataset rather than the one utilized by the original LLaMA.\n\nWe train the models on cloud TPU-v4s using EasyLM, a JAX based training pipeline we developed for training and fine-tuning large language models. We employ a combination of normal data parallelism and fully sharded data parallelism (also know as ZeRO stage 3) to balance the training throughput and memory usage. Overall we reach a throughput of over 2200 tokens / second / TPU-v4 chip for our 7B model.\n\nEvaluation\n\nWe evaluated OpenLLaMA on a wide range of tasks using lm-evaluation-harness. The LLaMA results are generated by running the original LLaMA model on the same evaluation metrics. We note that our results for the LLaMA model differ slightly from the original LLaMA paper, which we believe is a result of different evaluation protocols. Similar differences have been reported in this issue of lm-evaluation-harness. Additionally, we present the results of GPT-J, a 6B parameter model trained on the Pile dataset by EleutherAI.\n\nThe original LLaMA model was trained for 1 trillion tokens and GPT-J was trained for 500 billion tokens. We present the results in the table below. OpenLLaMA exhibits comparable performance to the original LLaMA and GPT-J across a majority of tasks, and outperforms them in some tasks.\n\nTask/Metric\tGPT-J 6B\tLLaMA 7B\tOpenLLaMA 7B\tOpenLLaMA 3B\tOpenLLaMA 13B 600BT\nanli_r1/acc\t0.32\t0.35\t0.33\t0.33\t0.33\nanli_r2/acc\t0.34\t0.34\t0.36\t0.32\t0.35\nanli_r3/acc\t0.35\t0.37\t0.38\t0.35\t0.38\narc_challenge/acc\t0.34\t0.39\t0.37\t0.34\t0.39\narc_challenge/acc_norm\t0.37\t0.41\t0.38\t0.37\t0.42\narc_easy/acc\t0.67\t0.68\t0.72\t0.69\t0.74\narc_easy/acc_norm\t0.62\t0.52\t0.68\t0.65\t0.70\nddboolq/acc\t0.50\t0.56\t0.53\t0.49\t0.71\nhellaswag/acc\t0.36\t0.36\t0.63\t0.43\t0.54\nhellaswag/acc_norm\t0.66\t0.73\t0.72\t0.67\t0.73\nopenbookqa/acc\t0.29\t0.29\t0.30\t0.27\t0.30\nopenbookqa/acc_norm\t0.38\t0.41\t0.40\t0.40\t0.41\npiqa/acc\t0.75\t0.78\t0.76\t0.75\t0.77\npiqa/acc_norm\t0.76\t0.78\t0.77\t0.76\t0.78\nrecord/em\t0.88\t0.91\t0.89\t0.88\t0.90\nrecord/f1\t0.89\t0.91\t0.90\t0.89\t0.90\nrte/acc\t0.54\t0.56\t0.60\t0.58\t0.65\ntruthfulqa_mc/mc1\t0.20\t0.21\t0.23\t0.22\t0.22\ntruthfulqa_mc/mc2\t0.36\t0.34\t0.35\t0.35\t0.35\nwic/acc\t0.50\t0.50\t0.51\t0.48\t0.49\nwinogrande/acc\t0.64\t0.68\t0.67\t0.62\t0.67\nAverage\t0.51\t0.53\t0.55\t0.52\t0.56\n\nWe removed the task CB and WSC from our benchmark, as our model performs suspiciously well on these two tasks. We hypothesize that there could be a benchmark data contamination in the training set.\n\nContact\n\nWe would love to get feedback from the community. If you have any questions, please open an issue or contact us.\n\nOpenLLaMA is developed by: Xinyang Geng* and Hao Liu* from Berkeley AI Research. *Equal Contribution\n\nAcknowledgment\n\nWe thank the Google TPU Research Cloud program for providing part of the computation resources. Weâ€™d like to specially thank Jonathan Caton from TPU Research Cloud for helping us organizing compute resources, Rafi Witten from the Google Cloud team and James Bradbury from the Google JAX team for helping us optimizing our training throughput. Weâ€™d also want to thank Charlie Snell, Gautier Izacard, Eric Wallace, Lianmin Zheng and our user community for the discussions and feedback.\n\nThe OpenLLaMA 13B model is trained in collaboration with Stability AI, and we thank Stability AI for providing the computation resources. Weâ€™d like to especially thank David Ha and Shivanshu Purohit for the coordinating the logistics and providing engineering support.\n\nReference\n\nIf you found OpenLLaMA useful in your research or applications, please cite using the following BibTeX:\n\n@software{openlm2023openllama,\n  author = {Geng, Xinyang and Liu, Hao},\n  title = {OpenLLaMA: An Open Reproduction of LLaMA},\n  month = May,\n  year = 2023,\n  url = {https://github.com/openlm-research/open_llama}\n}\n\n@software{together2023redpajama,\n  author = {Together Computer},\n  title = {RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset},\n  month = April,\n  year = 2023,\n  url = {https://github.com/togethercomputer/RedPajama-Data}\n}\n\n@article{touvron2023llama,\n  title={Llama: Open and efficient foundation language models},\n  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},\n  journal={arXiv preprint arXiv:2302.13971},\n  year={2023}\n}\n",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/convnextv2_ms",
    "project_name": "convnextv2_ms",
    "readme": "Original Text\nConvNeXt V2\n\nConvNeXt V2ï¼šä½¿ç”¨é®ç è‡ªç¼–ç å™¨å…±è®¾è®¡å¹¶æ‰©å±•å·ç§¯ç½‘ç»œ\n\nç®€ä»‹\n\næœ¬æ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å…¨å·ç§¯é®ç è‡ªç¼–ç å™¨æ¡†æ¶ä»¥åŠä¸€ç§æ–°çš„å…¨å±€å“åº”å½’ä¸€åŒ–ï¼ˆGRNï¼‰å±‚ï¼Œè¯¥å±‚å¯æ·»åŠ åˆ°ConvNeXtæ¶æ„ä¸­ä»¥å¢å¼ºé€šé“é—´ç‰¹å¾ç«äº‰ã€‚è¿™ç§è‡ªæˆ‘ç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚MAEï¼‰ä¸æ¶æ„æ”¹è¿›çš„å…±è®¾è®¡ï¼Œäº§ç”Ÿäº†ä¸€ç§æ–°çš„æ¨¡å‹å®¶æ—ConvNeXt V2ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—æé«˜äº†çº¯å·ç§¯ç½‘ç»œåœ¨å„ç§è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ImageNetåˆ†ç±»ã€COCOæ£€æµ‹å’ŒADE20Kåˆ†å‰²ã€‚[1]\n\nå›¾1. ConvNeXt V2æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ImageNet-1Kä¸Šå¤ç°çš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tè®­ç»ƒç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•° (M)\té…ç½®æ–‡ä»¶\tä¸‹è½½\nconvnextv2_tiny\tD910x8-G\t82.43\t95.98\t28.64\tyaml\tæƒé‡\nè¯´æ˜\nè®­ç»ƒç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º{è®¾å¤‡}x{åˆ†ç‰‡}-{MindSporeæ¨¡å¼}ï¼Œå…¶ä¸­MindSporeæ¨¡å¼å¯ä»¥æ˜¯G - å›¾æ¨¡å¼æˆ–F - pynativeæ¨¡å¼é…åˆmså‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-Gè¡¨ç¤ºåœ¨8ç‰‡Ascend 910 NPUä¸Šä½¿ç”¨å›¾æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šåœ¨ImageNet-1KéªŒè¯é›†ä¸ŠæŠ¥å‘Šçš„å‡†ç¡®ç‡ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡å·¥ä½œ\nå®‰è£…\n\nè¯·å‚è€ƒMindCVä¸­çš„å®‰è£…æŒ‡å—ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ImageNet-1Kæ•°æ®é›†ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹è½»æ¾å¤ç°æŠ¥å‘Šç»“æœã€‚å¯¹äºåœ¨å¤šä¸ªAscend 910è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/convnextv2/convnextv2_tiny_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­æ·»åŠ  --allow-run-as-root å‚æ•°ã€‚\n\nåŒæ ·åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\nå…³äºæ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹æ¬¡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹æ¬¡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹æ¬¡å¤§å°æ—¶çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨å¸Œæœ›åœ¨ä¸éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/convnextv2/convnextv2_tiny_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯\n\nä¸ºäº†éªŒè¯è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/convnextv2/convnextv2_tiny_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²\n\nè¯·å‚è€ƒ MindCV çš„éƒ¨ç½²æ•™ç¨‹ã€‚\n\nå‚è€ƒæ–‡çŒ®\n\n[1] Woo S, Debnath S, Hu R, ç­‰. ConvNeXt V2ï¼šä½¿ç”¨é®è”½è‡ªç¼–ç å™¨å…±åŒè®¾è®¡å¹¶æ‰©å±•å·ç§¯ç½‘ç»œ[J]. arXiv é¢„å°æœ¬ arXiv:2301.00808, 2023ã€‚",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/densenet_ms",
    "project_name": "densenet_ms",
    "readme": "DenseNet\n\nDensely Connected Convolutional Networks\n\nIntroduction\n\nRecent work has shown that convolutional networks can be substantially deeper, more accurate, and more efficient to train if they contain shorter connections between layers close to the input and those close to the output. Dense Convolutional Network (DenseNet) is introduced based on this observation, which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with \nLL\nL layers have \nLL\nL connections-one between each layer and its subsequent layer, DenseNet has \nL(L+1)2\\frac{L(L+1)}{2}\n2\nL(L+1)\n\tâ€‹\n\n direct connections. For each layer, the feature maps of all preceding layers are used as inputs, and their feature maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters.[1]\n\nFigure 1. Architecture of DenseNet [1]\n\nResults\n\nOur reproduced model performance on ImageNet-1K is reported as follows.\n\nModel\tContext\tTop-1 (%)\tTop-5 (%)\tParams (M)\tRecipe\tDownload\ndensenet121\tD910x8-G\t75.64\t92.84\t8.06\tyaml\tweights\ndensenet161\tD910x8-G\t79.09\t94.66\t28.90\tyaml\tweights\ndensenet169\tD910x8-G\t77.26\t93.71\t14.31\tyaml\tweights\ndensenet201\tD910x8-G\t78.14\t94.08\t20.24\tyaml\tweights\nNotes\nContext: Training context denoted as {device}x{pieces}-{MS mode}, where mindspore mode can be G - graph mode or F - pynative mode with ms function. For example, D910x8-G is for training on 8 pieces of Ascend 910 NPU using graph mode.\nTop-1 and Top-5: Accuracy reported on the validation set of ImageNet-1K.\nQuick Start\nPreparation\nInstallation\n\nPlease refer to the installation instruction in MindCV.\n\nDataset Preparation\n\nPlease download the ImageNet-1K dataset for model training and validation.\n\nTraining\nDistributed Training\n\nIt is easy to reproduce the reported results with the pre-defined training recipe. For distributed training on multiple Ascend 910 devices, please run\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/densenet/densenet_121_ascend.yaml --data_dir /path/to/imagenet\n\n\nIf the script is executed by the root user, the --allow-run-as-root parameter must be added to mpirun.\n\nSimilarly, you can train the model on multiple GPU devices with the above mpirun command.\n\nFor detailed illustration of all hyper-parameters, please refer to config.py.\n\nNote: As the global batch size (batch_size x num_devices) is an important hyper-parameter, it is recommended to keep the global batch size unchanged for reproduction or adjust the learning rate linearly to a new global batch size.\n\nStandalone Training\n\nIf you want to train or finetune the model on a smaller dataset without distributed training, please run:\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/densenet/densenet_121_ascend.yaml --data_dir /path/to/dataset --distribute False\n\nValidation\n\nTo validate the accuracy of the trained model, you can use validate.py and parse the checkpoint path with --ckpt_path.\n\npython validate.py -c configs/densenet/densenet_121_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\nDeployment\n\nPlease refer to the deployment tutorial in MindCV.\n\nReferences\n\n[1] Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708.",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/internlm_7b_base_ms",
    "project_name": "internlm_7b_base_ms",
    "readme": "InternLM\nÂ \nInternLM HOT\nÂ \n\nğŸ’»Github Repo â€¢ ğŸ¤”Reporting Issues\n\nModification\n\nModify the example code section.\n\nIntroduction\n\nInternLM has open-sourced a 7 billion parameter base model tailored for practical scenarios. The model has the following characteristics:\n\nIt leverages trillions of high-quality tokens for training to establish a powerful knowledge base.\nIt provides a versatile toolset for users to flexibly build their own workflows.\nInternLM-7B\nPerformance Evaluation\n\nWe conducted a comprehensive evaluation of InternLM using the open-source evaluation tool OpenCompass. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the OpenCompass leaderboard for more evaluation results.\n\nDatasets\\Models\tInternLM-Chat-7B\tInternLM-7B\tLLaMA-7B\tBaichuan-7B\tChatGLM2-6B\tAlpaca-7B\tVicuna-7B\nC-Eval(Val)\t53.2\t53.4\t24.2\t42.7\t50.9\t28.9\t31.2\nMMLU\t50.8\t51.0\t35.2*\t41.5\t46.0\t39.7\t47.3\nAGIEval\t42.5\t37.6\t20.8\t24.6\t39.0\t24.1\t26.4\nCommonSenseQA\t75.2\t59.5\t65.0\t58.8\t60.0\t68.7\t66.7\nBUSTM\t74.3\t50.6\t48.5\t51.3\t55.0\t48.8\t62.5\nCLUEWSC\t78.6\t59.1\t50.3\t52.8\t59.8\t50.3\t52.2\nMATH\t6.4\t7.1\t2.8\t3.0\t6.6\t2.2\t2.8\nGSM8K\t34.5\t31.2\t10.1\t9.7\t29.2\t6.0\t15.3\nHumanEval\t14.0\t10.4\t14.0\t9.2\t9.2\t9.2\t11.0\nRACE(High)\t76.3\t57.4\t46.9*\t28.1\t66.3\t40.7\t54.0\nThe evaluation results were obtained from OpenCompass 20230706 (some data marked with *, which means come from the original papers), and evaluation configuration can be found in the configuration files provided by OpenCompass.\nThe evaluation data may have numerical differences due to the version iteration of OpenCompass, so please refer to the latest evaluation results of OpenCompass.\n\nLimitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.\n\nUse with openMind\nfinetune\ncreate dataset\n\nRaw dataset downloadï¼šalpaca_data\n\npython example/dataset/alpaca_data_preprocess.py \\\n--mindrecord_schema internlm_alpaca \\\n--input_glob {path}/alpaca_data.json \\\n--output_file {path}/alpaca_processed/alpaca.mindrecord \\\n--seq_length 2048\n\ntrain\ncd example\nbash train.sh \"train_internlm_7b.py --train_dataset /{path}/alpaca.mindrecord\"\n\nOpen Source License\n\nThe code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow free commercial usage. To apply for a commercial license, please fill in the application form (English)/ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰. For other questions or collaborations, please contact internlm@pjlab.org.cn.\n\nä¿®æ”¹è¯´æ˜\n\nä¿®æ”¹äº†å¿«é€Ÿå¼€å§‹çš„ç¤ºä¾‹ä»£ç ã€‚\n\nç®€ä»‹\n\nInternLM ï¼Œå³ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹ï¼ŒåŒ…å«é¢å‘å®ç”¨åœºæ™¯çš„70äº¿å‚æ•°åŸºç¡€æ¨¡å‹ ï¼ˆInternLM-7Bï¼‰ã€‚æ¨¡å‹å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n\nä½¿ç”¨ä¸Šä¸‡äº¿é«˜è´¨é‡é¢„æ–™ï¼Œå»ºç«‹æ¨¡å‹è¶…å¼ºçŸ¥è¯†ä½“ç³»ï¼›\né€šç”¨å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œæ”¯æŒç”¨æˆ·çµæ´»è‡ªåŠ©æ­å»ºæµç¨‹ï¼›\nInternLM-7B\næ€§èƒ½è¯„æµ‹\n\næˆ‘ä»¬ä½¿ç”¨å¼€æºè¯„æµ‹å·¥å…· OpenCompass ä»å­¦ç§‘ç»¼åˆèƒ½åŠ›ã€è¯­è¨€èƒ½åŠ›ã€çŸ¥è¯†èƒ½åŠ›ã€æ¨ç†èƒ½åŠ›ã€ç†è§£èƒ½åŠ›äº”å¤§èƒ½åŠ›ç»´åº¦å¯¹InternLMå¼€å±•å…¨é¢è¯„æµ‹ï¼Œéƒ¨åˆ†è¯„æµ‹ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼Œæ¬¢è¿è®¿é—® OpenCompass æ¦œå• è·å–æ›´å¤šçš„è¯„æµ‹ç»“æœã€‚\n\næ•°æ®é›†\\æ¨¡å‹\tInternLM-Chat-7B\tInternLM-7B\tLLaMA-7B\tBaichuan-7B\tChatGLM2-6B\tAlpaca-7B\tVicuna-7B\nC-Eval(Val)\t53.2\t53.4\t24.2\t42.7\t50.9\t28.9\t31.2\nMMLU\t50.8\t51.0\t35.2*\t41.5\t46.0\t39.7\t47.3\nAGIEval\t42.5\t37.6\t20.8\t24.6\t39.0\t24.1\t26.4\nCommonSenseQA\t75.2\t59.5\t65.0\t58.8\t60.0\t68.7\t66.7\nBUSTM\t74.3\t50.6\t48.5\t51.3\t55.0\t48.8\t62.5\nCLUEWSC\t78.6\t59.1\t50.3\t52.8\t59.8\t50.3\t52.2\nMATH\t6.4\t7.1\t2.8\t3.0\t6.6\t2.2\t2.8\nGSM8K\t34.5\t31.2\t10.1\t9.7\t29.2\t6.0\t15.3\nHumanEval\t14.0\t10.4\t14.0\t9.2\t9.2\t9.2\t11.0\nRACE(High)\t76.3\t57.4\t46.9*\t28.1\t66.3\t40.7\t54.0\nä»¥ä¸Šè¯„æµ‹ç»“æœåŸºäº OpenCompass 20230706 è·å¾—ï¼ˆéƒ¨åˆ†æ•°æ®æ ‡æ³¨*ä»£è¡¨æ•°æ®æ¥è‡ªåŸå§‹è®ºæ–‡ï¼‰ï¼Œå…·ä½“æµ‹è¯•ç»†èŠ‚å¯å‚è§ OpenCompass ä¸­æä¾›çš„é…ç½®æ–‡ä»¶ã€‚\nè¯„æµ‹æ•°æ®ä¼šå›  OpenCompass çš„ç‰ˆæœ¬è¿­ä»£è€Œå­˜åœ¨æ•°å€¼å·®å¼‚ï¼Œè¯·ä»¥ OpenCompass æœ€æ–°ç‰ˆçš„è¯„æµ‹ç»“æœä¸ºä¸»ã€‚\n\nå±€é™æ€§ï¼š å°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬éå¸¸æ³¨é‡æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå°½åŠ›ä¿ƒä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆä¼¦ç†å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†å—é™äºæ¨¡å‹å¤§å°ä»¥åŠæ¦‚ç‡ç”ŸæˆèŒƒå¼ï¼Œæ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿå„ç§ä¸ç¬¦åˆé¢„æœŸçš„è¾“å‡ºï¼Œä¾‹å¦‚å›å¤å†…å®¹åŒ…å«åè§ã€æ­§è§†ç­‰æœ‰å®³å†…å®¹ï¼Œè¯·å‹¿ä¼ æ’­è¿™äº›å†…å®¹ã€‚ç”±äºä¼ æ’­ä¸è‰¯ä¿¡æ¯å¯¼è‡´çš„ä»»ä½•åæœï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…è´£ä»»ã€‚\n\né€šè¿‡openMindä½¿ç”¨\nå¾®è°ƒ\nåˆ›å»ºæ•°æ®é›†\n\nåŸå§‹æ•°æ®é›†ä¸‹è½½ï¼šalpaca_data\n\npython example/dataset/alpaca_data_preprocess.py \\\n--mindrecord_schema internlm_alpaca \\\n--input_glob {path}/alpaca_data.json \\\n--output_file {path}/alpaca_processed/alpaca.mindrecord \\\n--seq_length 2048\n\nè®­ç»ƒ\ncd example\nbash train.sh \"train_internlm_7b.py --train_dataset /{path}/alpaca.mindrecord\"\n\næ¨ç†\nimport argparse\nimport mindspore as ms\nfrom openmind import pipeline\n\nms.set_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\",\n                         model='MindSpore-Lab/internlm_7b_base',\n                         framework='ms',\n                         model_kwargs={\"use_past\":True},\n                         trust_remote_code=True)\n\ntext = \"ä½ å¥½\"\n\npipeline_result = pipeline_task(text, do_sample=False)\nprint(pipeline_result)\n\nå¼€æºè®¸å¯è¯\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆç”³è¯·è¡¨ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» internlm@pjlab.org.cnã€‚",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/internlm_7b_chat_ms",
    "project_name": "internlm_7b_chat_ms",
    "readme": "Original Text\nä¹¦ç”ŸÂ·æµ¦è¯­\nÂ \nInternLM çƒ­é—¨\nÂ \n\nğŸ’»ä»£ç ä»“åº“ â€¢ ğŸ¤”é—®é¢˜åé¦ˆ\n\næ›´æ–°å†…å®¹\n\nä¿®æ”¹äº†ç¤ºä¾‹ä»£ç éƒ¨åˆ†ã€‚\n\næ¨¡å‹ç®€ä»‹\n\nä¹¦ç”ŸÂ·æµ¦è¯­å¼€æºäº†é¢å‘å®ç”¨åœºæ™¯çš„70äº¿å‚æ•°åŸºç¡€æ¨¡å‹ä¸å¯¹è¯æ¨¡å‹ã€‚è¯¥æ¨¡å‹å…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š\n\né€šè¿‡ä¸‡äº¿çº§é«˜è´¨é‡è¯­æ–™è®­ç»ƒï¼Œæ„å»ºäº†å¼ºå¤§çš„çŸ¥è¯†ä½“ç³»ã€‚\næ”¯æŒ8kè¶…é•¿ä¸Šä¸‹æ–‡çª—å£ï¼Œå…·å¤‡æ›´é•¿çš„è¾“å…¥å¤„ç†èƒ½åŠ›å’Œæ›´å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚\næä¾›å¤šæ ·åŒ–å·¥å…·è°ƒç”¨ï¼Œæ”¯æŒç”¨æˆ·çµæ´»æ„å»ºå·¥ä½œæµç¨‹ã€‚\nInternLM-7B\næ€§èƒ½è¯„æµ‹\n\næˆ‘ä»¬ä½¿ç”¨å¼€æºè¯„æµ‹å·¥å…·OpenCompasså¯¹æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„æµ‹ï¼Œä¸»è¦ä»å­¦ç§‘èƒ½åŠ›ã€è¯­è¨€èƒ½åŠ›ã€çŸ¥è¯†èƒ½åŠ›ã€æ¨ç†èƒ½åŠ›å’Œç†è§£èƒ½åŠ›äº”å¤§ç»´åº¦è¿›è¡Œè¯„æµ‹ã€‚ä»¥ä¸‹æ˜¯éƒ¨åˆ†è¯„æµ‹ç»“æœï¼Œæ›´å¤šè¯„æµ‹ç»“æœè¯·å‚è§OpenCompassæ¦œå•ã€‚\n\nè¯„æµ‹é›†\\æ¨¡å‹\tInternLM-Chat-7B\tInternLM-7B\tLLaMA-7B\tBaichuan-7B\tChatGLM2-6B\tAlpaca-7B\tVicuna-7B\nC-Eval(éªŒè¯é›†)\t53.2\t53.4\t24.2\t42.7\t50.9\t28.9\t31.2\nMMLU\t50.8\t51.0\t35.2*\t41.5\t46.0\t39.7\t47.3\nAGIEval\t42.5\t37.6\t20.8\t24.6\t39.0\t24.1\t26.4\nå¸¸è¯†æ¨ç†\t75.2\t59.5\t65.0\t58.8\t60.0\t68.7\t66.7\nå¸ƒå°”æ¨ç†\t74.3\t50.6\t48.5\t51.3\t55.0\t48.8\t62.5\næŒ‡ä»£æ¶ˆè§£\t78.6\t59.1\t50.3\t52.8\t59.8\t50.3\t52.2\næ•°å­¦èƒ½åŠ›\t6.4\t7.1\t2.8\t3.0\t6.6\t2.2\t2.8\næ•°å­¦åº”ç”¨é¢˜\t34.5\t31.2\t10.1\t9.7\t29.2\t6.0\t15.3\nä»£ç ç”Ÿæˆ\t14.0\t10.4\t14.0\t9.2\t9.2\t9.2\t11.0\nè‹±è¯­é˜…è¯»\t76.3\t57.4\t46.9*\t28.1\t66.3\t40.7\t54.0\nä»¥ä¸Šè¯„æµ‹ç»“æœåŸºäºOpenCompass 20230706è·å–ï¼ˆéƒ¨åˆ†æ•°æ®æ ‡æ³¨*è¡¨ç¤ºæ•°æ®æ¥è‡ªåŸè®ºæ–‡ï¼‰ï¼Œè¯„æµ‹é…ç½®å¯å‚è€ƒOpenCompassæä¾›çš„é…ç½®æ–‡ä»¶ã€‚\nè¯„æµ‹æ•°æ®ä¼šå› OpenCompassçš„ç‰ˆæœ¬è¿­ä»£è€Œå­˜åœ¨æ•°å€¼å·®å¼‚ï¼Œè¯·ä»¥OpenCompassæœ€æ–°è¯„æµ‹ç»“æœä¸ºå‡†ã€‚\n\nå±€é™æ€§è¯´æ˜ï¼š å°½ç®¡æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°½åŠ›ç¡®ä¿æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œä¿ƒä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆä¼¦ç†å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†ç”±äºæ¨¡å‹è§„æ¨¡å’Œæ¦‚ç‡ç”ŸæˆèŒƒå¼çš„åŸå› ï¼Œä»æœ‰å¯èƒ½äº§ç”Ÿæ„å¤–è¾“å‡ºã€‚ä¾‹å¦‚ï¼Œç”Ÿæˆçš„å›å¤å¯èƒ½å­˜åœ¨åè§ã€æ­§è§†æˆ–å…¶ä»–æœ‰å®³å†…å®¹ã€‚è¯·å‹¿ä¼ æ’­æ­¤ç±»å†…å®¹ã€‚å› ä¼ æ’­æœ‰å®³ä¿¡æ¯å¯¼è‡´çš„ä»»ä½•åæœï¼Œæˆ‘ä»¬æ¦‚ä¸è´Ÿè´£ã€‚\n\nä½¿ç”¨openMind\nå¾®è°ƒ\nåˆ›å»ºæ•°æ®é›†\npython example/dataset/alpaca_data_preprocess.py \\\n--mindrecord_schema internlm_alpaca \\\n--input_glob {path}/alpaca_data.json \\\n--output_file {path}/alpaca_processed/alpaca.mindrecord \\\n--seq_length 2048\n\nè®­ç»ƒ\ncd example\nbash msrun.sh \"finetune.py --train_dataset /{path}/alpaca.mindrecord\"\n\næ¨ç†\nimport mindspore as ms\nfrom openmind import pipeline\n\nms.set_context(mode=0, device_id=0)\n\npipeline_task = pipeline(task=\"text_generation\",\n                         model='MindSpore-Lab/internlm_7b_chat',\n                         framework='ms',\n                         model_kwargs={\"use_past\":True},\n                         trust_remote_code=True)\n\ntext = \"ä½ å¥½\"\ntext = \"<s><s><|User|>:\" + text + \"<eoh>\\n<|Bot|>:\"\n\npipeline_result = pipeline_task(text, do_sample=False)\nprint(pipeline_result)\n\nå¼€æºè®¸å¯\n\nä»£ç é‡‡ç”¨ Apache-2.0 åè®®æˆæƒï¼Œæ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¹¶å…è®¸å…è´¹å•†ç”¨ã€‚å¦‚éœ€ç”³è¯·å•†ä¸šæˆæƒï¼Œè¯·å¡«å†™è‹±æ–‡ç”³è¯·è¡¨/ä¸­æ–‡ç”³è¯·è¡¨ã€‚å…¶ä»–é—®é¢˜æˆ–åˆä½œéœ€æ±‚ï¼Œè¯·è”ç³» internlm@pjlab.org.cnã€‚\n\næ›´æ–°è¯´æ˜\n\nä¼˜åŒ–äº†å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ä»£ç ã€‚\n\næ¨¡å‹æ¦‚è§ˆ\n\nInternLMï¼ˆä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹ï¼‰åŒ…å«å®ç”¨åœºæ™¯å¯¼å‘çš„70äº¿å‚æ•°åŸºç¡€æ¨¡å‹ä¸å¯¹è¯æ¨¡å‹ï¼ˆInternLM-7Bï¼‰ï¼Œå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒä¼˜åŠ¿ï¼š\n\nåŸºäºä¸‡äº¿çº§é«˜è´¨é‡è¯­æ–™è®­ç»ƒï¼Œæ„å»ºå¼ºå¤§çš„çŸ¥è¯†ä½“ç³»ï¼›\næ”¯æŒ8kè¶…é•¿ä¸Šä¸‹æ–‡çª—å£ï¼Œæå‡é•¿æ–‡æœ¬å¤„ç†ä¸å¤æ‚æ¨ç†èƒ½åŠ›ï¼›\nå…·å¤‡é€šç”¨å·¥å…·è°ƒç”¨åŠŸèƒ½ï¼Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰å·¥ä½œæµæ­å»ºï¼›\nInternLM-7B\næ€§èƒ½è¡¨ç°\n\né€šè¿‡å¼€æºè¯„ä¼°å·¥å…· OpenCompass å¯¹æ¨¡å‹è¿›è¡Œäº”å¤§ç»´åº¦å…¨é¢æµ‹è¯„ï¼Œéƒ¨åˆ†å…³é”®æŒ‡æ ‡å¦‚ä¸‹ï¼ˆå®Œæ•´ç»“æœè¯·æŸ¥é˜…OpenCompassæ¦œå•ï¼‰ï¼š\n\nè¯„æµ‹é›†\\æ¨¡å‹\tInternLM-Chat-7B\tInternLM-7B\tLLaMA-7B\tBaichuan-7B\tChatGLM2-6B\tAlpaca-7B\tVicuna-7B\nC-Eval(éªŒè¯é›†)\t53.2\t53.4\t24.2\t42.7\t50.9\t28.9\t31.2\nMMLU\t50.8\t51.0\t35.2*\t41.5\t46.0\t39.7\t47.3\nAGIEval\t42.5\t37.6\t20.8\t24.6\t39.0\t24.1\t26.4\nCommonSenseQA\t75.2\t59.5\t65.0\t58.8\t60.0\t68.7\t66.7\nBUSTM\t74.3\t50.6\t48.5\t51.3\t55.0\t48.8\t62.5\nCLUEWSC\t78.6\t59.1\t50.3\t52.8\t59.8\t50.3\t52.2\nMATH\t6.4\t7.1\t2.8\t3.0\t6.6\t2.2\t2.8\nGSM8K\t34.5\t31.2\t10.1\t9.7\t29.2\t6.0\t15.3\nHumanEval\t14.0\t10.4\t14.0\t9.2\t9.2\t9.2\t11.0\nRACE(é«˜é˜¶)\t76.3\t57.4\t46.9*\t28.1\t66.3\t40.7\t54.0\n\næ³¨ï¼šå¸¦*æ•°æ®å¼•è‡ªåŸè®ºæ–‡ï¼Œæµ‹è¯•é…ç½®è¯¦è§OpenCompassã€‚è¯„æµ‹ç»“æœå¯èƒ½éšå·¥å…·ç‰ˆæœ¬æ›´æ–°è€Œå˜åŒ–ï¼Œè¯·ä»¥æœ€æ–°ç‰ˆæœ¬ä¸ºå‡†ã€‚\n\nä½¿ç”¨é¡»çŸ¥ï¼š è™½ç„¶æˆ‘ä»¬åœ¨è®­ç»ƒä¸­ä¸¥æ ¼éµå¾ªä¼¦ç†è§„èŒƒï¼Œä½†ç”±äºæ¨¡å‹è§„æ¨¡ä¸æ¦‚ç‡ç”Ÿæˆæœºåˆ¶çš„é™åˆ¶ï¼Œå…¶è¾“å‡ºä»å¯èƒ½åŒ…å«åè§ã€æ­§è§†ç­‰æœ‰å®³å†…å®¹ã€‚è¯·å‹¿ä¼ æ’­æ­¤ç±»ä¿¡æ¯ï¼Œå› ä¼ æ’­ä¸è‰¯å†…å®¹å¯¼è‡´çš„ä¸€åˆ‡åæœä¸æœ¬é¡¹ç›®æ— å…³ã€‚\n\né€šè¿‡openMindä½¿ç”¨\nå¾®è°ƒæŒ‡å—\næ•°æ®é›†åˆ›å»º\npython example/dataset/alpaca_data_preprocess.py \\\n--mindrecord_schema internlm_alpaca \\\n--input_glob {path}/alpaca_data.json \\\n--output_file {path}/alpaca_processed/alpaca.mindrecord \\\n--seq_length 2048\n\nTraining\ncd example\nbash msrun.sh \"finetune.py --train_dataset /{path}/alpaca.mindrecord\"\n\nReasoning\ncd example\npython inference.py\n\nOpen Source License\n\nThe code in this repository is open-sourced under the Apache-2.0 license. The model weights are fully accessible for academic research, and free commercial use licenses can also be applied for (Application Form). For other inquiries or collaborations, please contact internlm@pjlab.org.cn.",
    "tags": "[\"Text Generation\", \"Transformers\", \"MindSpore\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/openMind/mobilevit_ms",
    "project_name": "mobilevit_ms",
    "readme": "Original Text\nMobileViT\n\nMobileViTï¼šè½»é‡çº§ã€é€šç”¨å‹ã€é€‚ç”¨äºç§»åŠ¨è®¾å¤‡çš„è§†è§‰å˜æ¢å™¨\n\nç®€ä»‹\n\nMobileViT æ˜¯ä¸€ç§ä¸ºç§»åŠ¨è®¾å¤‡è®¾è®¡çš„è½»é‡çº§ä¸”é€šç”¨å‹è§†è§‰å˜æ¢å™¨ã€‚MobileViT æä¾›äº†é€šè¿‡å˜æ¢å™¨è¿›è¡Œå…¨å±€ä¿¡æ¯å¤„ç†çš„ä¸åŒè§†è§’ï¼Œå³å˜æ¢å™¨ä½œä¸ºå·ç§¯ã€‚MobileViT åœ¨ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºåŸºäº CNN å’Œ ViT çš„ç½‘ç»œã€‚åœ¨ ImageNet-1k æ•°æ®é›†ä¸Šï¼ŒMobileViT åœ¨çº¦ 600 ä¸‡ä¸ªå‚æ•°ä¸‹è¾¾åˆ°äº† 78.4% çš„ top-1 å‡†ç¡®ç‡ï¼Œè¿™æ¯”å…·æœ‰ç›¸ä¼¼å‚æ•°é‡çš„ MobileNetv3ï¼ˆåŸºäº CNNï¼‰å’Œ DeITï¼ˆåŸºäº ViTï¼‰åˆ†åˆ«é«˜å‡º 3.2% å’Œ 6.2%ã€‚åœ¨ MS-COCO ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒMobileViT ç›¸å¯¹äºå…·æœ‰ç›¸ä¼¼å‚æ•°é‡çš„ MobileNetv3 çš„å‡†ç¡®ç‡é«˜å‡º 5.7%ã€‚\n\nå›¾ 1. MobileViT æ¶æ„ [1]\n\nç»“æœ\n\næˆ‘ä»¬åœ¨ ImageNet-1K æ•°æ®é›†ä¸Šå¤ç°çš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚\n\næ¨¡å‹\tè®­ç»ƒç¯å¢ƒ\tTop-1 (%)\tTop-5 (%)\tå‚æ•°é‡ (M)\té…ç½®æ–‡ä»¶\tæƒé‡ä¸‹è½½\nmobilevit_xx_small\tD910x8-G\t68.91\t88.91\t1.27\tyaml\tweights\nmobilevit_x_small\tD910x8-G\t74.99\t92.32\t2.32\tyaml\tweights\nmobilevit_small\tD910x8-G\t78.47\t94.18\t5.59\tyaml\tweights\nè¯´æ˜\nè®­ç»ƒç¯å¢ƒï¼šè®­ç»ƒç¯å¢ƒè¡¨ç¤ºä¸º {è®¾å¤‡}x{å—æ•°}-{MindSpore æ¨¡å¼}ï¼Œå…¶ä¸­ MindSpore æ¨¡å¼å¯ä»¥æ˜¯ G - å›¾æ¨¡å¼æˆ– F - pynative æ¨¡å¼ï¼Œå¹¶å¸¦æœ‰ ms å‡½æ•°ã€‚ä¾‹å¦‚ï¼ŒD910x8-G è¡¨ç¤ºåœ¨ 8 ç‰‡ Ascend 910 NPU ä¸Šä½¿ç”¨å›¾æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚\nTop-1 å’Œ Top-5ï¼šImageNet-1K éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ã€‚\nå¿«é€Ÿå…¥é—¨\nå‡†å¤‡\nå®‰è£…\n\nè¯·å‚è€ƒ MindCV çš„å®‰è£…è¯´æ˜ã€‚\n\næ•°æ®é›†å‡†å¤‡\n\nè¯·ä¸‹è½½ ImageNet-1K æ•°æ®é›†ä»¥ä¾›æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ä½¿ç”¨ã€‚\n\nè®­ç»ƒ\nåˆ†å¸ƒå¼è®­ç»ƒ\n\nä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒé…æ–¹è½»æ¾å¤ç°æŠ¥å‘Šç»“æœã€‚å¯¹äºåœ¨å¤šä¸ª Ascend 910 è®¾å¤‡ä¸Šè¿›è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·è¿è¡Œ\n\n# distributed training on multiple GPU/Ascend devices\nmpirun -n 8 python train.py --config configs/mobilevit/mobilevit_xx_small_ascend.yaml --data_dir /path/to/imagenet\n\n\nå¦‚æœè„šæœ¬ç”± root ç”¨æˆ·æ‰§è¡Œï¼Œå¿…é¡»åœ¨ mpirun å‘½ä»¤ä¸­åŠ å…¥ --allow-run-as-root å‚æ•°ã€‚\n\nåŒæ ·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸Šè¿° mpirun å‘½ä»¤åœ¨å¤šä¸ª GPU è®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚\n\næœ‰å…³æ‰€æœ‰è¶…å‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ config.pyã€‚\n\næ³¨æ„ï¼š ç”±äºå…¨å±€æ‰¹é‡å¤§å°ï¼ˆbatch_size x num_devicesï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œå»ºè®®åœ¨å¤ç°æ—¶ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ï¼Œæˆ–è€…åœ¨è°ƒæ•´åˆ°æ–°çš„å…¨å±€æ‰¹é‡å¤§å°æ—¶ï¼Œçº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ã€‚\n\nç‹¬ç«‹è®­ç»ƒ\n\nå¦‚æœæ‚¨å¸Œæœ›åœ¨æ— éœ€åˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œè¯·è¿è¡Œï¼š\n\n# standalone training on a CPU/GPU/Ascend device\npython train.py --config configs/mobilevit/mobilevit_xx_small_ascend.yaml --data_dir /path/to/dataset --distribute False\n\néªŒè¯æ¨¡å‹ç²¾åº¦\n\nä¸ºäº†éªŒè¯è®­ç»ƒåæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ validate.py å¹¶é€šè¿‡ --ckpt_path å‚æ•°è§£ææ£€æŸ¥ç‚¹è·¯å¾„ã€‚\n\npython validate.py -c configs/mobilevit/mobilevit_xx_small_ascend.yaml --data_dir /path/to/imagenet --ckpt_path /path/to/ckpt\n\néƒ¨ç½²\n\nè¯·å‚è€ƒMindCVçš„éƒ¨ç½²æ•™ç¨‹ã€‚",
    "tags": "[\"Image Classification\", \"Apache License 2.0\"]"
  },
  {
    "url": "https://gitcode.com/MooYeh/resnet50_ms",
    "project_name": "resnet50_ms",
    "readme": "Original Text\nResNet-50\n\nåœ¨224x224åˆ†è¾¨ç‡ä¸‹ï¼ŒåŸºäºImageNet-1kæ•°æ®é›†é¢„è®­ç»ƒçš„ResNetæ¨¡å‹ã€‚è¯¥æ¨¡å‹ç”±ä½•å‡¯æ˜ç­‰äººåœ¨è®ºæ–‡ã€ŠDeep Residual Learning for Image Recognitionã€‹ä¸­æå‡ºã€‚æ­¤å¤„ä¸ºMindSporeæ¡†æ¶çš„å®ç°ç‰ˆæœ¬ã€‚\n\nä½¿ç”¨æ–¹æ³•\n\nè¯·å‚è€ƒmindcvè·å–æ›´å¤šä¿¡æ¯ã€‚",
    "tags": "[\"English\", \"Apache License 2.0\", \"imagenet-1k\", \"image-classification\"]"
  },
  {
    "url": "https://gitcode.com/FlashAI/vision",
    "project_name": "vision",
    "readme": "flashaiå¤šæ¨¡æ€ç‰ˆæ•´åˆåŒ…ï¼Œè‡ªå¸¦æœ¬åœ°å¤§æ¨¡å‹ï¼Œå……åˆ†åˆ©ç”¨æœ¬åœ°AI,æä¾›æ–‡æ¡£ï¼ŒéŸ³é¢‘ï¼Œè§†é¢‘ï¼Œå›¾ç‰‡æ•°æ®çš„æœ¬åœ°åŒ–ç¿»è¯‘ï¼Œå®¡æ ¸ï¼Œæ ¡å¯¹ï¼Œæ€»ç»“åŠŸèƒ½,å…·æœ‰ç§å¯†ï¼Œç¨³å®šï¼Œå»‰ä»·çš„ç‰¹ç‚¹ã€‚\n\nFlashAIæ˜¯ä¸€æ¬¾æ­è½½æœ¬åœ°çŸ¥è¯†åº“çš„ç§æœ‰åŒ–å¤§æ¨¡å‹å·¥å…·é›†çš„åº”ç”¨ï¼Œæ— éœ€è”ç½‘ï¼Œæ— éœ€é…ç½®ï¼Œå¼€ç®±å³ç”¨ï¼Œå³å¯ä½“éªŒå¤§è¯­è¨€æ¨¡å‹æä¾›çš„æœ¬åœ°çŸ¥è¯†åº“ä»¥åŠå„ç§é«˜æ•ˆï¼ŒåŠŸèƒ½é½å…¨çš„æ•ˆç‡å·¥å…·åº”ç”¨ã€‚\n\nå®˜ç½‘:flashai.com.cn\n\nç‰¹ç‚¹ *æ— éœ€ä»»ä½•å®‰è£…é…ç½® *å®Œå…¨ç¦»çº¿ä½¿ç”¨ï¼Œç»å¯¹ä¿è¯æ•°æ®éšç§ *æ°¸ä¹…å…è´¹ *win10æˆ–mac os 12ä»¥ä¸Š *CPU+å†…å­˜å°±èƒ½è¿è¡Œï¼Œæœ‰GPUæ•ˆæœæ›´ä½³ *ä½é…æœºå™¨ä¹Ÿå¯ä»¥ä½¿ç”¨äº‘ç«¯å¤§æ¨¡å‹ç‰ˆæœ¬ *è‡ªç ”æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿï¼Œå¯è‡ªä¸»å¾®è°ƒæ¨¡å‹ *æ–‡ä»¶éƒ½æœ‰è¯ä¹¦ç­¾åï¼Œå®‰å…¨å¯é \n\nä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š\n\nä¼ä¸šä½¿ç”¨FlashAIè¿›è¡Œå†…éƒ¨æ–‡æ¡£çš„è‡ªåŠ¨ç¿»è¯‘å’Œç”Ÿæˆï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚\n\nå†…å®¹åˆ›ä½œè€…åˆ©ç”¨FlashAIå¿«é€Ÿç”Ÿæˆæ–‡ç« è‰ç¨¿ï¼ŒåŠ é€Ÿåˆ›ä½œæµç¨‹ã€‚\n\næ•™è‚²æœºæ„åˆ©ç”¨FlashAIè¾…åŠ©æ•™å­¦ææ–™çš„ç¼–å†™å’Œç¿»è¯‘å·¥ä½œã€‚\n\näº§å“ç‰¹è‰²ï¼š\n\näº‘ç«¯æ¨¡å‹æ”¯æŒï¼Œæ— éœ€æœ¬åœ°å¤§æ¨¡å‹å³å¯ä½¿ç”¨\n\nå¤šç‰ˆæœ¬æ”¯æŒï¼ŒåŒ…æ‹¬ä¸åŒå¤§å°å’ŒåŠŸèƒ½çš„æ¨¡å‹\n\né€‚ç”¨äºä¸åŒç¡¬ä»¶é…ç½®ï¼Œä»ä½é…åˆ°é«˜é…å‡æœ‰è¦†ç›–\n\nå†…å­˜å’ŒCPUè¦æ±‚æ˜ç¡®ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®è‡ªèº«æ¡ä»¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n\næ”¯æŒå¤šç§æ“ä½œç³»ç»Ÿï¼ŒåŒ…æ‹¬Windowså’ŒMac OS\n\næä¾›ä¸åŒå¤§å°çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚\n\nä½¿ç”¨æ•™ç¨‹ï¼š\n\nè®¿é—®FlashAIå®˜ç½‘å¹¶ä¸‹è½½é€‚åˆè‡ªå·±æ“ä½œç³»ç»Ÿçš„ç‰ˆæœ¬ã€‚\n\næ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç‰ˆæœ¬è¿›è¡Œå®‰è£…ã€‚\n\nå®‰è£…å®Œæˆåï¼Œæ ¹æ®ä½¿ç”¨è¯´æ˜è¿›è¡Œæ¨¡å‹çš„åˆå§‹åŒ–å’Œé…ç½®ã€‚\n\nä½¿ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚\n\næ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä¼˜åŒ–è¾“å‡ºç»“æœã€‚\n\nå®šæœŸæ›´æ–°æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½å’ŒåŠŸèƒ½ã€‚\n\nGemma3å¤§æ¨¡å‹æœ¬åœ°ä¸€é”®éƒ¨ç½²æ•´åˆåŒ…ï¼Œæä¾›1Bï¼Œ4Bï¼Œ12Bï¼Œ27Bæ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ï¼Œæ— éœ€ä»»ä½•é…ç½®ï¼Œé›†æˆå›¾å½¢ç•Œé¢ï¼Œè‡ªå¸¦æœ¬åœ°çŸ¥è¯†åº“ï¼Œç¦»çº¿è¿è¡Œï¼Œæ°¸ä¹…å…è´¹FlashAIä¸€é”®éƒ¨ç½²å¤§æ¨¡å‹\n\næµ‹æµ‹ä½ çš„ç”µè„‘å¯ä»¥è¿è¡Œä»€ä¹ˆå¤§æ¨¡å‹ mark.flashai.com.cn",
    "tags": "[\"Transformers\"]"
  },
  {
    "url": "https://gitcode.com/FlashAI/qwen",
    "project_name": "é€šä¹‰åƒé—®",
    "readme": "FlashAIæ˜¯ä¸€æ¬¾æ­è½½æœ¬åœ°çŸ¥è¯†åº“çš„ç§æœ‰åŒ–å¤§æ¨¡å‹å·¥å…·é›†çš„åº”ç”¨ï¼Œæ— éœ€è”ç½‘ï¼Œæ— éœ€é…ç½®ï¼Œå¼€ç®±å³ç”¨ï¼Œå³å¯ä½“éªŒå¤§è¯­è¨€æ¨¡å‹æä¾›çš„æœ¬åœ°çŸ¥è¯†åº“ä»¥åŠå„ç§é«˜æ•ˆï¼ŒåŠŸèƒ½é½å…¨çš„æ•ˆç‡å·¥å…·åº”ç”¨ã€‚\n\nå®˜ç½‘:flashai.com.cn\n\nç‰¹ç‚¹ *æ— éœ€ä»»ä½•å®‰è£…é…ç½® *å®Œå…¨ç¦»çº¿ä½¿ç”¨ï¼Œç»å¯¹ä¿è¯æ•°æ®éšç§ *æ°¸ä¹…å…è´¹ *win10æˆ–mac os 12ä»¥ä¸Š *CPU+å†…å­˜å°±èƒ½è¿è¡Œï¼Œæœ‰GPUæ•ˆæœæ›´ä½³ *ä½é…æœºå™¨ä¹Ÿå¯ä»¥ä½¿ç”¨äº‘ç«¯å¤§æ¨¡å‹ç‰ˆæœ¬ *è‡ªç ”æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿï¼Œå¯è‡ªä¸»å¾®è°ƒæ¨¡å‹ *æ–‡ä»¶éƒ½æœ‰è¯ä¹¦ç­¾åï¼Œå®‰å…¨å¯é \n\nä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š\n\nä¼ä¸šä½¿ç”¨FlashAIè¿›è¡Œå†…éƒ¨æ–‡æ¡£çš„è‡ªåŠ¨ç¿»è¯‘å’Œç”Ÿæˆï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚\n\nå†…å®¹åˆ›ä½œè€…åˆ©ç”¨FlashAIå¿«é€Ÿç”Ÿæˆæ–‡ç« è‰ç¨¿ï¼ŒåŠ é€Ÿåˆ›ä½œæµç¨‹ã€‚\n\næ•™è‚²æœºæ„åˆ©ç”¨FlashAIè¾…åŠ©æ•™å­¦ææ–™çš„ç¼–å†™å’Œç¿»è¯‘å·¥ä½œã€‚\n\näº§å“ç‰¹è‰²ï¼š\n\näº‘ç«¯æ¨¡å‹æ”¯æŒï¼Œæ— éœ€æœ¬åœ°å¤§æ¨¡å‹å³å¯ä½¿ç”¨\n\nå¤šç‰ˆæœ¬æ”¯æŒï¼ŒåŒ…æ‹¬ä¸åŒå¤§å°å’ŒåŠŸèƒ½çš„æ¨¡å‹\n\né€‚ç”¨äºä¸åŒç¡¬ä»¶é…ç½®ï¼Œä»ä½é…åˆ°é«˜é…å‡æœ‰è¦†ç›–\n\nå†…å­˜å’ŒCPUè¦æ±‚æ˜ç¡®ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®è‡ªèº«æ¡ä»¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n\næ”¯æŒå¤šç§æ“ä½œç³»ç»Ÿï¼ŒåŒ…æ‹¬Windowså’ŒMac OS\n\næä¾›ä¸åŒå¤§å°çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚\n\nä½¿ç”¨æ•™ç¨‹ï¼š\n\nè®¿é—®FlashAIå®˜ç½‘å¹¶ä¸‹è½½é€‚åˆè‡ªå·±æ“ä½œç³»ç»Ÿçš„ç‰ˆæœ¬ã€‚\n\næ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç‰ˆæœ¬è¿›è¡Œå®‰è£…ã€‚\n\nå®‰è£…å®Œæˆåï¼Œæ ¹æ®ä½¿ç”¨è¯´æ˜è¿›è¡Œæ¨¡å‹çš„åˆå§‹åŒ–å’Œé…ç½®ã€‚\n\nä½¿ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚\n\næ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä¼˜åŒ–è¾“å‡ºç»“æœã€‚\n\nå®šæœŸæ›´æ–°æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½å’ŒåŠŸèƒ½ã€‚\n\nDeepSeek R1å¤§æ¨¡å‹æœ¬åœ°ä¸€é”®éƒ¨ç½²æ•´åˆåŒ…ï¼Œæä¾›1.5Bï¼Œ7Bï¼Œ14Bï¼Œ32Bï¼Œ70Bæ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ï¼Œæ— éœ€ä»»ä½•é…ç½®ï¼Œé›†æˆå›¾å½¢ç•Œé¢ï¼Œè‡ªå¸¦æœ¬åœ°çŸ¥è¯†åº“ï¼Œç¦»çº¿è¿è¡Œï¼Œæ°¸ä¹…å…è´¹FlashAIä¸€é”®éƒ¨ç½²å¤§æ¨¡å‹\n\næµ‹æµ‹ä½ çš„ç”µè„‘å¯ä»¥è¿è¡Œä»€ä¹ˆå¤§æ¨¡å‹ mark.flashai.com.cn",
    "tags": "[\"Transformers\"]"
  },
  {
    "url": "https://gitcode.com/FlashAI/qwen3",
    "project_name": "qwen3",
    "readme": "æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ Qwen3 çš„æƒé‡ï¼Œè¯¥æ¨¡å‹åŒ…å« 2 ä¸ª MoE æ¨¡å‹å’Œ 6 ä¸ªå¯†é›†æ¨¡å‹ï¼Œè§„æ¨¡ä» 0.6B åˆ° 235Bã€‚æˆ‘ä»¬çš„æ——èˆ°æ¨¡å‹ Qwen3-235B-A22B åœ¨ç¼–ç ã€æ•°å­¦ã€é€šç”¨èƒ½åŠ›ç­‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¸ DeepSeek-R1ã€o1ã€o3-miniã€Grok-3 å’Œ Gemini-2.5-Pro ç­‰å…¶ä»–é¡¶çº§æ¨¡å‹ç›¸æ¯”æå…·ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œå°å‹ MoE æ¨¡å‹ Qwen3-30B-A3B çš„æ¿€æ´»å‚æ•°é‡æ˜¯ QwQ-32B çš„ 10 å€ï¼Œå³ä½¿æ˜¯åƒ Qwen3-4B è¿™æ ·çš„å¾®å‹æ¨¡å‹ä¹Ÿèƒ½ä¸ Qwen2.5-72B-Instruct çš„æ€§èƒ½ç›¸åª²ç¾ã€‚\n\nFlashAIæ˜¯ä¸€æ¬¾æ­è½½æœ¬åœ°çŸ¥è¯†åº“çš„ç§æœ‰åŒ–å¤§æ¨¡å‹å·¥å…·é›†çš„åº”ç”¨ï¼Œæ— éœ€è”ç½‘ï¼Œæ— éœ€é…ç½®ï¼Œå¼€ç®±å³ç”¨ï¼Œå³å¯ä½“éªŒå¤§è¯­è¨€æ¨¡å‹æä¾›çš„æœ¬åœ°çŸ¥è¯†åº“ä»¥åŠå„ç§é«˜æ•ˆï¼ŒåŠŸèƒ½é½å…¨çš„æ•ˆç‡å·¥å…·åº”ç”¨ã€‚\n\nå®˜ç½‘:flashai.com.cn\n\nç‰¹ç‚¹ *æ— éœ€ä»»ä½•å®‰è£…é…ç½® *å®Œå…¨ç¦»çº¿ä½¿ç”¨ï¼Œç»å¯¹ä¿è¯æ•°æ®éšç§ *æ°¸ä¹…å…è´¹ *win10æˆ–mac os 12ä»¥ä¸Š *CPU+å†…å­˜å°±èƒ½è¿è¡Œï¼Œæœ‰GPUæ•ˆæœæ›´ä½³ *ä½é…æœºå™¨ä¹Ÿå¯ä»¥ä½¿ç”¨äº‘ç«¯å¤§æ¨¡å‹ç‰ˆæœ¬ *è‡ªç ”æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿï¼Œå¯è‡ªä¸»å¾®è°ƒæ¨¡å‹ *æ–‡ä»¶éƒ½æœ‰è¯ä¹¦ç­¾åï¼Œå®‰å…¨å¯é \n\nä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š\n\nä¼ä¸šä½¿ç”¨FlashAIè¿›è¡Œå†…éƒ¨æ–‡æ¡£çš„è‡ªåŠ¨ç¿»è¯‘å’Œç”Ÿæˆï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚\n\nå†…å®¹åˆ›ä½œè€…åˆ©ç”¨FlashAIå¿«é€Ÿç”Ÿæˆæ–‡ç« è‰ç¨¿ï¼ŒåŠ é€Ÿåˆ›ä½œæµç¨‹ã€‚\n\næ•™è‚²æœºæ„åˆ©ç”¨FlashAIè¾…åŠ©æ•™å­¦ææ–™çš„ç¼–å†™å’Œç¿»è¯‘å·¥ä½œã€‚\n\näº§å“ç‰¹è‰²ï¼š\n\näº‘ç«¯æ¨¡å‹æ”¯æŒï¼Œæ— éœ€æœ¬åœ°å¤§æ¨¡å‹å³å¯ä½¿ç”¨\n\nå¤šç‰ˆæœ¬æ”¯æŒï¼ŒåŒ…æ‹¬ä¸åŒå¤§å°å’ŒåŠŸèƒ½çš„æ¨¡å‹\n\né€‚ç”¨äºä¸åŒç¡¬ä»¶é…ç½®ï¼Œä»ä½é…åˆ°é«˜é…å‡æœ‰è¦†ç›–\n\nå†…å­˜å’ŒCPUè¦æ±‚æ˜ç¡®ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®è‡ªèº«æ¡ä»¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n\næ”¯æŒå¤šç§æ“ä½œç³»ç»Ÿï¼ŒåŒ…æ‹¬Windowså’ŒMac OS\n\næä¾›ä¸åŒå¤§å°çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚\n\nä½¿ç”¨æ•™ç¨‹ï¼š\n\nè®¿é—®FlashAIå®˜ç½‘å¹¶ä¸‹è½½é€‚åˆè‡ªå·±æ“ä½œç³»ç»Ÿçš„ç‰ˆæœ¬ã€‚\n\næ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç‰ˆæœ¬è¿›è¡Œå®‰è£…ã€‚\n\nå®‰è£…å®Œæˆåï¼Œæ ¹æ®ä½¿ç”¨è¯´æ˜è¿›è¡Œæ¨¡å‹çš„åˆå§‹åŒ–å’Œé…ç½®ã€‚\n\nä½¿ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚\n\næ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä¼˜åŒ–è¾“å‡ºç»“æœã€‚\n\nå®šæœŸæ›´æ–°æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½å’ŒåŠŸèƒ½ã€‚\n\nDeepSeek R1å¤§æ¨¡å‹æœ¬åœ°ä¸€é”®éƒ¨ç½²æ•´åˆåŒ…ï¼Œæä¾›1.5Bï¼Œ7Bï¼Œ14Bï¼Œ32Bï¼Œ70Bæ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ï¼Œæ— éœ€ä»»ä½•é…ç½®ï¼Œé›†æˆå›¾å½¢ç•Œé¢ï¼Œè‡ªå¸¦æœ¬åœ°çŸ¥è¯†åº“ï¼Œç¦»çº¿è¿è¡Œï¼Œæ°¸ä¹…å…è´¹FlashAIä¸€é”®éƒ¨ç½²å¤§æ¨¡å‹\n\næµ‹æµ‹ä½ çš„ç”µè„‘å¯ä»¥è¿è¡Œä»€ä¹ˆå¤§æ¨¡å‹ mark.flashai.com.cn",
    "tags": "[\"Transformers\"]"
  },
  {
    "url": "https://gitcode.com/FlashAI/gemma3",
    "project_name": "gemma3",
    "readme": "FlashAIæ˜¯ä¸€æ¬¾æ­è½½æœ¬åœ°çŸ¥è¯†åº“çš„ç§æœ‰åŒ–å¤§æ¨¡å‹å·¥å…·é›†çš„åº”ç”¨ï¼Œæ— éœ€è”ç½‘ï¼Œæ— éœ€é…ç½®ï¼Œå¼€ç®±å³ç”¨ï¼Œå³å¯ä½“éªŒå¤§è¯­è¨€æ¨¡å‹æä¾›çš„æœ¬åœ°çŸ¥è¯†åº“ä»¥åŠå„ç§é«˜æ•ˆï¼ŒåŠŸèƒ½é½å…¨çš„æ•ˆç‡å·¥å…·åº”ç”¨ã€‚\n\nå®˜ç½‘:flashai.com.cn\n\nç‰¹ç‚¹ *æ— éœ€ä»»ä½•å®‰è£…é…ç½® *å®Œå…¨ç¦»çº¿ä½¿ç”¨ï¼Œç»å¯¹ä¿è¯æ•°æ®éšç§ *æ°¸ä¹…å…è´¹ *win10æˆ–mac os 12ä»¥ä¸Š *CPU+å†…å­˜å°±èƒ½è¿è¡Œï¼Œæœ‰GPUæ•ˆæœæ›´ä½³ *ä½é…æœºå™¨ä¹Ÿå¯ä»¥ä½¿ç”¨äº‘ç«¯å¤§æ¨¡å‹ç‰ˆæœ¬ *è‡ªç ”æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿï¼Œå¯è‡ªä¸»å¾®è°ƒæ¨¡å‹ *æ–‡ä»¶éƒ½æœ‰è¯ä¹¦ç­¾åï¼Œå®‰å…¨å¯é \n\nä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š\n\nä¼ä¸šä½¿ç”¨FlashAIè¿›è¡Œå†…éƒ¨æ–‡æ¡£çš„è‡ªåŠ¨ç¿»è¯‘å’Œç”Ÿæˆï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚\n\nå†…å®¹åˆ›ä½œè€…åˆ©ç”¨FlashAIå¿«é€Ÿç”Ÿæˆæ–‡ç« è‰ç¨¿ï¼ŒåŠ é€Ÿåˆ›ä½œæµç¨‹ã€‚\n\næ•™è‚²æœºæ„åˆ©ç”¨FlashAIè¾…åŠ©æ•™å­¦ææ–™çš„ç¼–å†™å’Œç¿»è¯‘å·¥ä½œã€‚\n\näº§å“ç‰¹è‰²ï¼š\n\näº‘ç«¯æ¨¡å‹æ”¯æŒï¼Œæ— éœ€æœ¬åœ°å¤§æ¨¡å‹å³å¯ä½¿ç”¨\n\nå¤šç‰ˆæœ¬æ”¯æŒï¼ŒåŒ…æ‹¬ä¸åŒå¤§å°å’ŒåŠŸèƒ½çš„æ¨¡å‹\n\né€‚ç”¨äºä¸åŒç¡¬ä»¶é…ç½®ï¼Œä»ä½é…åˆ°é«˜é…å‡æœ‰è¦†ç›–\n\nå†…å­˜å’ŒCPUè¦æ±‚æ˜ç¡®ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®è‡ªèº«æ¡ä»¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n\næ”¯æŒå¤šç§æ“ä½œç³»ç»Ÿï¼ŒåŒ…æ‹¬Windowså’ŒMac OS\n\næä¾›ä¸åŒå¤§å°çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚\n\nä½¿ç”¨æ•™ç¨‹ï¼š\n\nè®¿é—®FlashAIå®˜ç½‘å¹¶ä¸‹è½½é€‚åˆè‡ªå·±æ“ä½œç³»ç»Ÿçš„ç‰ˆæœ¬ã€‚\n\næ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç‰ˆæœ¬è¿›è¡Œå®‰è£…ã€‚\n\nå®‰è£…å®Œæˆåï¼Œæ ¹æ®ä½¿ç”¨è¯´æ˜è¿›è¡Œæ¨¡å‹çš„åˆå§‹åŒ–å’Œé…ç½®ã€‚\n\nä½¿ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚\n\næ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä¼˜åŒ–è¾“å‡ºç»“æœã€‚\n\nå®šæœŸæ›´æ–°æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½å’ŒåŠŸèƒ½ã€‚\n\nGemma3å¤§æ¨¡å‹æœ¬åœ°ä¸€é”®éƒ¨ç½²æ•´åˆåŒ…ï¼Œæä¾›1Bï¼Œ4Bï¼Œ12Bï¼Œ27Bæ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ï¼Œæ— éœ€ä»»ä½•é…ç½®ï¼Œé›†æˆå›¾å½¢ç•Œé¢ï¼Œè‡ªå¸¦æœ¬åœ°çŸ¥è¯†åº“ï¼Œç¦»çº¿è¿è¡Œï¼Œæ°¸ä¹…å…è´¹FlashAIä¸€é”®éƒ¨ç½²å¤§æ¨¡å‹\n\næµ‹æµ‹ä½ çš„ç”µè„‘å¯ä»¥è¿è¡Œä»€ä¹ˆå¤§æ¨¡å‹ mark.flashai.com.cn",
    "tags": "[\"Transformers\"]"
  },
  {
    "url": "https://gitcode.com/FlashAI/convert-lite",
    "project_name": "convert-lite",
    "readme": "FlashAIæ˜¯ä¸€æ¬¾æ­è½½æœ¬åœ°çŸ¥è¯†åº“çš„ç§æœ‰åŒ–å¤§æ¨¡å‹å·¥å…·é›†çš„åº”ç”¨ï¼Œæ— éœ€è”ç½‘ï¼Œæ— éœ€é…ç½®ï¼Œå¼€ç®±å³ç”¨ï¼Œå³å¯ä½“éªŒå¤§è¯­è¨€æ¨¡å‹æä¾›çš„æœ¬åœ°çŸ¥è¯†åº“ä»¥åŠå„ç§é«˜æ•ˆï¼ŒåŠŸèƒ½é½å…¨çš„æ•ˆç‡å·¥å…·åº”ç”¨ã€‚\n\nç‰¹ç‚¹ *æ— éœ€ä»»ä½•å®‰è£…é…ç½® *å®Œå…¨ç¦»çº¿ä½¿ç”¨ï¼Œç»å¯¹ä¿è¯æ•°æ®éšç§ *æ°¸ä¹…å…è´¹ *win10æˆ–mac os 12ä»¥ä¸Š *CPU+å†…å­˜å°±èƒ½è¿è¡Œï¼Œæœ‰GPUæ•ˆæœæ›´ä½³ *ä½é…æœºå™¨ä¹Ÿå¯ä»¥ä½¿ç”¨äº‘ç«¯å¤§æ¨¡å‹ç‰ˆæœ¬ *è‡ªç ”æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿï¼Œå¯è‡ªä¸»å¾®è°ƒæ¨¡å‹ *æ–‡ä»¶éƒ½æœ‰è¯ä¹¦ç­¾åï¼Œå®‰å…¨å¯é \n\nä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š\n\nä¼ä¸šä½¿ç”¨FlashAIè¿›è¡Œå†…éƒ¨æ–‡æ¡£çš„è‡ªåŠ¨ç¿»è¯‘å’Œç”Ÿæˆï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚\n\nå†…å®¹åˆ›ä½œè€…åˆ©ç”¨FlashAIå¿«é€Ÿç”Ÿæˆæ–‡ç« è‰ç¨¿ï¼ŒåŠ é€Ÿåˆ›ä½œæµç¨‹ã€‚\n\næ•™è‚²æœºæ„åˆ©ç”¨FlashAIè¾…åŠ©æ•™å­¦ææ–™çš„ç¼–å†™å’Œç¿»è¯‘å·¥ä½œã€‚\n\näº§å“ç‰¹è‰²ï¼š\n\näº‘ç«¯æ¨¡å‹æ”¯æŒï¼Œæ— éœ€æœ¬åœ°å¤§æ¨¡å‹å³å¯ä½¿ç”¨\n\nå¤šç‰ˆæœ¬æ”¯æŒï¼ŒåŒ…æ‹¬ä¸åŒå¤§å°å’ŒåŠŸèƒ½çš„æ¨¡å‹\n\né€‚ç”¨äºä¸åŒç¡¬ä»¶é…ç½®ï¼Œä»ä½é…åˆ°é«˜é…å‡æœ‰è¦†ç›–\n\nå†…å­˜å’ŒCPUè¦æ±‚æ˜ç¡®ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®è‡ªèº«æ¡ä»¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n\næ”¯æŒå¤šç§æ“ä½œç³»ç»Ÿï¼ŒåŒ…æ‹¬Windowså’ŒMac OS\n\næä¾›ä¸åŒå¤§å°çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚\n\nä½¿ç”¨æ•™ç¨‹ï¼š\n\nè®¿é—®FlashAIå®˜ç½‘å¹¶ä¸‹è½½é€‚åˆè‡ªå·±æ“ä½œç³»ç»Ÿçš„ç‰ˆæœ¬ã€‚\n\næ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç‰ˆæœ¬è¿›è¡Œå®‰è£…ã€‚\n\nå®‰è£…å®Œæˆåï¼Œæ ¹æ®ä½¿ç”¨è¯´æ˜è¿›è¡Œæ¨¡å‹çš„åˆå§‹åŒ–å’Œé…ç½®ã€‚\n\nä½¿ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚\n\næ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä¼˜åŒ–è¾“å‡ºç»“æœã€‚\n\nå®šæœŸæ›´æ–°æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½å’ŒåŠŸèƒ½ã€‚\n\nFlashAI Convert Lite æ”¯æŒå°†ä¸»æµçš„ PDFã€Wordã€Excelã€PPTã€HTML æ–‡ä»¶ï¼Œç”šè‡³å›¾ç‰‡ï¼ˆå†…ç½®å¼ºå¤§ OCR æ¨¡å‹è¯†åˆ«æ–‡å­—ï¼‰è½»æ¾è½¬æ¢ä¸ºç®€æ´é«˜æ•ˆçš„ Markdown æ ¼å¼ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒå°† Markdown åå‘å¯¼å‡ºä¸º Word æ–‡æ¡£ã€‚å‘Šåˆ«ç½‘ç»œä¾èµ–ï¼Œæ— éœ€æ³¨å†Œç™»å½•ï¼Œæ‚¨çš„æ‰€æœ‰æ“ä½œå’Œæ•°æ®éƒ½å®‰å…¨åœ°ä¿ç•™åœ¨æœ¬åœ°ã€‚ç®€æ´ç›´è§‚çš„å›¾å½¢åŒ–ç•Œé¢è®©æ“ä½œæ˜“å¦‚åæŒã€‚æœ€é‡è¦çš„æ˜¯ï¼Œå®ƒæ‰¿è¯ºæ°¸ä¹…å…è´¹ï¼ç«‹å³ä½“éªŒ FlashAI Convert Liteï¼Œè®©æ–‡æ¡£è½¬æ¢å˜å¾—å‰æ‰€æœªæœ‰çš„ç®€å•ã€é«˜æ•ˆå’Œå®‰å…¨ã€‚",
    "tags": "[\"Transformers\"]"
  },
  {
    "url": "https://gitcode.com/FlashAI/flashai-convert",
    "project_name": "flashai-convert",
    "readme": "FlashAIæ˜¯ä¸€æ¬¾æ­è½½æœ¬åœ°çŸ¥è¯†åº“çš„ç§æœ‰åŒ–å¤§æ¨¡å‹å·¥å…·é›†çš„åº”ç”¨ï¼Œæ— éœ€è”ç½‘ï¼Œæ— éœ€é…ç½®ï¼Œå¼€ç®±å³ç”¨ï¼Œå³å¯ä½“éªŒå¤§è¯­è¨€æ¨¡å‹æä¾›çš„æœ¬åœ°çŸ¥è¯†åº“ä»¥åŠå„ç§é«˜æ•ˆï¼ŒåŠŸèƒ½é½å…¨çš„æ•ˆç‡å·¥å…·åº”ç”¨ã€‚\n\nç‰¹ç‚¹ *æ— éœ€ä»»ä½•å®‰è£…é…ç½® *å®Œå…¨ç¦»çº¿ä½¿ç”¨ï¼Œç»å¯¹ä¿è¯æ•°æ®éšç§ *æ°¸ä¹…å…è´¹ *win10æˆ–mac os 12ä»¥ä¸Š *CPU+å†…å­˜å°±èƒ½è¿è¡Œï¼Œæœ‰GPUæ•ˆæœæ›´ä½³ *ä½é…æœºå™¨ä¹Ÿå¯ä»¥ä½¿ç”¨äº‘ç«¯å¤§æ¨¡å‹ç‰ˆæœ¬ *è‡ªç ”æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿï¼Œå¯è‡ªä¸»å¾®è°ƒæ¨¡å‹ *æ–‡ä»¶éƒ½æœ‰è¯ä¹¦ç­¾åï¼Œå®‰å…¨å¯é \n\nä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š\n\nä¼ä¸šä½¿ç”¨FlashAIè¿›è¡Œå†…éƒ¨æ–‡æ¡£çš„è‡ªåŠ¨ç¿»è¯‘å’Œç”Ÿæˆï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚\n\nå†…å®¹åˆ›ä½œè€…åˆ©ç”¨FlashAIå¿«é€Ÿç”Ÿæˆæ–‡ç« è‰ç¨¿ï¼ŒåŠ é€Ÿåˆ›ä½œæµç¨‹ã€‚\n\næ•™è‚²æœºæ„åˆ©ç”¨FlashAIè¾…åŠ©æ•™å­¦ææ–™çš„ç¼–å†™å’Œç¿»è¯‘å·¥ä½œã€‚\n\näº§å“ç‰¹è‰²ï¼š\n\näº‘ç«¯æ¨¡å‹æ”¯æŒï¼Œæ— éœ€æœ¬åœ°å¤§æ¨¡å‹å³å¯ä½¿ç”¨\n\nå¤šç‰ˆæœ¬æ”¯æŒï¼ŒåŒ…æ‹¬ä¸åŒå¤§å°å’ŒåŠŸèƒ½çš„æ¨¡å‹\n\né€‚ç”¨äºä¸åŒç¡¬ä»¶é…ç½®ï¼Œä»ä½é…åˆ°é«˜é…å‡æœ‰è¦†ç›–\n\nå†…å­˜å’ŒCPUè¦æ±‚æ˜ç¡®ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®è‡ªèº«æ¡ä»¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n\næ”¯æŒå¤šç§æ“ä½œç³»ç»Ÿï¼ŒåŒ…æ‹¬Windowså’ŒMac OS\n\næä¾›ä¸åŒå¤§å°çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚\n\nä½¿ç”¨æ•™ç¨‹ï¼š\n\nè®¿é—®FlashAIå®˜ç½‘å¹¶ä¸‹è½½é€‚åˆè‡ªå·±æ“ä½œç³»ç»Ÿçš„ç‰ˆæœ¬ã€‚\n\næ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç‰ˆæœ¬è¿›è¡Œå®‰è£…ã€‚\n\nå®‰è£…å®Œæˆåï¼Œæ ¹æ®ä½¿ç”¨è¯´æ˜è¿›è¡Œæ¨¡å‹çš„åˆå§‹åŒ–å’Œé…ç½®ã€‚\n\nä½¿ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚\n\næ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä¼˜åŒ–è¾“å‡ºç»“æœã€‚\n\nå®šæœŸæ›´æ–°æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½å’ŒåŠŸèƒ½ã€‚\n\nFlashAI Convert Lite æ”¯æŒå°†ä¸»æµçš„ PDFã€Wordã€Excelã€PPTã€HTML æ–‡ä»¶ï¼Œç”šè‡³å›¾ç‰‡ï¼ˆå†…ç½®å¼ºå¤§ OCR æ¨¡å‹è¯†åˆ«æ–‡å­—ï¼‰è½»æ¾è½¬æ¢ä¸ºç®€æ´é«˜æ•ˆçš„ Markdown æ ¼å¼ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒå°† Markdown åå‘å¯¼å‡ºä¸º Word æ–‡æ¡£ã€‚å‘Šåˆ«ç½‘ç»œä¾èµ–ï¼Œæ— éœ€æ³¨å†Œç™»å½•ï¼Œæ‚¨çš„æ‰€æœ‰æ“ä½œå’Œæ•°æ®éƒ½å®‰å…¨åœ°ä¿ç•™åœ¨æœ¬åœ°ã€‚ç®€æ´ç›´è§‚çš„å›¾å½¢åŒ–ç•Œé¢è®©æ“ä½œæ˜“å¦‚åæŒã€‚æœ€é‡è¦çš„æ˜¯ï¼Œå®ƒæ‰¿è¯ºæ°¸ä¹…å…è´¹ï¼ç«‹å³ä½“éªŒ FlashAI Convert Liteï¼Œè®©æ–‡æ¡£è½¬æ¢å˜å¾—å‰æ‰€æœªæœ‰çš„ç®€å•ã€é«˜æ•ˆå’Œå®‰å…¨ã€‚",
    "tags": "[\"Transformers\"]"
  },
  {
    "url": "https://gitcode.com/FlashAI/deepseek",
    "project_name": "deepseek",
    "readme": "FlashAIæ˜¯ä¸€æ¬¾æ­è½½æœ¬åœ°çŸ¥è¯†åº“çš„ç§æœ‰åŒ–å¤§æ¨¡å‹å·¥å…·é›†çš„åº”ç”¨ï¼Œæ— éœ€è”ç½‘ï¼Œæ— éœ€é…ç½®ï¼Œå¼€ç®±å³ç”¨ï¼Œå³å¯ä½“éªŒå¤§è¯­è¨€æ¨¡å‹æä¾›çš„æœ¬åœ°çŸ¥è¯†åº“ä»¥åŠå„ç§é«˜æ•ˆï¼ŒåŠŸèƒ½é½å…¨çš„æ•ˆç‡å·¥å…·åº”ç”¨ã€‚\n\nç‰¹ç‚¹ *æ— éœ€ä»»ä½•å®‰è£…é…ç½® *å®Œå…¨ç¦»çº¿ä½¿ç”¨ï¼Œç»å¯¹ä¿è¯æ•°æ®éšç§ *æ°¸ä¹…å…è´¹ *win10æˆ–mac os 12ä»¥ä¸Š *CPU+å†…å­˜å°±èƒ½è¿è¡Œï¼Œæœ‰GPUæ•ˆæœæ›´ä½³ *ä½é…æœºå™¨ä¹Ÿå¯ä»¥ä½¿ç”¨äº‘ç«¯å¤§æ¨¡å‹ç‰ˆæœ¬ *è‡ªç ”æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿï¼Œå¯è‡ªä¸»å¾®è°ƒæ¨¡å‹ *æ–‡ä»¶éƒ½æœ‰è¯ä¹¦ç­¾åï¼Œå®‰å…¨å¯é \n\nä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š\n\nä¼ä¸šä½¿ç”¨FlashAIè¿›è¡Œå†…éƒ¨æ–‡æ¡£çš„è‡ªåŠ¨ç¿»è¯‘å’Œç”Ÿæˆï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚\n\nå†…å®¹åˆ›ä½œè€…åˆ©ç”¨FlashAIå¿«é€Ÿç”Ÿæˆæ–‡ç« è‰ç¨¿ï¼ŒåŠ é€Ÿåˆ›ä½œæµç¨‹ã€‚\n\næ•™è‚²æœºæ„åˆ©ç”¨FlashAIè¾…åŠ©æ•™å­¦ææ–™çš„ç¼–å†™å’Œç¿»è¯‘å·¥ä½œã€‚\n\näº§å“ç‰¹è‰²ï¼š\n\näº‘ç«¯æ¨¡å‹æ”¯æŒï¼Œæ— éœ€æœ¬åœ°å¤§æ¨¡å‹å³å¯ä½¿ç”¨\n\nå¤šç‰ˆæœ¬æ”¯æŒï¼ŒåŒ…æ‹¬ä¸åŒå¤§å°å’ŒåŠŸèƒ½çš„æ¨¡å‹\n\né€‚ç”¨äºä¸åŒç¡¬ä»¶é…ç½®ï¼Œä»ä½é…åˆ°é«˜é…å‡æœ‰è¦†ç›–\n\nå†…å­˜å’ŒCPUè¦æ±‚æ˜ç¡®ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®è‡ªèº«æ¡ä»¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n\næ”¯æŒå¤šç§æ“ä½œç³»ç»Ÿï¼ŒåŒ…æ‹¬Windowså’ŒMac OS\n\næä¾›ä¸åŒå¤§å°çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚\n\nä½¿ç”¨æ•™ç¨‹ï¼š\n\nè®¿é—®FlashAIå®˜ç½‘å¹¶ä¸‹è½½é€‚åˆè‡ªå·±æ“ä½œç³»ç»Ÿçš„ç‰ˆæœ¬ã€‚\n\næ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç‰ˆæœ¬è¿›è¡Œå®‰è£…ã€‚\n\nå®‰è£…å®Œæˆåï¼Œæ ¹æ®ä½¿ç”¨è¯´æ˜è¿›è¡Œæ¨¡å‹çš„åˆå§‹åŒ–å’Œé…ç½®ã€‚\n\nä½¿ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚\n\næ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä¼˜åŒ–è¾“å‡ºç»“æœã€‚\n\nå®šæœŸæ›´æ–°æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½å’ŒåŠŸèƒ½ã€‚\n\nDeepSeek R1å¤§æ¨¡å‹æœ¬åœ°ä¸€é”®éƒ¨ç½²æ•´åˆåŒ…ï¼Œæä¾›1.5Bï¼Œ7Bï¼Œ14Bï¼Œ32Bï¼Œ70Bæ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ï¼Œæ— éœ€ä»»ä½•é…ç½®ï¼Œé›†æˆå›¾å½¢ç•Œé¢ï¼Œè‡ªå¸¦æœ¬åœ°çŸ¥è¯†åº“ï¼Œç¦»çº¿è¿è¡Œï¼Œæ°¸ä¹…å…è´¹FlashAIä¸€é”®éƒ¨ç½²å¤§æ¨¡å‹\n\næµ‹æµ‹ä½ çš„ç”µè„‘å¯ä»¥è¿è¡Œä»€ä¹ˆå¤§æ¨¡å‹ mark.flashai.com.cn",
    "tags": "[\"Transformers\"]"
  },
  {
    "url": "https://gitcode.com/FlashAI/server",
    "project_name": "server",
    "readme": "FlashAI Serveræ˜¯ä¸€æ¬¾æ­è½½æœ¬åœ°çŸ¥è¯†åº“çš„ç§æœ‰åŒ–å¤§æ¨¡å‹å·¥å…·é›†çš„åº”ç”¨ï¼Œæ— éœ€è”ç½‘ï¼Œæ— éœ€é…ç½®ï¼Œå¼€ç®±å³ç”¨ï¼Œå³å¯ä½“éªŒå¤§è¯­è¨€æ¨¡å‹æä¾›çš„æœ¬åœ°çŸ¥è¯†åº“ä»¥åŠå„ç§é«˜æ•ˆï¼ŒåŠŸèƒ½é½å…¨çš„æ•ˆç‡å·¥å…·åº”ç”¨ã€‚\n\nå®˜ç½‘ï¼šflashai.com.cn\n\nå±€åŸŸç½‘æ‰€æœ‰æœºå™¨å¯ä»¥é€šè¿‡webè®¿é—®å¤§æ¨¡å‹æœåŠ¡\n\nç‰¹ç‚¹ *æ— éœ€ä»»ä½•å®‰è£…é…ç½® *å®Œå…¨ç¦»çº¿ä½¿ç”¨ï¼Œç»å¯¹ä¿è¯æ•°æ®éšç§ *æ°¸ä¹…å…è´¹ *win10æˆ–mac os 12ä»¥ä¸Š *CPU+å†…å­˜å°±èƒ½è¿è¡Œï¼Œæœ‰GPUæ•ˆæœæ›´ä½³ *ä½é…æœºå™¨ä¹Ÿå¯ä»¥ä½¿ç”¨äº‘ç«¯å¤§æ¨¡å‹ç‰ˆæœ¬ *è‡ªç ”æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿï¼Œå¯è‡ªä¸»å¾®è°ƒæ¨¡å‹ *æ–‡ä»¶éƒ½æœ‰è¯ä¹¦ç­¾åï¼Œå®‰å…¨å¯é \n\nä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š\n\nä¼ä¸šä½¿ç”¨FlashAIè¿›è¡Œå†…éƒ¨æ–‡æ¡£çš„è‡ªåŠ¨ç¿»è¯‘å’Œç”Ÿæˆï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚\n\nå†…å®¹åˆ›ä½œè€…åˆ©ç”¨FlashAIå¿«é€Ÿç”Ÿæˆæ–‡ç« è‰ç¨¿ï¼ŒåŠ é€Ÿåˆ›ä½œæµç¨‹ã€‚\n\næ•™è‚²æœºæ„åˆ©ç”¨FlashAIè¾…åŠ©æ•™å­¦ææ–™çš„ç¼–å†™å’Œç¿»è¯‘å·¥ä½œã€‚\n\näº§å“ç‰¹è‰²ï¼š\n\näº‘ç«¯æ¨¡å‹æ”¯æŒï¼Œæ— éœ€æœ¬åœ°å¤§æ¨¡å‹å³å¯ä½¿ç”¨\n\nå¤šç‰ˆæœ¬æ”¯æŒï¼ŒåŒ…æ‹¬ä¸åŒå¤§å°å’ŒåŠŸèƒ½çš„æ¨¡å‹\n\né€‚ç”¨äºä¸åŒç¡¬ä»¶é…ç½®ï¼Œä»ä½é…åˆ°é«˜é…å‡æœ‰è¦†ç›–\n\nå†…å­˜å’ŒCPUè¦æ±‚æ˜ç¡®ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®è‡ªèº«æ¡ä»¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n\næ”¯æŒå¤šç§æ“ä½œç³»ç»Ÿï¼ŒåŒ…æ‹¬Windowså’ŒMac OS\n\næä¾›ä¸åŒå¤§å°çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚\n\nä½¿ç”¨æ•™ç¨‹ï¼š\n\nè®¿é—®FlashAIå®˜ç½‘å¹¶ä¸‹è½½é€‚åˆè‡ªå·±æ“ä½œç³»ç»Ÿçš„ç‰ˆæœ¬ã€‚\n\næ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç‰ˆæœ¬è¿›è¡Œå®‰è£…ã€‚\n\nå®‰è£…å®Œæˆåï¼Œæ ¹æ®ä½¿ç”¨è¯´æ˜è¿›è¡Œæ¨¡å‹çš„åˆå§‹åŒ–å’Œé…ç½®ã€‚\n\nä½¿ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚\n\næ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä¼˜åŒ–è¾“å‡ºç»“æœã€‚\n\nå®šæœŸæ›´æ–°æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½å’ŒåŠŸèƒ½ã€‚\n\nDeepSeek R1å¤§æ¨¡å‹æœ¬åœ°ä¸€é”®éƒ¨ç½²æ•´åˆåŒ…ï¼Œæä¾›1.5Bï¼Œ7Bï¼Œ14Bï¼Œ32Bï¼Œ70Bæ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ï¼Œæ— éœ€ä»»ä½•é…ç½®ï¼Œé›†æˆå›¾å½¢ç•Œé¢ï¼Œè‡ªå¸¦æœ¬åœ°çŸ¥è¯†åº“ï¼Œç¦»çº¿è¿è¡Œï¼Œæ°¸ä¹…å…è´¹FlashAIä¸€é”®éƒ¨ç½²å¤§æ¨¡å‹\n\næµ‹æµ‹ä½ çš„ç”µè„‘å¯ä»¥è¿è¡Œä»€ä¹ˆå¤§æ¨¡å‹ mark.flashai.com.cn",
    "tags": "[\"Transformers\"]"
  },
  {
    "url": "https://gitcode.com/tencent_hunyuan/Hunyuan3D-Part",
    "project_name": "Hunyuan3D-Part",
    "readme": "â˜¯ï¸ Hunyuan3D Part\nPipeline of our image to 3D part generation. It contains two key components, P3-SAM and X-Part. The holistic mesh is fed to part detection module P3-SAM to obtain the semantic features, part segmentations and part bounding boxes. Then X-Part generate the complete parts.\n\nP3-SAMï¼š Native 3DPart Segmentation.\nPaper: https://arxiv.org/abs/2509.06784.\nCode: https://github.com/Tencent-Hunyuan/Hunyuan3D-Part/tree/main/P3-SAM.\nProject Page: https://murcherful.github.io/P3-SAM/ .\nHuggingFace Demo: https://huggingface.co/spaces/tencent/Hunyuan3D-Part.\n\nX-Partï¼š high-fidelity and structure-coherent shapede composition\nPaper: https://arxiv.org/abs/2509.08643.\nCode: https://github.com/Tencent-Hunyuan/Hunyuan3D-Part/tree/main/XPart.\nProject Page: https://yanxinhao.github.io/Projects/X-Part/.\nHuggingFace Demo: https://huggingface.co/spaces/tencent/Hunyuan3D-Part.\n\nNotice\nThe current release is a light version of X-Part. The full-blood version is available on .\nFor X-Part, we recommend using â€‹â€‹scannedâ€‹â€‹ or â€‹â€‹AI-generated meshesâ€‹â€‹ (e.g., from Hunyuan3D V2.5 or V3.0) as input.\nP3-SAM can handle any input mesh.\nğŸ”— Citation\n\nIf you found this repository helpful, please cite our reports:\n\nP3-SAM\n@article{ma2025p3sam,\n  title={P3-sam: Native 3d part segmentation},\n  author={Ma, Changfeng and Li, Yang and Yan, Xinhao and Xu, Jiachen and Yang, Yunhan and Wang, Chunshi and Zhao, Zibo and Guo, Yanwen and Chen, Zhuo and Guo, Chunchao},\n  journal={arXiv preprint arXiv:2509.06784},\n  year={2025}\n}\n\nX-Part\n@article{yan2025xpart,\n  title={X-Part: high fidelity and structure coherent shape decomposition},\n  author={Yan, Xinhao and Xu, Jiachen and Li, Yang and Ma, Changfeng and Yang, Yunhan and Wang, Chunshi and Zhao, Zibo and Lai, Zeqiang and Zhao, Yunfei and Chen, Zhuo and others},\n  journal={arXiv preprint arXiv:2509.08643},\n  year={2025}\n}\n",
    "tags": "[\"Transformers\", \"Safetensors\", \"Hunyuan3D-2\", \"allenai/objaverse\", \"allenai/objaverse-xl\", \"3D-Generation\", \"Part-Segmentation\", \"Part-Generataion\", \"arxiv:2509.06784\", \"arxiv:2509.08643\"]"
  },
  {
    "url": "https://gitcode.com/tencent_hunyuan/Hunyuan3D-Omni",
    "project_name": "Hunyuan3D-Omni",
    "readme": "      \n\n\nHunyuan3D-Omni\n\nHunyuan3D-Omni is a unified framework for the controllable generation of 3D assets, which inherits the structure of Hunyuan3D 2.1. In contrast, Hunyuan3D-Omni constructs a unified control encoder to introduce additional control signals, including point cloud, voxel, skeleton, and bounding box.\n\nMulti-Modal Conditional Control\nBounding Box Control: Generate 3D models constrained by 3D bounding boxes\nPose Control: Create 3D human models with specific skeletal poses\nPoint Cloud Control: Generate 3D models guided by input point clouds\nVoxel Control: Create 3D models from voxel representations\nğŸ Models Zoo\n\nIt takes 10 GB VRAM for generation.\n\nModel\tDescription\tDate\tSize\tHuggingface\nHunyuan3D-Omni\tImage to Shape Model with multi-modal control\t2025-09-25\t3.3B\tDownload\nInstallation\nRequirements\n\nWe test our model with Python 3.10.\n\npip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\npip install -r requirements.txt\n\nUsage\nInference\nMulti-Modal Inference\npython inference.py --control_type <control_type> [--use_ema] [--flashvdm]\n\n\nThe control_type parameter has four available options:\n\npoint: Use point control type for inference.\nvoxel: Use voxel control type for inference.\nbbox: Use bounding box control type for inference.\npose: Use pose control type for inference.\n\nThe --use_ema flag enables the use of Exponential Moving Average (EMA) model for more stable inference.\n\nThe --flashvdm flag enables FlashVDM optimization for faster inference speed.\n\nPlease choose the appropriate control_type based on your requirements. For example, if you want to use the point control type, you can run:\n\npython inference.py --control_type point \npython inference.py --control_type point --use_ema\npython inference.py --control_type point --flashvdm\n\nAcknowledgements\n\nWe would like to thank the contributors to the TripoSG, Trellis, DINOv2, Stable Diffusion, FLUX, diffusers, HuggingFace, CraftsMan3D, Michelangelo, Hunyuan-DiT, HunyuanVideo, HunyuanWorld-1.0, and HunyuanWorld-Voyager repositories, for their open research and exploration.\n\nCitation\n\nIf you use this code in your research, please cite:\n\n@misc{hunyuan3d2025hunyuan3d,\n    title={Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material},\n    author={Tencent Hunyuan3D Team},\n    year={2025},\n    eprint={2506.15442},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\n@misc{hunyuan3d22025tencent,\n    title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},\n    author={Tencent Hunyuan3D Team},\n    year={2025},\n    eprint={2501.12202},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\n@misc{yang2024hunyuan3d,\n    title={Hunyuan3D 1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},\n    author={Tencent Hunyuan3D Team},\n    year={2024},\n    eprint={2411.02293},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\nStar History",
    "tags": "[\"Image-to-3D\", \"PyTorch\", \"Transformers\", \"Hunyuan3D-2\", \"English\", \"Chinese\", \"Other\", \"text-to-3d\"]"
  },
  {
    "url": "https://gitcode.com/ascend-tribe/openPangu-Embedded-7B-V1.1",
    "project_name": "openPangu-Embedded-7B-V1.1",
    "readme": "å¼€æºç›˜å¤ Embedded-7B-V1.1\n\nä¸­æ–‡ | English\n\n1. ç®€ä»‹\n\nopenPangu-Embedded-7B-V1.1 æ˜¯åŸºäºæ˜‡è…¾ NPU ä»é›¶è®­ç»ƒçš„é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°é‡ä¸º 7Bï¼ˆä¸å«è¯è¡¨Embeddingï¼‰ã€‚openPangu-Embedded-7B-V1.1 è®­ç»ƒäº†çº¦ 25T tokensï¼Œå…·å¤‡å¿«æ…¢æ€è€ƒèåˆä¸è‡ªé€‚åº”åˆ‡æ¢èƒ½åŠ›ã€‚\n\n2. æ¨¡å‹æ¶æ„\n\topenPangu-Embedded-7B-V1.1\nArchitecture\tDense\nParameters (Non-Embedding)\t7B\nNumber of Layers\t34\nHidden Dimension\t12800\nAttention Mechanism\tGQA\nNumber of Attention Heads\t32 for Qï¼Œ8 for KV\nVocabulary Size\t153k\nContext Length (Natively)\t32k\nPretraining Tokens\t25T\n3. æµ‹è¯„ç»“æœ\næµ‹è¯„é›†\tæµ‹è¯„æŒ‡æ ‡\tæ…¢æ€è€ƒv1.0\tæ…¢æ€è€ƒv1.1\tè‡ªé€‚åº”v1.1\né€šç”¨èƒ½åŠ›\t\t\t\t\nMMLU-Pro\tExact Match\t76.32\t75.54\t72.81\nCMMLU\tAcc\t75.59\t72.94\t72.18\nArenaHard_v0.1\tw/o style control\t85.80\t88.00\t84.60\nC-Eval\tAcc\t83.05\t84.92\t83.33\nGPQA-Diamond\tAvg@4\t70.54\t73.23\t73.74\næ•°å­¦èƒ½åŠ›\t\t\t\t\nMATH-500\tAvg@1\t95.00\t97.00\t96.00\nAIME24\tAvg@16\t71.57\t79.38\t79.02\nAIME25\tAvg@16\t58.24\t70.00\t70.21\nä»£ç èƒ½åŠ›\t\t\t\t\nLiveCodeBench\tAvg@2 (08/24~01/25)\t54.04\t58.27\t58.27\nMBPP+\tAvg@2\t76.06\t76.46\t75.66\n\næ³¨ï¼š è¯„æµ‹è¿‡ç¨‹ä¸­system prompt ä¸ºç©ºï¼Œä¸”ä¸æ·»åŠ ä»»ä½•é¢å¤–çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºã€‚è¯„æµ‹é‡‡ç”¨ 128k çš„åºåˆ—é•¿åº¦è¿›è¡Œã€‚\n\né™¤ç²¾åº¦å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨éƒ¨åˆ†æ•°æ®é›†ä¸Šç»Ÿè®¡äº†æ¨¡å‹çš„è¾“å‡ºé•¿åº¦ï¼Œé€šè¿‡æ•°æ®è´¨é‡é©±åŠ¨çš„å­¦ä¹ ç­–ç•¥ï¼Œè‡ªé€‚åº”å¿«æ…¢æ€è€ƒå¯ä»¥åœ¨åŸºæœ¬ä¸å½±å“ç²¾åº¦åœ°å‰æä¸‹ï¼Œæœ‰æ•ˆåœ°åœ¨ç®€å•ä»»åŠ¡ä¸Šè‡ªåŠ¨åˆ‡æ¢éƒ¨åˆ†è¾“å‡ºä¸ºå¿«æ€è€ƒï¼Œå¤§å¹…ç¼©çŸ­å¹³å‡è¾“å‡ºæ€ç»´é“¾é•¿åº¦ï¼ˆLengthï¼‰ï¼›åœ¨éš¾ä»»åŠ¡é€šè¿‡ä¿æŒæ…¢æ€è€ƒèƒ½åŠ›ï¼Œç²¾åº¦æŒå¹³çº¯æ…¢æ€è€ƒæ¨¡å‹ã€‚\n\næµ‹è¯„é›†\tæµ‹è¯„æŒ‡æ ‡\tæ…¢æ€è€ƒv1.1\tè‡ªé€‚åº”v1.1\né€šç”¨èƒ½åŠ›\t\t\t\nCMMLU\tAcc\t72.94\t72.18\n\tLength\t2574\t1338\nC-Eval\tAcc\t84.92\t83.33\n\tLength\t2484\t1723\næ•°å­¦èƒ½åŠ›\t\t\t\nAIME24\tAvg@16\t79.38\t79.02\n\tLength\t48229\t49656\nä»£ç èƒ½åŠ›\t\t\t\nLiveCodeBench\tAvg@2 (08/24~01/25)\t58.27\t58.27\n\tLength\t58140\t59307\n4. éƒ¨ç½²å’Œä½¿ç”¨\n4.1 ç¯å¢ƒå‡†å¤‡\nç¡¬ä»¶è§„æ ¼\n\nAtlas 800T A2 (64GB)ï¼Œé©±åŠ¨ä¸å›ºä»¶å®‰è£…åŒ…è·å–è¯·å‚ç…§ [Atlas 800T A2]ã€‚\n\nè½¯ä»¶ç¯å¢ƒ\næ“ä½œç³»ç»Ÿï¼šLinuxï¼ˆæ¨è openEuler>=24.03ï¼‰\nCANN==8.1.RC1ï¼Œå®‰è£…å‡†å¤‡åŠæµç¨‹è¯·å‚ç…§ [CANN Install]\npython==3.10\ntorch==2.1.0\ntorch-npu==2.1.0.post12\ntransformers==4.53.2\n\nä»¥ä¸Šè½¯ä»¶é…å¥—ç»è¿‡éªŒè¯ï¼Œç†è®ºå¯ä»¥æ”¯æŒæ›´é«˜ç‰ˆæœ¬ï¼Œå¦‚æœ‰ç–‘é—®ï¼Œå¯ä»¥æäº¤ issueã€‚\n\n4.2 æƒé‡å®Œæ•´æ€§æ ¡éªŒ\n\nè¯·å‚è€ƒä»¥ä¸‹æ–¹æ³•å¯¹ä¸‹è½½å†…å®¹è¿›è¡Œå®Œæ•´æ€§æ ¡éªŒï¼Œhash å€¼å­˜å‚¨åœ¨ checklist.chk æ–‡ä»¶ä¸­ã€‚\n\n#!/usr/bin/env bash\nARCH=$(uname -m)\nMODEL_PATH=\"${TARGET_FOLDER}/${MODEL_FOLDER_PATH}\"\ncd \"$MODEL_PATH\" || exit 1\nif [ \"$ARCH\" = \"arm64\" ]; then\n    sha256sum checklist.chk\nelse\n    sha256sum -c checklist.chk\nfi\n\n4.3 æ¨ç†æ ·ä¾‹\n\nä¸‹è¿°å†…å®¹æä¾› openPangu-Embedded-7B-V1.1 åœ¨ transformers æ¡†æ¶ä¸Šè¿›è¡Œæ¨ç†çš„ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼š\n\nè¿è¡Œå‰è¯·ä¿®æ”¹ generate.pyï¼Œæ·»åŠ æ¨¡å‹è·¯å¾„ã€‚\n\ncd inference\npython generate.py\n\n\nopenPangu-Embedded-7B-V1.1 æ¨¡å‹é»˜è®¤ä¸ºæ…¢æ€è€ƒæ¨¡å¼ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ‰‹æ®µåˆ‡æ¢è‡³å¿«æ…¢è‡ªé€‚åº”åˆ‡æ¢/å¿«æ€è€ƒæ¨¡å¼ï¼š\n\nåœ¨ä»£ç å®ä¾‹generate.pyä¸­ï¼Œauto_thinking_promptä¸no_thinking_promptå˜é‡çš„å®šä¹‰å±•ç¤ºäº†åˆ‡æ¢è‡³å¿«æ…¢è‡ªé€‚åº”æˆ–å¿«æ€è€ƒæ¨¡å¼çš„å…·ä½“å®ç°ï¼šé€šè¿‡åœ¨ç”¨æˆ·è¾“å…¥æœ«å°¾æ·»åŠ /auto_thinkæˆ–/no_thinkæ ‡è®°ï¼Œå¯å°†å½“å‰è½®æ¬¡åˆ‡æ¢è‡³å¿«æ…¢è‡ªé€‚åº”åˆ‡æ¢/å¿«æ€è€ƒæ¨¡å¼ã€‚\n4.4 ä½¿ç”¨æ¨ç†æ¡†æ¶\n\nvllm_ascendï¼šå‚è€ƒ[vllm_ascend_for_openpangu_embedded_7b.zh]\n\n5. æ¨¡å‹è®¸å¯è¯\n\né™¤æ–‡ä»¶ä¸­å¯¹å¼€æºè®¸å¯è¯å¦æœ‰çº¦å®šå¤–ï¼ŒopenPangu-Embedded-7B-V1.1 æ¨¡å‹æ ¹æ® OPENPANGU MODEL LICENSE AGREEMENT VERSION 1.0 æˆæƒï¼Œæ—¨åœ¨å…è®¸ä½¿ç”¨å¹¶ä¿ƒè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ¨¡å‹å­˜å‚¨åº“æ ¹ç›®å½•ä¸­çš„ LICENSE æ–‡ä»¶ã€‚\n\n6. å…è´£å£°æ˜\n\nç”±äº openPangu-Embedded-7B-V1.1ï¼ˆâ€œæ¨¡å‹â€ï¼‰æ‰€ä¾èµ–çš„æŠ€æœ¯å›ºæœ‰çš„æŠ€æœ¯é™åˆ¶ï¼Œä»¥åŠäººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹æ˜¯ç”±ç›˜å¤è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œåä¸ºæ— æ³•å¯¹ä»¥ä¸‹äº‹é¡¹åšå‡ºä»»ä½•ä¿è¯ï¼š\n\nå°½ç®¡è¯¥æ¨¡å‹çš„è¾“å‡ºç”± AI ç®—æ³•ç”Ÿæˆï¼Œä½†ä¸èƒ½æ’é™¤æŸäº›ä¿¡æ¯å¯èƒ½å­˜åœ¨ç¼ºé™·ã€ä¸åˆç†æˆ–å¼•èµ·ä¸é€‚çš„å¯èƒ½æ€§ï¼Œç”Ÿæˆçš„å†…å®¹ä¸ä»£è¡¨åä¸ºçš„æ€åº¦æˆ–ç«‹åœºï¼›\næ— æ³•ä¿è¯è¯¥æ¨¡å‹ 100% å‡†ç¡®ã€å¯é ã€åŠŸèƒ½é½å…¨ã€åŠæ—¶ã€å®‰å…¨ã€æ— é”™è¯¯ã€ä¸é—´æ–­ã€æŒç»­ç¨³å®šæˆ–æ— ä»»ä½•æ•…éšœï¼›\nè¯¥æ¨¡å‹çš„è¾“å‡ºå†…å®¹ä¸æ„æˆä»»ä½•å»ºè®®æˆ–å†³ç­–ï¼Œä¹Ÿä¸ä¿è¯ç”Ÿæˆçš„å†…å®¹çš„çœŸå®æ€§ã€å®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€åŠæ—¶æ€§ã€åˆæ³•æ€§ã€åŠŸèƒ½æ€§æˆ–å®ç”¨æ€§ã€‚ç”Ÿæˆçš„å†…å®¹ä¸èƒ½æ›¿ä»£åŒ»ç–—ã€æ³•å¾‹ç­‰é¢†åŸŸçš„ä¸“ä¸šäººå£«å›ç­”æ‚¨çš„é—®é¢˜ã€‚ç”Ÿæˆçš„å†…å®¹ä»…ä¾›å‚è€ƒï¼Œä¸ä»£è¡¨åä¸ºçš„ä»»ä½•æ€åº¦ã€ç«‹åœºæˆ–è§‚ç‚¹ã€‚æ‚¨éœ€è¦æ ¹æ®å®é™…æƒ…å†µåšå‡ºç‹¬ç«‹åˆ¤æ–­ï¼Œåä¸ºä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n7. åé¦ˆ\n\nå¦‚æœæœ‰ä»»ä½•æ„è§å’Œå»ºè®®ï¼Œè¯·æäº¤issueæˆ–è”ç³» openPangu@huawei.comã€‚",
    "tags": "[\"Unknown\"]"
  },
  {
    "url": "https://gitcode.com/ascend-tribe/openPangu-Embedded-1B-V1.1",
    "project_name": "openPangu-Embedded-1B-V1.1",
    "readme": "å¼€æºç›˜å¤ Embedded-1B-V1.1\n\nä¸­æ–‡ | English\n\n1.ç®€ä»‹\n\nopenPangu-Embedded-1B-V1.1 æ˜¯åŸºäºæ˜‡è…¾ NPU ä»é›¶è®­ç»ƒçš„é«˜æ•ˆè¯­è¨€æ¨¡å‹ï¼Œå‚æ•°é‡ä¸º 1Bï¼ˆä¸å«è¯è¡¨Embeddingï¼‰ï¼Œæ¨¡å‹ç»“æ„é‡‡ç”¨ 26 å±‚ Dense æ¶æ„ï¼Œè®­ç»ƒäº†çº¦ 10T tokensã€‚é€šè¿‡æ˜‡è…¾ Atlas 200I A2å¯ç”¨çš„æ¨¡å‹æ¶æ„è®¾è®¡ã€æ•°æ®å’Œè®­ç»ƒç­–ç•¥ä¼˜åŒ–ï¼ŒopenPangu-Embedded-1B-V1.1 åœ¨ä¿æŒç«¯ä¾§è¿è¡Œçš„è¦æ±‚ä¸‹è¾¾åˆ°äº†è¾ƒé«˜çš„ç²¾åº¦ã€‚å’ŒV1ç‰ˆæœ¬ç›¸æ¯”ï¼ŒopenPangu-Embedded-1B-V1.1åœ¨åè®­ç»ƒé˜¶æ®µè¿›ä¸€æ­¥åšäº†ç¦»çº¿On-policyè’¸é¦å’Œå¤šæºå¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼Œä»¥å¢å¼ºæ•°å­¦ã€ä»£ç ä»¥åŠé€šç”¨èƒ½åŠ›ã€‚\n\næ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è€ƒæŠ€æœ¯æŠ¥å‘Šã€‚\n\n2. æ¨¡å‹æ¶æ„\n\nopenPangu-Embedded-1B-V1.1 æ˜¯ä¸€ä¸ªä¸ºç«¯ä¾§è®¾å¤‡è¿è¡Œè€Œè®¾è®¡çš„é«˜æ•ˆå¿«æ€è€ƒè¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒæ˜‡è…¾ Atlas 200I A2ã€‚\n\n\topenPangu-Embedded-1B-V1.1\nArchitecture\tDense\nParameters (Non-Embedding)\t1B\nNumber of Layers\t26\nHidden Dimension\t1536\nAttention Mechanism\tGQA\nNumber of Attention Heads\t12 for Q, 6 for KV\nVocabulary Size\t153k\nContext Length (Natively)\t32k\nTraining Tokens\t10T\n3. æµ‹è¯„ç»“æœ\nè¯„æµ‹é›†\tæµ‹è¯„æŒ‡æ ‡\topenPangu-Embedded-1B-V1\topenPangu-Embedded-1B-V1.1\né€šç”¨èƒ½åŠ›\t\t\t\nMMLU\tAcc\t60.72\t65.08\nCMMLU\tAcc\t51.99\t55.65\nC-Eval\tAcc\t60.98\t63.02\nIF-Eval\tPrompt Strict\t56.56\t55.51\nCLUEWSC\tAcc\t68.55\t78.55\næ•°å­¦&æ¨ç†\t\t\t\nGSM8K\tAcc\t66.72\t82.76\nMATH-500\tAcc\t52.00\t81.83\nDROP\tF1\t50.31\t52.82\nGPQA-Diamond\tPass@1\t33.84\t41.70\nä»£ç èƒ½åŠ›\t\t\t\nMBPP\tPass@1\t54.09\t59.31\nHumanEval\tPass@1\t56.71\t66.66\n\næ³¨ï¼š è¯„æµ‹è¿‡ç¨‹ä¸­system prompt ä¸ºç©ºã€‚è¯„æµ‹ç»†èŠ‚è¯¦è§æŠ€æœ¯æŠ¥å‘Šã€‚\n\n4. éƒ¨ç½²å’Œä½¿ç”¨\n4.1 ç¯å¢ƒå‡†å¤‡\nç¡¬ä»¶è§„æ ¼\n\nAtlas 800T A2 (64GB)ï¼Œé©±åŠ¨ä¸å›ºä»¶å®‰è£…åŒ…è·å–è¯·å‚ç…§ [Atlas 800T A2]ã€‚\n\nè½¯ä»¶ç¯å¢ƒ\næ“ä½œç³»ç»Ÿï¼šLinuxï¼ˆæ¨è openEuler>=24.03ï¼‰\nCANN==8.1.RC1ï¼Œå®‰è£…å‡†å¤‡åŠæµç¨‹è¯·å‚ç…§ [CANN Install]\npython==3.10\ntorch==2.1.0\ntorch-npu==2.1.0.post12\ntransformers==4.53.2\n\nä»¥ä¸Šè½¯ä»¶é…å¥—ç»è¿‡éªŒè¯ï¼Œç†è®ºå¯ä»¥æ”¯æŒæ›´é«˜ç‰ˆæœ¬ï¼Œå¦‚æœ‰ç–‘é—®ï¼Œå¯ä»¥æäº¤ issueã€‚\n\n4.2 æƒé‡å®Œæ•´æ€§æ ¡éªŒ\n\nè¯·å‚è€ƒä»¥ä¸‹æ–¹æ³•å¯¹ä¸‹è½½å†…å®¹è¿›è¡Œå®Œæ•´æ€§æ ¡éªŒï¼Œhash å€¼å­˜å‚¨åœ¨ checklist.chk æ–‡ä»¶ä¸­ã€‚\n\n#!/usr/bin/env bash\nARCH=$(uname -m)\nMODEL_PATH=\"${TARGET_FOLDER}/${MODEL_FOLDER_PATH}\"\ncd \"$MODEL_PATH\" || exit 1\nif [ \"$ARCH\" = \"arm64\" ]; then\n    sha256sum checklist.chk\nelse\n    sha256sum -c checklist.chk\nfi\n\n4.3 æ¨ç†æ ·ä¾‹\n\nä¸‹è¿°å†…å®¹æä¾› openPangu-Embedded-1B-V1.1 åœ¨ transformers æ¡†æ¶ä¸Šè¿›è¡Œæ¨ç†çš„ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼š\n\nè¿è¡Œå‰è¯·ä¿®æ”¹ generate.pyï¼Œæ·»åŠ æ¨¡å‹è·¯å¾„ã€‚\n\ncd inference\npython generate.py\n\n4.4 ä½¿ç”¨æ¨ç†æ¡†æ¶\n\nvllm_ascendï¼š å‚è€ƒ[vllm_ascend_for_openpangu_embedded_1b.zh]\n\næ˜‡è…¾ Atlas 200I A2æ¨ç†ï¼š openPangu-Embedded-1B-V1.1 æ¨¡å‹æ¨ç†å·²é€‚é…æ˜‡è…¾ MindIE 2.2.T10ï¼ˆ[ä¸‹è½½é“¾æ¥]ï¼‰ï¼Œæ”¯æŒ OrangePi AIpro (æ˜‡è…¾ Atlas 200I A2) æ¨ç†éƒ¨ç½²ã€‚å±Šæ—¶å¯å‰å¾€ [æ˜‡è…¾ç¤¾åŒºModelZoo] ä¸‹è½½é€‚é…ï¼Œä¸‹è½½é•œåƒå‰éœ€è¦ç”³è¯·æƒé™ï¼Œè€å¿ƒç­‰å¾…æƒé™ç”³è¯·é€šè¿‡åï¼Œæ ¹æ®æŒ‡å—ä¸‹è½½å¯¹åº”ç‰ˆæœ¬æ–‡ä»¶å’Œå®‰è£…æŒ‡å¯¼å®Œæˆæ¨ç†éƒ¨ç½²ã€‚\n\n5. æ¨¡å‹è®¸å¯è¯\n\né™¤æ–‡ä»¶ä¸­å¯¹å¼€æºè®¸å¯è¯å¦æœ‰çº¦å®šå¤–ï¼ŒopenPangu-Embedded-1B-V1.1 æ¨¡å‹æ ¹æ® OPENPANGU MODEL LICENSE AGREEMENT VERSION 1.0 æˆæƒï¼Œæ—¨åœ¨å…è®¸ä½¿ç”¨å¹¶ä¿ƒè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ¨¡å‹å­˜å‚¨åº“æ ¹ç›®å½•ä¸­çš„ LICENSE æ–‡ä»¶ã€‚\n\n6. å…è´£å£°æ˜\n\nç”±äº openPangu-Embedded-1B-V1.1ï¼ˆâ€œæ¨¡å‹â€ï¼‰æ‰€ä¾èµ–çš„æŠ€æœ¯å›ºæœ‰çš„æŠ€æœ¯é™åˆ¶ï¼Œä»¥åŠäººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹æ˜¯ç”±ç›˜å¤è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œåä¸ºæ— æ³•å¯¹ä»¥ä¸‹äº‹é¡¹åšå‡ºä»»ä½•ä¿è¯ï¼š\n\nå°½ç®¡è¯¥æ¨¡å‹çš„è¾“å‡ºç”± AI ç®—æ³•ç”Ÿæˆï¼Œä½†ä¸èƒ½æ’é™¤æŸäº›ä¿¡æ¯å¯èƒ½å­˜åœ¨ç¼ºé™·ã€ä¸åˆç†æˆ–å¼•èµ·ä¸é€‚çš„å¯èƒ½æ€§ï¼Œç”Ÿæˆçš„å†…å®¹ä¸ä»£è¡¨åä¸ºçš„æ€åº¦æˆ–ç«‹åœºï¼›\næ— æ³•ä¿è¯è¯¥æ¨¡å‹ 100% å‡†ç¡®ã€å¯é ã€åŠŸèƒ½é½å…¨ã€åŠæ—¶ã€å®‰å…¨ã€æ— é”™è¯¯ã€ä¸é—´æ–­ã€æŒç»­ç¨³å®šæˆ–æ— ä»»ä½•æ•…éšœï¼›\nè¯¥æ¨¡å‹çš„è¾“å‡ºå†…å®¹ä¸æ„æˆä»»ä½•å»ºè®®æˆ–å†³ç­–ï¼Œä¹Ÿä¸ä¿è¯ç”Ÿæˆçš„å†…å®¹çš„çœŸå®æ€§ã€å®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€åŠæ—¶æ€§ã€åˆæ³•æ€§ã€åŠŸèƒ½æ€§æˆ–å®ç”¨æ€§ã€‚ç”Ÿæˆçš„å†…å®¹ä¸èƒ½æ›¿ä»£åŒ»ç–—ã€æ³•å¾‹ç­‰é¢†åŸŸçš„ä¸“ä¸šäººå£«å›ç­”æ‚¨çš„é—®é¢˜ã€‚ç”Ÿæˆçš„å†…å®¹ä»…ä¾›å‚è€ƒï¼Œä¸ä»£è¡¨åä¸ºçš„ä»»ä½•æ€åº¦ã€ç«‹åœºæˆ–è§‚ç‚¹ã€‚æ‚¨éœ€è¦æ ¹æ®å®é™…æƒ…å†µåšå‡ºç‹¬ç«‹åˆ¤æ–­ï¼Œåä¸ºä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n7. åé¦ˆ\n\nå¦‚æœæœ‰ä»»ä½•æ„è§å’Œå»ºè®®ï¼Œè¯·æäº¤issueæˆ–è”ç³» openPangu@huawei.comã€‚",
    "tags": "[\"Unknown\"]"
  }
]