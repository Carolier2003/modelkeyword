"""
ä¸»è¦å·¥ä½œæµç¨‹ - æ¨¡å‹å…³é”®è¯æå–ç³»ç»Ÿ
"""
import os
import argparse
import json
from datetime import datetime
from typing import List, Optional
import traceback

from models import ModelInfo, KeywordResult, save_to_json, load_from_json
from csv_reader import CSVModelReader
from ai_extractor import KeywordExtractor
from multi_platform_extractor import MultiPlatformExtractorSync


def detect_available_platforms() -> int:
    """æ£€æµ‹å¯ç”¨çš„APIå¹³å°æ•°é‡"""
    platforms = {
        "MOONSHOT_API_KEY": "æœˆä¹‹æš—é¢",
        "DASHSCOPE_API_KEY": "é˜¿é‡Œç™¾ç‚¼", 
        "QINIU_API_KEY": "ä¸ƒç‰›äº‘",
        "HUNYUAN_API_KEY": "è…¾è®¯æ··å…ƒ"
    }
    
    available_count = 0
    for key, name in platforms.items():
        if os.getenv(key):
            available_count += 1
    
    return available_count


class ModelKeywordExtractor:
    """æ¨¡å‹å…³é”®è¯æå–ä¸»ç¨‹åº"""
    
    def __init__(self, output_dir: str = "output", token: Optional[str] = None, use_multi_platform: bool = False):
        """
        åˆå§‹åŒ–æå–å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            token: å¯é€‰çš„è®¤è¯token
            use_multi_platform: æ˜¯å¦ä½¿ç”¨å¤šå¹³å°å¹¶å‘æå–
        """
        self.output_dir = output_dir
        self.token = token
        self.use_multi_platform = use_multi_platform
        self.ensure_output_dir()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.csv_reader = CSVModelReader(delay=0.1, token=token)  # CSVè¯»å–å™¨ï¼Œé›†æˆçˆ¬è™«åŠŸèƒ½
        
        # é€‰æ‹©æå–å™¨
        if use_multi_platform:
            print("ğŸš€ ä½¿ç”¨å¤šå¹³å°å¹¶å‘æå–å™¨")
            self.extractor = MultiPlatformExtractorSync()
        else:
            print("ğŸ“¡ ä½¿ç”¨å•å¹³å°æå–å™¨")
            self.extractor = KeywordExtractor()
    
    def ensure_output_dir(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
    
    def run_full_pipeline(self, max_models: int = 100, force_crawl: bool = False, use_csv: bool = True):
        """
        è¿è¡Œå®Œæ•´çš„æå–æµç¨‹
        
        Args:
            max_models: æœ€å¤§æ¨¡å‹æ•°é‡
            force_crawl: æ˜¯å¦å¼ºåˆ¶é‡æ–°çˆ¬å–
            use_csv: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨CSVæ–‡ä»¶
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¸ºæœ¬æ¬¡è¿è¡Œåˆ›å»ºä¸“å±æ–‡ä»¶å¤¹
        run_output_dir = os.path.join(self.output_dir, f"run_{timestamp}")
        os.makedirs(run_output_dir, exist_ok=True)
        print(f"ğŸ“ æœ¬æ¬¡è¾“å‡ºç›®å½•: {run_output_dir}")
        
        print("=" * 60)
        print("æ¨¡å‹å…³é”®è¯æå–ç³»ç»Ÿ")
        print("=" * 60)
        
        try:
            # æ­¥éª¤1: è·å–æ¨¡å‹ä¿¡æ¯
            models_file = os.path.join(run_output_dir, f"models_{timestamp}.json")
            models = self.crawl_or_load_models(max_models, force_crawl, models_file, use_csv)
            
            if not models:
                print("é”™è¯¯ï¼šæœªèƒ½è·å–ä»»ä½•æ¨¡å‹ä¿¡æ¯")
                return
            
            # æ­¥éª¤2: æå–å…³é”®è¯
            keywords_file = os.path.join(run_output_dir, f"keywords_{timestamp}.json")
            keyword_results = self.extract_keywords(models, keywords_file)
            
            if not keyword_results:
                print("é”™è¯¯ï¼šæœªèƒ½æå–ä»»ä½•å…³é”®è¯")
                return
            
            # æ­¥éª¤3: å»é‡å¤„ç†
            dedup_file = os.path.join(run_output_dir, f"keywords_dedup_{timestamp}.json")
            final_results = self.deduplicate_keywords(keyword_results, dedup_file)
            
            # æ­¥éª¤4: ç”ŸæˆæŠ¥å‘Š
            report_file = os.path.join(run_output_dir, f"report_{timestamp}.md")
            self.generate_report(keyword_results, final_results, report_file, total_attempted=len(models))
            
            print(f"\nâœ… æå–å®Œæˆï¼")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   ğŸ“ CSVè¯»å–æ¨¡å‹æ•°: {len(models)}")
            print(f"   âœ… æˆåŠŸæå–æ¨¡å‹æ•°: {len(final_results)}")
            print(f"   âŒ å¤±è´¥æ¨¡å‹æ•°: {len(models) - len(final_results)}")
            print(f"   ğŸ” å…³é”®è¯æ€»æ•°: {sum(len(r.keywords) for r in final_results)}")
            print(f"   ğŸ“ˆ æˆåŠŸç‡: {len(final_results)/len(models)*100:.1f}%")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
            print(f"   - æ¨¡å‹ä¿¡æ¯: {models_file}")
            print(f"   - åŸå§‹å…³é”®è¯: {keywords_file}")
            print(f"   - å»é‡å…³é”®è¯: {dedup_file}")
            print(f"   - åˆ†ææŠ¥å‘Š: {report_file}")
            print(f"   - CSVå¯¼å‡º: {report_file.replace('.md', '.csv')}")
            print(f"   - å…³é”®è¯åˆ—è¡¨: {report_file.replace('.md', '_keywords.txt')}")
            
        except Exception as e:
            print(f"âŒ è¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            traceback.print_exc()
    
    def crawl_or_load_models(self, max_models: int, force_crawl: bool, output_file: str, use_csv: bool = True) -> List[ModelInfo]:
        """
        è·å–æˆ–åŠ è½½æ¨¡å‹ä¿¡æ¯
        
        Args:
            max_models: æœ€å¤§æ¨¡å‹æ•°é‡
            force_crawl: æ˜¯å¦å¼ºåˆ¶é‡æ–°è·å–
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            use_csv: æ˜¯å¦ä½¿ç”¨CSVæ–‡ä»¶ï¼ˆä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼Œä½†åªæ”¯æŒCSVï¼‰
            
        Returns:
            æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
        """
        print(f"\nğŸ“– æ­¥éª¤1: ä»CSVæ–‡ä»¶è·å–æ¨¡å‹ä¿¡æ¯ (ç›®æ ‡æ•°é‡: {max_models})")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜æ–‡ä»¶
        cache_file = os.path.join(self.output_dir, "models_cache.json")
        
        if not force_crawl and os.path.exists(cache_file):
            print("å‘ç°ç¼“å­˜æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")
            try:
                cached_data = load_from_json(cache_file)
                if cached_data and len(cached_data) >= max_models:
                    models = [ModelInfo.from_dict(data) for data in cached_data[:max_models]]
                    print(f"âœ… ä»ç¼“å­˜åŠ è½½äº† {len(models)} ä¸ªæ¨¡å‹ä¿¡æ¯")
                    
                    # ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶
                    model_dicts = [model.to_dict() for model in models]
                    save_to_json(model_dicts, output_file)
                    
                    return models
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        
        # ä»CSVæ–‡ä»¶è·å–æ¨¡å‹ä¿¡æ¯
        print("å¼€å§‹ä»CSVæ–‡ä»¶è¯»å–æ¨¡å‹ä¿¡æ¯...")
        try:
            models = self.csv_reader.crawl_models(max_models, fetch_details=True)
        except Exception as e:
            print(f"âŒ CSVè¯»å–å¤±è´¥: {e}")
            return []
        
        if models:
            # ä¿å­˜åˆ°ç¼“å­˜å’Œè¾“å‡ºæ–‡ä»¶
            model_dicts = [model.to_dict() for model in models]
            save_to_json(model_dicts, cache_file)
            save_to_json(model_dicts, output_file)
            print(f"âœ… æˆåŠŸè¯»å– {len(models)} ä¸ªæ¨¡å‹ä¿¡æ¯")
        else:
            print("âŒ æœªèƒ½è¯»å–åˆ°ä»»ä½•æ¨¡å‹ä¿¡æ¯")
        
        return models
    
    def extract_keywords(self, models: List[ModelInfo], output_file: str) -> List[KeywordResult]:
        """
        æå–å…³é”®è¯
        
        Args:
            models: æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å…³é”®è¯æå–ç»“æœåˆ—è¡¨
        """
        print(f"\nğŸ¤– æ­¥éª¤2: æå–å…³é”®è¯ (æ¨¡å‹æ•°é‡: {len(models)})")
        
        # æ£€æŸ¥æ¨¡å‹æœ‰æ•ˆæ€§ï¼ˆé¡¹ç›®åç§°ä¸ä¸ºç©ºå³å¯ï¼‰
        valid_models = [m for m in models if m.project_name.strip()]
        print(f"æœ‰æ•ˆæ¨¡å‹æ•°é‡: {len(valid_models)}")
        
        # å¤±è´¥åŸå› ç»Ÿè®¡
        failure_stats = {
            'json_parse_error': 0,
            'insufficient_keywords': 0,
            'validation_error': 0,
            'api_error': 0,
            'other_error': 0
        }
        
        # è¯´æ˜READMEè·å–æ–¹å¼
        print(f"è¯´æ˜: READMEå†…å®¹å·²é€šè¿‡çˆ¬è™«è·å–ï¼ŒAIå°†åŸºäºçˆ¬å–çš„æ•°æ®è¿›è¡Œåˆ†æ")
        
        if not valid_models:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹ç”¨äºå…³é”®è¯æå–")
            return []
        
        # æ‰¹é‡æå–å…³é”®è¯
        keyword_results = self.extractor.extract_batch_keywords(valid_models)
        
        if keyword_results:
            # ä¿å­˜ç»“æœ
            results_data = [result.to_dict() for result in keyword_results]
            save_to_json(results_data, output_file)
            print(f"âœ… æˆåŠŸæå– {len(keyword_results)} ä¸ªæ¨¡å‹çš„å…³é”®è¯")
            
            # æ˜¾ç¤ºå¤±è´¥ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰å¤±è´¥çš„è¯ï¼‰
            failed_count = len(valid_models) - len(keyword_results)
            if failed_count > 0:
                print(f"\nğŸ“Š å¤±è´¥åˆ†æ:")
                print(f"   ğŸ’” å¤±è´¥æ¨¡å‹æ•°: {failed_count}")
                print(f"   ğŸ“‹ å»ºè®®æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯äº†è§£å…·ä½“å¤±è´¥åŸå› ")
        else:
            print("âŒ å…³é”®è¯æå–å¤±è´¥")
        
        return keyword_results
    
    def deduplicate_keywords(self, keyword_results: List[KeywordResult], output_file: str) -> List[KeywordResult]:
        """
        å…³é”®è¯å»é‡
        
        Args:
            keyword_results: åŸå§‹å…³é”®è¯ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å»é‡åçš„ç»“æœ
        """
        print(f"\nğŸ”„ æ­¥éª¤3: ä¿å­˜å…³é”®è¯ï¼ˆå»é‡å°†åœ¨æŠ¥å‘Šç”Ÿæˆæ—¶è¿›è¡Œï¼‰")
        
        # æ‰§è¡Œå»é‡
        dedup_results = self.extractor.deduplicate_keywords(keyword_results)
        
        if dedup_results:
            # ä¿å­˜å»é‡ç»“æœ
            results_data = [result.to_dict() for result in dedup_results]
            save_to_json(results_data, output_file)
            
            # ç»Ÿè®¡ä¿¡æ¯
            original_count = sum(len(r.keywords) for r in keyword_results)
            final_count = sum(len(r.keywords) for r in dedup_results)
            print(f"âœ… å»é‡å®Œæˆ: {original_count} â†’ {final_count} ä¸ªå…³é”®è¯")
        else:
            print("âŒ å»é‡å¤±è´¥")
        
        return dedup_results
    
    def _csv_deduplicate_keywords(self, keyword_results: List[KeywordResult]) -> List[KeywordResult]:
        """
        æ‰§è¡ŒCSVçº§åˆ«çš„å»é‡ï¼Œä¸generate_csv_outputä¸­çš„é€»è¾‘ä¸€è‡´
        
        Args:
            keyword_results: å…³é”®è¯ç»“æœåˆ—è¡¨
            
        Returns:
            å»é‡åçš„ç»“æœåˆ—è¡¨
        """
        # ç”¨äºå»é‡çš„å·²ä½¿ç”¨å…³é”®è¯é›†åˆï¼ˆä¿ç•™å…ˆç”Ÿæˆçš„ï¼‰
        used_keywords = set()
        dedup_results = []
        
        for result in keyword_results:
            filtered_keywords = []
            
            for kw in result.keywords:
                keyword = kw['keyword']
                
                # å»é‡ï¼šä¿ç•™å…ˆç”Ÿæˆçš„å…³é”®è¯
                if keyword not in used_keywords:
                    used_keywords.add(keyword)
                    filtered_keywords.append(kw)
            
            if filtered_keywords:
                # åˆ›å»ºæ–°çš„ç»“æœå¯¹è±¡ï¼ŒåŒ…å«å»é‡åçš„å…³é”®è¯
                dedup_result = KeywordResult(
                    model_url=result.model_url,
                    keywords=filtered_keywords
                )
                dedup_results.append(dedup_result)
        
        return dedup_results
    
    def generate_report(self, original_results: List[KeywordResult], final_results: List[KeywordResult], output_file: str, total_attempted: int = None):
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            original_results: åŸå§‹å…³é”®è¯ç»“æœï¼ˆæœªå»é‡ï¼‰
            final_results: æœ€ç»ˆå…³é”®è¯ç»“æœï¼ˆä¸original_resultsç›¸åŒï¼Œå› ä¸ºå»é‡åœ¨CSVé˜¶æ®µè¿›è¡Œï¼‰
            output_file: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        print(f"\nğŸ“‹ æ­¥éª¤4: ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        
        # å…ˆè¿›è¡ŒCSVå»é‡ï¼Œè·å–çœŸå®çš„å»é‡åæ•°æ®
        csv_dedup_results = self._csv_deduplicate_keywords(final_results)
        
        # ç»Ÿè®¡åˆ†æ
        total_models = len(final_results)
        total_keywords = sum(len(r.keywords) for r in csv_dedup_results)  # ä½¿ç”¨CSVå»é‡åçš„æ•°æ®
        original_keywords = sum(len(r.keywords) for r in original_results)
        
        # æŒ‰ç»´åº¦ç»Ÿè®¡ï¼ˆä½¿ç”¨CSVå»é‡åçš„ç»“æœï¼‰
        dimension_stats = {}
        final_keywords = []
        
        for result in csv_dedup_results:
            for kw in result.keywords:
                dimension = kw['dimension']
                dimension_stats[dimension] = dimension_stats.get(dimension, 0) + 1
                final_keywords.append(kw['keyword'])
        
        # åŸå§‹æ•°æ®ç»Ÿè®¡ï¼ˆç”¨äºé«˜é¢‘å…³é”®è¯åˆ†æï¼‰
        original_keyword_freq = {}
        for result in original_results:
            for kw in result.keywords:
                keyword = kw['keyword']
                original_keyword_freq[keyword] = original_keyword_freq.get(keyword, 0) + 1
        
        # ä½¿ç”¨ä¼ å…¥çš„å°è¯•æ€»æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æˆåŠŸæ•°é‡
        attempted_models = total_attempted if total_attempted else total_models
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_content = f"""# æ¨¡å‹å…³é”®è¯æå–åˆ†ææŠ¥å‘Š

## æ¦‚è§ˆç»Ÿè®¡

- ğŸ“Š **CSVè¯»å–æ¨¡å‹æ•°**: {attempted_models}
- âœ… **æˆåŠŸæå–æ¨¡å‹æ•°**: {total_models}
- âŒ **å¤±è´¥æ¨¡å‹æ•°**: {attempted_models - total_models}
- ğŸ“ˆ **æˆåŠŸç‡**: {total_models/attempted_models*100:.1f}%
- ğŸ” **åŸå§‹å…³é”®è¯æ€»æ•°**: {original_keywords}
- âœ‚ï¸ **å»é‡åå…³é”®è¯æ€»æ•°**: {total_keywords}
- ğŸ“‰ **å»é‡ç‡**: {(1-total_keywords/original_keywords)*100:.1f}%
- ğŸ“Š **å¹³å‡æ¯æ¨¡å‹å…³é”®è¯æ•°**: {total_keywords/total_models:.1f}
- ğŸ• **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ç»´åº¦åˆ†å¸ƒ

| ç»´åº¦ | å…³é”®è¯æ•°é‡ | å æ¯” |
|------|------------|------|
"""
        
        for dimension, count in sorted(dimension_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = count / total_keywords * 100
            report_content += f"| {dimension} | {count} | {percentage:.1f}% |\n"
        
        report_content += f"""
## åŸå§‹æ•°æ®é«˜é¢‘å…³é”®è¯åˆ†æ

> åŸºäºå»é‡å‰çš„åŸå§‹æå–æ•°æ®ï¼Œå±•ç¤ºæ•´ä¸ªæ•°æ®é›†ä¸­æœ€å¸¸è§çš„å…³é”®è¯

| æ’å | å…³é”®è¯ | åŸå§‹å‡ºç°æ¬¡æ•° | æœ€ç»ˆä¿ç•™æ¬¡æ•° |
|------|--------|-------------|-------------|
"""
        
        # ç»Ÿè®¡æœ€ç»ˆç»“æœä¸­çš„å…³é”®è¯é¢‘ç‡
        final_keyword_freq = {}
        for kw in final_keywords:
            final_keyword_freq[kw] = final_keyword_freq.get(kw, 0) + 1
        
        top_original_keywords = sorted(original_keyword_freq.items(), key=lambda x: x[1], reverse=True)[:20]
        for i, (keyword, original_freq) in enumerate(top_original_keywords, 1):
            final_freq = final_keyword_freq.get(keyword, 0)
            report_content += f"| {i} | {keyword} | {original_freq} | {final_freq} |\n"
        
        # æ·»åŠ æ‰€æœ‰å…³é”®è¯åˆ—è¡¨éƒ¨åˆ†
        report_content += """
## æ‰€æœ‰å…³é”®è¯åˆ—è¡¨

"""
        
        # æŒ‰ç»´åº¦åˆ†ç»„æ˜¾ç¤ºæ‰€æœ‰å…³é”®è¯ï¼ˆæœ€ç»ˆç»“æœï¼‰
        keywords_by_dimension = {}
        for result in final_results:
            for kw in result.keywords:
                dimension = kw['dimension']
                if dimension not in keywords_by_dimension:
                    keywords_by_dimension[dimension] = []
                keywords_by_dimension[dimension].append(kw['keyword'])
        
        # ä¸ºæ¯ä¸ªç»´åº¦æ·»åŠ å…³é”®è¯åˆ—è¡¨
        for dimension in sorted(keywords_by_dimension.keys()):
            keywords = sorted(set(keywords_by_dimension[dimension]))  # å»é‡å¹¶æ’åº
            report_content += f"\n### {dimension} ({len(keywords)}ä¸ª)\n\n"
            
            # å°†å…³é”®è¯åˆ†æˆå¤šè¡Œæ˜¾ç¤ºï¼Œæ¯è¡Œæœ€å¤š5ä¸ª
            for i in range(0, len(keywords), 5):
                line_keywords = keywords[i:i+5]
                report_content += "- " + " â€¢ ".join(f"**{kw}**" for kw in line_keywords) + "\n"
        
        report_content += """
## è¯¦ç»†ç»“æœ

"""
        
        # æ·»åŠ æ¯ä¸ªæ¨¡å‹çš„è¯¦ç»†ç»“æœï¼ˆä½¿ç”¨CSVå»é‡åçš„ç»“æœï¼‰
        for result in csv_dedup_results:
            model_name = result.model_url.split('/')[-2:] if '/' in result.model_url else [result.model_url]
            model_name = '/'.join(model_name)
            
            # å°†URLä¸­çš„gitcode.comæ›¿æ¢ä¸ºai.gitcode.com
            ai_url = result.model_url.replace('gitcode.com', 'ai.gitcode.com')
            
            report_content += f"\n### {model_name}\n\n"
            report_content += f"**URL**: {ai_url}\n\n"
            report_content += "**å…³é”®è¯åˆ—è¡¨**:\n\n"
            
            for kw in result.keywords:
                report_content += f"- **{kw['keyword']}** ({kw['dimension']}): {kw['reason']}\n"
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {output_file}")
        
        # ç”ŸæˆCSVæ–‡ä»¶
        csv_file = output_file.replace('.md', '.csv')
        self.generate_csv_output(final_results, csv_file)
        print(f"âœ… CSVæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {csv_file}")
        
        # ç”Ÿæˆçº¯æ–‡æœ¬å…³é”®è¯åˆ—è¡¨
        txt_file = output_file.replace('.md', '_keywords.txt')
        self.generate_keywords_txt(final_results, txt_file)
        print(f"âœ… å…³é”®è¯åˆ—è¡¨æ–‡ä»¶ç”Ÿæˆå®Œæˆ: {txt_file}")
    
    def generate_csv_output(self, keyword_results: List[KeywordResult], csv_file: str):
        """
        ç”ŸæˆCSVæ ¼å¼çš„å…³é”®è¯è¾“å‡ºæ–‡ä»¶
        
        Args:
            keyword_results: å…³é”®è¯ç»“æœåˆ—è¡¨
            csv_file: CSVæ–‡ä»¶è·¯å¾„
        """
        import csv
        
        print(f"\nğŸ“Š ç”ŸæˆCSVè¾“å‡ºæ–‡ä»¶...")
        
        # ç”¨äºå»é‡çš„å·²ä½¿ç”¨å…³é”®è¯é›†åˆï¼ˆä¿ç•™å…ˆç”Ÿæˆçš„ï¼Œä¸åŒºåˆ†å¤§å°å†™ï¼‰
        used_keywords_lower = set()  # å­˜å‚¨å°å†™ç‰ˆæœ¬ç”¨äºæ¯”è¾ƒ
        csv_data = []
        
        for result in keyword_results:
            # æå–é¡¹ç›®åç§°ï¼ˆä»URLä¸­è·å–ï¼‰
            project_name = result.model_url.split('/')[-1] if '/' in result.model_url else result.model_url
            if result.model_url.count('/') >= 2:
                # å¦‚æœURLåŒ…å«ç”¨æˆ·å/é¡¹ç›®åæ ¼å¼ï¼Œæå–æœ€åä¸¤éƒ¨åˆ†
                parts = result.model_url.rstrip('/').split('/')
                if len(parts) >= 2:
                    project_name = '/'.join(parts[-2:])
            
            # å¤„ç†æ¯ä¸ªå…³é”®è¯ï¼ˆæŒ‰ç”Ÿæˆé¡ºåºï¼Œå»é‡ä¿ç•™å…ˆç”Ÿæˆçš„ï¼‰
            for kw in result.keywords:
                keyword = kw['keyword']
                keyword_lower = keyword.lower()  # è½¬æ¢ä¸ºå°å†™ç”¨äºæ¯”è¾ƒ
                
                # å»é‡ï¼šä¸åŒºåˆ†å¤§å°å†™ï¼Œä¿ç•™å…ˆç”Ÿæˆçš„å…³é”®è¯
                if keyword_lower not in used_keywords_lower:
                    used_keywords_lower.add(keyword_lower)
                    # å°†URLä¸­çš„gitcode.comæ›¿æ¢ä¸ºai.gitcode.com
                    ai_url = result.model_url.replace('gitcode.com', 'ai.gitcode.com')
                    csv_data.append({
                        'é¡¹ç›®é“¾æ¥': ai_url,
                        'é¡¹ç›®åç§°': project_name,
                        'é«˜äº®è¯': keyword
                    })
        
        # å†™å…¥CSVæ–‡ä»¶
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            if csv_data:
                fieldnames = ['é¡¹ç›®é“¾æ¥', 'é¡¹ç›®åç§°', 'é«˜äº®è¯']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                # å†™å…¥è¡¨å¤´
                writer.writeheader()
                
                # å†™å…¥æ•°æ®
                for row in csv_data:
                    writer.writerow(row)
        
        print(f"ğŸ“Š CSVç»Ÿè®¡:")
        print(f"   ğŸ“ æ€»é¡¹ç›®æ•°: {len(keyword_results)}")
        print(f"   ğŸ” å»é‡å‰å…³é”®è¯æ•°: {sum(len(r.keywords) for r in keyword_results)}")
        print(f"   âœ‚ï¸ å»é‡åå…³é”®è¯æ•°: {len(csv_data)}")
        print(f"   ğŸ“‰ å»é‡ç‡: {(1 - len(csv_data) / sum(len(r.keywords) for r in keyword_results)) * 100:.1f}%")
    
    def generate_keywords_txt(self, keyword_results: List[KeywordResult], txt_file: str):
        """
        ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼çš„å…³é”®è¯åˆ—è¡¨æ–‡ä»¶ï¼ˆæ— markdownæ ¼å¼ï¼‰
        
        Args:
            keyword_results: å…³é”®è¯ç»“æœåˆ—è¡¨
            txt_file: æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        """
        print(f"\nğŸ“„ ç”Ÿæˆçº¯æ–‡æœ¬å…³é”®è¯åˆ—è¡¨...")
        
        # æŒ‰ç»´åº¦åˆ†ç»„æ˜¾ç¤ºæ‰€æœ‰å…³é”®è¯
        keywords_by_dimension = {}
        for result in keyword_results:
            for kw in result.keywords:
                dimension = kw['dimension']
                if dimension not in keywords_by_dimension:
                    keywords_by_dimension[dimension] = []
                keywords_by_dimension[dimension].append(kw['keyword'])
        
        # ç”Ÿæˆçº¯æ–‡æœ¬å†…å®¹
        txt_content = "æ‰€æœ‰å…³é”®è¯åˆ—è¡¨\n"
        txt_content += "=" * 50 + "\n\n"
        
        # ä¸ºæ¯ä¸ªç»´åº¦æ·»åŠ å…³é”®è¯åˆ—è¡¨
        for dimension in sorted(keywords_by_dimension.keys()):
            keywords = sorted(set(keywords_by_dimension[dimension]))  # å»é‡å¹¶æ’åº
            txt_content += f"{dimension} ({len(keywords)}ä¸ª)\n"
            txt_content += "-" * 30 + "\n"
            
            # å°†å…³é”®è¯åˆ†æˆå¤šè¡Œæ˜¾ç¤ºï¼Œæ¯è¡Œæœ€å¤š8ä¸ª
            for i in range(0, len(keywords), 8):
                line_keywords = keywords[i:i+8]
                txt_content += " â€¢ ".join(line_keywords) + "\n"
            
            txt_content += "\n"
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        total_unique_keywords = len(set(kw for keywords in keywords_by_dimension.values() for kw in keywords))
        total_keywords = sum(len(r.keywords) for r in keyword_results)
        
        txt_content += "ç»Ÿè®¡ä¿¡æ¯\n"
        txt_content += "=" * 50 + "\n"
        txt_content += f"æ€»æ¨¡å‹æ•°: {len(keyword_results)}\n"
        txt_content += f"å…³é”®è¯æ€»æ•°: {total_keywords}\n"
        txt_content += f"å»é‡åå…³é”®è¯æ•°: {total_unique_keywords}\n"
        txt_content += f"å¹³å‡æ¯æ¨¡å‹å…³é”®è¯æ•°: {total_keywords/len(keyword_results):.1f}\n"
        
        # ä¿å­˜æ–‡ä»¶
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(txt_content)
        
        print(f"ğŸ“„ çº¯æ–‡æœ¬ç»Ÿè®¡:")
        print(f"   ğŸ“ ç»´åº¦æ•°: {len(keywords_by_dimension)}")
        print(f"   ğŸ”¤ å»é‡åå…³é”®è¯æ•°: {total_unique_keywords}")
        print(f"   ğŸ“Š æ€»å…³é”®è¯æ•°: {total_keywords}")
    
    def test_single_model(self, model_url: str):
        """
        æµ‹è¯•å•ä¸ªæ¨¡å‹çš„å…³é”®è¯æå–ï¼ˆå·²å¼ƒç”¨ï¼‰
        
        Args:
            model_url: æ¨¡å‹URL
        """
        print(f"ğŸ§ª æµ‹è¯•å•ä¸ªæ¨¡å‹: {model_url}")
        print("âŒ å•ä¸ªæ¨¡å‹æµ‹è¯•åŠŸèƒ½å·²ç§»é™¤")
        print("ğŸ’¡ è¯·ä½¿ç”¨CSVæ–‡ä»¶æ‰¹é‡å¤„ç†: python keyword_extractor.py --max-models 10")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AIæ¨¡å‹å…³é”®è¯æå–ç³»ç»Ÿ")
    parser.add_argument("--max-models", type=int, default=10, help="æœ€å¤§æ¨¡å‹æ•°é‡ (é»˜è®¤: 10)")
    parser.add_argument("--force-crawl", action="store_true", help="å¼ºåˆ¶é‡æ–°è·å–æ¨¡å‹ä¿¡æ¯")
    parser.add_argument("--test-url", type=str, help="æµ‹è¯•å•ä¸ªæ¨¡å‹URL")
    parser.add_argument("--output-dir", type=str, default="output", help="è¾“å‡ºç›®å½• (é»˜è®¤: output)")
    parser.add_argument("--token", type=str, help="å¯é€‰çš„è®¤è¯token")
    
    args = parser.parse_args()
    
    # è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„APIå¹³å°æ•°é‡
    available_platforms = detect_available_platforms()
    use_multi_platform = available_platforms > 1
    
    print(f"ğŸ” æ£€æµ‹åˆ° {available_platforms} ä¸ªå¯ç”¨çš„APIå¹³å°")
    if use_multi_platform:
        print("ğŸš€ è‡ªåŠ¨å¯ç”¨å¤šå¹³å°å¹¶å‘æ¨¡å¼")
    else:
        print("ğŸ“¡ ä½¿ç”¨å•å¹³å°æ¨¡å¼")
    
    # åˆ›å»ºæå–å™¨
    extractor = ModelKeywordExtractor(
        output_dir=args.output_dir, 
        token=args.token,
        use_multi_platform=use_multi_platform
    )
    
    if args.test_url:
        # æµ‹è¯•å•ä¸ªæ¨¡å‹
        extractor.test_single_model(args.test_url)
    else:
        # è¿è¡Œå®Œæ•´æµç¨‹
        extractor.run_full_pipeline(
            max_models=args.max_models,
            force_crawl=args.force_crawl,
            use_csv=True  # å§‹ç»ˆä½¿ç”¨CSVæ–‡ä»¶
        )


if __name__ == "__main__":
    main()
