#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSVæ¨¡å‹è¯»å–å™¨
ç”¨äºä»CSVæ–‡ä»¶è¯»å–æ¨¡å‹ä¿¡æ¯å¹¶è½¬æ¢ä¸ºModelInfoå¯¹è±¡
"""

import csv
from urllib.parse import urlparse, urlunparse
from typing import List, Dict
from models import ModelInfo
from hf_scraper import scrape_hf_model_sync

# ===== å…¨å±€é…ç½® =====
# åªéœ€è¦åœ¨è¿™é‡Œä¿®æ”¹CSVæ–‡ä»¶è·¯å¾„ï¼Œå…¶ä»–åœ°æ–¹ä¼šè‡ªåŠ¨ä½¿ç”¨
DEFAULT_CSV_FILE = "é«˜äº®è¯éœ€æ±‚1113-v2.csv"
# ===================


class CSVModelReader:
    """CSVæ¨¡å‹è¯»å–å™¨"""
    
    def __init__(self, csv_file: str = None, delay: float = 0.5, token: str = None):
        """
        åˆå§‹åŒ–CSVè¯»å–å™¨
        
        Args:
            csv_file: CSVæ–‡ä»¶è·¯å¾„ï¼ˆNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
            delay: çˆ¬å–å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            token: å¯é€‰çš„è®¤è¯token
        """
        self.csv_file = csv_file or DEFAULT_CSV_FILE
        self.delay = delay
        self.token = token
    
    def clean_url(self, url: str) -> str:
        """
        æ¸…ç†URLï¼Œå»æ‰/model-inferenceç­‰è·¯å¾„åç¼€ï¼Œç¡®ä¿è®¿é—®ä¸»é¡µé¢
        
        Args:
            url: åŸå§‹URL
            
        Returns:
            æ¸…ç†åçš„URL
        """
        if not url:
            return url
        
        # è§£æURL
        parsed = urlparse(url)
        
        # è·å–è·¯å¾„éƒ¨åˆ†
        path = parsed.path.rstrip('/')
        
        # éœ€è¦ç§»é™¤çš„è·¯å¾„åç¼€åˆ—è¡¨
        suffixes_to_remove = [
            '/model-inference',
            '/model-inference/',
            '/inference',
            '/inference/',
            '/files',
            '/files/',
            '/tree',
            '/tree/',
        ]
        
        # ç§»é™¤è·¯å¾„åç¼€
        for suffix in suffixes_to_remove:
            if path.endswith(suffix):
                path = path[:-len(suffix)]
                break
        
        # é‡æ–°æ„å»ºURL
        cleaned_url = urlunparse((
            parsed.scheme,
            parsed.netloc,
            path,
            parsed.params,
            parsed.query,
            parsed.fragment
        ))
        
        return cleaned_url.rstrip('/')
    
    def read_csv_data(self, max_models: int = None) -> List[Dict]:
        """
        ä»CSVæ–‡ä»¶è¯»å–æ¨¡å‹æ•°æ®
        
        Args:
            max_models: æœ€å¤§æ¨¡å‹æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            
        Returns:
            æ¨¡å‹æ•°æ®åˆ—è¡¨
        """
        print(f"ğŸ“– å¼€å§‹è¯»å–CSVæ–‡ä»¶: {self.csv_file}")
        
        models = []
        try:
            with open(self.csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                
                for row in reader:
                    # æ£€æŸ¥å®¡æ ¸çŠ¶æ€å’Œæ˜¯å¦å…¬å¼€
                    audit_status = row.get('å®¡æ ¸çŠ¶æ€', '')
                    is_public = row.get('æ˜¯å¦å…¬å¼€', '')
                    
                    # åªå¤„ç†å®¡æ ¸é€šè¿‡ä¸”å…¬å¼€çš„æ¨¡å‹
                    if audit_status == '2' and is_public == '1':
                        models.append(row)
                        
                        # é™åˆ¶æ•°é‡
                        if max_models and len(models) >= max_models:
                            break
            
            print(f"âœ… ä»CSVè¯»å–åˆ° {len(models)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ¨¡å‹")
            return models
            
        except Exception as e:
            print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def convert_csv_to_model_info(self, csv_model: Dict) -> ModelInfo:
        """
        å°†CSVæ•°æ®è½¬æ¢ä¸ºModelInfoå¯¹è±¡
        
        Args:
            csv_model: CSVè¡Œæ•°æ®
            
        Returns:
            ModelInfoå¯¹è±¡
        """
        # æå–åŸºæœ¬ä¿¡æ¯
        # å…¼å®¹ä¸åŒçš„åˆ—åï¼šä¼˜å…ˆä½¿ç”¨"é¡¹ç›®åç§°"ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨"æƒ³ä¹°åç§°"
        project_name = csv_model.get('é¡¹ç›®åç§°', '') or csv_model.get('æƒ³ä¹°åç§°', '')
        project_url = csv_model.get('é¡¹ç›®ç½‘å€', '')
        
        # æ¸…ç†URLï¼Œå»æ‰/model-inferenceç­‰è·¯å¾„åç¼€
        cleaned_url = self.clean_url(project_url)
        
        # åˆ›å»ºModelInfoå¯¹è±¡ï¼ˆä¸åŒ…å«READMEå’Œæ ‡ç­¾ï¼Œè¿™äº›éœ€è¦çˆ¬å–ï¼‰
        model_info = ModelInfo(
            url=cleaned_url,
            project_name=project_name,
            readme="",  # ç©ºå­—ç¬¦ä¸²ï¼Œéœ€è¦çˆ¬å–
            tags=[]     # ç©ºåˆ—è¡¨ï¼Œéœ€è¦çˆ¬å–
        )
        
        return model_info
    
    def crawl_models(self, max_models: int = None, fetch_details: bool = False) -> List[ModelInfo]:
        """
        ä»CSVæ–‡ä»¶çˆ¬å–æ¨¡å‹ä¿¡æ¯

        Args:
            max_models: æœ€å¤§æ¨¡å‹æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            fetch_details: æ˜¯å¦è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆREADMEå’Œæ ‡ç­¾ï¼‰

        Returns:
            ModelInfoå¯¹è±¡åˆ—è¡¨
        """
        print(f"ğŸ“– å¼€å§‹ä»CSVæ–‡ä»¶è·å–æ¨¡å‹ä¿¡æ¯ (æœ€å¤§æ•°é‡: {max_models or 'å…¨éƒ¨'})")

        # ä»CSVè¯»å–åŸºç¡€æ•°æ®
        csv_models = self.read_csv_data(max_models)
        if not csv_models:
            return []

        # è½¬æ¢ä¸ºModelInfoå¯¹è±¡
        models = []
        for csv_model in csv_models:
            try:
                model_info = self.convert_csv_to_model_info(csv_model)

                # å¦‚æœéœ€è¦è·å–è¯¦ç»†ä¿¡æ¯ï¼Œåˆ™è°ƒç”¨çˆ¬è™«
                if fetch_details:
                    model_info = self.get_model_detail_from_scraper(model_info)

                models.append(model_info)

            except Exception as e:
                print(f"âš ï¸  è½¬æ¢æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
                continue

        print(f"âœ… æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹ä¿¡æ¯")
        return models

    def get_model_detail_from_scraper(self, model_info: ModelInfo) -> ModelInfo:
        """
        ä½¿ç”¨çˆ¬è™«è·å–æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯

        Args:
            model_info: åŸºç¡€æ¨¡å‹ä¿¡æ¯

        Returns:
            åŒ…å«è¯¦ç»†ä¿¡æ¯çš„ModelInfoå¯¹è±¡
        """
        print(f"æ­£åœ¨ä½¿ç”¨çˆ¬è™«è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯: {model_info.project_name}")

        try:
            # ä½¿ç”¨çˆ¬è™«è·å–è¯¦ç»†ä¿¡æ¯
            scraped_data = scrape_hf_model_sync(model_info.url, self.token)

            # æ›´æ–°æ¨¡å‹ä¿¡æ¯
            model_info.readme = scraped_data.get('readme', '')
            model_info.tags = scraped_data.get('tags', [])

            print(f"âœ… æˆåŠŸè·å–æ¨¡å‹ä¿¡æ¯: READMEé•¿åº¦={len(model_info.readme)}, æ ‡ç­¾æ•°={len(model_info.tags)}")
            return model_info

        except Exception as e:
            print(f"âŒ çˆ¬å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            # è¿”å›åŸå§‹æ¨¡å‹ä¿¡æ¯
            return model_info
