"""
CSVæ•°æ®è¯»å–å™¨ - ä»æœ¬åœ°CSVæ–‡ä»¶è¯»å–HuggingFaceæ¨¡å‹æ•°æ®ï¼Œå¹¶ä½¿ç”¨çˆ¬è™«è·å–è¯¦ç»†ä¿¡æ¯
"""
import csv
import requests
import time
import json
from typing import List, Optional
from urllib.parse import urlparse
from tqdm import tqdm
from bs4 import BeautifulSoup

from models import ModelInfo
from hf_scraper import scrape_hf_model_sync


class CSVModelReader:
    """CSVæ¨¡å‹æ•°æ®è¯»å–å™¨"""
    
    def __init__(self, csv_file: str = "huggingfaceæ¨¡å‹æ•°æ®_202509241526.csv", delay: float = 0.1, token: Optional[str] = None):
        """
        åˆå§‹åŒ–CSVè¯»å–å™¨
        
        Args:
            csv_file: CSVæ–‡ä»¶è·¯å¾„
            delay: çˆ¬å–è¯¦ç»†ä¿¡æ¯æ—¶çš„å»¶è¿Ÿæ—¶é—´
            token: å¯é€‰çš„è®¤è¯token
        """
        self.csv_file = csv_file
        self.delay = delay
        self.token = token
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36'
        })
    
    def read_csv_data(self, max_models: int = 100) -> List[dict]:
        """
        ä»CSVæ–‡ä»¶è¯»å–ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹æ•°æ®
        
        Args:
            max_models: æœ€å¤§æ¨¡å‹æ•°é‡
            
        Returns:
            ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹æ•°æ®åˆ—è¡¨
        """
        models = []
        
        try:
            print(f"ğŸ“– å¼€å§‹è¯»å–CSVæ–‡ä»¶: {self.csv_file}")
            
            with open(self.csv_file, 'r', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                
                for row in reader:
                    # æ£€æŸ¥æ˜¯å¦ç¬¦åˆç­›é€‰æ¡ä»¶ï¼šå®¡æ ¸çŠ¶æ€ä¸º2ï¼Œæ˜¯å¦å…¬å¼€ä¸º1
                    if (row.get('å®¡æ ¸çŠ¶æ€') == '2' and 
                        row.get('æ˜¯å¦å…¬å¼€') == '1' and 
                        row.get('é¡¹ç›®åç§°') and 
                        row.get('é¡¹ç›®ç½‘å€')):
                        
                        models.append({
                            'id': row.get('é¡¹ç›®ID', ''),
                            'name': row.get('é¡¹ç›®åç§°', ''),
                            'url': row.get('é¡¹ç›®ç½‘å€', ''),
                            'audit_status': row.get('å®¡æ ¸çŠ¶æ€', ''),
                            'is_public': row.get('æ˜¯å¦å…¬å¼€', '')
                        })
                        
                        if len(models) >= max_models:
                            break
            
            print(f"âœ… ä»CSVè¯»å–åˆ° {len(models)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ¨¡å‹")
            return models
            
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°CSVæ–‡ä»¶: {self.csv_file}")
            return []
        except Exception as e:
            print(f"âŒ è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return []
    
    def convert_csv_to_model_info(self, csv_model: dict) -> ModelInfo:
        """
        å°†CSVæ•°æ®è½¬æ¢ä¸ºModelInfoå¯¹è±¡
        
        Args:
            csv_model: CSVä¸­çš„æ¨¡å‹æ•°æ®
            
        Returns:
            ModelInfoå¯¹è±¡
        """
        project_name = csv_model['name']
        model_url = csv_model['url']
        
        # ä»URLä¸­æå–æ›´è§„èŒƒçš„é¡¹ç›®åç§°
        parsed_url = urlparse(model_url)
        if parsed_url.path:
            path_parts = parsed_url.path.strip('/').split('/')
            if len(path_parts) >= 2:
                # å»æ‰å¯èƒ½çš„å‰ç¼€å¦‚ hf_mirrors
                if path_parts[0] in ['hf_mirrors', 'mirrors']:
                    if len(path_parts) >= 3:
                        project_name = '/'.join(path_parts[1:])
                    else:
                        project_name = '/'.join(path_parts[1:]) if len(path_parts) > 1 else path_parts[-1]
                else:
                    project_name = '/'.join(path_parts[-2:]) if len(path_parts) >= 2 else path_parts[-1]
        
        return ModelInfo(
            url=model_url,
            project_name=project_name,
            readme="",  # ç”±AIè®¿é—®URLè·å–
            tags=[]     # ç”±AIè®¿é—®URLè·å–
        )
    
    def get_model_detail_from_scraper(self, model_info: ModelInfo) -> ModelInfo:
        """
        ä½¿ç”¨çˆ¬è™«è·å–æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            model_info: åŸºç¡€æ¨¡å‹ä¿¡æ¯
            
        Returns:
            åŒ…å«è¯¦ç»†ä¿¡æ¯çš„ModelInfoå¯¹è±¡
        """
        try:
            print(f"æ­£åœ¨ä½¿ç”¨çˆ¬è™«è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯: {model_info.project_name}")
            
            # ä½¿ç”¨çˆ¬è™«è·å–è¯¦ç»†ä¿¡æ¯
            scraped_data = scrape_hf_model_sync(model_info.url, self.token)
            
            if scraped_data and scraped_data.get("name") != "Error":
                # æ›´æ–°æ¨¡å‹ä¿¡æ¯
                model_info.readme = scraped_data.get("readme", "")
                
                # è§£ææ ‡ç­¾JSONå­—ç¬¦ä¸²
                tags_json = scraped_data.get("tags", "[]")
                try:
                    tags = json.loads(tags_json)
                    model_info.tags = tags
                except json.JSONDecodeError:
                    print(f"âš ï¸ æ ‡ç­¾JSONè§£æå¤±è´¥: {tags_json}")
                    model_info.tags = []
                
                print(f"âœ… æˆåŠŸè·å–æ¨¡å‹ä¿¡æ¯: READMEé•¿åº¦={len(model_info.readme)}, æ ‡ç­¾æ•°={len(model_info.tags)}")
            else:
                print(f"âŒ çˆ¬è™«è·å–å¤±è´¥: {model_info.url}")
                # ä¿æŒåŸæœ‰çš„ç©ºå€¼
                model_info.readme = ""
                model_info.tags = []
            
            return model_info
            
        except Exception as e:
            print(f"âŒ çˆ¬è™«è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯å¤±è´¥ {model_info.url}: {e}")
            # ä¿æŒåŸæœ‰çš„ç©ºå€¼
            model_info.readme = ""
            model_info.tags = []
            return model_info

    def get_model_detail_from_web(self, model_info: ModelInfo) -> ModelInfo:
        """
        ä»ç½‘é¡µè·å–æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼Œå·²åºŸå¼ƒï¼‰
        
        Args:
            model_info: åŸºç¡€æ¨¡å‹ä¿¡æ¯
            
        Returns:
            åŒ…å«è¯¦ç»†ä¿¡æ¯çš„ModelInfoå¯¹è±¡
        """
        try:
            print(f"æ­£åœ¨è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯: {model_info.project_name}")
            
            response = self.session.get(model_info.url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–READMEå†…å®¹
            readme = self._extract_readme(soup)
            model_info.readme = readme
            
            # æå–æ ‡ç­¾
            tags = self._extract_tags(soup)
            model_info.tags = tags
            
            return model_info
            
        except Exception as e:
            print(f"è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯å¤±è´¥ {model_info.url}: {e}")
            return model_info
    
    def _extract_readme(self, soup: BeautifulSoup) -> str:
        """æå–READMEå†…å®¹"""
        readme_content = ""
        
        # å¸¸è§çš„READMEå®¹å™¨é€‰æ‹©å™¨
        readme_selectors = [
            'div[data-target="readme-toc.content"]',
            '.markdown-body',
            '#readme',
            '.readme',
            'article',
            '[class*="readme"]',
            '[id*="readme"]',
            '.description'
        ]
        
        for selector in readme_selectors:
            readme_div = soup.select_one(selector)
            if readme_div:
                readme_content = readme_div.get_text(strip=True)
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ä¸“é—¨çš„READMEåŒºåŸŸï¼Œå°è¯•è·å–ä¸»è¦å†…å®¹
        if not readme_content:
            content_selectors = [
                'main',
                '.content',
                '.main-content',
                'article',
                '.description'
            ]
            
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    readme_content = content_div.get_text(strip=True)
                    break
        
        return readme_content[:5000]  # é™åˆ¶é•¿åº¦é¿å…è¿‡å¤§
    
    def _extract_tags(self, soup: BeautifulSoup) -> List[str]:
        """æå–é¡¹ç›®æ ‡ç­¾"""
        tags = []
        
        # æŸ¥æ‰¾æ ‡ç­¾çš„å¸¸è§ç»“æ„
        tag_selectors = [
            '.tag', '.badge', '.label', '.topic-tag',
            '[class*="tag"]', '[class*="badge"]', '[class*="label"]',
            '.aiHubTag', '.model-tag'
        ]
        
        for selector in tag_selectors:
            tag_elements = soup.select(selector)
            for elem in tag_elements:
                tag_text = elem.get_text(strip=True)
                if tag_text and tag_text not in tags and len(tag_text) < 50:  # è¿‡æ»¤è¿‡é•¿çš„æ–‡æœ¬
                    tags.append(tag_text)
            
            if tags:  # å¦‚æœæ‰¾åˆ°æ ‡ç­¾å°±åœæ­¢
                break
        
        return tags[:15]  # é™åˆ¶æ ‡ç­¾æ•°é‡
    
    def crawl_models(self, max_models: int = 100, fetch_details: bool = True) -> List[ModelInfo]:
        """
        ä»CSVæ–‡ä»¶è·å–æ¨¡å‹ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨çˆ¬è™«è·å–è¯¦ç»†ä¿¡æ¯
        
        Args:
            max_models: æœ€å¤§æ¨¡å‹æ•°é‡
            fetch_details: æ˜¯å¦è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆç°åœ¨é»˜è®¤ä¸ºTrueï¼‰
            
        Returns:
            æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
        """
        # ä»CSVè¯»å–åŸºç¡€æ•°æ®
        csv_models = self.read_csv_data(max_models)
        
        if not csv_models:
            print("âŒ æ— æ³•ä»CSVè·å–æ¨¡å‹æ•°æ®")
            return []
        
        # è½¬æ¢ä¸ºModelInfoå¯¹è±¡
        model_infos = []
        
        print(f"å¼€å§‹å¤„ç† {len(csv_models)} ä¸ªæ¨¡å‹...")
        
        for i, csv_model in enumerate(csv_models, 1):
            try:
                print(f"\nè¿›åº¦: {i}/{len(csv_models)}")
                
                # è½¬æ¢åŸºæœ¬ä¿¡æ¯
                model_info = self.convert_csv_to_model_info(csv_model)
                
                # ä½¿ç”¨çˆ¬è™«è·å–è¯¦ç»†ä¿¡æ¯
                if fetch_details:
                    model_info = self.get_model_detail_from_scraper(model_info)
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    if i < len(csv_models):  # æœ€åä¸€ä¸ªä¸éœ€è¦å»¶è¿Ÿ
                        time.sleep(self.delay)
                
                model_infos.append(model_info)
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ¨¡å‹å¤±è´¥: {e}")
                continue
        
        print(f"\nâœ… æˆåŠŸå¤„ç† {len(model_infos)} ä¸ªæ¨¡å‹çš„ä¿¡æ¯")
        return model_infos


def test_csv_reader():
    """æµ‹è¯•CSVè¯»å–å™¨åŠŸèƒ½"""
    reader = CSVModelReader(delay=1.0)
    
    # æµ‹è¯•è·å–å°‘é‡æ¨¡å‹
    models = reader.crawl_models(max_models=5, fetch_details=True)
    
    for model in models:
        print(f"\nURL: {model.url}")
        print(f"é¡¹ç›®åç§°: {model.project_name}")
        print(f"æ ‡ç­¾: {', '.join(model.tags)}")
        print(f"READMEé•¿åº¦: {len(model.readme)}")
        print(f"READMEé¢„è§ˆ: {model.readme[:200]}...")
        print("-" * 50)


if __name__ == "__main__":
    test_csv_reader()
